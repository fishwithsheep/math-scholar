{"Machine Learning in Finance": {"2503.08697": "|**2025-03-06**|**Matrix H-theory approach to stock market fluctuations**|Luan M. T. de Moraes, Ant\u00f4nio M. S. Macedo, Raydonal Ospina et.al.|[2503.08697](http://arxiv.org/abs/2503.08697)|null|26 pages, 10 figures. Published on Physical Review E|We introduce matrix H theory, a framework for analyzing collective behavior arising from multivariate stochastic processes with hierarchical structure. The theory models the joint distribution of the multiple variables (the measured signal) as a compound of a large-scale multivariate distribution with the distribution of a slowly fluctuating background. The background is characterized by a hierarchical stochastic evolution of internal degrees of freedom, representing the correlations between stocks at different time scales. As in its univariate version, the matrix H-theory formalism also has two universality classes: Wishart and inverse Wishart, enabling a concise description of both the background and the signal probability distributions in terms of Meijer G-functions with matrix argument. Empirical analysis of daily returns of stocks within the S&P500 demonstrates the effectiveness of matrix H theory in describing fluctuations in stock markets. These findings contribute to a deeper understanding of multivariate hierarchical processes and offer potential for developing more informed portfolio strategies in financial markets.|\n", "2503.08696": "|**2025-03-05**|**Multimodal Stock Price Prediction: A Case Study of the Russian Securities Market**|Kasymkhan Khubiev, Mikhail Semenov et.al.|[2503.08696](http://arxiv.org/abs/2503.08696)|null|NSCF-2024, PROGRAM SYSTEMS: THEORY AND APPLICATIONS|Classical asset price forecasting methods primarily rely on numerical data, such as price time series, trading volumes, limit order book data, and technical analysis indicators. However, the news flow plays a significant role in price formation, making the development of multimodal approaches that combine textual and numerical data for improved prediction accuracy highly relevant. This paper addresses the problem of forecasting financial asset prices using the multimodal approach that combines candlestick time series and textual news flow data. A unique dataset was collected for the study, which includes time series for 176 Russian stocks traded on the Moscow Exchange and 79,555 financial news articles in Russian. For processing textual data, pre-trained models RuBERT and Vikhr-Qwen2.5-0.5b-Instruct (a large language model) were used, while time series and vectorized text data were processed using an LSTM recurrent neural network. The experiments compared models based on a single modality (time series only) and two modalities, as well as various methods for aggregating text vector representations. Prediction quality was estimated using two key metrics: Accuracy (direction of price movement prediction: up or down) and Mean Absolute Percentage Error (MAPE), which measures the deviation of the predicted price from the true price. The experiments showed that incorporating textual modality reduced the MAPE value by 55%. The resulting multimodal dataset holds value for the further adaptation of language models in the financial sector. Future research directions include optimizing textual modality parameters, such as the time window, sentiment, and chronological order of news messages.|\n", "2503.03612": "|**2025-03-10**|**Large language models in finance : what is financial sentiment?**|Kemal Kirtac, Guido Germano et.al.|[2503.03612](http://arxiv.org/abs/2503.03612)|null||Financial sentiment has become a crucial yet complex concept in finance, increasingly used in market forecasting and investment strategies. Despite its growing importance, there remains a need to define and understand what financial sentiment truly represents and how it can be effectively measured. We explore the nature of financial sentiment and investigate how large language models (LLMs) contribute to its estimation. We trace the evolution of sentiment measurement in finance, from market-based and lexicon-based methods to advanced natural language processing techniques. The emergence of LLMs has significantly enhanced sentiment analysis, providing deeper contextual understanding and greater accuracy in extracting sentiment from financial text. We examine how BERT-based models, such as RoBERTa and FinBERT, are optimized for structured sentiment classification, while GPT-based models, including GPT-4, OPT, and LLaMA, excel in financial text generation and real-time sentiment interpretation. A comparative analysis of bidirectional and autoregressive transformer architectures highlights their respective roles in investor sentiment analysis, algorithmic trading, and financial decision-making. By exploring what financial sentiment is and how it is estimated within LLMs, we provide insights into the growing role of AI-driven sentiment analysis in finance.|\n", "2503.02680": "|**2025-03-04**|**VWAP Execution with Signature-Enhanced Transformers: A Multi-Asset Learning Approach**|Remi Genet et.al.|[2503.02680](http://arxiv.org/abs/2503.02680)|**[link](https://github.com/remigenet/DynamicVWAPTransformer)**||In this paper I propose a novel approach to Volume Weighted Average Price (VWAP) execution that addresses two key practical challenges: the need for asset-specific model training and the capture of complex temporal dependencies. Building upon my recent work in dynamic VWAP execution arXiv:2502.18177, I demonstrate that a single neural network trained across multiple assets can achieve performance comparable to or better than traditional asset-specific models. The proposed architecture combines a transformer-based design inspired by arXiv:2406.02486 with path signatures for capturing geometric features of price-volume trajectories, as in arXiv:2406.17890. The empirical analysis, conducted on hourly cryptocurrency trading data from 80 trading pairs, shows that the globally-fitted model with signature features (GFT-Sig) achieves superior performance in both absolute and quadratic VWAP loss metrics compared to asset-specific approaches. Notably, these improvements persist for out-of-sample assets, demonstrating the model's ability to generalize across different market conditions. The results suggest that combining global parameter sharing with signature-based feature extraction provides a scalable and robust approach to VWAP execution, offering significant practical advantages over traditional asset-specific implementations.|\n", "2503.02518": "|**2025-03-04**|**Extrapolating the long-term seasonal component of electricity prices for forecasting in the day-ahead market**|Katarzyna Ch\u0119\u0107, Bartosz Uniejewski, Rafa\u0142 Weron et.al.|[2503.02518](http://arxiv.org/abs/2503.02518)|null||Recent studies provide evidence that decomposing the electricity price into the long-term seasonal component (LTSC) and the remaining part, predicting both separately, and then combining their forecasts can bring significant accuracy gains in day-ahead electricity price forecasting. However, not much attention has been paid to predicting the LTSC, and the last 24 hourly values of the estimated pattern are typically copied for the target day. To address this gap, we introduce a novel approach which extracts the trend-seasonal pattern from a price series extrapolated using price forecasts for the next 24 hours. We assess it using two 5-year long test periods from the German and Spanish power markets, covering the Covid-19 pandemic, the 2021/2022 energy crisis, and the war in Ukraine. Considering parsimonious autoregressive and LASSO-estimated models, we find that improvements in predictive accuracy range from 3\\% to 15\\% in terms of the root mean squared error and exceed 1\\% in terms of profits from a realistic trading strategy involving day-ahead bidding and battery storage.|\n", "2503.08693": "|**2025-03-02**|**Liquidity-adjusted Return and Volatility, and Autoregressive Models**|Qi Deng, Zhong-guo Zhou et.al.|[2503.08693](http://arxiv.org/abs/2503.08693)|null||We construct liquidity-adjusted return and volatility using purposely designed liquidity metrics (liquidity jump and liquidity diffusion) that incorporate additional liquidity information. Based on these measures, we introduce a liquidity-adjusted ARMA-GARCH framework to address the limitations of traditional ARMA-GARCH models, which are not effectively in modeling illiquid assets with high liquidity variability, such as cryptocurrencies. We demonstrate that the liquidity-adjusted model improves model fit for cryptocurrencies, with greater volatility sensitivity to past shocks and reduced volatility persistence of erratic past volatility. Our model is validated by the empirical evidence that the liquidity-adjusted mean-variance (LAMV) portfolios outperform the traditional mean-variance (TMV) portfolios.|\n", "2503.00603": "|**2025-03-01**|**Understanding the Commodity Futures Term Structure Through Signatures**|Hari P. Krishnan, Stephan Sturm et.al.|[2503.00603](http://arxiv.org/abs/2503.00603)|null|19 pages, 1 figure|Signature methods have been widely and effectively used as a tool for feature extraction in statistical learning methods, notably in mathematical finance. They lack, however, interpretability: in the general case, it is unclear why signatures actually work. The present article aims to address this issue directly, by introducing and developing the concept of signature perturbations. In particular, we construct a regular perturbation of the signature of the term structure of log prices for various commodities, in terms of the convenience yield. Our perturbation expansion and rigorous convergence estimates help explain the success of signature-based classification of commodities markets according to their term structure, with the volatility of the convenience yield as the major discriminant.|\n", "2502.20978": "|**2025-03-04**|**Using quantile time series and historical simulation to forecast financial risk multiple steps ahead**|Richard Gerlach, Antonio Naimoli, Giuseppe Storti et.al.|[2502.20978](http://arxiv.org/abs/2502.20978)|null||A method for quantile-based, semi-parametric historical simulation estimation of multiple step ahead Value-at-Risk (VaR) and Expected Shortfall (ES) models is developed. It uses the quantile loss function, analogous to how the quasi-likelihood is employed by standard historical simulation methods. The returns data are scaled by the estimated quantile series, then resampling is employed to estimate the forecast distribution one and multiple steps ahead, allowing tail risk forecasting. The proposed method is applicable to any data or model where the relationship between VaR and ES does not change over time and can be extended to allow a measurement equation incorporating realized measures, thus including Realized GARCH and Realized CAViaR type models. Its finite sample properties, and its comparison with existing historical simulation methods, are evaluated via a simulation study. A forecasting study assesses the relative accuracy of the 1% and 2.5% VaR and ES one-day-ahead and ten-day-ahead forecasting results for the proposed class of models compared to several competitors.|\n", "2503.08692": "|**2025-02-27**|**Detecting Crypto Pump-and-Dump Schemes: A Thresholding-Based Approach to Handling Market Noise**|Mahya Karbalaii et.al.|[2503.08692](http://arxiv.org/abs/2503.08692)|null||We propose a simple yet robust unsupervised model to detect pump-and-dump events on tokens listed on the Poloniex Exchange platform. By combining threshold-based criteria with exponentially weighted moving averages (EWMA) and volatility measures, our approach effectively distinguishes genuine anomalies from minor trading fluctuations, even for tokens with low liquidity and prolonged inactivity. These characteristics present a unique challenge, as standard anomaly-detection methods often over-flag negligible volume spikes. Our framework overcomes this issue by tailoring both price and volume thresholds to the specific trading patterns observed, resulting in a model that balances high true-positive detection with minimal noise.|\n", "2502.19305": "|**2025-02-26**|**Corporate Fraud Detection in Rich-yet-Noisy Financial Graph**|Shiqi Wang, Zhibo Zhang, Libing Fang et.al.|[2502.19305](http://arxiv.org/abs/2502.19305)|**[link](https://github.com/wangskyGit/KeHGN-R)**||Corporate fraud detection aims to automatically recognize companies that conduct wrongful activities such as fraudulent financial statements or illegal insider trading. Previous learning-based methods fail to effectively integrate rich interactions in the company network. To close this gap, we collect 18-year financial records in China to form three graph datasets with fraud labels. We analyze the characteristics of the financial graphs, highlighting two pronounced issues: (1) information overload: the dominance of (noisy) non-company nodes over company nodes hinders the message-passing process in Graph Convolution Networks (GCN); and (2) hidden fraud: there exists a large percentage of possible undetected violations in the collected data. The hidden fraud problem will introduce noisy labels in the training dataset and compromise fraud detection results. To handle such challenges, we propose a novel graph-based method, namely, Knowledge-enhanced GCN with Robust Two-stage Learning (${\\rm KeGCN}_{R}$), which leverages Knowledge Graph Embeddings to mitigate the information overload and effectively learns rich representations. The proposed model adopts a two-stage learning method to enhance robustness against hidden frauds. Extensive experimental results not only confirm the importance of interactions but also show the superiority of ${\\rm KeGCN}_{R}$ over a number of strong baselines in terms of fraud detection effectiveness and robustness.|\n", "2502.18177": "|**2025-02-25**|**Recurrent Neural Networks for Dynamic VWAP Execution: Adaptive Trading Strategies with Temporal Kolmogorov-Arnold Networks**|Remi Genet et.al.|[2502.18177](http://arxiv.org/abs/2502.18177)|**[link](https://github.com/remigenet/deepdynamicvwap)**||The execution of Volume Weighted Average Price (VWAP) orders remains a critical challenge in modern financial markets, particularly as trading volumes and market complexity continue to increase. In my previous work arXiv:2502.13722, I introduced a novel deep learning approach that demonstrated significant improvements over traditional VWAP execution methods by directly optimizing the execution problem rather than relying on volume curve predictions. However, that model was static because it employed the fully linear approach described in arXiv:2410.21448, which is not designed for dynamic adjustment. This paper extends that foundation by developing a dynamic neural VWAP framework that adapts to evolving market conditions in real time. We introduce two key innovations: first, the integration of recurrent neural networks to capture complex temporal dependencies in market dynamics, and second, a sophisticated dynamic adjustment mechanism that continuously optimizes execution decisions based on market feedback. The empirical analysis, conducted across five major cryptocurrency markets, demonstrates that this dynamic approach achieves substantial improvements over both traditional methods and our previous static implementation, with execution performance gains of 10 to 15% in liquid markets and consistent outperformance across varying conditions. These results suggest that adaptive neural architectures can effectively address the challenges of modern VWAP execution while maintaining computational efficiency suitable for practical deployment.|\n", "2502.17967": "|**2025-02-25**|**LLM Knows Geometry Better than Algebra: Numerical Understanding of LLM-Based Agents in A Trading Arena**|Tianmi Ma, Jiawei Du, Wenxin Huang et.al.|[2502.17967](http://arxiv.org/abs/2502.17967)|**[link](https://github.com/wekjsdvnm/agent-trading-arena)**||Recent advancements in large language models (LLMs) have significantly improved performance in natural language processing tasks. However, their ability to generalize to dynamic, unseen tasks, particularly in numerical reasoning, remains a challenge. Existing benchmarks mainly evaluate LLMs on problems with predefined optimal solutions, which may not align with real-world scenarios where clear answers are absent. To bridge this gap, we design the Agent Trading Arena, a virtual numerical game simulating complex economic systems through zero-sum games, where agents invest in stock portfolios. Our experiments reveal that LLMs, including GPT-4o, struggle with algebraic reasoning when dealing with plain-text stock data, often focusing on local details rather than global trends. In contrast, LLMs perform significantly better with geometric reasoning when presented with visual data, such as scatter plots or K-line charts, suggesting that visual representations enhance numerical reasoning. This capability is further improved by incorporating the reflection module, which aids in the analysis and interpretation of complex data. We validate our findings on NASDAQ Stock dataset, where LLMs demonstrate stronger reasoning with visual data compared to text. Our code and data are publicly available at https://github.com/wekjsdvnm/Agent-Trading-Arena.git.|\n", "2502.17044": "|**2025-02-24**|**A data-driven econo-financial stress-testing framework to estimate the effect of supply chain networks on financial systemic risk**|Jan Fialkowski, Christian Diem, Andr\u00e1s Borsos et.al.|[2502.17044](http://arxiv.org/abs/2502.17044)|**[link](https://github.com/JanFialkowski/FSRI_Plus)**||Supply chain disruptions constitute an often underestimated risk for financial stability. As in financial networks, systemic risks in production networks arises when the local failure of one firm impacts the production of others and might trigger cascading disruptions that affect significant parts of the economy. Here, we study how systemic risk in production networks translates into financial systemic risk through a mechanism where supply chain contagion leads to correlated bank-firm loan defaults. We propose a financial stress-testing framework for micro- and macro-prudential applications that features a national firm level supply chain network in combination with interbank network layers. The model is calibrated by using a unique data set including about 1 million firm-level supply links, practically all bank-firm loans, and all interbank loans in a small European economy. As a showcase we implement a real COVID-19 shock scenario on the firm level. This model allows us to study how the disruption dynamics in the real economy can lead to interbank solvency contagion dynamics. We estimate to what extent this amplifies financial systemic risk. We discuss the relative importance of these contagion channels and find an increase of interbank contagion by 70% when production network contagion is present. We then examine the financial systemic risk firms bring to banks and find an increase of up to 28% in the presence of the interbank contagion channel. This framework is the first financial systemic risk model to take agent-level dynamics of the production network and shocks of the real economy into account which opens a path for directly, and event-driven understanding of the dynamical interaction between the real economy and financial systems.|\n", "2502.16023": "|**2025-02-22**|**Contrastive Similarity Learning for Market Forecasting: The ContraSim Framework**|Nicholas Vinden, Raeid Saqur, Zining Zhu et.al.|[2502.16023](http://arxiv.org/abs/2502.16023)|null|8 pages, 3 appendices|We introduce the Contrastive Similarity Space Embedding Algorithm (ContraSim), a novel framework for uncovering the global semantic relationships between daily financial headlines and market movements. ContraSim operates in two key stages: (I) Weighted Headline Augmentation, which generates augmented financial headlines along with a semantic fine-grained similarity score, and (II) Weighted Self-Supervised Contrastive Learning (WSSCL), an extended version of classical self-supervised contrastive learning that uses the similarity metric to create a refined weighted embedding space. This embedding space clusters semantically similar headlines together, facilitating deeper market insights. Empirical results demonstrate that integrating ContraSim features into financial forecasting tasks improves classification accuracy from WSJ headlines by 7%. Moreover, leveraging an information density analysis, we find that the similarity spaces constructed by ContraSim intrinsically cluster days with homogeneous market movement directions, indicating that ContraSim captures market dynamics independent of ground truth labels. Additionally, ContraSim enables the identification of historical news days that closely resemble the headlines of the current day, providing analysts with actionable insights to predict market trends by referencing analogous past events.|\n", "2502.15611": "|**2025-02-21**|**Network topology of the Euro Area interbank market**|Ilias Aarab, Thomas Gottron et.al.|[2502.15611](http://arxiv.org/abs/2502.15611)|null|This is the preprint version of the paper published in: Aarab, I.,   Gottron, T. (2024). Network Topology of the Euro Area Interbank Market. In:   Mingione, M., Vichi, M., Zaccaria, G. (eds) *High-quality and Timely   Statistics*. CESS 2022. Studies in Theoretical and Applied Statistics.   Springer, Cham. https://doi.org/10.1007/978-3-031-63630-1_1|The rapidly increasing availability of large amounts of granular financial data, paired with the advances of big data related technologies induces the need of suitable analytics that can represent and extract meaningful information from such data. In this paper we propose a multi-layer network approach to distill the Euro Area (EA) banking system in different distinct layers. Each layer of the network represents a specific type of financial relationship between banks, based on various sources of EA granular data collections. The resulting multi-layer network allows one to describe, analyze and compare the topology and structure of EA banks from different perspectives, eventually yielding a more complete picture of the financial market. This granular information representation has the potential to enable researchers and practitioners to better apprehend financial system dynamics as well as to support financial policies to manage and monitor financial risk from a more holistic point of view.|\n", "2502.15458": "|**2025-02-21**|**Clustered Network Connectedness: A New Measurement Framework with Application to Global Equity Markets**|Bastien Buchwalter, Francis X. Diebold, Kamil Yilmaz et.al.|[2502.15458](http://arxiv.org/abs/2502.15458)|null||Network connections, both across and within markets, are central in countless economic contexts. In recent decades, a large literature has developed and applied flexible methods for measuring network connectedness and its evolution, based on variance decompositions from vector autoregressions (VARs), as in Diebold and Yilmaz (2014). Those VARs are, however, typically identified using full orthogonalization (Sims, 1980), or no orthogonalization (Koop, Pesaran, and Potter, 1996; Pesaran and Shin, 1998), which, although useful, are special and extreme cases of a more general framework that we develop in this paper. In particular, we allow network nodes to be connected in \"clusters\", such as asset classes, industries, regions, etc., where shocks are orthogonal across clusters (Sims style orthogonalized identification) but correlated within clusters (Koop-Pesaran-Potter-Shin style generalized identification), so that the ordering of network nodes is relevant across clusters but irrelevant within clusters. After developing the clustered connectedness framework, we apply it in a detailed empirical exploration of sixteen country equity markets spanning three global regions.|\n", "2502.15853": "|**2025-02-21**|**Multi-Agent Stock Prediction Systems: Machine Learning Models, Simulations, and Real-Time Trading Strategies**|Daksh Dave, Gauransh Sawhney, Vikhyat Chauhan et.al.|[2502.15853](http://arxiv.org/abs/2502.15853)|null||This paper presents a comprehensive study on stock price prediction, leveragingadvanced machine learning (ML) and deep learning (DL) techniques to improve financial forecasting accuracy. The research evaluates the performance of various recurrent neural network (RNN) architectures, including Long Short-Term Memory (LSTM) networks, Gated Recurrent Units (GRU), and attention-based models. These models are assessed for their ability to capture complex temporal dependencies inherent in stock market data. Our findings show that attention-based models outperform other architectures, achieving the highest accuracy by capturing both short and long-term dependencies. This study contributes valuable insights into AI-driven financial forecasting, offering practical guidance for developing more accurate and efficient trading systems.|\n", "2502.14479": "|**2025-02-20**|**Modelling the term-structure of default risk under IFRS 9 within a multistate regression framework**|Arno Botha, Tanja Verster, Roland Breedt et.al.|[2502.14479](http://arxiv.org/abs/2502.14479)|null|33 pages, 8192 words, 12 figures|The lifetime behaviour of loans is notoriously difficult to model, which can compromise a bank's financial reserves against future losses, if modelled poorly. Therefore, we present a data-driven comparative study amongst three techniques in modelling a series of default risk estimates over the lifetime of each loan, i.e., its term-structure. The behaviour of loans can be described using a nonstationary and time-dependent semi-Markov model, though we model its elements using a multistate regression-based approach. As such, the transition probabilities are explicitly modelled as a function of a rich set of input variables, including macroeconomic and loan-level inputs. Our modelling techniques are deliberately chosen in ascending order of complexity: 1) a Markov chain; 2) beta regression; and 3) multinomial logistic regression. Using residential mortgage data, our results show that each successive model outperforms the previous, likely as a result of greater sophistication. This finding required devising a novel suite of simple model diagnostics, which can itself be reused in assessing sampling representativeness and the performance of other modelling techniques. These contributions surely advance the current practice within banking when conducting multistate modelling. Consequently, we believe that the estimation of loss reserves will be more timeous and accurate under IFRS 9.|\n", "2502.14431": "|**2025-02-20**|**Causality Analysis of COVID-19 Induced Crashes in Stock and Commodity Markets: A Topological Perspective**|Buddha Nath Sharma, Anish Rai, SR Luwang et.al.|[2502.14431](http://arxiv.org/abs/2502.14431)|null||The paper presents a comprehensive causality analysis of the US stock and commodity markets during the COVID-19 crash. The dynamics of different sectors are also compared. We use Topological Data Analysis (TDA) on multidimensional time-series to identify crashes in stock and commodity markets. The Wasserstein Distance WD shows distinct spikes signaling the crash for both stock and commodity markets. We then compare the persistence diagrams of stock and commodity markets using the WD metric. A significant spike in the $WD$ between stock and commodity markets is observed during the crisis, suggesting significant topological differences between the markets. Similar spikes are observed between the sectors of the US market as well. Spikes obtained may be due to either a difference in the magnitude of crashes in the two markets (or sectors), or from the temporal lag between the two markets suggesting information flow. We study the Granger-causality between stock and commodity markets and also between different sectors. The results show a bidirectional Granger-causality between commodity and stock during the crash period, demonstrating the greater interdependence of financial markets during the crash. However, the overall analysis shows that the causal direction is from stock to commodity. A pairwise Granger-causal analysis between US sectors is also conducted. There is a significant increase in the interdependence between the sectors during the crash period. TDA combined with Granger-causality effectively analyzes the interdependence and sensitivity of different markets and sectors.|\n", "2502.15822": "|**2025-02-20**|**Financial fraud detection system based on improved random forest and gradient boosting machine (GBM)**|Tianzuo Hu et.al.|[2502.15822](http://arxiv.org/abs/2502.15822)|null||This paper proposes a financial fraud detection system based on improved Random Forest (RF) and Gradient Boosting Machine (GBM). Specifically, the system introduces a novel model architecture called GBM-SSRF (Gradient Boosting Machine with Simplified and Strengthened Random Forest), which cleverly combines the powerful optimization capabilities of the gradient boosting machine (GBM) with improved randomization. The computational efficiency and feature extraction capabilities of the Simplified and Strengthened Random Forest (SSRF) forest significantly improve the performance of financial fraud detection. Although the traditional random forest model has good classification capabilities, it has high computational complexity when faced with large-scale data and has certain limitations in feature selection. As a commonly used ensemble learning method, the GBM model has significant advantages in optimizing performance and handling nonlinear problems. However, GBM takes a long time to train and is prone to overfitting problems when data samples are unbalanced. In response to these limitations, this paper optimizes the random forest based on the structure, reducing the computational complexity and improving the feature selection ability through the structural simplification and enhancement of the random forest. In addition, the optimized random forest is embedded into the GBM framework, and the model can maintain efficiency and stability with the help of GBM's gradient optimization capability. Experiments show that the GBM-SSRF model not only has good performance, but also has good robustness and generalization capabilities, providing an efficient and reliable solution for financial fraud detection.|\n"}, "Deep Learning in Finance": {"2503.10285": "|**2025-03-13**|**Unifying monitoring and modelling of water concentration levels in surface waters**|Peter B Sorensen, Anders Nielsen, Peter E Holm et.al.|[2503.10285](http://arxiv.org/abs/2503.10285)|null|41 pages, 11 figures, Developed to support the Danish EPA|Accurate prediction of expected concentrations is essential for effective catchment management, requiring both extensive monitoring and advanced modeling techniques. However, due to limitations in the equation solving capacity, the integration of monitoring and modeling has been suffering suboptimal statistical approaches. This limitation results in models that can only partially leverage monitoring data, thus being an obstacle for realistic uncertainty assessments by overlooking critical correlations between both measurements and model parameters. This study presents a novel solution that integrates catchment monitoring and a unified hieratical statistical catchment modeling that employs a log-normal distribution for residuals within a left-censored likelihood function to address measurements below detection limits. This enables the estimation of concentrations within sub-catchments in conjunction with a source/fate sub-catchment model and monitoring data. This approach is possible due to a model builder R package denoted RTMB. The proposed approach introduces a statistical paradigm based on a hierarchical structure, capable of accommodating heterogeneous sampling across various sampling locations and the authors suggest that this also will encourage further refinement of other existing modeling platforms within the scientific community to improve synergy with monitoring programs. The application of the method is demonstrated through an analysis of nickel concentrations in Danish surface waters.|\n", "2503.10032": "|**2025-03-13**|**A Neumann-Neumann Acceleration with Coarse Space for Domain Decomposition of Extreme Learning Machines**|Chang-Ock Lee, Byungeun Ryoo et.al.|[2503.10032](http://arxiv.org/abs/2503.10032)|null|21 pages, 6 figures, 6 tables|Extreme learning machines (ELMs), which preset hidden layer parameters and solve for last layer coefficients via a least squares method, can typically solve partial differential equations faster and more accurately than Physics Informed Neural Networks. However, they remain computationally expensive when high accuracy requires large least squares problems to be solved. Domain decomposition methods (DDMs) for ELMs have allowed parallel computation to reduce training times of large systems. This paper constructs a coarse space for ELMs, which enables further acceleration of their training. By partitioning interface variables into coarse and non-coarse variables, selective elimination introduces a Schur complement system on the non-coarse variables with the coarse problem embedded. Key to the performance of the proposed method is a Neumann-Neumann acceleration that utilizes the coarse space. Numerical experiments demonstrate significant speedup compared to a previous DDM method for ELMs.|\n", "2503.09409": "|**2025-03-12**|**AI-based Framework for Robust Model-Based Connector Mating in Robotic Wire Harness Installation**|Claudius Kienle, Benjamin Alt, Finn Schneider et.al.|[2503.09409](http://arxiv.org/abs/2503.09409)|null|6 pages, 6 figures, 4 tables, submitted to the 2025 IEEE 21st   International Conference on Automation Science and Engineering|Despite the widespread adoption of industrial robots in automotive assembly, wire harness installation remains a largely manual process, as it requires precise and flexible manipulation. To address this challenge, we design a novel AI-based framework that automates cable connector mating by integrating force control with deep visuotactile learning. Our system optimizes search-and-insertion strategies using first-order optimization over a multimodal transformer architecture trained on visual, tactile, and proprioceptive data. Additionally, we design a novel automated data collection and optimization pipeline that minimizes the need for machine learning expertise. The framework optimizes robot programs that run natively on standard industrial controllers, permitting human experts to audit and certify them. Experimental validations on a center console assembly task demonstrate significant improvements in cycle times and robustness compared to conventional robot programming approaches. Videos are available under https://claudius-kienle.github.io/AppMuTT.|\n", "2503.09345": "|**2025-03-12**|**Large-scale Thermo-Mechanical Simulation of Laser Beam Welding Using High-Performance Computing: A Qualitative Reproduction of Experimental Results**|Tommaso Bevilacqua, Andrey Gumenyuk, Niloufar Habibi et.al.|[2503.09345](http://arxiv.org/abs/2503.09345)|null||Laser beam welding is a non-contact joining technique that has gained significant importance in the course of the increasing degree of automation in industrial manufacturing. This process has established itself as a suitable joining tool for metallic materials due to its non-contact processing, short cycle times, and small heat-affected zones. One potential problem, however, is the formation of solidification cracks, which particularly affects alloys with a pronounced melting range. Since solidification cracking is influenced by both temperature and strain rate, precise measurement technologies are of crucial importance. For this purpose, as an experimental setup, a Controlled Tensile Weldability (CTW) test combined with a local deformation measurement technique is used.   The aim of the present work is the development of computational methods and software tools to numerically simulate the CTW. The numerical results are compared with those obtained from the experimental CTW. In this study, an austenitic stainless steel sheet is selected. A thermo-elastoplastic material behavior with temperature-dependent material parameters is assumed. The time-dependent problem is first discretized in time and then the resulting nonlinear problem is linearized with Newton's method. For the discretization in space, finite elements are used. In order to obtain a sufficiently accurate solution, a large number of finite elements has to be used. In each Newton step, this yields a large linear system of equations that has to be solved. Therefore, a highly parallel scalable solver framework, based on the software library PETSc, was used to solve this computationally challenging problem on a high-performance computing architecture. Finally, the experimental results and the numerical simulations are compared, showing to be qualitatively in good agreement.|\n", "2503.09655": "|**2025-03-12**|**A Deep Reinforcement Learning Approach to Automated Stock Trading, using xLSTM Networks**|Faezeh Sarlakifar, Mohammadreza Mohammadzadeh Asl, Sajjad Rezvani Khaledi et.al.|[2503.09655](http://arxiv.org/abs/2503.09655)|null||Traditional Long Short-Term Memory (LSTM) networks are effective for handling sequential data but have limitations such as gradient vanishing and difficulty in capturing long-term dependencies, which can impact their performance in dynamic and risky environments like stock trading. To address these limitations, this study explores the usage of the newly introduced Extended Long Short Term Memory (xLSTM) network in combination with a deep reinforcement learning (DRL) approach for automated stock trading. Our proposed method utilizes xLSTM networks in both actor and critic components, enabling effective handling of time series data and dynamic market environments. Proximal Policy Optimization (PPO), with its ability to balance exploration and exploitation, is employed to optimize the trading strategy. Experiments were conducted using financial data from major tech companies over a comprehensive timeline, demonstrating that the xLSTM-based model outperforms LSTM-based methods in key trading evaluation metrics, including cumulative return, average profitability per trade, maximum earning rate, maximum pullback, and Sharpe ratio. These findings mark the potential of xLSTM for enhancing DRL-based stock trading systems.|\n", "2503.09198": "|**2025-03-12**|**A 3d particle visualization system for temperature management**|Benoit Lange, Nancy Rodriguez, William Puech et.al.|[2503.09198](http://arxiv.org/abs/2503.09198)|null||This paper deals with a 3D visualization technique proposed to analyze and manage energy efficiency from a data center. Data are extracted from sensors located in the IBM Green Data Center in Montpellier France. These sensors measure different information such as hygrometry, pressure and temperature. We want to visualize in real-time the large among of data produced by these sensors. A visualization engine has been designed, based on particles system and a client server paradigm. In order to solve performance problems, a Level Of Detail solution has been developed. These methods are based on the earlier work introduced by J. Clark in 1976. In this paper we introduce a particle method used for this work and subsequently we explain different simplification methods we have applied to improve our solution.|\n", "2503.09647": "|**2025-03-12**|**Leveraging LLMS for Top-Down Sector Allocation In Automated Trading**|Ryan Quek Wei Heng, Edoardo Vittori, Keane Ong et.al.|[2503.09647](http://arxiv.org/abs/2503.09647)|null||This paper introduces a methodology leveraging Large Language Models (LLMs) for sector-level portfolio allocation through systematic analysis of macroeconomic conditions and market sentiment. Our framework emphasizes top-down sector allocation by processing multiple data streams simultaneously, including policy documents, economic indicators, and sentiment patterns. Empirical results demonstrate superior risk-adjusted returns compared to traditional cross momentum strategies, achieving a Sharpe ratio of 2.51 and portfolio return of 8.79% versus -0.61 and -1.39% respectively. These results suggest that LLM-based systematic macro analysis presents a viable approach for enhancing automated portfolio allocation decisions at the sector level.|\n", "2503.08953": "|**2025-03-11**|**Capturing Lifecycle System Degradation in Digital Twin Model Updating**|Yifan Tang, Mostafa Rahmani Dehaghani, G. Gary Wang et.al.|[2503.08953](http://arxiv.org/abs/2503.08953)|null|32 pages, 25 figures|Digital twin (DT) has emerged as a powerful tool to facilitate monitoring, control, and other decision-making tasks in real-world engineering systems. Online update methods have been proposed to update DT models. Considering the degradation behavior in the system lifecycle, these methods fail to enable DT models to predict the system responses affected by the system degradation over time. To alleviate this problem, degradation models of measurable parameters have been integrated into DT construction. However, identifying the degradation parameters relies on prior knowledge of the system and expensive experiments. To mitigate those limitations, this paper proposes a lifelong update method for DT models to capture the effects of system degradation on system responses without any prior knowledge and expensive offline experiments on the system. The core idea in the work is to represent the system degradation during the lifecycle as the dynamic changes of DT configurations (i.e., model parameters with a fixed model structure) at all degradation stages. During the lifelong update process, an Autoencoder is adopted to reconstruct the model parameters of all hidden layers simultaneously, so that the latent features taking into account the dependencies among hidden layers are obtained for each degradation stage. The dynamic behavior of latent features among successive degradation stages is then captured by a long short-term memory model, which enables prediction of the latent feature at any unseen stage. Based on the predicted latent features, the model configuration at future degradation stage is reconstructed to determine the new DT model, which predicts the system responses affected by the degradation at the same stage. The test results on two engineering datasets demonstrate that the proposed update method could capture effects of system degradation on system responses during the lifecycle.|\n", "2503.08904": "|**2025-03-11**|**Towards Efficient Parametric State Estimation in Circulating Fuel Reactors with Shallow Recurrent Decoder Networks**|Stefano Riva, Carolina Introini, J. Nathan Kutz et.al.|[2503.08904](http://arxiv.org/abs/2503.08904)|**[link](https://github.com/ermete-lab/nushred)**|arXiv admin note: text overlap with arXiv:2409.12550|The recent developments in data-driven methods have paved the way to new methodologies to provide accurate state reconstruction of engineering systems; nuclear reactors represent particularly challenging applications for this task due to the complexity of the strongly coupled physics involved and the extremely harsh and hostile environments, especially for new technologies such as Generation-IV reactors. Data-driven techniques can combine different sources of information, including computational proxy models and local noisy measurements on the system, to robustly estimate the state. This work leverages the novel Shallow Recurrent Decoder architecture to infer the entire state vector (including neutron fluxes, precursors concentrations, temperature, pressure and velocity) of a reactor from three out-of-core time-series neutron flux measurements alone. In particular, this work extends the standard architecture to treat parametric time-series data, ensuring the possibility of investigating different accidental scenarios and showing the capabilities of this approach to provide an accurate state estimation in various operating conditions. This paper considers as a test case the Molten Salt Fast Reactor (MSFR), a Generation-IV reactor concept, characterised by strong coupling between the neutronics and the thermal hydraulics due to the liquid nature of the fuel. The promising results of this work are further strengthened by the possibility of quantifying the uncertainty associated with the state estimation, due to the considerably low training cost. The accurate reconstruction of every characteristic field in real-time makes this approach suitable for monitoring and control purposes in the framework of a reactor digital twin.|\n", "2503.08283": "|**2025-03-11**|**Nonlinear optimals and their role in sustaining turbulence in channel flow**|Dario Klingenberg, Rich R. Kerswell et.al.|[2503.08283](http://arxiv.org/abs/2503.08283)|null||We investigate the energy transfer from the mean profile to velocity fluctuations in channel flow by calculating nonlinear optimal disturbances,i.e. the initial condition of a given finite energy that achieves the highest possible energy growth during a given fixed time horizon. It is found that for a large range of time horizons and initial disturbance energies, the nonlinear optimal exhibits streak spacing and amplitude consistent with DNS at least at Re_tau = 180, which suggests that they isolate the relevant physical mechanisms that sustain turbulence. Moreover, the time horizon necessary for a nonlinear disturbance to outperform a linear optimal is consistent with previous DNS-based estimates using eddy turnover time, which offers a new perspective on how some turbulent time scales are determined.|\n", "2503.08163": "|**2025-03-11**|**XAI4Extremes: An interpretable machine learning framework for understanding extreme-weather precursors under climate change**|Jiawen Wei, Aniruddha Bora, Vivek Oommen et.al.|[2503.08163](http://arxiv.org/abs/2503.08163)|null||Extreme weather events are increasing in frequency and intensity due to climate change. This, in turn, is exacting a significant toll in communities worldwide. While prediction skills are increasing with advances in numerical weather prediction and artificial intelligence tools, extreme weather still present challenges. More specifically, identifying the precursors of such extreme weather events and how these precursors may evolve under climate change remain unclear. In this paper, we propose to use post-hoc interpretability methods to construct relevance weather maps that show the key extreme-weather precursors identified by deep learning models. We then compare this machine view with existing domain knowledge to understand whether deep learning models identified patterns in data that may enrich our understanding of extreme-weather precursors. We finally bin these relevant maps into different multi-year time periods to understand the role that climate change is having on these precursors. The experiments are carried out on Indochina heatwaves, but the methodology can be readily extended to other extreme weather events worldwide.|\n", "2503.07834": "|**2025-03-10**|**Network Analysis of Uniswap: Centralization and Fragility in the Decentralized Exchange Market**|Tao Yan, Claudio J. Tessone et.al.|[2503.07834](http://arxiv.org/abs/2503.07834)|null||The Uniswap is a Decentralized Exchange (DEX) protocol that facilitates automatic token exchange without the need for traditional order books. Every pair of tokens forms a liquidity pool on Uniswap, and each token can be paired with any other token to create liquidity pools. This characteristic motivates us to employ a complex network approach to analyze the features of the Uniswap market. This research presents a comprehensive analysis of the Uniswap network using complex network methods. The network on October 31, 2023, is built to observe its recent features, showcasing both scale-free and core-periphery properties. By employing node and edge-betweenness metrics, we detect the most important tokens and liquidity pools. Additionally, we construct daily networks spanning from the beginning of Uniswap V2 on May 5, 2020, until October 31, 2023, and our findings demonstrate that the network becomes increasingly fragile over time. Furthermore, we conduct a robustness analysis by simulating the deletion of nodes to estimate the impact of some extreme events such as the Terra collapse. The results indicate that the Uniswap network exhibits robustness, yet it is notably fragile when deleting tokens with high betweenness centrality. This finding highlights that, despite being a decentralized exchange, Uniswap exhibits significant centralization tendencies in terms of token network connectivity and the distribution of TVL across nodes (tokens) and edges (liquidity pools).|\n", "2503.07462": "|**2025-03-10**|**Simultaneous Energy Harvesting and Bearing Fault Detection using Piezoelectric Cantilevers**|P. Peralta-Braz, M. M. Alamdari, C. T. Chou et.al.|[2503.07462](http://arxiv.org/abs/2503.07462)|null||Bearings are critical components in industrial machinery, yet their vulnerability to faults often leads to costly breakdowns. Conventional fault detection methods depend on continuous, high-frequency vibration sensing, digitising, and wireless transmission to the cloud-an approach that significantly drains the limited energy reserves of battery-powered sensors, accelerating their depletion and increasing maintenance costs. This work proposes a fundamentally different approach: rather than using instantaneous vibration data, we employ piezoelectric energy harvesters (PEHs) tuned to specific frequencies and leverage the cumulative harvested energy over time as the key diagnostic feature. By directly utilising the energy generated from the machinery's vibrations, we eliminate the need for frequent analog-to-digital conversions and data transmission, thereby reducing energy consumption at the sensor node and extending its operational lifetime. To validate this approach, we use a numerical PEH model and publicly available acceleration datasets, examining various PEH designs with different natural frequencies. We also consider the influence of the classification algorithm, the number of devices, and the observation window duration. The results demonstrate that the harvested energy reliably indicates bearing faults across a range of conditions and severities. By converting vibration energy into both a power source and a diagnostic feature, our solution offers a more sustainable, low-maintenance strategy for fault detection in smart machinery.|\n", "2503.07440": "|**2025-03-10**|**Early signs of stuck pipe detection based on Crossformer**|Bo Cao, Yu Song, Jin Yang et.al.|[2503.07440](http://arxiv.org/abs/2503.07440)|null|33 pages,9 figure|Stuck pipe incidents are one of the major challenges in drilling engineering,leading to massive time loss and additional costs.To address the limitations of insufficient long sequence modeling capability,the difficulty in accurately establishing warning threshold,and the lack of model interpretability in existing methods,we utilize Crossformer for early signs of detection indicating potential stuck events in order to provide guidance for on-site drilling engineers and prevent stuck pipe incidents.The sliding window technique is integrated into Crossformer to allow it to output and display longer outputs,the improved Crossformer model is trained using normal time series drilling data to generate predictions for various parameters at each time step.The relative reconstruction error of model is regard as the risk of stuck pipe,thereby considering data that the model can't predict as anomalies,which represent the early signs of stuck pipe incidents.The multi-step prediction capability of Crossformer and relative reconstruction error are combined to assess stuck pipe risk at each time step in advance.We partition the reconstruction error into modeling error and error due to anomalous data fluctuations,furthermore,the dynamic warning threshold and warning time for stuck pipe incidents are determined using the probability density function of reconstruction errors from normal drilling data.The results indicate that our method can effectively detect early signs of stuck pipe incidents during the drilling process.Crossformer exhibits superior modeling and predictive capabilities compared with other deep learning models.Transformer-based models with multi-step prediction capability are more suitable for stuck pipe prediction compared to the current single-step prediction models.|\n", "2503.07684": "|**2025-03-10**|**What is missing from existing Lithium-Sulfur models to capture coin-cell behaviour?**|Miss. Elizabeth Olisa Monica Marinescu et.al.|[2503.07684](http://arxiv.org/abs/2503.07684)|null|27 pages, 7 figures, conferences presented: ModVal 2025, ECS 2025|Lithium-sulfur (Li-S) batteries offer a promising alternative to current lithium-ion (Li-ion) batteries, with a high theoretical energy density, improved safety and high abundance, low cost of materials. For Li-S to reach commercial application, it is essential to understand how the behaviour scales between cell formats; new material development is predominately completed at coin-cell level, whilst pouch-cells will be used for commercial applications. Differences such as reduced electrolyte-to-sulfur (E/S) ratios and increased geometric size at larger cell formats contribute to the behavioural differences, in terms of achievable capacity, cyclability and potential degradation mechanisms.   This work focuses on the steps required to capture and test coin-cell behaviour, building upon the existing models within the literature, which predominately focus on pouch-cells. The areas investigated throughout this study, to improve the capability of the model in terms of scaling ability and causality of predictions, include the cathode surface area, precipitation dynamics and C-rate dependence.|\n", "2503.07231": "|**2025-03-10**|**An Analytics-Driven Approach to Enhancing Supply Chain Visibility with Graph Neural Networks and Federated Learning**|Ge Zheng, Alexandra Brintrup et.al.|[2503.07231](http://arxiv.org/abs/2503.07231)|null|15 pages, 5 figures, 5 tables, submitted to a journal|In today's globalised trade, supply chains form complex networks spanning multiple organisations and even countries, making them highly vulnerable to disruptions. These vulnerabilities, highlighted by recent global crises, underscore the urgent need for improved visibility and resilience of the supply chain. However, data-sharing limitations often hinder the achievement of comprehensive visibility between organisations or countries due to privacy, security, and regulatory concerns. Moreover, most existing research studies focused on individual firm- or product-level networks, overlooking the multifaceted interactions among diverse entities that characterise real-world supply chains, thus limiting a holistic understanding of supply chain dynamics. To address these challenges, we propose a novel approach that integrates Federated Learning (FL) and Graph Convolutional Neural Networks (GCNs) to enhance supply chain visibility through relationship prediction in supply chain knowledge graphs. FL enables collaborative model training across countries by facilitating information sharing without requiring raw data exchange, ensuring compliance with privacy regulations and maintaining data security. GCNs empower the framework to capture intricate relational patterns within knowledge graphs, enabling accurate link prediction to uncover hidden connections and provide comprehensive insights into supply chain networks. Experimental results validate the effectiveness of the proposed approach, demonstrating its ability to accurately predict relationships within country-level supply chain knowledge graphs. This enhanced visibility supports actionable insights, facilitates proactive risk management, and contributes to the development of resilient and adaptive supply chain strategies, ensuring that supply chains are better equipped to navigate the complexities of the global economy.|\n", "2503.07150": "|**2025-03-10**|**Simulating programmable morphing of shape memory polymer beam systems with complex geometry and topology**|Giulio Ferri, Enzo Marino et.al.|[2503.07150](http://arxiv.org/abs/2503.07150)|null||We propose a novel approach to the analysis of programmable geometrically exact shear deformable beam systems made of shape memory polymers. The proposed method combines the viscoelastic Generalized Maxwell model with the Williams, Landel and Ferry relaxation principle, enabling the reproduction of the shape memory effect of structural systems featuring complex geometry and topology. Very high efficiency is pursued by discretizing the differential problem in space through the isogeometric collocation (IGA-C) method. The method, in addition to the desirable attributes of isogeometric analysis (IGA), such as exactness of the geometric reconstruction of complex shapes and high-order accuracy, circumvents the need for numerical integration since it discretizes the problem in the strong form. Other distinguishing features of the proposed formulation are: i) ${\\rm SO}(3)$-consistency for the linearization of the problem and for the time stepping; ii) minimal (finite) rotation parametrization, that means only three rotational unknowns are used; iii) no additional unknowns are needed to account for the rate-dependent material compared to the purely elastic case. Through different numerical applications involving challenging initial geometries, we show that the proposed formulation possesses all the sought attributes in terms of programmability of complex systems, geometric flexibility, and high order accuracy.|\n", "2503.06926": "|**2025-03-10**|**Effect of Selection Format on LLM Performance**|Yuchen Han, Yucheng Wu, Jeffrey Willard et.al.|[2503.06926](http://arxiv.org/abs/2503.06926)|null||This paper investigates a critical aspect of large language model (LLM) performance: the optimal formatting of classification task options in prompts. Through an extensive experimental study, we compared two selection formats -- bullet points and plain English -- to determine their impact on model performance. Our findings suggest that presenting options via bullet points generally yields better results, although there are some exceptions. Furthermore, our research highlights the need for continued exploration of option formatting to drive further improvements in model performance.|\n", "2503.06769": "|**2025-03-09**|**Modular Photobioreactor Fa\u00e7ade Systems for Sustainable Architecture: Design, Fabrication, and Real-Time Monitoring**|Xiujin Liu et.al.|[2503.06769](http://arxiv.org/abs/2503.06769)|null|21 pages, 22 figures, 3 tables|This paper proposes an innovative solution to the growing issue of greenhouse gas emissions: a closed photobioreactor (PBR) fa\\c{c}ade system to mitigate greenhouse gas (GHG) concentrations. With digital fabrication technology, this study explores the transition from traditional, single function building facades to multifunctional, integrated building systems. It introduces a photobioreactor (PBR) fa\\c{c}ade system to mitigate greenhouse gas (GHG) concentrations while addressing the challenge of large-scale prefabricated components transportation. This research introduces a novel approach by designing the fa\\c{c}ade system as modular, user-friendly and transportation-friendly bricks, enabling the creation of a user-customized and self-assembled photobioreactor (PBR) system. The single module in the system is proposed to be \"neutralization bricks\", which embedded with algae and equipped with an air circulation system, facilitating the photobioreactor (PBR)'s functionality. A connection system between modules allows for easy assembly by users, while a limited variety of brick styles ensures modularity in manufacturing without sacrificing customization and diversity. The system is also equipped with an advanced microalgae status detection algorithm, which allows users to monitor the condition of the microalgae using monocular camera. This functionality ensures timely alerts and notifications for users to replace the algae, thereby optimizing the operational efficiency and sustainability of the algae cultivation process.|\n", "2503.06663": "|**2025-03-09**|**Energy-Adaptive Checkpoint-Free Intermittent Inference for Low Power Energy Harvesting Systems**|Sahidul Islam, Wei Wei, Jishnu Banarjee et.al.|[2503.06663](http://arxiv.org/abs/2503.06663)|null||Deep neural network (DNN) inference in energy harvesting (EH) devices poses significant challenges due to resource constraints and frequent power interruptions. These power losses not only increase end-to-end latency, but also compromise inference consistency and accuracy, as existing checkpointing and restore mechanisms are prone to errors. Consequently, the quality of service (QoS) for DNN inference on EH devices is severely impacted. In this paper, we propose an energy-adaptive DNN inference mechanism capable of dynamically transitioning the model into a low-power mode by reducing computational complexity when harvested energy is limited. This approach ensures that end-to-end latency requirements are met. Additionally, to address the limitations of error-prone checkpoint-and-restore mechanisms, we introduce a checkpoint-free intermittent inference framework that ensures consistent, progress-preserving DNN inference during power failures in energy-harvesting systems.|\n"}, "Reinforcement Learning in Finance": {"2305.07466": "|**2023-04-29**|**Systematic Review on Reinforcement Learning in the Field of Fintech**|Nadeem Malibari, Iyad Katib, Rashid Mehmood et.al.|[2305.07466](http://arxiv.org/abs/2305.07466)|null|31 pages, 15 figures, 7 tables|Applications of Reinforcement Learning in the Finance Technology (Fintech) have acquired a lot of admiration lately. Undoubtedly Reinforcement Learning, through its vast competence and proficiency, has aided remarkable results in the field of Fintech. The objective of this systematic survey is to perform an exploratory study on a correlation between reinforcement learning and Fintech to highlight the prediction accuracy, complexity, scalability, risks, profitability and performance. Major uses of reinforcement learning in finance or Fintech include portfolio optimization, credit risk reduction, investment capital management, profit maximization, effective recommendation systems, and better price setting strategies. Several studies have addressed the actual contribution of reinforcement learning to the performance of financial institutions. The latest studies included in this survey are publications from 2018 onward. The survey is conducted using PRISMA technique which focuses on the reporting of reviews and is based on a checklist and four-phase flow diagram. The conducted survey indicates that the performance of RL-based strategies in Fintech fields proves to perform considerably better than other state-of-the-art algorithms. The present work discusses the use of reinforcement learning algorithms in diverse decision-making challenges in Fintech and concludes that the organizations dealing with finance can benefit greatly from Robo-advising, smart order channelling, market making, hedging and options pricing, portfolio optimization, and optimal execution.|\n", "2206.14267": "|**2022-06-28**|**Applications of Reinforcement Learning in Finance -- Trading with a Double Deep Q-Network**|Frensi Zejnullahu, Maurice Moser, Joerg Osterrieder et.al.|[2206.14267](http://arxiv.org/abs/2206.14267)|null||This paper presents a Double Deep Q-Network algorithm for trading single assets, namely the E-mini S&P 500 continuous futures contract. We use a proven setup as the foundation for our environment with multiple extensions. The features of our trading agent are constantly being expanded to include additional assets such as commodities, resulting in four models. We also respond to environmental conditions, including costs and crises. Our trading agent is first trained for a specific time period and tested on new data and compared with the long-and-hold strategy as a benchmark (market). We analyze the differences between the various models and the in-sample/out-of-sample performance with respect to the environment. The experimental results show that the trading agent follows an appropriate behavior. It can adjust its policy to different circumstances, such as more extensive use of the neutral position when trading costs are present. Furthermore, the net asset value exceeded that of the benchmark, and the agent outperformed the market in the test set. We provide initial insights into the behavior of an agent in a financial domain using a DDQN algorithm. The results of this study can be used for further development.|\n", "2112.04553": "|**2023-02-28**|**Recent Advances in Reinforcement Learning in Finance**|Ben Hambly, Renyuan Xu, Huining Yang et.al.|[2112.04553](http://arxiv.org/abs/2112.04553)|null|60 pages, 1 figure|The rapid changes in the finance industry due to the increasing amount of data have revolutionized the techniques on data processing and data analysis and brought new theoretical and computational challenges. In contrast to classical stochastic control theory and other analytical approaches for solving financial decision-making problems that heavily reply on model assumptions, new developments from reinforcement learning (RL) are able to make full use of the large amount of financial data with fewer model assumptions and to improve decisions in complex financial environments. This survey paper aims to review the recent developments and use of RL approaches in finance. We give an introduction to Markov decision processes, which is the setting for many of the commonly used RL approaches. Various algorithms are then introduced with a focus on value and policy based methods that do not require any model assumptions. Connections are made with neural networks to extend the framework to encompass deep RL algorithms. Our survey concludes by discussing the application of these RL algorithms in a variety of decision-making problems in finance, including optimal execution, portfolio optimization, option pricing and hedging, market making, smart order routing, and robo-advising.|\n"}, "Time Series Forecasting": {"2503.10198": "|**2025-03-13**|**Deep Learning for Time Series Forecasting: A Survey**|Xiangjie Kong, Zhenghao Chen, Weiyao Liu et.al.|[2503.10198](http://arxiv.org/abs/2503.10198)|null||Time series forecasting (TSF) has long been a crucial task in both industry and daily life. Most classical statistical models may have certain limitations when applied to practical scenarios in fields such as energy, healthcare, traffic, meteorology, and economics, especially when high accuracy is required. With the continuous development of deep learning, numerous new models have emerged in the field of time series forecasting in recent years. However, existing surveys have not provided a unified summary of the wide range of model architectures in this field, nor have they given detailed summaries of works in feature extraction and datasets. To address this gap, in this review, we comprehensively study the previous works and summarize the general paradigms of Deep Time Series Forecasting (DTSF) in terms of model architectures. Besides, we take an innovative approach by focusing on the composition of time series and systematically explain important feature extraction methods. Additionally, we provide an overall compilation of datasets from various domains in existing works. Finally, we systematically emphasize the significant challenges faced and future research directions in this field.|\n", "2503.09791": "|**2025-03-12**|**Minimal Time Series Transformer**|Joni-Kristian K\u00e4m\u00e4r\u00e4inen et.al.|[2503.09791](http://arxiv.org/abs/2503.09791)|null|8 pages, 8 figures|Transformer is the state-of-the-art model for many natural language processing, computer vision, and audio analysis problems. Transformer effectively combines information from the past input and output samples in auto-regressive manner so that each sample becomes aware of all inputs and outputs. In sequence-to-sequence (Seq2Seq) modeling, the transformer processed samples become effective in predicting the next output. Time series forecasting is a Seq2Seq problem. The original architecture is defined for discrete input and output sequence tokens, but to adopt it for time series, the model must be adapted for continuous data. This work introduces minimal adaptations to make the original transformer architecture suitable for continuous value time series data.|\n", "2503.09656": "|**2025-03-12**|**LLM-PS: Empowering Large Language Models for Time Series Forecasting with Temporal Patterns and Semantics**|Jialiang Tang, Shuo Chen, Chen Gong et.al.|[2503.09656](http://arxiv.org/abs/2503.09656)|null||Time Series Forecasting (TSF) is critical in many real-world domains like financial planning and health monitoring. Recent studies have revealed that Large Language Models (LLMs), with their powerful in-contextual modeling capabilities, hold significant potential for TSF. However, existing LLM-based methods usually perform suboptimally because they neglect the inherent characteristics of time series data. Unlike the textual data used in LLM pre-training, the time series data is semantically sparse and comprises distinctive temporal patterns. To address this problem, we propose LLM-PS to empower the LLM for TSF by learning the fundamental \\textit{Patterns} and meaningful \\textit{Semantics} from time series data. Our LLM-PS incorporates a new multi-scale convolutional neural network adept at capturing both short-term fluctuations and long-term trends within the time series. Meanwhile, we introduce a time-to-text module for extracting valuable semantics across continuous time intervals rather than isolated time points. By integrating these patterns and semantics, LLM-PS effectively models temporal dependencies, enabling a deep comprehension of time series and delivering accurate forecasts. Intensive experimental results demonstrate that LLM-PS achieves state-of-the-art performance in both short- and long-term forecasting tasks, as well as in few- and zero-shot settings.|\n", "2503.08473": "|**2025-03-11**|**Data Driven Decision Making with Time Series and Spatio-temporal Data**|Bin Yang, Yuxuan Liang, Chenjuan Guo et.al.|[2503.08473](http://arxiv.org/abs/2503.08473)|null|This paper is accepted by ICDE 2025|Time series data captures properties that change over time. Such data occurs widely, ranging from the scientific and medical domains to the industrial and environmental domains. When the properties in time series exhibit spatial variations, we often call the data spatio-temporal. As part of the continued digitalization of processes throughout society, increasingly large volumes of time series and spatio-temporal data are available. In this tutorial, we focus on data-driven decision making with such data, e.g., enabling greener and more efficient transportation based on traffic time series forecasting. The tutorial adopts the holistic paradigm of \"data-governance-analytics-decision.\" We first introduce the data foundation of time series and spatio-temporal data, which is often heterogeneous. Next, we discuss data governance methods that aim to improve data quality. We then cover data analytics, focusing on five desired characteristics: automation, robustness, generality, explainability, and resource efficiency. We finally cover data-driven decision making strategies and briefly discuss promising research directions. We hope that the tutorial will serve as a primary resource for researchers and practitioners who are interested in value creation from time series and spatio-temporal data.|\n", "2503.08328": "|**2025-03-11**|**MFRS: A Multi-Frequency Reference Series Approach to Scalable and Accurate Time-Series Forecasting**|Liang Yu, Lai Tu, Xiang Bai et.al.|[2503.08328](http://arxiv.org/abs/2503.08328)|null||Multivariate time-series forecasting holds immense value across diverse applications, requiring methods to effectively capture complex temporal and inter-variable dynamics. A key challenge lies in uncovering the intrinsic patterns that govern predictability, beyond conventional designs, focusing on network architectures to explore latent relationships or temporal dependencies. Inspired by signal decomposition, this paper posits that time series predictability is derived from periodic characteristics at different frequencies. Consequently, we propose a novel time series forecasting method based on multi-frequency reference series correlation analysis. Through spectral analysis on long-term training data, we identify dominant spectral components and their harmonics to design base-pattern reference series. Unlike signal decomposition, which represents the original series as a linear combination of basis signals, our method uses a transformer model to compute cross-attention between the original series and reference series, capturing essential features for forecasting. Experiments on major open and synthetic datasets show state-of-the-art performance. Furthermore, by focusing on attention with a small number of reference series rather than pairwise variable attention, our method ensures scalability and broad applicability. The source code is available at: https://github.com/yuliang555/MFRS|\n", "2503.08271": "|**2025-03-11**|**LangTime: A Language-Guided Unified Model for Time Series Forecasting with Proximal Policy Optimization**|Wenzhe Niu, Zongxia Xie, Yanru Sun et.al.|[2503.08271](http://arxiv.org/abs/2503.08271)|null||Recent research has shown an increasing interest in utilizing pre-trained large language models (LLMs) for a variety of time series applications. However, there are three main challenges when using LLMs as foundational models for time series forecasting: (1) Cross-domain generalization. (2) Cross-modality alignment. (3) Error accumulation in autoregressive frameworks. To address these challenges, we proposed LangTime, a language-guided unified model for time series forecasting that incorporates cross-domain pre-training with reinforcement learning-based fine-tuning. Specifically, LangTime constructs Temporal Comprehension Prompts (TCPs), which include dataset-wise and channel-wise instructions, to facilitate domain adaptation and condense time series into a single token, enabling LLMs to understand better and align temporal data. To improve autoregressive forecasting, we introduce TimePPO, a reinforcement learning-based fine-tuning algorithm. TimePPO mitigates error accumulation by leveraging a multidimensional rewards function tailored for time series and a repeat-based value estimation strategy. Extensive experiments demonstrate that LangTime achieves state-of-the-art cross-domain forecasting performance, while TimePPO fine-tuning effectively enhances the stability and accuracy of autoregressive forecasting.|\n", "2503.06928": "|**2025-03-10**|**FinTSBridge: A New Evaluation Suite for Real-world Financial Prediction with Advanced Time Series Models**|Yanlong Wang, Jian Xu, Tiantian Gao et.al.|[2503.06928](http://arxiv.org/abs/2503.06928)|null|ICLR 2025 Workshop Advances in Financial AI|Despite the growing attention to time series forecasting in recent years, many studies have proposed various solutions to address the challenges encountered in time series prediction, aiming to improve forecasting performance. However, effectively applying these time series forecasting models to the field of financial asset pricing remains a challenging issue. There is still a need for a bridge to connect cutting-edge time series forecasting models with financial asset pricing. To bridge this gap, we have undertaken the following efforts: 1) We constructed three datasets from the financial domain; 2) We selected over ten time series forecasting models from recent studies and validated their performance in financial time series; 3) We developed new metrics, msIC and msIR, in addition to MSE and MAE, to showcase the time series correlation captured by the models; 4) We designed financial-specific tasks for these three datasets and assessed the practical performance and application potential of these forecasting models in important financial problems. We hope the developed new evaluation suite, FinTSBridge, can provide valuable insights into the effectiveness and robustness of advanced forecasting models in finanical domains.|\n", "2503.06867": "|**2025-03-10**|**Enhancing Time Series Forecasting via Logic-Inspired Regularization**|Jianqi Zhang, Jingyao Wang, Xingchen Shen et.al.|[2503.06867](http://arxiv.org/abs/2503.06867)|null||Time series forecasting (TSF) plays a crucial role in many applications. Transformer-based methods are one of the mainstream techniques for TSF. Existing methods treat all token dependencies equally. However, we find that the effectiveness of token dependencies varies across different forecasting scenarios, and existing methods ignore these differences, which affects their performance. This raises two issues: (1) What are effective token dependencies? (2) How can we learn effective dependencies? From a logical perspective, we align Transformer-based TSF methods with the logical framework and define effective token dependencies as those that ensure the tokens as atomic formulas (Issue 1). We then align the learning process of Transformer methods with the process of obtaining atomic formulas in logic, which inspires us to design a method for learning these effective dependencies (Issue 2). Specifically, we propose Attention Logic Regularization (Attn-L-Reg), a plug-and-play method that guides the model to use fewer but more effective dependencies by making the attention map sparse, thereby ensuring the tokens as atomic formulas and improving prediction performance. Extensive experiments and theoretical analysis confirm the effectiveness of Attn-L-Reg.|\n", "2503.06216": "|**2025-03-08**|**A Novel Distributed PV Power Forecasting Approach Based on Time-LLM**|Huapeng Lin, Miao Yu et.al.|[2503.06216](http://arxiv.org/abs/2503.06216)|null|23 pages, 8 figures|Distributed photovoltaic (DPV) systems are essential for advancing renewable energy applications and achieving energy independence. Accurate DPV power forecasting can optimize power system planning and scheduling while significantly reducing energy loss, thus enhancing overall system efficiency and reliability. However, solar energy's intermittent nature and DPV systems' spatial distribution create significant forecasting challenges. Traditional methods often rely on costly external data, such as numerical weather prediction (NWP) and satellite images, which are difficult to scale for smaller DPV systems. To tackle this issue, this study has introduced an advanced large language model (LLM)-based time series forecasting framework Time-LLM to improve the DPV power forecasting accuracy and generalization ability. By reprogramming, the framework aligns historical power data with natural language modalities, facilitating efficient modeling of time-series data. Then Qwen2.5-3B model is integrated as the backbone LLM to process input data by leveraging its pattern recognition and inference abilities, achieving a balance between efficiency and performance. Finally, by using a flatten and linear projection layer, the LLM's high-dimensional output is transformed into the final forecasts. Experimental results indicate that Time-LLM outperforms leading recent advanced time series forecasting models, such as Transformer-based methods and MLP-based models, achieving superior accuracy in both short-term and long-term forecasting. Time-LLM also demonstrates exceptional adaptability in few-shot and zero-shot learning scenarios. To the best of the authors' knowledge, this study is the first attempt to explore the application of LLMs to DPV power forecasting, which can offer a scalable solution that eliminates reliance on costly external data sources and improve real-world forecasting accuracy.|\n", "2503.06079": "|**2025-03-08**|**Fixing the Pitfalls of Probabilistic Time-Series Forecasting Evaluation by Kernel Quadrature**|Masaki Adachi, Masahiro Fujisawa, Michael A Osborne et.al.|[2503.06079](http://arxiv.org/abs/2503.06079)|null|11 pages, 6 figures|Despite the significance of probabilistic time-series forecasting models, their evaluation metrics often involve intractable integrations. The most widely used metric, the continuous ranked probability score (CRPS), is a strictly proper scoring function; however, its computation requires approximation. We found that popular CRPS estimators--specifically, the quantile-based estimator implemented in the widely used GluonTS library and the probability-weighted moment approximation--both exhibit inherent estimation biases. These biases lead to crude approximations, resulting in improper rankings of forecasting model performance when CRPS values are close. To address this issue, we introduced a kernel quadrature approach that leverages an unbiased CRPS estimator and employs cubature construction for scalable computation. Empirically, our approach consistently outperforms the two widely used CRPS estimators.|\n", "2503.05108": "|**2025-03-07**|**TS-LIF: A Temporal Segment Spiking Neuron Network for Time Series Forecasting**|Shibo Feng, Wanjin Feng, Xingyu Gao et.al.|[2503.05108](http://arxiv.org/abs/2503.05108)|null||Spiking Neural Networks (SNNs) offer a promising, biologically inspired approach for processing spatiotemporal data, particularly for time series forecasting. However, conventional neuron models like the Leaky Integrate-and-Fire (LIF) struggle to capture long-term dependencies and effectively process multi-scale temporal dynamics. To overcome these limitations, we introduce the Temporal Segment Leaky Integrate-and-Fire (TS-LIF) model, featuring a novel dual-compartment architecture. The dendritic and somatic compartments specialize in capturing distinct frequency components, providing functional heterogeneity that enhances the neuron's ability to process both low- and high-frequency information. Furthermore, the newly introduced direct somatic current injection reduces information loss during intra-neuronal transmission, while dendritic spike generation improves multi-scale information extraction. We provide a theoretical stability analysis of the TS-LIF model and explain how each compartment contributes to distinct frequency response characteristics. Experimental results show that TS-LIF outperforms traditional SNNs in time series forecasting, demonstrating better accuracy and robustness, even with missing data. TS-LIF advances the application of SNNs in time-series forecasting, providing a biologically inspired approach that captures complex temporal dynamics and offers potential for practical implementation in diverse forecasting scenarios. The source code is available at https://github.com/kkking-kk/TS-LIF.|\n", "2503.04956": "|**2025-03-06**|**Boltzmann convolutions and Welford mean-variance layers with an application to time series forecasting and classification**|Daniel Andrew Coulson, Martin T. Wells et.al.|[2503.04956](http://arxiv.org/abs/2503.04956)|null|40 pages, 7 figures, 11 tables|In this paper we propose a novel problem called the ForeClassing problem where the loss of a classification decision is only observed at a future time point after the classification decision has to be made. To solve this problem, we propose an approximately Bayesian deep neural network architecture called ForeClassNet for time series forecasting and classification. This network architecture forces the network to consider possible future realizations of the time series, by forecasting future time points and their likelihood of occurring, before making its final classification decision. To facilitate this, we introduce two novel neural network layers, Welford mean-variance layers and Boltzmann convolutional layers. Welford mean-variance layers allow networks to iteratively update their estimates of the mean and variance for the forecasted time points for each inputted time series to the network through successive forward passes, which the model can then consider in combination with a learned representation of the observed realizations of the time series for its classification decision. Boltzmann convolutional layers are linear combinations of approximately Bayesian convolutional layers with different filter lengths, allowing the model to learn multitemporal resolution representations of the input time series, and which resolutions to focus on within a given Boltzmann convolutional layer through a Boltzmann distribution. Through several simulation scenarios and two real world applications we demonstrate ForeClassNet achieves superior performance compared with current state of the art methods including a near 30% improvement in test set accuracy in our financial example compared to the second best performing model.|\n", "2503.07649": "|**2025-03-06**|**TS-RAG: Retrieval-Augmented Generation based Time Series Foundation Models are Stronger Zero-Shot Forecaster**|Kanghui Ning, Zijie Pan, Yu Liu et.al.|[2503.07649](http://arxiv.org/abs/2503.07649)|null||Recently, Large Language Models (LLMs) and Foundation Models (FMs) have become prevalent for time series forecasting tasks. However, fine-tuning large language models (LLMs) for forecasting enables the adaptation to specific domains but may not generalize well across diverse, unseen datasets. Meanwhile, existing time series foundation models (TSFMs) lack inherent mechanisms for domain adaptation and suffer from limited interpretability, making them suboptimal for zero-shot forecasting. To this end, we present TS-RAG, a retrieval-augmented generation based time series forecasting framework that enhances the generalization capability and interpretability of TSFMs. Specifically, TS-RAG leverages pre-trained time series encoders to retrieve semantically relevant time series segments from a dedicated knowledge database, incorporating contextual patterns for the given time series query. Next, we develop a learnable Mixture-of-Experts (MoE)-based augmentation module, which dynamically fuses retrieved time series patterns with the TSFM's representation of the input query, improving forecasting accuracy without requiring task-specific fine-tuning. Thorough empirical studies on seven public benchmark datasets demonstrate that TS-RAG achieves state-of-the-art zero-shot forecasting performance, outperforming TSFMs by up to 6.51% across diverse domains and showcasing desired interpretability.|\n", "2503.04218": "|**2025-03-06**|**Hedging with Sparse Reward Reinforcement Learning**|Yiheng Ding, Gangnan Yuan, Dewei Zuo et.al.|[2503.04218](http://arxiv.org/abs/2503.04218)|null||Derivatives, as a critical class of financial instruments, isolate and trade the price attributes of risk assets such as stocks, commodities, and indices, aiding risk management and enhancing market efficiency. However, traditional hedging models, constrained by assumptions such as continuous trading and zero transaction costs, fail to satisfy risk control requirements in complex and uncertain real-world markets.   With advances in computing technology and deep learning, data-driven trading strategies are becoming increasingly prevalent. This thesis proposes a derivatives hedging framework integrating deep learning and reinforcement learning. The framework comprises a probabilistic forecasting model and a hedging agent, enabling market probability prediction, derivative pricing, and hedging.   Specifically, we design a spatiotemporal attention-based probabilistic financial time series forecasting Transformer to address the scarcity of derivatives hedging data. A low-rank attention mechanism compresses high-dimensional assets into a low-dimensional latent space, capturing nonlinear asset relationships. The Transformer models sequential dependencies within this latent space, improving market probability forecasts and constructing an online training environment for downstream hedging tasks.   Additionally, we incorporate generalized geometric Brownian motion to develop a risk-neutral pricing approach for derivatives. We model derivatives hedging as a reinforcement learning problem with sparse rewards and propose a behavior cloning-based recurrent proximal policy optimization (BC-RPPO) algorithm. This pretraining-finetuning framework significantly enhances the hedging agent's performance. Numerical experiments in the U.S. and Chinese financial markets demonstrate our method's superiority over traditional approaches.|\n", "2503.04118": "|**2025-03-06**|**TimeFound: A Foundation Model for Time Series Forecasting**|Congxi Xiao, Jingbo Zhou, Yixiong Xiao et.al.|[2503.04118](http://arxiv.org/abs/2503.04118)|null||We present TimeFound, an encoder-decoder transformer-based time series foundation model for out-of-the-box zero-shot forecasting. To handle time series data from various domains, TimeFound employs a multi-resolution patching strategy to capture complex temporal patterns at multiple scales. We pre-train our model with two sizes (200M and 710M parameters) on a large time-series corpus comprising both real-world and synthetic datasets. Over a collection of unseen datasets across diverse domains and forecasting horizons, our empirical evaluations suggest that TimeFound can achieve superior or competitive zero-shot forecasting performance, compared to state-of-the-art time series foundation models.|\n", "2503.03729": "|**2025-03-05**|**Graph-Augmented LSTM for Forecasting Sparse Anomalies in Graph-Structured Time Series**|Sneh Pillai et.al.|[2503.03729](http://arxiv.org/abs/2503.03729)|null|12 pages|Detecting anomalies in time series data is a critical task across many domains. The challenge intensifies when anomalies are sparse and the data are multivariate with relational dependencies across sensors or nodes. Traditional univariate anomaly detectors struggle to capture such cross-node dependencies, particularly in sparse anomaly settings. To address this, we propose a graph-augmented time series forecasting approach that explicitly integrates the graph of relationships among time series into an LSTM forecasting model. This enables the model to detect rare anomalies that might otherwise go unnoticed in purely univariate approaches. We evaluate the approach on two benchmark datasets - the Yahoo Webscope S5 anomaly dataset and the METR-LA traffic sensor network - and compare the performance of the Graph-Augmented LSTM against LSTM-only, ARIMA, and Prophet baselines. Results demonstrate that the graph-augmented model achieves significantly higher precision and recall, improving F1-score by up to 10% over the best baseline|\n", "2503.03594": "|**2025-03-09**|**Small but Mighty: Enhancing Time Series Forecasting with Lightweight LLMs**|Haoran Fan, Bin Li, Yixuan Weng et.al.|[2503.03594](http://arxiv.org/abs/2503.03594)|null|20 pages, 10 figures|While LLMs have demonstrated remarkable potential in time series forecasting, their practical deployment remains constrained by excessive computational demands and memory footprints. Existing LLM-based approaches typically suffer from three critical limitations: Inefficient parameter utilization in handling numerical time series patterns; Modality misalignment between continuous temporal signals and discrete text embeddings; and Inflexibility for real-time expert knowledge integration. We present SMETimes, the first systematic investigation of sub-3B parameter SLMs for efficient and accurate time series forecasting. Our approach centers on three key innovations: A statistically-enhanced prompting mechanism that bridges numerical time series with textual semantics through descriptive statistical features; A adaptive fusion embedding architecture that aligns temporal patterns with language model token spaces through learnable parameters; And a dynamic mixture-of-experts framework enabled by SLMs' computational efficiency, adaptively combining base predictions with domain-specific models. Extensive evaluations across seven benchmark datasets demonstrate that our 3B-parameter SLM achieves state-of-the-art performance on five primary datasets while maintaining 3.8x faster training and 5.2x lower memory consumption compared to 7B-parameter LLM baselines. Notably, the proposed model exhibits better learning capabilities, achieving 12.3% lower MSE than conventional LLM. Ablation studies validate that our statistical prompting and cross-modal fusion modules respectively contribute 15.7% and 18.2% error reduction in long-horizon forecasting tasks. By redefining the efficiency-accuracy trade-off landscape, this work establishes SLMs as viable alternatives to resource-intensive LLMs for practical time series forecasting. Code and models are available at https://github.com/xiyan1234567/SMETimes.|\n", "2503.02836": "|**2025-03-04**|**SeqFusion: Sequential Fusion of Pre-Trained Models for Zero-Shot Time-Series Forecasting**|Ting-Ji Huang, Xu-Yang Chen, Han-Jia Ye et.al.|[2503.02836](http://arxiv.org/abs/2503.02836)|**[link](https://github.com/Tingji2419/SeqFusion)**||Unlike traditional time-series forecasting methods that require extensive in-task data for training, zero-shot forecasting can directly predict future values given a target time series without additional training data. Current zero-shot approaches primarily rely on pre-trained generalized models, with their performance often depending on the variety and relevance of the pre-training data, which can raise privacy concerns. Instead of collecting diverse pre-training data, we introduce SeqFusion in this work, a novel framework that collects and fuses diverse pre-trained models (PTMs) sequentially for zero-shot forecasting. Based on the specific temporal characteristics of the target time series, SeqFusion selects the most suitable PTMs from a batch of pre-collected PTMs, performs sequential predictions, and fuses all the predictions while using minimal data to protect privacy. Each of these PTMs specializes in different temporal patterns and forecasting tasks, allowing SeqFusion to select by measuring distances in a shared representation space of the target time series with each PTM. Experiments demonstrate that SeqFusion achieves competitive accuracy in zero-shot forecasting compared to state-of-the-art methods.|\n", "2503.02609": "|**2025-03-04**|**Lightweight Channel-wise Dynamic Fusion Model: Non-stationary Time Series Forecasting via Entropy Analysis**|Tianyu Jia, Zongxia Xie, Yanru Sun et.al.|[2503.02609](http://arxiv.org/abs/2503.02609)|null||Non-stationarity is an intrinsic property of real-world time series and plays a crucial role in time series forecasting. Previous studies primarily adopt instance normalization to attenuate the non-stationarity of original series for better predictability. However, instance normalization that directly removes the inherent non-stationarity can lead to three issues: (1) disrupting global temporal dependencies, (2) ignoring channel-specific differences, and (3) producing over-smoothed predictions. To address these issues, we theoretically demonstrate that variance can be a valid and interpretable proxy for quantifying non-stationarity of time series. Based on the analysis, we propose a novel lightweight \\textit{C}hannel-wise \\textit{D}ynamic \\textit{F}usion \\textit{M}odel (\\textit{CDFM}), which selectively and dynamically recovers intrinsic non-stationarity of the original series, while keeping the predictability of normalized series. First, we design a Dual-Predictor Module, which involves two branches: a Time Stationary Predictor for capturing stable patterns and a Time Non-stationary Predictor for modeling global dynamics patterns. Second, we propose a Fusion Weight Learner to dynamically characterize the intrinsic non-stationary information across different samples based on variance. Finally, we introduce a Channel Selector to selectively recover non-stationary information from specific channels by evaluating their non-stationarity, similarity, and distribution consistency, enabling the model to capture relevant dynamic features and avoid overfitting. Comprehensive experiments on seven time series datasets demonstrate the superiority and generalization capabilities of CDFM.|\n", "2503.01157": "|**2025-03-03**|**Unify and Anchor: A Context-Aware Transformer for Cross-Domain Time Series Forecasting**|Xiaobin Hong, Jiawen Zhang, Wenzhong Li et.al.|[2503.01157](http://arxiv.org/abs/2503.01157)|null|20 pages, 12 figures, 8 tables, conference under review|The rise of foundation models has revolutionized natural language processing and computer vision, yet their best practices to time series forecasting remains underexplored. Existing time series foundation models often adopt methodologies from these fields without addressing the unique characteristics of time series data. In this paper, we identify two key challenges in cross-domain time series forecasting: the complexity of temporal patterns and semantic misalignment. To tackle these issues, we propose the ``Unify and Anchor\" transfer paradigm, which disentangles frequency components for a unified perspective and incorporates external context as domain anchors for guided adaptation. Based on this framework, we introduce ContexTST, a Transformer-based model that employs a time series coordinator for structured representation and the Transformer blocks with a context-informed mixture-of-experts mechanism for effective cross-domain generalization. Extensive experiments demonstrate that ContexTST advances state-of-the-art forecasting performance while achieving strong zero-shot transferability across diverse domains.|\n"}, "Numerical Analysis": {"2503.10612": "|**2025-03-13**|**Approximation technique for preserving the minimum principle on the entropy for the compressible Euler Equations**|Bennett Clayton, Eric J. Tovar et.al.|[2503.10612](http://arxiv.org/abs/2503.10612)|null||This paper is concerned with constructing an invariant-domain preserving approximation technique for the compressible Euler equations that preserves the minimum principle on the physical entropy. We show that any numerical method that can be written as a convex combination of good auxiliary states will satisfy the minimum principle on the physical entropy provided the equation of state satisfies some mild assumptions. Furthermore, we derive a wave speed estimate in an extended Riemann problem necessary for constructing the auxiliary states with desired properties. Finally, we numerically illustrate the proposed methodology.|\n", "2503.10607": "|**2025-03-13**|**Utilizing discrete variable representations for decoherence-accurate numerical simulation of superconducting circuits**|Brittany Richman, C. J. Lobb, Jacob M. Taylor et.al.|[2503.10607](http://arxiv.org/abs/2503.10607)|null|26 pages, 14 figures|Given the prevalence of superconducting platforms for uses in quantum computing and quantum sensing, the simulation of quantum superconducting circuits has become increasingly important for identifying system characteristics and modeling their relevant dynamics. Various numerical tools and software packages have been developed with this purpose in mind, typically utilizing the harmonic oscillator basis or the charge basis to represent a Hamiltonian. In this work, we instead consider the use of discrete variable representations (DVRs) to model superconducting circuits. In particular, we use `sinc DVRs' of both charge number and phase to approximate the eigenenergies of several prototypical examples, exploring their use and effectiveness in the numerical analysis of superconducting circuits. We find that not only are these DVRs capable of achieving decoherence-accurate simulation, i.e., accuracy at the resolution of experiments subject to decay, decoherence, and dephasing, they also demonstrate improvements in efficiency with smaller basis sizes and better convergence over standard approaches, showing that DVRs are an advantageous alternative for representing superconducting circuits.|\n", "2503.10562": "|**2025-03-13**|**Discontinuous Galerkin discretization of conservative dynamical low-rank approximation schemes for the Vlasov-Poisson equation**|Andr\u00e9 Uschmajew, Andreas Zeiser et.al.|[2503.10562](http://arxiv.org/abs/2503.10562)|null||A numerical dynamical low-rank approximation (DLRA) scheme for the solution of the Vlasov-Poisson equation is presented. Based on the formulation of the DLRA equations as Friedrichs' systems in a continuous setting, it combines recently proposed conservative DLRA methods with a discontinuous Galerkin discretization. The resulting scheme is shown to ensure mass and momentum conservation at the discrete level. In addition, a new formulation of the conservative integrator is proposed which facilitates a projector splitting integrator. Numerical experiments validate our approach in one- and two-dimensional simulations of Landau damping. As a demonstration of feasibility, it is also shown that the rank-adaptive unconventional integrator can be combined with mesh adaptivity.|\n", "2503.10552": "|**2025-03-13**|**Mathematical and numerical methods for understanding immune cell motion during wound healing**|Giulia Lupi, Seol Ah Park, Martin Ambroz et.al.|[2503.10552](http://arxiv.org/abs/2503.10552)|null||In this paper, we propose a new workflow to analyze macrophage motion during wound healing. These immune cells are attracted to the wound after an injury and they move showing both directional and random motion. Thus, first, we smooth the trajectories and we separate the random from the directional parts of the motion. The smoothing model is based on curve evolution where the curve motion is influenced by the smoothing term and the attracting term. Once we obtain the random sub-trajectories, we analyze them using the mean squared displacement to characterize the type of diffusion. Finally, we compute the velocities on the smoothed trajectories and use them as sparse samples to reconstruct the wound attractant field. To do that, we consider a minimization problem for the vector components and lengths, which leads to solving the Laplace equation with Dirichlet conditions for the sparse samples and zero Neumann boundary conditions on the domain boundary.|\n", "2503.10492": "|**2025-03-13**|**Meta-learning characteristics and dynamics of quantum systems**|Lucas Schorling, Pranav Vaidhyanathan, Jonas Schuff et.al.|[2503.10492](http://arxiv.org/abs/2503.10492)|null|6+1 pages, 4 figures. L. Schorling and P. Vaidhyanathan contributed   equally to this work|While machine learning holds great promise for quantum technologies, most current methods focus on predicting or controlling a specific quantum system. Meta-learning approaches, however, can adapt to new systems for which little data is available, by leveraging knowledge obtained from previous data associated with similar systems. In this paper, we meta-learn dynamics and characteristics of closed and open two-level systems, as well as the Heisenberg model. Based on experimental data of a Loss-DiVincenzo spin-qubit hosted in a Ge/Si core/shell nanowire for different gate voltage configurations, we predict qubit characteristics i.e. $g$-factor and Rabi frequency using meta-learning. The algorithm we introduce improves upon previous state-of-the-art meta-learning methods for physics-based systems by introducing novel techniques such as adaptive learning rates and a global optimizer for improved robustness and increased computational efficiency. We benchmark our method against other meta-learning methods, a vanilla transformer, and a multilayer perceptron, and demonstrate improved performance.|\n", "2503.10487": "|**2025-03-13**|**Sediment Concentration Estimation via Multiscale Inverse Problem and Stochastic Homogenization**|Jiwei Li, Lingyun Qiu, Zhongjing Wang et.al.|[2503.10487](http://arxiv.org/abs/2503.10487)|null|20 pages, 7 figures, submitted to Archive for Rational Mechanics and   Analysis|In this work, we contribute to the broader understanding of inverse problems by introducing a versatile multiscale modeling framework tailored to the challenges of sediment concentration estimation. Specifically, we propose a novel approach for sediment concentration measurement in water flow, modeled as a multiscale inverse medium problem. To address the multiscale nature of the sediment distribution, we treat it as an inhomogeneous random field and use the homogenization theory in deriving the effective medium model. The inverse problem is formulated as the reconstruction of the effective medium model, specifically, the sediment concentration, from partial boundary measurements. Additionally, we develop numerical algorithms to improve the efficiency and accuracy of solving this inverse problem. Our numerical experiments demonstrate the effectiveness of the proposed model and methods in producing accurate sediment concentration estimates, offering new insights into sediment concentration measurement in complex environments.|\n", "2503.10478": "|**2025-03-13**|**Multiscale simulation of interacting turbulent and rarefied gas flows in the DSMC framework**|Liyan Luo, Songyan Tian, Lei Wu et.al.|[2503.10478](http://arxiv.org/abs/2503.10478)|null||A multiscale stochastic-deterministic coupling method is proposed to investigate the complex interactions between turbulent and rarefied gas flows within a unified framework. This method intermittently integrates the general synthetic iterative scheme with the shear stress transport turbulence model into the direct simulation Monte Carlo (DSMC) approach, enabling the simulation of gas flows across the free-molecular, transition, slip, and turbulent regimes. First, the macroscopic synthetic equations, derived directly from DSMC, are coupled with the turbulence model to establish a constitutive relation that incorporates not only turbulent and laminar transport coefficients but also higher-order terms accounting for rarefaction effects. Second, the macroscopic properties, statistically sampled over specific time intervals in DSMC, along with the turbulent properties provided by the turbulence model, serve as initial conditions for solving the macroscopic synthetic equations. Finally, the simulation particles in DSMC are updated based on the macroscopic properties obtained from the synthetic equations. Numerical simulations demonstrate that the proposed method asymptotically converges to either the turbulence model or DSMC results, adaptively adjusting to different flow regimes. Then, this coupling method is applied to simulate an opposing jet surrounded by hypersonic rarefied gas flows, revealing significant variations in surface properties due to the interplay of turbulent and rarefied effects. This study presents an efficient methodology for simulating the complex interplay between rarefied and turbulent flows, establishing a foundational framework for investigating the coupled effects of turbulence, hypersonic conditions, and chemical reactions in rarefied gas dynamics in the future.|\n", "2503.10453": "|**2025-03-13**|**Proceedings of the WAVES 2024 Conference**|Laurent Gizon et.al.|[2503.10453](http://arxiv.org/abs/2503.10453)|null||Proceedings of 16th International Conference on Mathematical and Numerical Aspects of Wave Propagation held at the Harnack House, Berlin, Germany, 30 June - 5 July, 2024.|\n", "2503.10402": "|**2025-03-13**|**Efficient and stable derivative-free Steffensen algorithm for root finding**|Alexandre Wagemakers, Vipul Periwal et.al.|[2503.10402](http://arxiv.org/abs/2503.10402)|null||We explore a family of numerical methods, based on the Steffensen divided difference iterative algorithm, that do not evaluate the derivative of the objective functions. The family of methods achieves second-order convergence with two function evaluations per iteration with marginal additional computational cost. An important side benefit of the method is the improvement in stability for different initial conditions compared to the vanilla Steffensen method. We present numerical results for scalar functions, fields, and scalar fields. This family of methods outperforms the Steffensen method with respect to standard quantitative metrics in most cases.|\n", "2503.10314": "|**2025-03-13**|**A rotation-based geometrically nonlinear spectral Reissner--Mindlin shell element**|Nima Azizi, Wolfgang Dornisch et.al.|[2503.10314](http://arxiv.org/abs/2503.10314)|null||In this paper, we propose a geometrically nonlinear spectral shell element based on Reissner--Mindlin kinematics using a rotation-based formulation with additive update of the discrete nodal rotation vector. The formulation is provided in matrix notation in detail. The use of a director vector, as opposed to multi-parameter shell models, significantly reduces the computational cost by minimizing the number of degrees of freedom. Additionally, we highlight the advantages of the spectral element method (SEM) in combination with Gauss-Lobatto-Legendre quadrature regarding the computational costs to generate the element stiffness matrix. To assess the performance of the new formulation for large deformation analysis, we compare it to three other numerical methods. One of these methods is a non-isoparametric SEM shell using the geometry definition of isogeometric analysis (IGA), while the other two are IGA shell formulations which differ in the rotation interpolation. All formulations base on Rodrigues' rotation tensor. Through the solution of various challenging numerical examples, it is demonstrated that although IGA benefits from an exact geometric representation, its influence on solution accuracy is less significant than that of shape function characteristics and rotational formulations. Furthermore, we show that the proposed SEM shell, despite its simpler rotational formulation, can produce results comparable to the most accurate and complex version of IGA. Finally, we discuss the optimal SEM strategy, emphasizing the effectiveness of employing coarser meshes with higher-order elements.|\n", "2503.10279": "|**2025-03-13**|**Numerically robust Gaussian state estimation with singular observation noise**|Nicholas Kr\u00e4mer, Filip Tronarp et.al.|[2503.10279](http://arxiv.org/abs/2503.10279)|null||This article proposes numerically robust algorithms for Gaussian state estimation with singular observation noise. Our approach combines a series of basis changes with Bayes' rule, transforming the singular estimation problem into a nonsingular one with reduced state dimension. In addition to ensuring low runtime and numerical stability, our proposal facilitates marginal-likelihood computations and Gauss-Markov representations of the posterior process. We analyse the proposed method's computational savings and numerical robustness and validate our findings in a series of simulations.|\n", "2503.10263": "|**2025-03-13**|**KARL -- A Monte Carlo model for atomic and molecular processes in the tritium atmosphere of the KATRIN experiment**|Christian Sendlinger, Jonas Kellerer, Felix Spanier et.al.|[2503.10263](http://arxiv.org/abs/2503.10263)|null|accepted for publication in Computer Physics Communications, 60   pages, 28 figures|A new parallelized simulation code is presented, which uses a Monte Carlo method to determine particle spectra in the KATRIN source. Reaction chains are generated from the decay of tritium within the source. The code includes all relevant processes: elastic scattering, ionization, excitation (electric, vibrational, rotational), recombination and various clustering processes. The main emphasis of the code is the calculation of particle spectra and particle densities and currents at specific points within the source. It features a new technique to determine these quantities. It also calculates target fields for the interaction of particles with each other as it is needed for recombination processes. The code has been designed for the KATRIN experiment but is easily adapt-able for other tritium based experiments like Project 8. Geometry and background tritium gas flow can be given as user input. The code is parallelized using MPI and writes output using HDF5. Input to the simulation is read from a JSON description.|\n", "2503.10251": "|**2025-03-13**|**Numerical Error Analysis of Large Language Models**|Stanislav Budzinskiy, Wenyi Fang, Longbin Zeng et.al.|[2503.10251](http://arxiv.org/abs/2503.10251)|null||Large language models based on transformer architectures have become integral to state-of-the-art natural language processing applications. However, their training remains computationally expensive and exhibits instabilities, some of which are expected to be caused by finite-precision computations. We provide a theoretical analysis of the impact of round-off errors within the forward pass of a transformer architecture which yields fundamental bounds for these effects. In addition, we conduct a series of numerical experiments which demonstrate the practical relevance of our bounds. Our results yield concrete guidelines for choosing hyperparameters that mitigate round-off errors, leading to more robust and stable inference.|\n", "2503.10244": "|**2025-03-13**|**Thermal Management of Lithium-Ion Batteries: A Comparative Study of Phase Change Materials and Air-Cooling Systems Equipped with Fins**|Masoumeh Karimi Kisomi et.al.|[2503.10244](http://arxiv.org/abs/2503.10244)|null||Lithium-ion batteries are extensively utilized as the primary power source for electric vehicles due to their high energy density, environmental friendliness and lightweight nature. However, their performance and safety are highly dependent on operating temperature. Therefore, a battery thermal management system (BTMS) is essential to ensure the reliable operation and safety of electric vehicles. This study presents a battery thermal management system incorporating phase change material (PCM) and air cooling in a cylindrical lithium-ion cell with fins to enhance heat dissipation. The effects of each system on maximum and minimum temperature, and temperature uniformity along the battery cell are analyzed. Additionally, the impact of fins in both systems is evaluated against a finless cell. A numerical analysis utilizing ANSYS software and the finite volume method (FVM) is performed to evaluate the cooling performance of the systems. The results show that PCM reduces both the maximum and minimum temperatures compared to the air cooling system due to the phase change mechanism. In the finless battery case, the maximum temperature decreases from 316 K to 304 K when using PCM instead of the air cooling system. Also, in the same fin-based battery, the minimum temperature decreases from 307 K to 302 K by using PCM instead of the air cooling system, leading to improved temperature stability. The results indicate that, in general, the fins help reduce the maximum cell temperature when compared to the case without fins in both cases. Using rectangular fins reduces the maximum temperature by approximately 3% compared to a finless battery in the air cooling system. Additionally, the presence of fins reduces the temperature difference along the battery, ensuring a more uniform temperature distribution, such that, in the PCM system with rectangular fins, the temperature difference remains below 1 K.|\n", "2503.10221": "|**2025-03-13**|**New More Efficient A-WENO Schemes**|Shaoshuai Chu, Alexander Kurganov, Ruixiao Xin et.al.|[2503.10221](http://arxiv.org/abs/2503.10221)|null||We develop new more efficient A-WENO schemes for both hyperbolic systems of conservation laws and nonconservative hyperbolic systems. The new schemes are a very simple modification of the existing A-WENO schemes: They are obtained by a more efficient evaluation of the high-order correction terms. We conduct several numerical experiments to demonstrate the performance of the introduced schemes.|\n", "2503.10199": "|**2025-03-13**|**Optimal Estimation and Uncertainty Quantification for Stochastic Inverse Problems via Variational Bayesian Methods**|Ruibiao Song, Liying Zhang et.al.|[2503.10199](http://arxiv.org/abs/2503.10199)|null||The Bayesian inversion method demonstrates significant potential for solving inverse problems, enabling both point estimation and uncertainty quantification. However, Bayesian maximum a posteriori (MAP) estimation may become unstable when handling data from diverse distributions (e.g., solutions of stochastic partial differential equations (SPDEs)). Additionally, Monte Carlo sampling methods are computationally expensive. To address these challenges, we propose a novel two-stage optimization method based on optimal control theory and variational Bayesian methods. This method not only achieves stable solutions for stochastic inverse problems but also efficiently quantifies the uncertainty of the solutions. In the first stage, we introduce a new weighting formulation to ensure the stability of the Bayesian MAP estimation. In the second stage, we derive the necessary condition to efficiently quantify the uncertainty of the solutions, by combining the new weighting formula with variational inference. Furthermore, we establish an error estimation theorem that relates the exact solution to the optimally estimated solution under different amounts of observed data. Finally, the efficiency of the proposed method is demonstrated through numerical examples.|\n", "2503.10196": "|**2025-03-13**|**A filtered Lie splitting method for the Zakharov system with low regularity estimates**|Lun Ji, Hang Li, Chunmei Su et.al.|[2503.10196](http://arxiv.org/abs/2503.10196)|null||In this paper, we present an error estimate for the filtered Lie splitting scheme applied to the Zakharov system, characterized by solutions exhibiting very low regularity across all dimensions. Our findings are derived from the application of multilinear estimates established within the framework of discrete Bourgain spaces. Specifically, we demonstrate that when the solution $(E,z,z_t) \\in H^{s+r+1/2}\\times H^{s+r}\\times H^{s+r-1}$, the error in $H^{r+1/2}\\times H^{r}\\times H^{r-1}$ is $\\mathcal{O}(\\tau^{s/2})$ for $s\\in(0,2]$, where $r=\\max(0,\\frac d2-1)$. To the best of our knowledge, this represents the first explicit error estimate for the splitting method based on the original Zakharov system, as well as the first instance where low regularity error estimates for coupled equations have been considered within the Bourgain framework. Furthermore, numerical experiments confirm the validity of our theoretical results.|\n", "2503.10194": "|**2025-03-13**|**Surrogate modeling of resonant behavior in scattering problems through adaptive rational approximation and sketching**|Davide Pradovera, Ralf Hiptmair, Ilaria Perugia et.al.|[2503.10194](http://arxiv.org/abs/2503.10194)|null||This paper describes novel algorithms for the identification of (almost-)resonant behavior in scattering problems. Our methods, relying on rational approximation, aim at building surrogate models of what we call \"field amplification\", defined as the norm of the solution operator of the scattering problem, which we express through boundary-integral equations. To provide our techniques with theoretical foundations, we first derive results linking the field amplification to the spectral properties of the operator that defines the scattering problem. Such results are then used to justify the use of rational approximation in the surrogate-modeling task. Some of our proposed methods apply rational approximation in a \"standard\" way, building a rational approximant for either the solution operator directly or, in the interest of computational efficiency, for a randomly \"sketched\" version of it. Our other \"hybrid\" approaches are more innovative, combining rational-approximation-assisted root-finding with approximation using radial basis functions. Three key features of our methods are that (i) they are agnostic of the strategy used to discretize the scattering problem, (ii) they do not require any computations involving non-real wavenumbers, and (iii) they can adjust to different settings through the use of adaptive sampling strategies. We carry out some numerical experiments involving 2D scatterers to compare our approaches. In our tests, two of our approaches (one standard, one hybrid) emerge as the best performers, with one or the other being preferable, depending on whether emphasis is placed on accuracy or efficiency.|\n", "2503.10179": "|**2025-03-13**|**Highly efficient norm preserving numerical schemes for micromagnetic energy minimization based on SAV method**|Jiajun Zhan, Lei Yang, Jiayun He et.al.|[2503.10179](http://arxiv.org/abs/2503.10179)|null||In this paper, two efficient and magnetization norm preserving numerical schemes based on the scalar auxiliary variable (SAV) method are developed for calculating the ground state in micromagnetic structures. The first SAV scheme is based on the original SAV method for the gradient flow model, while the second scheme features an updated scalar auxiliary variable to better align with the associated energy. To address the challenging constraint of pointwise constant magnetization length, an implicit projection method is designed, and verified by both SAV schemes. Both proposed SAV schemes partially preserve energy dissipation and exhibit exceptional efficiency, requiring two linear systems with constant coefficients to be solved. The computational efficiency is further enhanced by applying the Discrete Cosine Transform during the solving process. Numerical experiments demonstrate that our SAV schemes outperform commonly used numerical methods in terms of both efficiency and stability.|\n", "2503.10172": "|**2025-03-13**|**On convergence of greedy block nonlinear Kaczmarz methods with momentum**|Naiyu Jiang, Wendi Bao, Lili Xing et.al.|[2503.10172](http://arxiv.org/abs/2503.10172)|null||In this paper, for solving nonlinear systems we propose two pseudoinverse-free greedy block methods with momentum by combining the residual-based weighted nonlinear Kaczmarz and heavy ball methods. Without the full column rank assumptions on Jacobi matrices of nonlinear systems, we provide a thorough convergence analysis, and derive upper bounds for the convergence rates of the new methods. Numerical experiments demonstrate that the proposed methods with momentum are much more effective than the existing ones.|\n", "2503.13388": "|**2025-03-17**|**A mathematical model for a universal digital quantum computer with an application to the Grover-Rudolph algorithm**|Antonio Falc\u00f3, Daniela Falc\u00f3--Pomares, Hermann G. Matthies et.al.|[2503.13388](http://arxiv.org/abs/2503.13388)|null||In this work, we develop a novel mathematical framework for universal digital quantum computation using algebraic probability theory. We rigorously define quantum circuits as finite sequences of elementary quantum gates and establish their role in implementing unitary transformations. A key result demonstrates that every unitary matrix in \\(\\mathrm{U}(N)\\) can be expressed as a product of elementary quantum gates, leading to the concept of a universal dictionary for quantum computation. We apply this framework to the construction of quantum circuits that encode probability distributions, focusing on the Grover-Rudolph algorithm. By leveraging controlled quantum gates and rotation matrices, we design a quantum circuit that approximates a given probability density function. Numerical simulations, conducted using Qiskit, confirm the theoretical predictions and validate the effectiveness of our approach. These results provide a rigorous foundation for quantum circuit synthesis within an algebraic probability framework and offer new insights into the encoding of probability distributions in quantum algorithms. Potential applications include quantum machine learning, circuit optimization, and experimental implementations on real quantum hardware.|\n", "2503.13364": "|**2025-03-17**|**Demonstration of a Tunable Non-Hermitian Nonlinear Microwave Dimer**|Juan S. Salcedo-Gallo, Michiel Burgelman, Vincent P. Flynn et.al.|[2503.13364](http://arxiv.org/abs/2503.13364)|null|10 pages, 4 figures, 73 references|Achieving and controlling non-reciprocity in engineered photonic structures is of fundamental interest in science and engineering. Here, we introduce a tunable, non-Hermitian, nonlinear microwave dimer designed to precisely implement phase-non-reciprocal hopping dynamics between two spatially separated cavities at room temperature. Our system incorporates simple components such as three-dimensional microwave cavities, unidirectional amplifiers, digital attenuators, and a digital phase shifter. By dividing the energy transfer into forward and backward paths, our platform enables precise control over the amplitude and phase of the propagating signals in each direction. Through a combination of theoretical and numerical analysis, we model the dynamics of the system under different operating conditions, including a parameter regime where the gain not only compensates for but significantly exceeds the inherent loss. Our model quantitatively reproduces the observed weak-drive transmission spectra, the amplitude and frequency of self-sustained limit cycles, and the synchronization effect between the limit cycle and an external microwave tone. Our results may have implications in areas ranging from sensing and synthetic photonic materials to neuromorphic computing and quantum networks, while providing new insight into the interplay between non-Hermitian and nonlinear dynamics.|\n", "2503.13354": "|**2025-03-17**|**Parameter-free structure-texture image decomposition by unrolling**|Laura Girometti, Jean-Fran\u00e7ois Aujol, Antoine Guennec et.al.|[2503.13354](http://arxiv.org/abs/2503.13354)|null|To be published in Conference Proceedings: Scale Space and   Variational Method in Computer Vision, 2025|In this work, we propose a parameter-free and efficient method to tackle the structure-texture image decomposition problem. In particular, we present a neural network LPR-NET based on the unrolling of the Low Patch Rank model. On the one hand, this allows us to automatically learn parameters from data, and on the other hand to be computationally faster while obtaining qualitatively similar results compared to traditional iterative model-based methods. Moreover, despite being trained on synthetic images, numerical experiments show the ability of our network to generalize well when applied to natural images.|\n", "2503.13311": "|**2025-03-17**|**Numerical Hopf-Lax formulae for Hamilton-Jacobi equations on unstructured geometries**|Simone Cacace, Roberto Ferretti, Giulia Tatafiore et.al.|[2503.13311](http://arxiv.org/abs/2503.13311)|null||We consider a scheme of Semi-Lagrangian (SL) type for the numerical solution of Hamilton-Jacobi (HJ) equation on unstructured triangular grids. As it is well known, SL schemes are not well suited for unstructured grids, due to the cost of the point location phase; this drawback is augmented by the need for repeated minimization. In this work, we propose a scheme that works only on the basis of node values and connectivity of the grid. In a first version, we obtain a monotone scheme; then, applying a quadratic refinement to the numerical solution, we improve accuracy at the price of some extra computational cost. The scheme can be applied to both time-dependent and stationary HJ equations; in the latter case, we also study the construction of a fast policy iteration solver. We perform a theoretical analysis of the two versions, and validate them with an extensive set of examples, both in the time-dependent and in the stationary case.|\n", "2503.13298": "|**2025-03-17**|**From Few-Shot Optimal Control to Few-Shot Learning**|Roman Chertovskih, Nikolay Pogodaev, Maxim Staritsyn et.al.|[2503.13298](http://arxiv.org/abs/2503.13298)|null|6 pages|We present an approach to solving unconstrained nonlinear optimal control problems for a broad class of dynamical systems. This approach involves lifting the nonlinear problem to a linear ``super-problem'' on a dual Banach space, followed by a non-standard ``exact'' variational analysis, -- culminating in a descent method that achieves rapid convergence with minimal iterations. We investigate the applicability of this framework to mean-field control and discuss its perspectives for the analysis of information propagation in self-interacting neural networks.|\n", "2503.13284": "|**2025-03-17**|**Bayesian identification of material parameters in viscoelastic structures as an inverse problem in a semigroup setting**|Rebecca Rothermel, Thomas Schuster et.al.|[2503.13284](http://arxiv.org/abs/2503.13284)|null|32 pages, 7 figures, 17 tables|The article considers the nonlinear inverse problem of identifying the material parameters in viscoelastic structures based on a generalized Maxwell model. The aim is to reconstruct the model parameters from stress data acquired from a relaxation experiment, where the number of Maxwell elements, and thus the number of material parameters themselves, are assumed to be unknown. This implies that the forward operator acts on a Cartesian product of a semigroup (of integers) and a Hilbert space and demands for an extension of existing regularization theory. We develop a stable reconstruction procedure by applying Bayesian inversion to this setting. We use an appropriate binomial prior which takes the integer setting for the number of Maxwell elements into account and at the same time computes the underlying material parameters. We extend the regularization theory for inverse problems to this special setup and prove existence, stability and convergence of the computed solution. The theoretical results are evaluated by extensive numerical tests.|\n", "2503.13248": "|**2025-03-17**|**Neural network-based Godunov corrections for approximate Riemann solvers using bi-fidelity learning**|Akshay Thakur, Matthew J. Zahr et.al.|[2503.13248](http://arxiv.org/abs/2503.13248)|null|22 pages, 16 figures|The Riemann problem is fundamental in the computational modeling of hyperbolic partial differential equations, enabling the development of stable and accurate upwind schemes. While exact solvers provide robust upwinding fluxes, their high computational cost necessitates approximate solvers. Although approximate solvers achieve accuracy in many scenarios, they produce inaccurate solutions in certain cases. To overcome this limitation, we propose constructing neural network-based surrogate models, trained using supervised learning, designed to map interior and exterior conservative state variables to the corresponding exact flux. Specifically, we propose two distinct approaches: one utilizing a vanilla neural network and the other employing a bi-fidelity neural network. The performance of the proposed approaches is demonstrated through applications to one-dimensional and two-dimensional partial differential equations, showcasing their robustness and accuracy.|\n", "2503.13235": "|**2025-03-17**|**Emergent B2 chemical orderings in the AlTiVNb and AlTiCrMo refractory high-entropy superalloys studied via first-principles theory and atomistic modelling**|Christopher D. Woodgate, Hubert J. Naguszewski, David Redka et.al.|[2503.13235](http://arxiv.org/abs/2503.13235)|null|18 pages, 9 figures|We study the thermodynamics and phase stability of the AlTiVNb and AlTiCrMo refractory high-entropy superalloys using a combination of \\textit{ab initio} electronic structure theory -- namely a concentration wave analysis -- and atomistic Monte Carlo simulations. Our multiscale approach is suitable both for examining atomic short-range order in the solid solution, as well as for studying the emergence of long-range crystallographic order with decreasing temperature. In both alloys considered in this work, in alignment with experimental observations, we predict a B2 (CsCl) chemical ordering emerging at high temperatures, which is driven primarily by Al and Ti, with other elements expressing weaker site preferences. The predicted B2 ordering temperature for AlTiVNb is higher than that for AlTiCrMo. These chemical orderings are discussed in terms of the alloys' electronic structure, with hybridisation between the $sp$ states of Al and the $d$ states of the transition metals understood to play an important role. Within our modelling, the chemically ordered B2 phases for both alloys have an increased predicted residual resistivity compared to the A2 (disordered bcc) phases. These increased resistivity values are understood to originate in a reduction in the electronic density of states at the Fermi level, in conjunction with qualitative changes to the alloys' smeared-out Fermi surfaces. These results highlight the close connections between composition, structure, and physical properties in this technologically relevant class of materials.|\n", "2503.13193": "|**2025-03-17**|**The deep multi-FBSDE method: a robust deep learning method for coupled FBSDEs**|Kristoffer Andersson, Adam Andersson, Cornelis W. Oosterlee et.al.|[2503.13193](http://arxiv.org/abs/2503.13193)|null|18 pages, 11 figures|We introduce the deep multi-FBSDE method for robust approximation of coupled forward-backward stochastic differential equations (FBSDEs), focusing on cases where the deep BSDE method of Han, Jentzen, and E (2018) fails to converge. To overcome the convergence issues, we consider a family of FBSDEs that are equivalent to the original problem in the sense that they satisfy the same associated partial differential equation (PDE). Our algorithm proceeds in two phases: first, we approximate the initial condition for the FBSDE family, and second, we approximate the original FBSDE using the initial condition approximated in the first phase. Numerical experiments show that our method converges even when the standard deep BSDE method does not.|\n", "2503.13170": "|**2025-03-17**|**A~posteriori error analysis for optimization with PDE constraints**|Fernando Gaspoz, Christian Kreuzer, Andreas Veeser et.al.|[2503.13170](http://arxiv.org/abs/2503.13170)|null||We consider finite element solutions to optimization problems, where the state depends on the possibly constrained control through a linear partial differential equation. Basing upon a reduced and rescaled optimality system, we derive a posteriori bounds capturing the approximation of the state, the adjoint state, the control and the observation. The upper and lower bounds show a gap, which grows with decreasing cost or Tikhonov regularization parameter. This growth is mitigated compared to previous results and can be countered by refinement if control and observation involve compact operators. Numerical results illustrate these properties for model problems with distributed and boundary control.|\n", "2503.13126": "|**2025-03-17**|**Error analysis of the Strang splitting for the 3D semilinear wave equation with finite-energy data**|Maximilian Ruff et.al.|[2503.13126](http://arxiv.org/abs/2503.13126)|null|39 pages|We study a variant of the Strang splitting for the time integration of the semilinear wave equation under the finite-energy condition on the torus $\\mathbb{T}^3$. In the case of a cubic nonlinearity, we show almost second-order convergence in $L^2$ and almost first-order convergence in $H^1$. If the nonlinearity has a quartic form instead, we show an analogous convergence result with an order reduced by 1/2. To our knowledge these are the best convergence results available for the 3D cubic and quartic wave equations under the finite-energy condition. Our approach relies on continuous- and discrete-time Strichartz estimates. We also make use of the integration and summation by parts formulas to exploit cancellations in the error terms. Moreover, error bounds for a full discretization using the Fourier pseudo-spectral method in space are given. Finally, we discuss a numerical example indicating the sharpness of our theoretical results.|\n", "2503.13094": "|**2025-03-17**|**Preserving invariant domains and strong approximation of stochastic differential equations**|Utku Erdogan, Gabriel Lord et.al.|[2503.13094](http://arxiv.org/abs/2503.13094)|null||In this paper, we develop numerical methods for solving Stochastic Differential Equations (SDEs) with solutions that evolve within a hypercube $D$ in $\\mathbb{R}^d$. Our approach is based on a convex combination of two numerical flows, both of which are constructed from positivity preserving methods. The strong convergence of the Euler version of the method is proven to be of order $\\tfrac{1}{2}$, and numerical examples are provided to demonstrate that, in some cases, first-order convergence is observed in practice. We compare the Euler and Milstein versions of these new methods to existing domain preservation methods in the literature and observe our methods are robust, more widely applicable and that the error constant is in most cases superior.|\n", "2503.13093": "|**2025-03-17**|**Localized Dynamic Mode Decomposition with Temporally Adaptive Partitioning**|Qiuqi Li, Chang Liu, Yifei Yang et.al.|[2503.13093](http://arxiv.org/abs/2503.13093)|null|24 pages, 15 figures, 3 tables|Dynamic Mode Decomposition (DMD) is a widely used data-driven algorithm for predicting the future states of dynamical systems. However, its standard formulation often struggles with poor long-term predictive accuracy. To address this limitation, we propose a localized DMD framework that improves prediction performance by integrating DMD's strong short-term forecasting capabilities with time-domain decomposition techniques. Our approach segments the time domain of the dynamical system, independently constructing snapshot matrices and performing localized predictions within each segment. We first introduce a localized DMD method with predefined partitioning, which is simple to implement, and then extend it to an adaptive partitioning strategy that enhances prediction accuracy, robustness, and generalizability. Furthermore, we conduct an error analysis that provides the upper bound of the local and global truncation error for our method. To demonstrate the effectiveness of our approach, we apply it to four benchmark problems: Burgers' equation, the Allen-Cahn equation, the nonlinear Schrodinger equation, and Maxwell's equations. Numerical results show that our method significantly improves both predictive accuracy and computational efficiency.|\n", "2503.13032": "|**2025-03-17**|**Miniaturization-Oriented Design of Spline-Parameterized UWB Antenna for In-Door Positioning Applications**|Adrian Bekasiewicz, Tom Dhaene, Ivo Couckuyt et.al.|[2503.13032](http://arxiv.org/abs/2503.13032)|null||Design of ultra-wideband antennas for in-door localization applications is a challenging task that involves development of geometry that ensures appropriate balance between the size and performance. In this work, a topologically-flexible monopole has been generated using a stratified framework which embeds a gradient-based trust-region (TR) optimization algorithm in a meta-loop that gradually increases structure dimensionality. The optimization has been performed using a composite objective function that maintains acceptable size/performance trade-off. The final design features a reflection below -10 dB within the UWB spectrum and a small footprint of only 182 mm2. The considered method has been benchmarked against a standard TR-based routine executed directly on a multi-dimensional representation of the antenna model.|\n", "2503.13029": "|**2025-03-17**|**Specification-Oriented Automatic Design of Topologically Agnostic Antenna Structure**|Adrian Bekasiewicz, Mariusz Dzwonkowski, Tom Dhaene et.al.|[2503.13029](http://arxiv.org/abs/2503.13029)|null||Design of antennas for modern applications is a challenging task that combines cognition-driven development of topology intertwined with tuning of its parameters using rigorous numerical optimization. However, the process can be streamlined by neglecting the engineering insight in favor of automatic de-termination of structure geometry. In this work, a specification-oriented design of topologically agnostic antenna is considered. The radiator is developed using a bi-stage algorithm that involves min-max classification of randomly-generated topologies followed by local tuning of the promising designs using a trust-region optimization applied to a feature-based representation of the structure frequency response. The automatically generated antenna is characterized by -10 dB bandwidth of over 600 MHz w.r.t. the center frequency of 6.5 GHz and a dual-lobe radiation pattern. The obtained performance figures make the radiator of use for in-door positioning applications. The design method has been favorably compared against the frequency-based trust-region optimization.|\n", "2503.13027": "|**2025-03-17**|**Giant energy density nitride dielectrics enabled by a paraelectric-metaparaelectric phase transition**|Zhijie Liu, Xingyue Ma, Lan Chen et.al.|[2503.13027](http://arxiv.org/abs/2503.13027)|null|18 pages, 5 figures|Electrostatic dielectric capacitors are foundational to advance the electronics and electric power devices due to their ultrafast charging/discharging capability and high-power density. However, the low energy density limits the potential for next generation devices in terms of miniaturization and integration. We propose a strategy that relies on inducing a field-driven phase transition that we denote paraelectric-metaparaelectric, which yields an ultrahigh energy density in III-nitrides. III-nitride compounds (Al, Sc, B)N with certain cation concentrations possess a nonpolar hexagonal ground phase which could transform into a polar wurtzite phase under a very large electric field, which is denoted as metaparaelectric with nearly null hysteresis P-E loop. This paraelectric-metaparaelectric transition leads to a polarization saturation at large electric field. The corresponding P-E loop displays a giant energy density of 308 J/cm$^3$ with high efficiency nearly 100%. The proposed paraelectric-metaparaelectric phase transition strategy in nitrides opens an avenue to design of next generation high performance dielectrics.|\n", "2503.13019": "|**2025-03-17**|**TR-Based Antenna Design with Forward FD: the Effects of Step Size on the Optimization Performance**|Adrian Bekasiewicz, Slawomir Koziel, Tom Dhaene et.al.|[2503.13019](http://arxiv.org/abs/2503.13019)|null||Numerical methods are important tools for design of modern antennas. Trust-region (TR) methods coupled with data-efficient surrogates based on finite differentiation (FD) represent a popular class of antenna design algorithms. However, TR performance is subject to FD setup, which is normally determined a priori based on rules-of-thumb. In this work, the effect of FD perturbations on the performance of TR-based design is evaluated on a case study basis concerning a total of 80 optimizations of a planar antenna structure. The obtained results demonstrate that, for the considered radiator, the performance of the final designs obtained using different FD setups may vary by as much as 18 dB (and by over 4 dB on average). At the same time, the a priori perturbations in a range between 1.5% and 3% (w.r.t. the initial design) seem to be suitable for maintaining (relatively) consistent and high-quality results.|\n", "2503.13015": "|**2025-03-17**|**High-performance and reliable probabilistic Ising machine based on simulated quantum annealing**|Eleonora Raimondo, Esteban Garz\u00f3n, Yixin Shao et.al.|[2503.13015](http://arxiv.org/abs/2503.13015)|null||Probabilistic computing with pbits is emerging as a computational paradigm for machine learning and for facing combinatorial optimization problems (COPs) with the so-called probabilistic Ising machines (PIMs). From a hardware point of view, the key elements that characterize a PIM are the random number generation, the nonlinearity, the network of coupled pbits, and the energy minimization algorithm. Regarding the latter, in this work we show that PIMs using the simulated quantum annealing (SQA) schedule exhibit better performance as compared to simulated annealing and parallel tempering in solving a number of COPs, such as maximum satisfiability problems, planted Ising problem, and travelling salesman problem. Additionally, we design and simulate the architecture of a fully connected CMOS based PIM able to run the SQA algorithm having a spin-update time of 8 ns with a power consumption of 0.22 mW. Our results also show that SQA increases the reliability and the scalability of PIMs by compensating for device variability at an algorithmic level enabling the development of their implementation combining CMOS with different technologies such as spintronics. This work shows that the characteristics of the SQA are hardware agnostic and can be applied in the co-design of any hybrid analog digital Ising machine implementation. Our results open a promising direction for the implementation of a new generation of reliable and scalable PIMs.|\n", "2503.13007": "|**2025-03-17**|**On the characteristic structure of the adjoint Euler equations with application to supersonic flows**|Carlos Lozano, Jorge Ponsin et.al.|[2503.13007](http://arxiv.org/abs/2503.13007)|null|23 pages|We review the characteristic structure of the two-dimensional adjoint Euler equations. We derive the compatibility and jump conditions along characteristics and show that the characteristic information can be used to obtain exact predictions for the adjoint variables in certain supersonic flows.|\n", "2503.12949": "|**2025-03-17**|**Tunable topological protection in Rydberg lattices via a novel quantum Monte Carlo approach**|Pranay Patil, Owen Benton et.al.|[2503.12949](http://arxiv.org/abs/2503.12949)|null|15 pages, 17 figures|Rydberg atom arrays have recently been conjectured to host $Z_2$ quantum spin liquids (QSLs) in certain parameter regimes. Due to the strong interactions between these atoms, it is not possible to analytically study these systems, and one must resort to Monte Carlo sampling of the path integral to reach definite conclusions. We use a tailored update, specifically designed to target the low energy excitations of the QSL. This allows us to reliably simulate Rydberg atoms on a triangular lattice in the proposed QSL regime. We identify a correlated paramagnetic phase at low temperatures which hosts topological protection similar to a $Z_2$ spin liquid up to a length scale tuned by Hamiltonian parameters. However, this correlated paramagnet seems to be continuously connected to the trivial paramagnetic regime and thus does not seem to be a true QSL. This result indicates the feasibility of Rydberg atom arrays to act as topological qubits.|\n", "2503.15460": "|**2025-03-19**|**pyTTN: An Open Source Toolbox for Open and Closed System Quantum Dynamics Simulations Using Tree Tensor Networks**|Lachlan P Lindoy, Daniel Rodrigo-Albert, Yannic Rath et.al.|[2503.15460](http://arxiv.org/abs/2503.15460)|**[link](https://gitlab.npl.co.uk/qsm/pyttn/)**|31 pages, 25 figures (main manuscript); 7 pages, 3 figures   (supplemental information)|We present the Python Tree Tensor Network package (pyTTN) for the evaluation of dynamical properties of closed and open quantum systems that makes use of Tree Tensor Network (TTN), or equivalently the multi-layer multiconfiguration time-dependent Hartree (ML-MCTDH), based representations of wavefunctions. This package includes several features allowing for easy setup of zero- and finite-temperature calculations for general Hamiltonians using single and multi-set TTN ans\\\"atze with an adaptive bond dimension through the use of subspace expansion techniques. All core features are implemented in C++ with Python bindings provided to simplify the use of this package. In addition to these core features, pyTTN provides several tools for setting up efficient simulation of open quantum system dynamics, including the use of the TTN ansatz to represent the auxiliary density operator space for the simulation of the Hierarchical Equation of Motion (HEOM) method and generalised pseudomode methods; furthermore we demonstrate that the two approaches are equivalent up to a non-unitary normal mode transformation acting on the pseudomode degrees of freedom. We present a set of applications of the package, starting with the widely used benchmark case of the photo-excitation dynamics of 24 mode pyrazine, following which we consider a more challenging model describing the exciton dynamics at the interface of a $n$-oligothiophene donor-C$_{60}$ fullerene acceptor system. Finally, we consider applications to open quantum systems, including the spin-boson model, a set of extended dissipative spin models, and an Anderson impurity model. By combining ease of use, an efficient implementation, as well as an extendable design allowing for the addition of future extensions, pyTTN can be integrated in a wide range of computational modelling software.|\n", "2503.15441": "|**2025-03-19**|**A discontinuity-capturing neural network with categorical embedding and its application to anisotropic elliptic interface problems**|Wei-Fan Hu, Te-Sheng Lin, Ming-Chih Lai et.al.|[2503.15441](http://arxiv.org/abs/2503.15441)|null||In this paper, we propose a discontinuity-capturing shallow neural network with categorical embedding to represent piecewise smooth functions. The network comprises three hidden layers, a discontinuity-capturing layer, a categorical embedding layer, and a fully-connected layer. Under such a design, we show that a piecewise smooth function, even with a large number of pieces, can be approximated by a single neural network with high prediction accuracy. We then leverage the proposed network model to solve anisotropic elliptic interface problems. The network is trained by minimizing the mean squared error loss of the system. Our results show that, despite its simple and shallow structure, the proposed neural network model exhibits comparable efficiency and accuracy to traditional grid-based numerical methods.|\n", "2503.15397": "|**2025-03-19**|**A High Order IMEX Method for Generalized Korteweg de-Vries Equations**|Seth Gerberding et.al.|[2503.15397](http://arxiv.org/abs/2503.15397)|null||In this paper, we introduce a high order space-time approximation of generalized Korteweg de-Vries equations. More specifically, the method uses continuous $H^1$-conforming finite elements for the spatial approximation and implicit-explicit methods for the temporal approximation. The method is high order in both space, provably stable, and mass-conservative. The scheme is formulated, its properties are proven, and numerical simulations are provided to illustrate the proposed methodology.|\n", "2503.15391": "|**2025-03-19**|**Modeling crystal defects using defect-informed neural networks**|Ziduo Yang, Xiaoqing Liu, Xiuying Zhang et.al.|[2503.15391](http://arxiv.org/abs/2503.15391)|null||Machine learning has revolutionized the study of crystalline materials for enabling rapid predictions and discovery. However, most AI-for-Materials research to date has focused on ideal crystals, whereas real-world materials inevitably contain defects that play a critical role in modern functional technologies. The defects and dopants break geometric symmetry and increase interaction complexity, posing particular challenges for traditional ML models. Addressing these challenges requires models that are able to capture sparse defect-driven effects in crystals while maintaining adaptability and precision. Here, we introduce Defect-Informed Equivariant Graph Neural Network (DefiNet), a model specifically designed to accurately capture defect-related interactions and geometric configurations in point-defect structures. Trained on 14,866 defect structures, DefiNet achieves highly accurate structural predictions in a single step, avoiding the time-consuming iterative processes in modern ML relaxation models and possible error accumulation from iteration. We further validates DefiNet's accuracy by using density functional theory (DFT) relaxation on DefiNet-predicted structures. For most defect structures, regardless of defect complexity or system size, only 3 ionic steps are required to reach the DFT-level ground state. Finally, comparisons with scanning transmission electron microscopy (STEM) images confirm DefiNet's scalability and extrapolation beyond point defects, positioning it as a groundbreaking tool for defect-focused materials research.|\n", "2503.15278": "|**2025-03-20**|**Symplectic integration of guiding-center equations in canonical coordinates for general toroidal fields**|Christopher G. Albert, Georg S. Grassler, Sergei V. Kasilov et.al.|[2503.15278](http://arxiv.org/abs/2503.15278)|null||Symplectic integrators with long-term preservation of integrals of motion are introduced for the guiding-center model of plasma particles in toroidal magnetic fields of general topology. An efficient transformation to canonical coordinates from cylindrical and flux-like coordinates is discussed and applied using one component of the magnetic vector potential as a spatial coordinate. This choice is efficient in both, theoretical and numerical developments and marks a generalization of magnetic flux coordinates. The transformation enables the application of conventional symplectic integration schemes formulated in canonical coordinates, as well as variational integrators on the guiding-center system, without requiring magnetic flux coordinates. Symplectic properties and superior efficiency of the implicit midpoint scheme compared to conventional non-symplectic methods are demonstrated on perturbed tokamak fields with magnetic islands and stochastic regions. The presented results mark a crucial step towards gyrokinetic models that conserve physical invariants.|\n", "2503.15258": "|**2025-03-19**|**A Lie algebra view of matrix splittings**|Michele Benzi, Milo Viviani et.al.|[2503.15258](http://arxiv.org/abs/2503.15258)|null|26 pages|In this paper we use some basic facts from the theory of (matrix) Lie groups and algebras to show that many of the classical matrix splittings used to construct stationary iterative methods and preconditioniers for Krylov subspace methods can be interpreted as linearizations of matrix factorizations. Moreover, we show that new matrix splittings are obtained when we specialize these splittings to some of the classical matrix groups and their Lie and Jordan algebras. As an example, we derive structured generalizations of the HSS (Hermitian/skew-Hermitian) iteration, and provide sufficient conditions for their convergence.|\n", "2503.15188": "|**2025-03-19**|**Convergence analysis of SPH method on irregular particle distributions for the Poisson equation**|Zhonghua Qiao, Yifan Wei et.al.|[2503.15188](http://arxiv.org/abs/2503.15188)|null||The accuracy of particle approximation in Smoothed Particle Hydrodynamics (SPH) method decreases due to irregular particle distributions, especially for second-order derivatives. This study aims to enhance the accuracy of SPH method and analyze its convergence with irregular particle distributions. By establishing regularity conditions for particle distributions, we ensure that the local truncation error of traditional SPH formulations, including first and second derivatives, achieves second-order accuracy. Our proposed method, the volume reconstruction SPH method, guarantees these regularity conditions while preserving the discrete maximum principle. Benefiting from the discrete maximum principle, we conduct a rigorous global error analysis in the $L^\\infty$-norm for the Poisson equation with variable coefficients, achieving second-order convergence. Numerical examples are presented to validate the theoretical findings.|\n", "2503.15187": "|**2025-03-19**|**Thermal Enskog-Vlasov Lattice Boltzmann model with phase separation**|Sergiu Busuioc, Victor Sofonea et.al.|[2503.15187](http://arxiv.org/abs/2503.15187)|null|29 pages, 9 figures|An Enskog-Vlasov finite-difference Lattice Boltzmann (EV-FDLB) for liquid-vapor systems with variable temperature is introduced. The model involves both the simplified Enskog collision operator and the self-consistent force field which accounts for the long-range interaction between the fluid particles. Full-range Gauss-Hermite quadratures were used for the discretization of the momentum space. The numerical solutions of the Enskog-Vlasov equation obtained employing the EV-FDLB model and the Direct Simulation Monte Carlo (DSMC)-like particle method (PM) are compared. Reasonable agreement is found between the two approaches when simulating the liquid-vapor phase separation and the liquid slab evaporation.|\n", "2503.15149": "|**2025-03-19**|**Machine learning surrogate models of many-body dispersion interactions in polymer melts**|Zhaoxiang Shen, Ra\u00fal I. Sosa, Jakub Lengiewicz et.al.|[2503.15149](http://arxiv.org/abs/2503.15149)|null||Accurate prediction of many-body dispersion (MBD) interactions is essential for understanding the van der Waals forces that govern the behavior of many complex molecular systems. However, the high computational cost of MBD calculations limits their direct application in large-scale simulations. In this work, we introduce a machine learning surrogate model specifically designed to predict MBD forces in polymer melts, a system that demands accurate MBD description and offers structural advantages for machine learning approaches. Our model is based on a trimmed SchNet architecture that selectively retains the most relevant atomic connections and incorporates trainable radial basis functions for geometric encoding. We validate our surrogate model on datasets from polyethylene, polypropylene, and polyvinyl chloride melts, demonstrating high predictive accuracy and robust generalization across diverse polymer systems. In addition, the model captures key physical features, such as the characteristic decay behavior of MBD interactions, providing valuable insights for optimizing cutoff strategies. Characterized by high computational efficiency, our surrogate model enables practical incorporation of MBD effects into large-scale molecular simulations.|\n", "2503.15125": "|**2025-03-19**|**A Spectral Approach to Optimal Control of the Fokker-Planck Equation**|Dante Kalise, Lucas M. Moschen, Grigorios A. Pavliotis et.al.|[2503.15125](http://arxiv.org/abs/2503.15125)|null|6 pages, 4 figures|In this paper, we present a spectral optimal control framework for Fokker-Planck equations based on the standard ground state transformation that maps the Fokker-Planck operator to a Schrodinger operator. Our primary objective is to accelerate convergence toward the (unique) steady state. To fulfill this objective, a gradient-based iterative algorithm with Pontryagin's maximum principle and Barzilai-Borwein update is developed to compute time-dependent controls. Numerical experiments on two-dimensional ill-conditioned normal distributions and double-well potentials demonstrate that our approach effectively targets slow-decaying modes, thus increasing the spectral gap.|\n", "2503.15121": "|**2025-03-19**|**Analytic adjoint solution for incompressible potential flows**|Carlos Lozano, Jorge Ponsin et.al.|[2503.15121](http://arxiv.org/abs/2503.15121)|null|20 pages|We obtain the analytic adjoint solution for two-dimensional (2D) incompressible potential flow for a cost function measuring aerodynamic force using the connection of the adjoint approach to Green's functions and also by establishing and exploiting its relation to the adjoint incompressible Euler equations. By comparison with the analytic solution, it is shown that the naive approach based on solving Laplace's equation for the adjoint variables can be ill-defined. The analysis of the boundary behavior of the analytic solution is used to discuss the proper formulation of the adjoint problem as well as the mechanism for incorporating the Kutta condition in the adjoint formulation|\n", "2503.15105": "|**2025-03-19**|**Control, Optimal Transport and Neural Differential Equations in Supervised Learning**|Minh-Nhat Phung, Minh-Binh Tran et.al.|[2503.15105](http://arxiv.org/abs/2503.15105)|null||From the perspective of control theory, neural differential equations (neural ODEs) have become an important tool for supervised learning. In the fundamental work of Ruiz-Balet and Zuazua (SIAM REVIEW 2023), the authors pose an open problem regarding the connection between control theory, optimal transport theory, and neural differential equations. More precisely, they inquire how one can quantify the closeness of the optimal flows in neural transport equations to the true dynamic optimal transport. In this work, we propose a construction of neural differential equations that converge to the true dynamic optimal transport in the limit, providing a significant step in solving the formerly mentioned open problem.|\n", "2503.15080": "|**2025-03-19**|**Exponentially Tilted Thermodynamic Maps (expTM): Predicting Phase Transitions Across Temperature, Pressure, and Chemical Potential**|Suemin Lee, Ruiyu Wang, Lukas Herron et.al.|[2503.15080](http://arxiv.org/abs/2503.15080)|null||Predicting and characterizing phase transitions is crucial for understanding generic physical phenomena such as crystallization, protein folding and others. However, directly observing phase transitions is not always easy, and often one has limited observations far from the phase boundary and measured under some specific thermodynamic conditions. In this study, we propose a statistical physics and Generative AI driven framework that can take such limited information to generate samples of different phases under arbitrary thermodynamic conditions, which we name Exponentially Tilted Thermodynamic Maps (expTM). The central idea is to map collected data into a tractable simple prior expressed as an exponentially tilted Gaussian. We demonstrate how the variance and mean of the prior can be correlated with pairs of thermodynamic control variables, including temperature, pressure, and chemical potential. This gives us the ability to generate thermodynamically correct samples under any values of the control variables. To demonstrate the practical applicability of this approach, we use expTM to sample the lattice gas models with the Grand Canonical ensemble, capturing phase transitions under varying chemical potentials and temperatures. We further demonstrate how expTM can model the isothermal-isobaric ensemble, with which we predict different phases of CO2 under varying pressure conditions. Both examples are trained on very limited data far from the phase boundary. These results establish expTM as a robust tool for understanding phase transitions across diverse thermodynamic conditions requiring only a small number of observations.|\n", "2503.15077": "|**2025-03-19**|**Efficient forward and inverse uncertainty quantification for dynamical systems based on dimension reduction and Kriging surrogate modeling in functional space**|Zhouzhou Song, Weiyun Xu, Marcos A. Valdebenito et.al.|[2503.15077](http://arxiv.org/abs/2503.15077)|null||Surrogate models are extensively employed for forward and inverse uncertainty quantification in complex, computation-intensive engineering problems. Nonetheless, constructing high-accuracy surrogate models for complex dynamical systems with limited training samples continues to be a challenge, as capturing the variability in high-dimensional dynamical system responses with a small training set is inherently difficult. This study introduces an efficient Kriging modeling framework based on functional dimension reduction (KFDR) for conducting forward and inverse uncertainty quantification in dynamical systems. By treating the responses of dynamical systems as functions of time, the proposed KFDR method first projects these responses onto a functional space spanned by a set of predefined basis functions, which can deal with noisy data by adding a roughness regularization term. A few key latent functions are then identified by solving the functional eigenequation, mapping the time-variant responses into a low-dimensional latent functional space. Subsequently, Kriging surrogate models with noise terms are constructed in the latent space. With an inverse mapping established from the latent space to the original output space, the proposed approach enables accurate and efficient predictions for dynamical systems. Finally, the surrogate model derived from KFDR is directly utilized for efficient forward and inverse uncertainty quantification of the dynamical system. Through three numerical examples, the proposed method demonstrates its ability to construct highly accurate surrogate models and perform uncertainty quantification for dynamical systems accurately and efficiently.|\n", "2503.15051": "|**2025-03-19**|**Ab initio study of pressure-induced phase transition, band gaps and X-ray photoemission valence band spectra of YVO$_4$**|M. Werwi\u0144ski, J. Kaczkowski, P. Le\u015bniak et.al.|[2503.15051](http://arxiv.org/abs/2503.15051)|null||High-pressure induced structural transition from zircon-type phase into scheelite-type phase in YVO$_4$ is studied using ab initio calculations. Several structures with compressed volumes are evaluated, where for every considered volume the c/a ratio and atomic positions are optimised. The transition pressure and transition volume change are calculated. The reports on YVO$_4$ electronic structure and electronic band gap behaviour are followed by results of X-Ray photoemission spectra XPS calculations. Most of our theoretical predictions are compared with experimental results taken from literature.|\n", "2503.14978": "|**2025-03-19**|**Inferring diffusivity from killed diffusion**|Richard Nickl, Fanny Seizilles et.al.|[2503.14978](http://arxiv.org/abs/2503.14978)|null|30 pages, 4 figures|We consider diffusion of independent molecules in an insulated Euclidean domain with unknown diffusivity parameter. At a random time and position, the molecules may bind and stop diffusing in dependence of a given `binding potential'. The binding process can be modeled by an additive random functional corresponding to the canonical construction of a `killed' diffusion Markov process. We study the problem of conducting inference on the infinite-dimensional diffusion parameter from a histogram plot of the `killing' positions of the process. We show first that these positions follow a Poisson point process whose intensity measure is determined by the solution of a certain Schr\\\"odinger equation. The inference problem can then be re-cast as a non-linear inverse problem for this PDE, which we show to be consistently solvable in a Bayesian way under natural conditions on the initial state of the diffusion, provided the binding potential is not too `aggressive'. In the course of our proofs we obtain novel posterior contraction rate results for high-dimensional Poisson count data that are of independent interest. A numerical illustration of the algorithm by standard MCMC methods is also provided.|\n", "2503.14972": "|**2025-03-19**|**Effect of Gd and Co content on electrochemical and electronic properties of La$_{1.5}$Mg$_{0.5}$Ni$_7$ alloys: a combined experimental and first-principles study**|Miros\u0142aw Werwi\u0144ski, Andrzej Szajek, Agnieszka Marczy\u0144ska et.al.|[2503.14972](http://arxiv.org/abs/2503.14972)|null||In this work, we investigate the effect of Gd and Co substitutions on the electrochemical and electronic properties of La$_{1.5}$Mg$_{0.5}$Ni$_7$ alloy. Two series of La$_{1.5-x}$Gd$_x$Mg$_{0.5}$Ni$_7$ ($x$ = 0.0, 0.25, 1.0) and La$_{1.5}$Mg$_{0.5}$Ni$_{7-y}$Co$_y$ ($y$ = 0.0, 0.5, 1.5) alloys are produced using mechanical alloying technique. The X-ray diffraction indicates multiphase character of the samples, with the majority of hexagonal Ce$_2$Ni$_7$-type and rhombohedral Gd$_2$Co$_7$-type structures of (La,Mg)$_2$Ni$_7$ phase. Partial substitutions of La by Gd or Ni by Co in La$_{1.5}$Mg$_{0.5}$Ni$_7$ phase result in increase of cycle stability of the metal hydride (MH$_x$) electrodes. All considered alloys reach the maximum discharge capacity after three charging-discharging cycles. Two optimal compositions, in respect of electrochemical properties, are subsequently investigated by the X-ray photoelectron spectroscopy (XPS). The experimental analysis of the valence band is further extended by the density functional theory (DFT) calculations. We also discuss the effects of alloying and site preference of dopants on the position of the van Hove-type singularity, as observed in the electronic densities of states (DOS) in proximity of the Fermi level.|\n", "2503.14952": "|**2025-03-19**|**Effect of substitution La by Mg on electrochemical and electronic properties in La$_{2-x}$Mg$_x$Ni$_7$ alloys: a combined experimental and ab initio studies**|Miros\u0142aw Werwi\u0144ski, Andrzej Szajek, Agnieszka Marczy\u0144ska et.al.|[2503.14952](http://arxiv.org/abs/2503.14952)|null||La-Mg-Ni-based alloys are promising negative electrode materials for 3rd generation of Ni-MH$_x$ batteries. In this work, we investigate the effect of Mg substitution on the electrochemical and electronic properties of La$_{2-x}$Mg$_x$Ni$_7$ materials. The mechanical alloying technique is used to produce a series of La$_{2-x}$Mg$_x$Ni$_7$ alloys ($x$ = 0.00, 0.25, 0.50 and 0.75). The X-ray diffraction measurements indicate multiphase character of the samples with majority (La,Mg)$_2$Ni$_7$ phases of hexagonal Ce$_2$Ni$_7$-type and rhombohedral Gd$_2$Co$_7$-type. Electrochemical measurements show how the maximum discharge capacity ($C_{max}$) increases with Mg concentration and that reach the highest value of 304 mAh/g for La$_{1.5}$Mg$_{0.5}$Ni$_7$ ($x$ = 0.5). The experimental efforts are followed by the density functional theory (DFT) calculations performed with the full-potential local-orbital minimum-basis scheme (FPLO). To simulate chemical disorder, we use the coherent potential approximation (CPA). The calculations are focused on the La$_{1.5}$Mg$_{0.5}$Ni$_7$ composition with the highest measured value of $C_{max}$. Additionally, several other structures are considered as reference points. We find that hexagonal and rhombohedral structures of La$_2$Ni$_7$ have almost identical total energies, which is in a good agreement with a coexistence of both phases in the samples. The calculated site preferences of Mg in both Ce$_2$Ni$_7$-type and Gd$_2$Co$_7$-type La$_{1.5}$Mg$_{0.5}$Ni$_7$ phases are consistent with the previous experimental data. Furthermore, the valence band of the nanocrystalline La$_{1.5}$Mg$_{0.5}$Ni$_7$ sample is investigated by X-ray photoelectron spectroscopy (XPS). The experimental XPS are interpreted based on the corresponding spectra calculated with DFT.|\n", "2503.14947": "|**2025-03-19**|**Image Restoration Models with Optimal Transport and Total Variation Regularization**|Weijia Huang, Zhongyi Huang, Wenli Yang et.al.|[2503.14947](http://arxiv.org/abs/2503.14947)|null||In this paper, we propose image restoration models using optimal transport (OT) and total variation regularization. We present theoretical results of the proposed models based on the relations between the dual Lipschitz norm from OT and the G-norm introduced by Yves Meyer. We design a numerical method based on the Primal-Dual Hybrid Gradient (PDHG) algorithm for the Wasserstain distance and the augmented Lagrangian method (ALM) for the total variation, and the convergence analysis of the proposed numerical method is established. We also consider replacing the total variation in our model by one of its modifications developed in \\cite{zhu}, with the aim of suppressing the stair-casing effect and preserving image contrasts. Numerical experiments demonstrate the features of the proposed models.|\n", "2503.14913": "|**2025-03-19**|**A PINN-enriched finite element method for linear elliptic problems**|Xiao Chen, Yixin Luo, Jingrun Chen et.al.|[2503.14913](http://arxiv.org/abs/2503.14913)|null||In this paper, we propose a hybrid method that combines finite element method (FEM) and physics-informed neural network (PINN) for solving linear elliptic problems. This method contains three steps: (1) train a PINN and obtain an approximate solution $u_{\\theta}$; (2) enrich the finite element space with $u_{\\theta}$; (3) obtain the final solution by FEM in the enriched space. In the second step, the enriched space is constructed by addition $v + u_{\\theta}$ or multiplication $v \\cdot u_{\\theta}$, where $v$ belongs to the standard finite element space. We conduct the convergence analysis for the proposed method. Compared to the standard FEM, the same convergence order is obtained and higher accuracy can be achieved when solution derivatives are well approximated in PINN. Numerical examples from one dimension to three dimensions verify these theoretical results. For some examples, the accuracy of the proposed method can be reduced by a couple of orders of magnitude compared to the standard FEM.|\n", "2503.16388": "|**2025-03-20**|**A Mixed-FEM approximation with uniform conservation of the exponential stability for a class of anisotropic port-Hamiltonian system and its application to LQ control**|Luis A. Mora, Kirsten Morris et.al.|[2503.16388](http://arxiv.org/abs/2503.16388)|null||In this manuscript, we present a mixed finite element discretization for a class of boundary-damped anisotropic port-Hamiltonian systems. Using a multiplier method, we demonstrate that the resulting approximation model uniformly preserves the exponential stability of the uncontrolled system, establishing a lower bound for the exponential decay rate that is independent of the mesh size. This property is illustrated through the spatial discretization of a piezoelectric beam. Furthermore, we show how the uniform preservation of exponential stability by the proposed model aids in the convergence of controllers derived from an infinite-time linear quadratic control design, in comparison to models obtained from the standard finite-element method.|\n", "2503.16312": "|**2025-03-20**|**Near-Linear Runtime for a Classical Matrix Preconditioning Algorithm**|Xufeng Cai, Jason M. Altschuler, Jelena Diakonikolas et.al.|[2503.16312](http://arxiv.org/abs/2503.16312)|null||In 1960, Osborne proposed a simple iterative algorithm for matrix balancing with outstanding numerical performance. Today, it is the default preconditioning procedure before eigenvalue computation and other linear algebra subroutines in mainstream software packages such as Python, Julia, MATLAB, EISPACK, LAPACK, and more. Despite its widespread usage, Osborne's algorithm has long resisted theoretical guarantees for its runtime: the first polynomial-time guarantees were obtained only in the past decade, and recent near-linear runtimes remain confined to variants of Osborne's algorithm with important differences that make them simpler to analyze but empirically slower. In this paper, we address this longstanding gap between theory and practice by proving that Osborne's original algorithm -- the de facto preconditioner in practice -- in fact has a near-linear runtime. This runtime guarantee (1) is optimal in the input size up to at most a single logarithm, (2) is the first runtime for Osborne's algorithm that does not dominate the runtime of downstream tasks like eigenvalue computation, and (3) improves upon the theoretical runtimes for all other variants of Osborne's algorithm.|\n", "2503.16240": "|**2025-03-20**|**Machine learning identifies nullclines in oscillatory dynamical systems**|Bartosz Prokop, Jimmy Billen, Nikita Frolov et.al.|[2503.16240](http://arxiv.org/abs/2503.16240)|null|7 pages, 4 figures|We introduce CLINE (Computational Learning and Identification of Nullclines), a neural network-based method that uncovers the hidden structure of nullclines from oscillatory time series data. Unlike traditional approaches aiming at direct prediction of system dynamics, CLINE identifies static geometric features of the phase space that encode the (non)linear relationships between state variables. It overcomes challenges such as multiple time scales and strong nonlinearities while producing interpretable results convertible into symbolic differential equations. We validate CLINE on various oscillatory systems, showcasing its effectiveness.|\n", "2503.16225": "|**2025-03-20**|**Energy-Adaptive Riemannian Conjugate Gradient Method for Density Functional Theory**|Daniel Peterseim, Jonas P\u00fcschel, Tatjana Stykel et.al.|[2503.16225](http://arxiv.org/abs/2503.16225)|null||This paper presents a novel Riemannian conjugate gradient method for the Kohn-Sham energy minimization problem in density functional theory (DFT), with a focus on non-metallic crystal systems. We introduce an energy-adaptive metric that preconditions the Kohn-Sham model, significantly enhancing optimization efficiency. Additionally, a carefully designed shift strategy and several algorithmic improvements make the implementation comparable in performance to highly optimized self-consistent field iterations. The energy-adaptive Riemannian conjugate gradient method has a sound mathematical foundation, including stability and convergence, offering a reliable and efficient alternative for DFT-based electronic structure calculations in computational chemistry.|\n", "2503.16222": "|**2025-03-20**|**Efficient Bayesian Computation Using Plug-and-Play Priors for Poisson Inverse Problems**|Teresa Klatzer, Savvas Melidonis, Marcelo Pereyra et.al.|[2503.16222](http://arxiv.org/abs/2503.16222)|null|31 pages, 17 figures|This paper introduces a novel plug-and-play (PnP) Langevin sampling methodology for Bayesian inference in low-photon Poisson imaging problems, a challenging class of problems with significant applications in astronomy, medicine, and biology. PnP Langevin sampling algorithms offer a powerful framework for Bayesian image restoration, enabling accurate point estimation as well as advanced inference tasks, including uncertainty quantification and visualization analyses, and empirical Bayesian inference for automatic model parameter tuning. However, existing PnP Langevin algorithms are not well-suited for low-photon Poisson imaging due to high solution uncertainty and poor regularity properties, such as exploding gradients and non-negativity constraints. To address these challenges, we propose two strategies for extending Langevin PnP sampling to Poisson imaging models: (i) an accelerated PnP Langevin method that incorporates boundary reflections and a Poisson likelihood approximation and (ii) a mirror sampling algorithm that leverages a Riemannian geometry to handle the constraints and the poor regularity of the likelihood without approximations. The effectiveness of these approaches is demonstrated through extensive numerical experiments and comparisons with state-of-the-art methods.|\n", "2503.16210": "|**2025-03-20**|**On the convergence of split exponential integrators for semilinear parabolic problems**|Marco Caliari, Fabio Cassini, Lukas Einkemmer et.al.|[2503.16210](http://arxiv.org/abs/2503.16210)|null||Splitting the exponential-like $\\varphi$ functions, which typically appear in exponential integrators, is attractive in many situations since it can dramatically reduce the computational cost of the procedure. However, depending on the employed splitting, this can result in order reduction. The aim of this paper is to analyze different such split approximations. We perform the analysis for semilinear problems in the abstract framework of commuting semigroups and derive error bounds that depend, in particular, on whether the vector (to which the $\\varphi$ functions are applied) satisfies appropriate boundary conditions. We then present the convergence analysis for two split versions of a second-order exponential Runge--Kutta integrator in the context of analytic semigroups, and show that one suffers from order reduction while the other does not. Numerical results for semidiscretized parabolic PDEs confirm the theoretical findings.|\n", "2503.16209": "|**2025-03-20**|**Instance optimal function recovery -- samples, decoders and asymptotic performance**|Moritz Moeller, Kateryna Pozharska, Tino Ullrich et.al.|[2503.16209](http://arxiv.org/abs/2503.16209)|null|2 Figures|In this paper we study non-linear sampling recovery of multivariate functions using techniques from compressed sensing. In the first part of the paper we prove that square root Lasso $({\\tt rLasso})$ with a particular choice of the regularization parameter $\\lambda>0$ as well as orthogonal matching pursuit $({\\tt OMP})$ after sufficiently many iterations provide noise blind decoders which efficiently recover multivariate functions from random samples. In contrast to basis pursuit the decoders $({\\tt rLasso})$ and $({\\tt OMP})$ do not require any additional information on the width of the function class in $L_\\infty$ and lead to instance optimal recovery guarantees. In the second part of the paper we relate the findings to linear recovery methods such as least squares $({\\tt Lsqr})$ or Smolyak's algorithm $({\\tt Smolyak})$ and compare the performance in a model situation, namely periodic multivariate functions with $L_p$-bounded mixed derivative will be approximated in $L_q$. The main observation is the fact, that $({\\tt rLasso})$ and $({\\tt OMP})$ outperform Smolyak's algorithm (sparse grids) in various situations, where $1<p<2\\leq q<\\infty$. For $q=2$ they even outperform any linear method including $({\\tt Lsqr})$ in combination with recently proposed subsampled random points.|\n", "2503.16202": "|**2025-03-20**|**3D Stochastic Geometry Model for Aerial Vehicle-Relayed Ground-Air-Satellite Connectivity**|Yulei Wang, Yalin Liu, Yaru Fu et.al.|[2503.16202](http://arxiv.org/abs/2503.16202)|null||Due to their flexibility, aerial vehicles (AVs), such as unmanned aerial vehicles and airships, are widely employed as relays to assist communications between massive ground users (GUs) and satellites, forming an AV-relayed ground-air-satellite solution (GASS). In GASS, the deployment of AVs is crucial to ensure overall performance from GUs to satellites. This paper develops a stochastic geometry-based analytical model for GASS under Matern hard-core point process (MHCPP) distributed AVs. The 3D distributions of AVs and GUs are modeled by considering their locations on spherical surfaces in the presence of high-altitude satellites. Accordingly, we derive an overall connectivity analytical model for GASS, which includes the average performance of AV-relayed two-hop transmissions. Extensive numerical results validate the accuracy of the connectivity model and provide essential insights for configuring AV deployments.|\n", "2503.16196": "|**2025-03-20**|**An interior penalty DG method with correct and minimal averages, jumps and penalties for the miscible displacement problem of nonnegative characteristic form, and SUPG-type error estimates under low regularity, dominating Darcy velocity**|Zhijie Du, Huoyuan Duan, Roger C E Tan et.al.|[2503.16196](http://arxiv.org/abs/2503.16196)|null||An interior penalty DG method is proposed for the steady-state linear partial differential equations of nonnegative characteristic form, suitable for mixed second-order elliptic-parabolic and first-order hyperbolic equations. Due to the different natures of the elliptic, parabolic, and hyperbolic equations. In the new DG method, the averages, jumps and penalties are minimal, correctly and only imposed on the diffusion-diffusion element boundaries, in addition to the well-known upwind jumps associating with the advection velocity. For the advection-dominated problem, the penalties can be further reduced only being imposed on the diffusion-dominated subset of the diffusion-diffusion element boundaries.This is based on the novel, crucial technique about the multiple partitions of the set of the interelement boundaries into a number of subsets with respect to the diffusion and to the advection and on the consistency result we have proven. The new DG method is the first DG method and the first time that the continuity and discontinuity of the solution are correctly identified and justified of the general steady-state linear partial differential equations of nonnegative characteristic form. The new DG method and its analysis are applied to the miscible displacement problem of vanishing diffusion coefficient and of low regularity, dominating Darcy flow velocity which lives in $H(\\operatorname{div};\\Omega)\\cap \\prod_{j=1}^J (H^r(D_j))^d$ for $r<1$ other than the usual assumption $(W^{1,\\infty}(\\Omega))^d$. We prove the SUPG-type error estimates $\\mathcal{O}(h^{\\ell+\\frac{1}{2}})$ for any element polynomial of degree $\\ell\\ge 1$ on generally shaped and nonconforming meshes, where the convergence order is independent of the regularity of the advection velocity. The SUPG-type error estimates obtained are new and the first time known under the low regularity of the advection velocity.|\n", "2503.16176": "|**2025-03-20**|**Nonnegative Biquadratic Tensors**|Chunfeng Cui, Liqun Qi et.al.|[2503.16176](http://arxiv.org/abs/2503.16176)|null||An M-eigenvalue of a nonnegative biquadratic tensor is referred to as an M$^+$-eigenvalue if it has a pair of nonnegative M-eigenvectors. If furthermore that pair of M-eigenvectors is positive, then that M$^+$-eigenvalue is called an M$^{++}$-eigenvalue. A nonnegative biquadratic tensor always has at least one M$^+$ eigenvalue, and the largest M$^+$-eigenvalue is also the largest M-eigenvalue and the M-spectral radius. In the case of an irreducible nonnegative biquadratic tensor, all the M$^+$-eigenvalues are M$^{++}$-eigenvalues. Although the M$^+$-eigenvalues of irreducible nonnegative biquadratic tensors are not unique in general, we establish a sufficient condition to ensure their uniqueness. For an irreducible nonnegative biquadratic tensor, the largest M$^+$-eigenvalue has a max-min characterization, while the smallest M$^+$-eigenvalue has a min-max characterization. A Collatz algorithm for computing the largest M$^+$-eigenvalues is proposed. Numerical results are reported.|\n", "2503.16119": "|**2025-03-20**|**Transverse Nucleon Single-Spin Asymmetry for Single-Inclusive Hadron and Jet Production at NLO Accuracy**|Daniel Rein, Marc Schlegel, Patrick Tollk\u00fchn et.al.|[2503.16119](http://arxiv.org/abs/2503.16119)|null|79 pages, 27 figures|We investigate the single-spin asymmetry for the single-inclusive production of hadrons and jets in collisions of transversely polarized nucleons and unpolarized leptons, $\\ell N^\\uparrow \\to (h\\,\\mathrm{or\\,jet})X$. We compute the spin-dependent cross section within collinear twist-3 factorization in perturbative QCD at next-to-leading order (NLO) accuracy. In this approach, multiparton correlations generate a non-vanishing effect. For the present paper, we focus on correlations in the nucleon initial-state rather than in the fragmentation process. We explicitly verify that collinear twist-3 factorization is valid at the one-loop level. Our analytical results show that at NLO the relevant multiparton correlation functions in the nucleon are probed on their full support in momentum fractions. Our numerical analysis for collisions at the Electron-Ion Collider indicates that the NLO corrections can be large and are sensitive to the functional form of the twist-3 correlation functions.|\n", "2503.16110": "|**2025-03-20**|**Compact implicit high resolution numerical method for solving transport problems with sorption isotherms**|Dagmar Zakova, Peter Frolkovic et.al.|[2503.16110](http://arxiv.org/abs/2503.16110)|null||This study investigates numerical methods to solve nonlinear transport problems characterized by various sorption isotherms with a focus on the Freundlich type of isotherms. We describe and compare second order accurate numerical schemes, focusing on implicit methods, to effectively model transport phenomena without stability restriction on the choice of time steps. Furthermore, a high resolution form of the method is proposed that limits a priori the second order accurate scheme towards first order accuracy to keep the values of numerical solutions in a physically acceptable range.   Through numerical experiments, we demonstrate the effectiveness of high resolution methods in minimizing oscillations near discontinuities, thereby enhancing solution plausibility. The observed convergence rates confirm that the second order accurate schemes achieve expected accuracy for smooth solutions and that they yield significant improvements when compared with the results of the first order scheme. As the computational cost of the compact implicit method seems to be comparable to similar explicit ones with a clear profit of unconditional stability, this research provides a practical tool toward numerical simulations of nonlinear transport phenomena applicable in various fields such as contaminant transport in porous media or column liquid chromatography.|\n", "2503.16078": "|**2025-03-20**|**Magnetic skyrmions embedded in a vortex**|Haobing Zhang, Xintao Fan, Weiwei Wang et.al.|[2503.16078](http://arxiv.org/abs/2503.16078)|null||Magnetic vortices and skyrmions represent two fundamental classes of topological spin textures in ferromagnetic systems, distinguished by their unique stabilization mechanisms and degrees of freedom. Vortices, characterized by circular in-plane magnetization (chirality) and out-of-plane core polarization, naturally arise in confined geometries due to the interplay between exchange and dipolar interactions. In contrast, skyrmions typically require the Dzyaloshinskii-Moriya interaction for stabilization and exhibit fixed chirality-polarity relationships. Through micromagnetic simulations, we reveal that these seemingly distinct topological states can coexist, forming a novel composite state termed the \\textit{n}-skyrmion vortex, which represents a skyrmion-embedded vortex state. These composite states possess quantized topological charges $Q$ that follow the relation $Q_{\\text{total}} = Q_{\\text{vortex}} + nQ_{\\text{skyrmion}}$, where $n$ denotes the number of embedded skyrmions. Similar to vortices, these states exhibit independent chirality and polarity and are energetically degenerate.|\n", "2503.16062": "|**2025-03-20**|**Constraint Phase Space Formulations for Finite-State Quantum Systems: The Relation between Commutator Variables and Complex Stiefel Manifolds**|Youhao Shang, Xiangsong Cheng, Jian Liu et.al.|[2503.16062](http://arxiv.org/abs/2503.16062)|null||We have recently developed the \\textit{constraint} coordinate-momentum \\textit{phase space} (CPS) formulation for finite-state quantum systems. It has been implemented for the electronic subsystem in nonadiabatic transition dynamics to develop practical trajectory-based approaches. In the generalized CPS formulation for the mapping Hamiltonian of the classical mapping model with commutator variables (CMMcv) method [\\textit{J. Phys. Chem. A} \\textbf{2021}, 125, 6845-6863], each {connected} component of the generalized CPS is the \\textit{complex Stiefel manifold} labeled by the eigenvalue set of the mapping kernel. Such a phase space structure allows for exact trajectory-based dynamics for pure discrete (electronic) degrees of freedom (DOFs), where the equations of motion of each trajectory are isomorphic to the time-dependent Schr\\\"odinger equation. We employ covariant kernels {within the generalized CPS framework} to develop two approaches that naturally yield exact evaluation of time correlation functions (TCFs) for pure discrete (electronic) DOFs. In addition, we briefly discuss the phase space mapping formalisms where the contribution of each trajectory to the integral expression of the {TCF} of population dynamics is strictly positive semi-definite. The generalized CPS formulation also indicates that the equations of motion in phase space mapping model I of our previous work [\\textit{J. Chem. Phys.} \\textbf{2016}, 145, 204105; \\textbf{2017}, 146, 024110; \\textbf{2019}, 151, 024105] lead to a complex Stiefel manifold $\\mathrm{U}(F)/\\mathrm{U}(F-2)$. It is expected that the generalized CPS formulation has implications for simulations of both nonadiabatic transition dynamics and many-body quantum dynamics for spins/bosons/fermions.|\n", "2503.16037": "|**2025-03-20**|**Scattering graph method for 3D radiative transfer**|Antti Mikkonen, Anssi Koskinen, Johanna Tamminen et.al.|[2503.16037](http://arxiv.org/abs/2503.16037)|null||A novel method for monochromatic scalar 3D radiative transfer, designed primarily for modelling remote sensing imaging, is presented. For simulating an observation of an imaging satellite instrument, the method uses a heuristic scattering coupling function to model the inter-pixel scattering of radiation, which is represented with a graph. The GPU-capable code implementation of the method, TURSCA, was validated against two established 3D RT models, Siro and SHDOM with relative agreement at 3% and 6%, respectively. The method opens up new avenues of research, especially in satellite-based remote sensing of atmospheres.|\n", "2503.16028": "|**2025-03-20**|**Sequential Monte Carlo with Gaussian Mixture Distributions for Infinite-Dimensional Statistical Inverse Problems**|Haoyu Lu, Junxiong Jia, Deyu Meng et.al.|[2503.16028](http://arxiv.org/abs/2503.16028)|null|35 pages|By formulating the inverse problem of partial differential equations (PDEs) as a statistical inference problem, the Bayesian approach provides a general framework for quantifying uncertainties. In the inverse problem of PDEs, parameters are defined on an infinite-dimensional function space, and the PDEs induce a computationally intensive likelihood function. Additionally, sparse data tends to lead to a multi-modal posterior. These features make it difficult to apply existing sequential Monte Carlo (SMC) algorithms. To overcome these difficulties, we propose new conditions for the likelihood functions, construct a Gaussian mixture based preconditioned Crank-Nicolson transition kernel, and demonstrate the universal approximation property of the infinite-dimensional Gaussian mixture probability measure. By combining these three novel tools, we propose a new SMC algorithm, named SMC-GM. For this new algorithm, we obtain a convergence theorem that allows Gaussian priors, illustrating that the sequential particle filter actually reproduces the true posterior distribution. Furthermore, the proposed new algorithm is rigorously defined on the infinite-dimensional function space, naturally exhibiting the discretization-invariant property. Numerical experiments demonstrate that the new approach has a strong ability to probe the multi-modality of the posterior, significantly reduces the computational burden, and numerically exhibits the discretization-invariant property (important for large-scale problems).|\n", "2503.16010": "|**2025-03-20**|**Patch-based learning of adaptive Total Variation parameter maps for blind image denoising**|Claudio Fantasia, Luca Calatroni, Xavier Descombes et.al.|[2503.16010](http://arxiv.org/abs/2503.16010)|null||We consider a patch-based learning approach defined in terms of neural networks to estimate spatially adaptive regularisation parameter maps for image denoising with weighted Total Variation and test it to situations when the noise distribution is unknown. As an example, we consider situations where noise could be either Gaussian or Poisson and perform preliminary model selection by a standard binary classification network. Then, we define a patch-based approach where at each image pixel an optimal weighting between TV regularisation and the corresponding data fidelity is learned in a supervised way using reference natural image patches upon optimisation of SSIM and in a sliding window fashion. Extensive numerical results are reported for both noise models, showing significant improvement w.r.t. results obtained by means of optimal scalar regularisation.|\n", "2503.15994": "|**2025-03-20**|**A framework for efficient reduced order modelling in the Julia programming language**|Nicholas Mueller, Santiago Badia et.al.|[2503.15994](http://arxiv.org/abs/2503.15994)|null|14 pages, 6 figures|In this paper we propose ROManifolds, a Julia-based package geared towards the numerical approximation of parameterized partial differential equations (PDEs) with a rich set of linear reduced order models (ROMs). The library favors extendibility and productivity, thanks to an expressive high level API, and the efficiency attained by the Julia just-in-time compiler. The implementation of the package is PDE agnostic, meaning that the same code can be used to solve a wide range of equations, including linear, nonlinear, single-field, multi-field, steady and unsteady problems. We highlight the main innovations of ROManifolds, we detail its implementation principles, we introduce its building blocks by providing usage examples, and we solve a fluid dynamics problem described by the Navier-Stokes equations in a 3d geometry.|\n", "2503.15959": "|**2025-03-20**|**Orbital-Free Density Functional Theory for Periodic Solids: Construction of the Pauli Potential**|Sangita Majumdar, Zekun Shi, Giovanni Vignale et.al.|[2503.15959](http://arxiv.org/abs/2503.15959)|null||The practical success of density functional theory (DFT) is largely credited to the Kohn-Sham approach, which enables the exact calculation of the non-interacting electron kinetic energy via an auxiliary noninteracting system. Yet, the realization of DFT's full potential awaits the discovery of a direct link between the electron density, $n$, and the non-interacting kinetic energy, $T_{S}[n]$. In this work, we address two key challenges towards this objective. First, we introduce a new algorithm for directly solving the constrained minimization problem yielding $T_{S}[n]$ for periodic densities -- a class of densities that, in spite of its central importance for materials science, has received limited attention in the literature. Second, we present a numerical procedure that allows us to calculate the functional derivative of $T_{S}[n]$ with respect to the density at constant electron number, also known as the Kohn-Sham potential $V_{S}[n](\\rv)$. Lastly, the algorithm is augmented with a subroutine that computes the ``derivative discontinuity\", i.e., the spatially uniform jump in $V_{S}[n](\\rv)$ which occurs upon increasing or decreasing the total number of electrons. This feature allows us to distinguish between ``insulating\" and ``conducting\" densities for non interacting electrons. The code integrates key methodological innovations, such as the use of an adaptive basis set (``equidensity orbitals\") for wave function expansion and the QR decomposition to accelerate the implementation of the orthogonality constraint. Notably, we derive a closed-form expression for the Pauli potential in one dimension, expressed solely in terms of the input density, without relying on Kohn-Sham eigenvalues and eigenfunctions. We validate this method on one-dimensional periodic densities, achieving results within ``chemical accuracy\".|\n", "2503.15912": "|**2025-03-20**|**Normal and inverse magnetocaloric effects in structurally disordered Laves phase Y$_{1-x}$Gd$_{x}$Co$_{2}$ (0 $\\leq$ x $\\leq$ 1) compounds**|Natalia Pierunek, Zbigniew \u015aniadecki, Miros\u0142aw Werwi\u0144ski et.al.|[2503.15912](http://arxiv.org/abs/2503.15912)|null||Magnetic and magnetocaloric properties of Y$_{1-x}$Gd$_{x}$Co$_{2}$ compounds, where x = 0.2, 0.4, 0.6, 0.8 and 1.0, were investigated experimentally and theoretically. Crystal structures were characterized by X-ray diffraction (Rietveld analysis) and investigated samples possess the MgCu$_{2}$-type single phase with Fd-3m space group. Melt-spinning process introduced a chemical and topological disorder, which directly affected the magnetic properties. Refrigerant capacity (RC), strictly connected to the full width at half maximum $\\delta$TFWHM of the $\\Delta$S$_M$(T) curve and the maximum of magnetic entropy changes $\\Delta$S$_{Mpk}$(T)(T,$\\Delta$H), increases from 29 to 148 J/kg with replacement of Y by Gd atoms from x = 0.2 to x = 0.8. RC and $\\delta$TFWHM indicate the presence of disorder. Temperature dependences of magnetic entropy change $\\Delta$S$_M$(T,$\\Delta$H) and RC were measured in as-quenched and annealed state for Y$_{0.4}$Gd$_{0.6}$Co$_{2}$. This particular composition was chosen for detailed investigation mainly due to its Curie point (T$_C$ = 282 K), which is close to the room temperature. After isothermal annealing ($\\tau_a$ = 60 min, Ta = 700$^o$C) RC decreased from 122 to 104 J/kg, which clearly indicates the homogenization of the heat treated sample. Furthermore, observed inverse magnetocaloric effect is associated with the presence of antiferromagnetically coupled Gd and Co magnetic moments. The phase transition temperature increases with increasing Gd content from 74 to 407 K for Y$_{0.8}$Gd$_{0.2}$Co$_{2}$ and GdCo2, respectively. Within the FPLO-LDA DFT method, the non-magnetic ground state for YCo$_{2}$ and the magnetic ground state for GdCo$_{2}$ are predicted in agreement with experiment. The dependence of calculated total and species-resolved magnetic moments on Gd concentration reasonably agrees with available experimental data.|\n", "2503.17357": "|**2025-03-21**|**Filtered Rayleigh-Ritz is all you need**|Ryan Abbott, Daniel C. Hackett, George T. Fleming et.al.|[2503.17357](http://arxiv.org/abs/2503.17357)|null|22+7 pages, 0 figures, 1 table|Recent work has shown that the (block) Lanczos algorithm can be used to extract approximate energy spectra and matrix elements from (matrices of) correlation functions in quantum field theory, and identified exact coincidences between Lanczos analysis methods and others. In this work, we note another coincidence: the Lanczos algorithm is equivalent to the well-known Rayleigh-Ritz method applied to Krylov subspaces. Rayleigh-Ritz provides optimal eigenvalue approximations within subspaces; we find that spurious-state filtering allows these optimality guarantees to be retained in the presence of statistical noise. We explore the relation between Lanczos and Prony's method, their block generalizations, generalized pencil of functions (GPOF), and methods based on the generalized eigenvalue problem (GEVP), and find they all fall into a larger \"Prony-Ritz equivalence class\", identified as all methods which solve a finite-dimensional spectrum exactly given sufficient correlation function (matrix) data. This equivalence allows simpler and more numerically stable implementations of (block) Lanczos analyses.|\n", "2503.17314": "|**2025-03-21**|**Numerical Investigation of Preferential Flow Paths in Enzymatically Induced Calcite Precipitation supported by Bayesian Model Analysis**|Rebecca Kohlhaas, Johannes Hommel, Felix Weinhardt et.al.|[2503.17314](http://arxiv.org/abs/2503.17314)|null||The usability of enzymatically induced calcium carbonate precipitation (EICP) as a method for altering porous-media properties, soil stabilization, or biocementation depends on our ability to predict the spatial distribution of the precipitated calcium carbonate in porous media. While current REV-scale models are able to reproduce the main features of laboratory experiments, they neglect effects like the formation of preferential flow paths and the appearance of multiple polymorphs of calcium carbonate with differing properties. We show that extending an existing EICP model by the conceptual assumption of a mobile precipitate, amorphous calcium carbonate (ACC), allows for the formation of preferential flow paths when the initial porosity is heterogeneous. We apply sensitivity analysis and Bayesian inference to gain an understanding of the influence of characteristic parameters of ACC that are uncertain or unknown and compare two variations of the model based on different formulations of the ACC detachment term to analyse the plausibility of our hypothesis. An arbitrary Polynomial Chaos (aPC) surrogate model is trained based on the full model and used to reduce the computational cost of this study.|\n", "2503.17283": "|**2025-03-21**|**Energy Efficiency trends in HPC: what high-energy and astrophysicists need to know**|Estela Suarez, Jorge Amaya, Martin Frank et.al.|[2503.17283](http://arxiv.org/abs/2503.17283)|null||The growing energy demands of HPC systems have made energy efficiency a critical concern for system developers and operators. However, HPC users are generally less aware of how these energy concerns influence the design, deployment, and operation of supercomputers even though they experience the consequences. This paper examines the implications of HPC's energy consumption, providing an overview of current trends aimed at improving energy efficiency. We describe how hardware innovations such as energy-efficient processors, novel system architectures, power management techniques, and advanced scheduling policies do have a direct impact on how applications need to be programmed and executed on HPC systems. For application developers, understanding how these new systems work and how to analyse and report the performances of their own software is critical in the dialog with HPC system designers and administrators. The paper aims to raise awareness about energy efficiency among users, particularly in the high energy physics and astrophysics domains, offering practical advice on how to analyse and optimise applications to reduce their energy consumption without compromising on performance.|\n", "2503.17265": "|**2025-03-21**|**Learning to Solve Related Linear Systems**|Disha Hegde, Jon Cockayne et.al.|[2503.17265](http://arxiv.org/abs/2503.17265)|null||Solving multiple parametrised related systems is an essential component of many numerical tasks. Borrowing strength from the solved systems and learning will make this process faster. In this work, we propose a novel probabilistic linear solver over the parameter space. This leverages information from the solved linear systems in a regression setting to provide an efficient posterior mean and covariance. We advocate using this as companion regression model for the preconditioned conjugate gradient method, and discuss the favourable properties of the posterior mean and covariance as the initial guess and preconditioner. We also provide several design choices for this companion solver. Numerical experiments showcase the benefits of using our novel solver in a hyperparameter optimisation problem.|\n", "2503.17234": "|**2025-03-21**|**High Accuracy Techniques Based Adaptive Finite Element Methods for Elliptic PDEs**|Jingjing Xiao, Ying Liu, Nianyu Yi et.al.|[2503.17234](http://arxiv.org/abs/2503.17234)|null||This paper aims to develop an efficient adaptive finite element method for the second-order elliptic problem. Although the theory for adaptive finite element methods based on residual-type a posteriori error estimator and bisection refinement has been well established, in practical computations, the use of non-asymptotic exact of error estimator and the excessive number of adaptive iteration steps often lead to inefficiency of the adaptive algorithm. We propose an efficient adaptive finite element method based on high-accuracy techniques including the superconvergence recovery technique and high-quality mesh optimization. The centroidal Voronoi Delaunay triangulation mesh optimization is embedded in the mesh adaption to provide high-quality mesh, and then assure that the superconvergence property of the recovered gradient and the asymptotical exactness of the error estimator. A tailored adaptive strategy, which could generate high-quality meshes with a target number of vertices, is developed to ensure the adaptive computation process terminated within $7$ steps. The effectiveness and robustness of the adaptive algorithm is numerically demonstrated.|\n", "2503.17190": "|**2025-03-21**|**Babu\u0161ka's paradox in a nonlinear bending model**|S\u00f6ren Bartels, Andrea Bonito, Peter Hornung et.al.|[2503.17190](http://arxiv.org/abs/2503.17190)|null||The Babu\\v{s}ka or plate paradox concerns the failure of convergence when a domain with curved boundary is approximated by polygonal domains in linear bending problems with simple support boundary conditions. It can be explained via a boundary integral representation of the total Gaussian curvature that is part of the Kirchhoff--Love bending energy. It is shown that the paradox also occurs for a nonlinear bending-folding model which enforces vanishing Gaussian curvature. A simple remedy that is compatible with simplicial finite element methods to avoid wrong convergence is devised.|\n", "2503.17145": "|**2025-03-21**|**Numerical Simulations of Fully Eulerian Fluid-Structure Contact Interaction using a Ghost-Penalty Cut Finite Element Approach**|Stefan Frei, Tobias Knoke, Marc C. Steinbach et.al.|[2503.17145](http://arxiv.org/abs/2503.17145)|null|29 pages, 8 figures, 3 tables|In this work, we develop a cut-based unfitted finite element formulation for solving nonlinear, nonstationary fluid-structure interaction with contact in Eulerian coordinates. In the Eulerian description fluid flow modeled by the incompressible Navier-Stokes equations remains in Eulerian coordinates, while elastic solids are transformed from Lagrangian coordinates into the Eulerian system. A monolithic description is adopted. For the spatial discretization, we employ an unfitted finite element method with ghost penalties based on inf-sup stable finite elements. To handle contact, we use a relaxation of the contact condition in combination with a unified Nitsche approach that takes care implicitly of the switch between fluid-structure interaction and contact conditions. The temporal discretization is based on a backward Euler scheme with implicit extensions of solutions at the previous time step. The nonlinear system is solved with a semi-smooth Newton's method with line search. Our formulation, discretization and implementation are substantiated with an elastic falling ball that comes into contact with the bottom boundary, constituting a challenging state-of-the-art benchmark.|\n", "2503.17012": "|**2025-03-21**|**Learning Non-Ideal Single Vortex Flows Using the Differentiable Vortex Particle Method**|Ziqi Ji, Gang Du, Penghao Duan et.al.|[2503.17012](http://arxiv.org/abs/2503.17012)|null||This study extends the differentiable vortex particle method (DVPM) beyond idealized flow scenarios to encompass more realistic, non-ideal conditions, including viscous flow and flow subjected to non-conservative body forces. We establish the Lamb-Oseen vortex as a benchmark case, representing a fundamental viscous single vortex flow in fluid mechanics. This selection offers significant analytical advantages, as the Lamb-Oseen vortex possesses an exact analytical solution derived from the Navier-Stokes (NS) equations, thereby providing definitive ground truth data for training and validation purposes. Through rigorous evaluation across a spectrum of Reynolds numbers, we demonstrate that DVPM achieves superior accuracy in modeling the Lamb-Oseen vortex compared to conventional convolutional neural networks (CNNs) and physics-informed neural networks (PINNs). Our results substantiate DVPM's robust capabilities in modeling non-ideal single vortex flows, establishing its distinct advantages over contemporary deep learning methodologies in fluid dynamics applications. The dataset and source code are publicly available on GitHub at the following link: https://github.com/jh36714753/Learning_Non-Ideal_Single_Vortex_Flows.|\n", "2503.16968": "|**2025-03-21**|**Modelling Material Injection Into Porous Structures With the Theory of Porous Media Under Non-isothermal Conditions**|Jan-S\u00f6ren L. V\u00f6lter, Zubin Trivedi, Tim Ricken et.al.|[2503.16968](http://arxiv.org/abs/2503.16968)|null|21 pages, 13 figures|In this work, the Theory of Porous Media (TPM) is employed to model percutaneous vertebroplasty, a medical procedure in which acrylic cement is injected into cancellous vertebral bone. Previously, isothermal macroscale models have been derived to describe this material injection and the mechanical interactions which arise. However, the temperature of the injected cement is typically below the human body temperature, necessitating the extension of these models to the non-isothermal case. Following the modelling principles of the TPM and considering local thermal non-equilibrium conditions, our model introduces three energy balances as well as additional constitutive relations. If restricted to local thermal equilibrium conditions, our model equations are in agreement with other examples of TPM-based models. We observe that our model elicits physically reasonable behaviour in numerical simulations which employ parameter values and initial and boundary conditions relevant for our application. Noting that we neglect capillary effects, we claim our model to be thermodynamically consistent despite the employment of simplifying assumptions during its derivation, such as the Coleman and Noll procedure.|\n", "2503.16877": "|**2025-03-21**|**A fourth-order cut-cell method for solving the two-dimensional advection-diffusion equation with moving boundaries**|Kaiyi Liang, Yuke Zhu, Jiyu Liu et.al.|[2503.16877](http://arxiv.org/abs/2503.16877)|null||We propose a fourth-order cut-cell method for solving the two-dimensional advection-diffusion equation with moving boundaries on a Cartesian grid. We employ the ARMS technique to give an explicit and accurate representation of moving boundaries, and introduce a cell-merging technique to overcome discontinuities caused by topological changes in cut cells and the small cell problem. We use a polynomial interpolation technique base on poised lattice generation to achieve fourth-order spatial discretization, and use a fourth-order implicit-explicit Runge-Kutta scheme for time integration. Numerical tests are performed on various moving regions, with advection velocity both matching and differing from boundary velocity, which demonstrate the fourth-order accuracy of the proposed method.|\n", "2503.16827": "|**2025-03-21**|**Discontinuous Galerkin Representation of the Maxwell-J\u00fcttner Distribution**|Grant Johnson, Ammar Hakim, James Juno et.al.|[2503.16827](http://arxiv.org/abs/2503.16827)|null||Kinetic simulations of relativistic gases and plasmas are critical for understanding diverse astrophysical and terrestrial systems, but the accurate construction of the relativistic Maxwellian, the Maxwell-J\\\"uttner (MJ) distribution, on a discrete simulation grid is challenging. Difficulties arise from the finite velocity bounds of the domain, which may not capture the entire distribution function, as well as errors introduced by projecting the function onto a discrete grid. Here we present a novel scheme for iteratively correcting the moments of the projected distribution applicable to all grid-based discretizations of the relativistic kinetic equation. In addition, we describe how to compute the needed nonlinear quantities, such as Lorentz boost factors, in a discontinuous Galerkin (DG) scheme through a combination of numerical quadrature and weak operations. The resulting method accurately captures the distribution function and ensures that the moments match the desired values to machine precision.|\n", "2503.16792": "|**2025-03-21**|**Numerical simulation of wormhole propagation with the mixed hybridized discontinuous Galerkin finite element method**|Jiansong Zhang, Jiang Zhu, Yiming Wang et.al.|[2503.16792](http://arxiv.org/abs/2503.16792)|null|18pages, 2 figures|The acid treatment of carbonate reservoirs is a widely employed technique for enhancing the productivity of oil and gas reservoirs. In this paper, we present a novel combined hybridized mixed discontinuous Galerkin (HMDG) finite element method to simulate the dissolution process near the wellbore, commonly referred to as the wormhole phenomenon. The primary contribution of this work lies in the application of hybridization techniques to both the pressure and concentration equations. Additionally, an upwind scheme is utilized to address convection-dominant scenarios, and a ``cut-off\" operator is introduced to maintain the boundedness of porosity. Compared to traditional discontinuous Galerkin methods, the proposed approach results in a global system with fewer unknowns and sparser stencils, thereby significantly reducing computational costs. We analyze the existence and uniqueness of the new combined method and derive optimal error estimates using the developed technique. Numerical examples are provided to validate the theoretical analysis.|\n", "2503.16784": "|**2025-03-21**|**Multi-property directed generative design of inorganic materials through Wyckoff-augmented transfer learning**|Shuya Yamazaki, Wei Nong, Ruiming Zhu et.al.|[2503.16784](http://arxiv.org/abs/2503.16784)|null||Accelerated materials discovery is an urgent demand to drive advancements in fields such as energy conversion, storage, and catalysis. Property-directed generative design has emerged as a transformative approach for rapidly discovering new functional inorganic materials with multiple desired properties within vast and complex search spaces. However, this approach faces two primary challenges: data scarcity for functional properties and the multi-objective optimization required to balance competing tasks. Here, we present a multi-property-directed generative framework designed to overcome these limitations and enhance site symmetry-compliant crystal generation beyond P1 (translational) symmetry. By incorporating Wyckoff-position-based data augmentation and transfer learning, our framework effectively handles sparse and small functional datasets, enabling the generation of new stable materials simultaneously conditioned on targeted space group, band gap, and formation energy. Using this approach, we identified previously unknown thermodynamically and lattice-dynamically stable semiconductors in tetragonal, trigonal, and cubic systems, with bandgaps ranging from 0.13 to 2.20 eV, as validated by density functional theory (DFT) calculations. Additionally, we assessed their thermoelectric descriptors using DFT, indicating their potential suitability for thermoelectric applications. We believe our integrated framework represents a significant step forward in generative design of inorganic materials.|\n", "2503.16765": "|**2025-03-21**|**A thermodynamically consistent phase-field model for mass transport with interfacial reaction and deformation**|Zhaoyang Wang, Huaxiong Huang, Ping Lin et.al.|[2503.16765](http://arxiv.org/abs/2503.16765)|null||In this paper, a thermodynamically consistent phase-field model is proposed to describe the mass transport and reaction processes of multiple species in a fluid. A key feature of this model is that reactions between different species occur only at the interface, and may induce deformation of the interface. For the governing equations derived based on the energy variational method, we propose a structure-preserving numerical scheme that satisfies the mass conservation and energy dissipation laws at the discrete level. Furthermore, we carry out a rigorous error analysis of the time-discrete scheme for a simplified case. A series of numerical experiments are conducted to validate the effectiveness of the model as well as the accuracy and stability of the scheme. In particular, we simulate microvessels with straight and bifurcated structures to illustrate the risk of microaneurysm formation.|\n", "2503.16717": "|**2025-03-20**|**Random-sketching Techniques to Enhance the Numerically Stability of Block Orthogonalization Algorithms for s-step GMRES**|Ichitaro Yamazaki, Andrew J. Higgins, Erik G. Boman et.al.|[2503.16717](http://arxiv.org/abs/2503.16717)|null|14 pages|We integrate random sketching techniques into block orthogonalization schemes needed for s-step GMRES. The resulting block orthogonalization schemes generate the basis vectors whose overall orthogonality error is bounded by machine precision as long as each of the corresponding block vectors are numerically full rank. We implement these randomized block orthogonalization schemes using standard distributed-memory linear algebra kernels for s-step GMRES available in the Trilinos software packages. Our performance results on the Perlmutter supercomputer (with four NVIDIA A100 GPUs per node) demonstrate that these randomized techniques can enhance the numerical stability of the orthogonalization and overall solver, without a significant increase in the execution time.|\n", "2503.16665": "|**2025-03-20**|**Evolution of Shock Structures and QPOs After Halting BHL Accretion onto Kerr Black Hole**|Orhan Donmez et.al.|[2503.16665](http://arxiv.org/abs/2503.16665)|null|29 pages, 7 figures, 3 Tables. Suggestions and comments are welcome|One of the mechanisms responsible for disk formation around the black holes is Bondi-Hoyle-Lyttleton (BHL) accretion.The fact that BHL accretion can be interrupted by various astrophysical phenomena, such as stellar winds or astrophysical jets, makes it crucial to study the behavior of shock cones formed by BHL accretion around black holes once the accretion process is halted. Investigating the new plasma structures that emerge in these scenarios can provide insights into observational results. In this context, a new plasma structure forming around the Kerr black hole has been numerically modeled as a function of the black hole spin parameter and the asymptotic velocity of BHL accretion. The numerical analysis revealed that high spin (a/M=0.9) and supersonic flow ( M > 1) are necessary conditions for low-frequency quasi-periodic oscillations (LFQPOs) formation. On the other hand, the fundamental mode of the high-frequency quasi-periodic oscillations (HFQPOs) are found to be independent of both the black hole spin and asymptotic velocity and are instead governed by general relativistic effects. Additionally, the study demonstrated that for 3:2 and 2:1 resonance states to form, nonlinear couplings needs to be occurred when the black hole rotates rapidly. These couplings produce harmonic frequencies, providing an explanation for the observed quasi-periodic oscillation (QPO) resonances in black hole binaries. These findings align with precession models and nonlinear resonance models, both of which play a crucial role in QPO generation. Finally, the LFQPOs and HFQPOs obtained from numerical simulations are consistent with the observed QPO frequencies in the microquasars GRS 1915+105 and XTE J1550-564, as well as in the AGN REJ1034+396, which harbors a supermassive black hole at its center.|\n", "2503.19847": "|**2025-03-25**|**Ab-initio simulation of excited-state potential energy surfaces with transferable deep quantum Monte Carlo**|Zeno Sch\u00e4tzle, P. Bern\u00e1t Szab\u00f3, Alice Cuzzocrea et.al.|[2503.19847](http://arxiv.org/abs/2503.19847)|null|21 pages, 4 figures|The accurate quantum chemical calculation of excited states is a challenging task, often requiring computationally demanding methods. When entire ground and excited potential energy surfaces (PESs) are desired, e.g., to predict the interaction of light excitation and structural changes, one is often forced to use cheaper computational methods at the cost of reduced accuracy. Here we introduce a novel method for the geometrically transferable optimization of neural network wave functions that leverages weight sharing and dynamical ordering of electronic states. Our method enables the efficient prediction of ground and excited-state PESs and their intersections at the highest accuracy, demonstrating up to two orders of magnitude cost reduction compared to single-point calculations. We validate our approach on three challenging excited-state PESs, including ethylene, the carbon dimer, and the methylenimmonium cation, indicating that transferable deep-learning QMC can pave the way towards highly accurate simulation of excited-state dynamics.|\n", "2503.19814": "|**2025-03-25**|**Machine Learning and Data-Driven Methods in Computational Surface and Interface Science**|Lukas H\u00f6rmann, Wojciech G. Stark, Reinhard J. Maurer et.al.|[2503.19814](http://arxiv.org/abs/2503.19814)|null|27 pages, 5 figures|Nanoscale design of surfaces and interfaces is essential for modern technologies like organic LEDs, batteries, fuel cells, superlubricating surfaces, and heterogeneous catalysis. However, these systems often exhibit complex surface reconstructions and polymorphism, with properties influenced by kinetic processes and dynamic behavior. A lack of accurate and scalable simulation tools has limited computational modeling of surfaces and interfaces. Recently, machine learning and data-driven methods have expanded the capabilities of theoretical modeling, enabling, for example, the routine use of machine-learned interatomic potentials to predict energies and forces across numerous structures. Despite these advances, significant challenges remain, including the scarcity of large, consistent datasets and the need for computational and data-efficient machine learning methods. Additionally, a major challenge lies in the lack of accurate reference data and electronic structure methods for interfaces. Density Functional Theory, while effective for bulk materials, is less reliable for surfaces, and too few accurate experimental studies on interface structure and stability exist. Here, we will sketch the current state of data-driven methods and machine learning in computational surface science and provide a perspective on how these methods will shape the field in the future.|\n", "2503.19807": "|**2025-03-25**|**Probabilistic combination of loads in topology optimization designs via cumulative damage criteria**|Luis Irastorza-Valera, Luis Saucedo-Mora et.al.|[2503.19807](http://arxiv.org/abs/2503.19807)|null||Topology optimization (TO) is a well-established methodology for structural design under user-defined constraints, e.g. minimum volume and maximum stiffness. However, such methods have traditionally been applied to static, deterministic loading, in which modulus, position and direction are known and invariant. This is against the probabilistic load combination used in the structural engineering designs, and entails two important shortcomings.   The first one is related to maintenance and reliability: static loading fails to consider naturally occurring uncertainties in the loading process, measurements or regular service; also ignoring (quantitatively) unforeseen phenomena such as vibrations, and the material's behavior is assumed linear isotropic, ignoring fatigue, plasticity and anisotropy in functionally-graded materials. The second one concerns optimality itself: often times, the structure presented as \"optimal\" in fact over-estimates loading and thus wastes material by exceeding its real needs and/or distributing it poorly throughout the design dominion.   In this article, a probabilistic framework is presented: uncertain and pseudo-dynamic loading is introduced to create robust topologies via a reinforced SIMP scheme with embedded penalization addressing fatigue damage, layer direction, mechanical response (traction/compression) and yield limit (von Mises equivalent stress). This computationally efficient framework is applied to various loading scenarios, generating diverse designs for the same volume fraction constraint with improved and more realistic performances. Under the proposed method, if loads are permanent and damage isotropic, the methodology converges to the traditional (deterministic) topological optimization results. Future ramifications of this work are pondered, especially regarding metamaterial design.|\n", "2503.19806": "|**2025-03-25**|**New analytic formulae for memory and prediction functions in reservoir computers with time delays**|Peyton Mullarkey, Sarah Marzen et.al.|[2503.19806](http://arxiv.org/abs/2503.19806)|null|9 pages, 4 figures|Time delays increase the effective dimensionality of reservoirs, thus suggesting that time delays in reservoirs can enhance their performance, particularly their memory and prediction abilities. We find new closed-form expressions for memory and prediction functions of linear time-delayed reservoirs in terms of the power spectrum of the input and the reservoir transfer function. We confirm this relationship numerically for some time-delayed reservoirs using simulations, including when the reservoir can be linearized but is actually nonlinear. Finally, we use these closed-form formulae to address the utility of multiple time delays in linear reservoirs in order to perform memory and prediction, finding similar results to previous work on nonlinear reservoirs. We hope these closed-form formulae can be used to understand memory and predictive capabilities in time-delayed reservoirs.|\n", "2503.19784": "|**2025-03-25**|**Adaptive refinement in defeaturing problems via an equilibrated flux a posteriori error estimator**|Annalisa Buffa, Denise Grappein, Rafael V\u00e1zquez et.al.|[2503.19784](http://arxiv.org/abs/2503.19784)|null||An adaptive refinement strategy, based on an equilibrated flux a posteriori error estimator, is proposed in the context of defeaturing problems. Defeaturing consists in removing features from complex domains in order to ease the meshing process, and to reduce the computational burden of simulations. It is a common procedure, for example, in computer aided design for simulation based manufacturing. However, depending on the problem at hand, the effect of geometrical simplification on the accuracy of the solution may be detrimental. The proposed adaptive strategy is hence twofold: starting from a defeatured geometry it allows both for standard mesh refinement and geometrical refinement, which consists in choosing, at each step, which features need to be included into the geometry in order to significantly increase the accuracy of the solution. With respect to other estimators that were previously proposed in the context of defeaturing, the use of an equilibrated flux reconstruction allows us to avoid the evaluation of the numerical flux on the boundary of features. This makes the estimator and the adaptive strategy particularly well-suited for finite element discretizations, in which the numerical flux is typically discontinuous across element edges. The inclusion of the features during the adaptive process is tackled by a CutFEM strategy, in order to preserve the non conformity of the mesh to the feature boundary and never remesh the computational domain as the features are added. Hence, the estimator also accounts for the error introduced by weakly imposing the boundary conditions on the boundary of the added features.|\n", "2503.19732": "|**2025-03-25**|**Grid-Free Evaluation of Phonon-Limited Electronic Relaxation Times and Transport Properties**|Nenad Vukmirovi\u0107 et.al.|[2503.19732](http://arxiv.org/abs/2503.19732)|null||Present calculations of electrical transport properties of materials require evaluations of electron-phonon coupling constants on dense predefined grids of electron and phonon momenta and performing the sums over these momenta. In this work, we present the methodology for calculation of carrier relaxation times and electrical transport properties without the use of a predefined grid. The relaxation times are evaluated by integrating out the delta function that ensures energy conservation and performing an average over the angular components of phonon momentum. The charge carrier mobility is then evaluated as a sum over appropriately sampled electronic momenta. We illustrate our methodology by applying to the Fr{\\\"o}hlich model and to a real semiconducting material ZnTe. We find that rather accurate results can be obtained with a modest number of electron and phonon momenta, on the order of one hundred each, regardless of the carrier effective mass.|\n", "2503.19723": "|**2025-03-25**|**Role of spatial embedding and planarity in shaping the topology of the Street Networks**|Ritish Khetarpal, Aradhana Singh et.al.|[2503.19723](http://arxiv.org/abs/2503.19723)|null|17 pages, 8 main figures, 6 supplementary figures|The topology of city street networks (SNs) is constrained by spatial embedding, requiring non-crossing links and preventing random node placement or overlap. Here, we analyzed SNs of $33$ Indian cities to explore how the spatial embedding and the planarity jointly shape their topology. Overall, we found that all the studied SNs have small-world properties with higher clustering and efficiency. The efficiency of the empirical networks is even higher than that of the corresponding degree of preserved random networks. This increased efficiency can be explained by Dijkstra's path-length distribution, which closely fits a right-skewed normal or log-normal distribution. Moreover, we observed that the connectivity of the streets is length-dependent: the smaller streets connect preferably to the smaller streets, while longer streets tend to connect with the longer counterparts. This length-dependent connectivity is more profound in the empirical SNs than in the corresponding degree preserved random and random planar networks. However, planar networks maintaining the empirical spatial coordinates replicate the connectivity behavior of empirical SNs, highlighting the influence of spatial embedding. Moreover, the robustness of the cities in terms of resilience to random errors and targeted attacks is independent of the SN's size, indicating other factors, such as geographical constraints, substantially influence network stability.|\n", "2503.19720": "|**2025-03-25**|**Defects and Impurity Properties of VN precipitates in ARAFM Steels: Modelling using a Universal Machine Learning Potential and Experimental Validation**|R. S. Stroud, C. Reynolds, T. Melichar et.al.|[2503.19720](http://arxiv.org/abs/2503.19720)|null|14 pages, 4 figures|VN precipitates used to strengthen ARAFM steels for fusion applications, have been shown to undergo dissolution under high Fe ion irradiation doses of 100 dpa at dose rates of 10^-3 dpa/s at 600 C. Here, point defects and solute substitutions have been studied using atom probe tomography (APT), universal machine learning interatomic potentials (uMLIPs), and density functional theory. Through a combination of transmission electron microscopy (TEM), APT, and atomic scale calculations, N-vacancies and substitutional Cr are found to be present in VN precipitates prior to irradiation. Ternary convex hulls were calculated for ten VNX (X=Cr, Fe, C, Si, Mn, W, Ta, B, S, P) systems with an uMLIP. These calculations predict Fe, P, Mn, and Si to be unstable solutes in the VN precipitate. Therefore, Fe, P, Mn and Si implantation via collision cascades is found to be a possible mechanism driving dissolution of VN.|\n", "2503.19701": "|**2025-03-25**|**Enhanced gradient recovery-based a posteriori error estimator and adaptive finite element method for elliptic equations**|Ying Liu, Jingjing Xiao, Nianyu Yi et.al.|[2503.19701](http://arxiv.org/abs/2503.19701)|null||Recovery type a posteriori error estimators are popular, particularly in the engineering community, for their computationally inexpensive, easy to implement, and generally asymptotically exactness. Unlike the residual type error estimators, one can not establish upper and lower a posteriori error bounds for the classical recovery type error estimators without the saturation assumption. In this paper, we first present three examples to show the unsatisfactory performance in the practice of standard residual or recovery-type error estimators, then, an improved gradient recovery-based a posteriori error estimator is constructed. The proposed error estimator contains two parts, one is the difference between the direct and post-processed gradient approximations, and the other is the residual of the recovered gradient. The reliability and efficiency of the enhanced estimator are derived. Based on the improved recovery-based error estimator and the newest-vertex bisection refinement method with a tailored mark strategy, an adaptive finite element algorithm is designed. We then prove the convergence of the adaptive method by establishing the contraction of gradient error plus oscillation. Numerical experiments are provided to illustrate the asymptotic exactness of the new recovery-based a posteriori error estimator and the high efficiency of the corresponding adaptive algorithm.|\n", "2503.19687": "|**2025-03-25**|**Excitability and travelling waves in renewable active matter**|Abhishek M, Ankit Dhanuka, Deb Sankar Banerjee et.al.|[2503.19687](http://arxiv.org/abs/2503.19687)|null|28 pages, 12 figures|Activity and renewability are distinctive features of living matter, and constitute a new class of materials that we term renewable active matter. A striking example is the cell cytoskeleton, where myosin filaments bind to the actin meshwork, apply contractile stresses and undergo continual stress/strain dependent turnover, thus acting as both force generators and sensors. As a consequence of nonreciprocity, arising from the independence of action and response, such living matter exhibits unusual mechanical properties like, segregation without attraction, fragility and force chains. Here we show that the interplay between activity and turnover gives rise to mechanical excitability in the form of travelling waves and pulses, and spatiotemporal chaos. We provide a systematic study of the nucleation, movement and shape of the travelling pulse, and present a boundary layer analysis to establish the existence of homoclinic orbits. Our analytical results are supported by detailed numerical analysis of the governing partial differential equations. This study has implications for the observed mechanical excitability in a variety of cellular contexts such as in isolated adherent cells and confluent cells within tissues.|\n", "2503.19684": "|**2025-03-25**|**Characteristic boundary conditions for Hybridizable Discontinuous Galerkin methods**|Jan Ellmenreich, Matteo Giacomini, Antonio Huerta et.al.|[2503.19684](http://arxiv.org/abs/2503.19684)|null||In this work we introduce the concept of characteristic boundary conditions (CBCs) within the framework of Hybridizable Discontinuous Galerkin (HDG) methods, including both the Navier-Stokes characteristic boundary conditions (NSCBCs) and a novel approach to generalized characteristic relaxation boundary conditions (GRCBCs). CBCs are based on the characteristic decomposition of the compressible Euler equations and are designed to prevent the reflection of waves at the domain boundaries. We show the effectiveness of the proposed method for weakly compressible flows through a series of numerical experiments by comparing the results with common boundary conditions in the HDG setting and reference solutions available in the literature. In particular, HDG with CBCs show superior performance minimizing the reflection of vortices at artificial boundaries, for both inviscid and viscous flows.|\n", "2503.19675": "|**2025-03-25**|**Tailoring nuclear spins order with defects: a Quantum-TCAD study**|Gaetano Calogero, Ioannis Deretzis, Giuseppe Fisicaro et.al.|[2503.19675](http://arxiv.org/abs/2503.19675)|null||The full design of relevant systems for quantum applications, ranging from quantum simulation to sensing, is presented using a combination of atomistic methods. A prototypical system features a two-dimensional ordered distribution of spins interacting with out-of-plane spin drivers/probes. It could be realized in wide-bandgap semiconductors through open-volume point defects and functionalized surfaces with low Miller indexes. We study the case of defect electron spins (driver / probe) interacting via hyperfine coupling with $S=1/2$ nuclear spins of H atoms chemisorbed onto \\hkl(001) and \\hkl(111) 3C-SiC surfaces. We simulate the system fabrication processes with super lattice kinetic Monte Carlo, demonstrating that epitaxial growth under time-dependent conditions is a viable method for achieving controlled abundance or depletion of near-surface point defects. Quantum features are evaluated by means of extensive numerical analysis at a full quantum mechanical level based on calibrated models of interacting spin systems. This analysis includes both stationary (relative stability of ordered states) and time-dependent (protocols) conditions, achieved varying the model parameters (in our case the atomic structure and the external field). We identify a rich scenario of metastable spin-waves in the quantum simulation setting. The interaction between protocols and variable system configurations could hinder the effectiveness of the preparation/measurement phases.|\n", "2503.19624": "|**2025-03-25**|**Derivative polynomials and infinite series for squigonometric functions**|Bart S. van Lith et.al.|[2503.19624](http://arxiv.org/abs/2503.19624)|null|23 pages, 3 figures|All squigonometric functions admit derivatives that can be expressed as polynomials of the squine and cosquine. We introduce a general framework that allows us to determine these polynomials recursively. We also provide an explicit formula for all coefficients of these polynomials. This also allows us to provide an explicit expression for the MacLaurin series coefficients of all squigonometric functions. We further discuss some methods that can compute the squigonometric functions up to any given tolerance over all of the real line.|\n", "2503.19620": "|**2025-03-25**|**Optimization through In-Context Learning and Iterative LLM Prompting for Nuclear Engineering Design Problems**|M. Rizki Oktavian, Anirudh Tunga, Amandeep Bakshi et.al.|[2503.19620](http://arxiv.org/abs/2503.19620)|null|Codes and data are available upon request|The optimization of nuclear engineering designs, such as nuclear fuel assembly configurations, involves managing competing objectives like reactivity control and power distribution. This study explores the use of Optimization by Prompting, an iterative approach utilizing large language models (LLMs), to address these challenges. The method is straightforward to implement, requiring no hyperparameter tuning or complex mathematical formulations. Optimization problems can be described in plain English, with only an evaluator and a parsing script needed for execution. The in-context learning capabilities of LLMs enable them to understand problem nuances, therefore, they have the potential to surpass traditional metaheuristic optimization methods. This study demonstrates the application of LLMs as optimizers to Boiling Water Reactor (BWR) fuel lattice design, showing the capability of commercial LLMs to achieve superior optimization results compared to traditional methods.|\n", "2503.19487": "|**2025-03-25**|**Asymptotic-preserving and positivity-preserving discontinuous Galerkin method for the semiconductor Boltzmann equation in the diffusive scaling**|Huan Ding, Liu Liu, Xinghui Zhong et.al.|[2503.19487](http://arxiv.org/abs/2503.19487)|null||In this paper, we develop an asymptotic-preserving and positivity-preserving discontinuous Galerkin (DG) method for solving the semiconductor Boltzmann equation in the diffusive scaling. We first formulate the diffusive relaxation system based on the even-odd decomposition method, which allows us to split into one relaxation step and one transport step. We adopt a robust implicit scheme that can be explicitly implemented for the relaxation step that involves the stiffness of the collision term, while the third-order strong-stability-preserving Runge-Kutta method is employed for the transport step. We couple this temporal scheme with the DG method for spatial discretization, which provides additional advantages including high-order accuracy, $h$-$p$ adaptivity, and the ability to handle arbitrary unstructured meshes. A positivity-preserving limiter is further applied to preserve physical properties of numerical solutions. The stability analysis using the even-odd decomposition is conducted for the first time. We demonstrate the accuracy and performance of our proposed scheme through several numerical examples.|\n", "2503.19483": "|**2025-03-25**|**Empirical Hyper Element Integration Method (EHEIM) with Unified Integration Criteria for Efficient Hyper Reduced FE$^2$ Simulations**|Nils Lange, Geralf H\u00fctter, Bjoern Kiefer et.al.|[2503.19483](http://arxiv.org/abs/2503.19483)|null||Numerical homogenization for mechanical multiscale modeling by means of the finite element method (FEM) is an elegant way of obtaining structure-property relations, if the behavior of the constituents of the lower scale is well understood. However, the computational costs of this so-called FE$^2$ method are so high that reduction methods are essential. While the construction of a reduced basis for the microscopic nodal displacements using proper orthogonal decomposition (POD) has become a standard technique, the reduction of the computational effort for the projected nodal forces, the so-called hyper reduction, is an additional challenge, for which different strategies have been proposed in the literature. The empirical cubature method (ECM), which has been proven to be very robust, implemented the conservation of the total volume is used as a constraint in the resulting optimization problem, while energy-based criteria have been proposed in other contributions.   The present contribution presents a unified integration criteria concept, involving the aforementioned criteria, among others. These criteria are used both with a Gauss point-based as well as with an element-based hyper reduction scheme, the latter retaining full compatibility with the common modular finite element framework. The methods are combined with a previously proposed clustered training strategy and a monolithic solver. Numerical examples empirically demonstrate that the additional criteria improve the accuracy for a given number of modes. Vice verse, less modes and thus lower computational costs are required to reach a given level of accuracy.|\n", "2503.19424": "|**2025-03-25**|**A linear, unconditionally stable, second order decoupled method for the nematic liquid crystal flows with SAV approach**|Ruonan Cao, Nianyu Yi et.al.|[2503.19424](http://arxiv.org/abs/2503.19424)|null||In this paper, we present a second order, linear, fully decoupled, and unconditionally energy stable scheme for solving the Erickson-Leslie model. This approach integrates the pressure correction method with a scalar auxiliary variable technique. We rigorously demonstrate the unconditional energy stability of the proposed scheme. Furthermore, we present several numerical experiments to validate its convergence order, stability, and computational efficiency.|\n", "2503.19379": "|**2025-03-25**|**Kernel compensation method for Maxwell eigenproblem with mimetic finite difference discretization**|Chenhao Jin, Yinhua Xia, Yan Xu et.al.|[2503.19379](http://arxiv.org/abs/2503.19379)|null|9 figures. This work has been accepted for publication in Numerical   Methods for Partial Differential Equations|We present a kernel compensation method for Maxwell eigenproblem for photonic crystals to avoid the infinite-dimensional kernels that cause many difficulties in the calculation of energy gaps. The quasi-periodic problem is first transformed into a periodic one on the cube by the Floquet-Bloch theory. Then the compensation operator is introduced in Maxwell's equation with the shifted curl operator. The discrete problem depends on the compatible discretization of the de Rham complex, which is implemented by the mimetic finite difference method in this paper. We prove that the compensation term exactly fills up the kernel of the original problem and avoids spurious eigenvalues. Also, we propose an efficient preconditioner and its FFT and multigrid solvers, which allow parallel computing. Numerical experiments for different three-dimensional lattices are performed to validate the accuracy and effectiveness of the method.|\n", "2503.19346": "|**2025-03-25**|**A Wong--Zakai resonance-based integrator for nonlinear Schr\u00f6dinger equation with white noise dispersion**|Jianbo Cui, Georg Maierhofer et.al.|[2503.19346](http://arxiv.org/abs/2503.19346)|null|39 pages|We introduce a novel approach to numerical approximation of nonlinear Schr\\\"odinger equation with white noise dispersion in the regime of low-regularity solutions. Approximating such solutions in the stochastic setting is particularly challenging due to randomized frequency interactions and presents a compelling challenge for the construction of tailored schemes. In particular, we design the first resonance-based schemes for this equation, which achieve provable convergence for solutions of much lower regularity than previously required. A crucial ingredient in this construction is the Wong--Zakai approximation of stochastic dispersive system, which introduces piecewise linear phases that capture nonlinear frequency interactions and can subsequently be approximated to construct resonance-based schemes. We prove the well-posedness of the Wong--Zakai approximated equation and establish its proximity to the original full stochastic dispersive system. Based on this approximation, we demonstrate an improved strong convergence rate for our new scheme, which exploits the stochastic nature of the dispersive terms. Finally, we provide numerical experiments underlining the favourable performance of our novel method in practice.|\n", "2503.19333": "|**2025-03-25**|**E-PINNs: Epistemic Physics-Informed Neural Networks**|Ashish S. Nair, Bruno Jacob, Amanda A. Howard et.al.|[2503.19333](http://arxiv.org/abs/2503.19333)|null|27 pages, 13 figures|Physics-informed neural networks (PINNs) have demonstrated promise as a framework for solving forward and inverse problems involving partial differential equations. Despite recent progress in the field, it remains challenging to quantify uncertainty in these networks. While approaches such as Bayesian PINNs (B-PINNs) provide a principled approach to capturing uncertainty through Bayesian inference, they can be computationally expensive for large-scale applications. In this work, we propose Epistemic Physics-Informed Neural Networks (E-PINNs), a framework that leverages a small network, the \\emph{epinet}, to efficiently quantify uncertainty in PINNs. The proposed approach works as an add-on to existing, pre-trained PINNs with a small computational overhead. We demonstrate the applicability of the proposed framework in various test cases and compare the results with B-PINNs using Hamiltonian Monte Carlo (HMC) posterior estimation and dropout-equipped PINNs (Dropout-PINNs). Our experiments show that E-PINNs provide similar coverage to B-PINNs, with often comparable sharpness, while being computationally more efficient. This observation, combined with E-PINNs' more consistent uncertainty estimates and better calibration compared to Dropout-PINNs for the examples presented, indicates that E-PINNs offer a promising approach in terms of accuracy-efficiency trade-off.|\n", "2503.21663": "|**2025-03-27**|**DiPolMol-Py: A Python package for calculations for $^2\u03a3$ ground-state molecules**|Bethan Humphreys, Alex J. Matthies, Hannah J. Williams et.al.|[2503.21663](http://arxiv.org/abs/2503.21663)|null|25 pages, 5 figures|We present the python package DiPolMol-Py, which can be used to calculate the rotational and hyperfine structure of $^2\\Sigma$ molecules. The calculations can be performed in the presence of dc magnetic fields, dc electric fields and far off-resonant optical fields. We additionally include functions to calculate the polarisability of the molecule and the transition dipole moment between different energy eigenstates. The package is applicable to many of the molecules which can be laser cooled, specifically the alkaline earth fluorides. We provide a constants file which includes many of the required literature values for CaF, SrF and BaF. Additional species can easily be added by updating this file.|\n", "2503.21658": "|**2025-03-27**|**Numerical Analysis of the Stability of Iron Dust Bunsen Flames**|Thijs Hazenberg, Daniel Braig, Johannes Mich et.al.|[2503.21658](http://arxiv.org/abs/2503.21658)|null|Submitted to ECM-PROCI track|This article presents numerical simulations of the response of an iron dust Bunsen flame to particle seeding changes. A validated numerical model is used to study the impact of particle seeding fluctuations on flame stability. Simulations are conducted for the Bunsen setup in the right-side up and up-side down configuration. No significant differences in flame response are identified in flame stability between the right-side up and up-side down configurations. We find that the Bunsen flame is surprisingly robust to abrupt changes in particle loading. The sudden change in particle loading does not excite any intrinsic instabilities in the flame. Based on our results, the iron dust flames are robust to imposed fluctuations. We hypothesize that this is due to the lack of a feedback mechanism between the burned temperature and the heat release rate. This mechanism is present in conventional, chemistry-driven, gaseous flames. However, such a mechanism is absent in iron dust flames because the combustion of individual iron particles is limited by oxygen diffusion, which is insensitive to temperature.|\n", "2503.21653": "|**2025-03-27**|**Strong convergence and stability of stochastic theta method for time-changed stochastic differential equations with local Lipschitz coefficients**|Jingwei Chen, Jun Ye, Jinwen Chen et.al.|[2503.21653](http://arxiv.org/abs/2503.21653)|null||In this paper, the stochastic theta (ST) method is investigated for a class of stochastic differential equations driven by a time-changed Brownian motion, whose coefficients are time-space-dependent and satisfy the local Lipschitz condition. It is proved that under the local Lipschitz and some additional assumptions, the ST method with $\\theta\\in[1/2,1]$ is strongly convergent. It is also obtained that, for all positive stepsizes, the ST method with $\\theta\\in[1/2,1]$ is asymptotically mean square stable under a coercivity condition. With some restrictions on the stepsize, the ST method with $\\theta\\in[0,1/2)$ is asymptotically mean square stable under a stronger assumption. Some numerical simulations are presented to illustrate the theoretical results.|\n", "2503.21626": "|**2025-03-27**|**Inverse Lax-Wendroff boundary treatment for solving conservation laws with finite difference HWENO methods**|Guangyao Zhu, Yan Jiang, Zhuang Zhao et.al.|[2503.21626](http://arxiv.org/abs/2503.21626)|null||This paper presents a novel inverse Lax-Wendroff (ILW) boundary treatment for finite difference Hermite weighted essentially non-oscillatory (HWENO) schemes to solve hyperbolic conservation laws on arbitrary geometries. The complex geometric domain is divided by a uniform Cartesian grid, resulting in challenge in boundary treatment. The proposed ILW boundary treatment could provide high order approximations of both solution values and spatial derivatives at ghost points outside the computational domain. Distinct from existing ILW approaches, our boundary treatment constructs the extrapolation via optimized through a least squares formulation, coupled with the spatial derivatives at the boundary obtained via the ILW procedure. Theoretical analysis indicates that compared with other ILW methods, our proposed one would require fewer terms by using the relatively complicated ILW procedure and thus improve computational efficiency while preserving accuracy and stability. The effectiveness and robustness of the method are validated through numerical experiments.|\n", "2503.21618": "|**2025-03-27**|**A shifted Laplace rational filter for large-scale eigenvalue problems**|Biyi Wang, Karl Meerbergen, Raf Vandebril et.al.|[2503.21618](http://arxiv.org/abs/2503.21618)|null||We present a rational filter for computing all eigenvalues of a symmetric definite eigenvalue problem lying in an interval on the real axis. The linear systems arising from the filter embedded in the subspace iteration framework, are solved via a preconditioned Krylov method.   The choice of the poles of the filter is based on two criteria. On the one hand, the filter should enhance the eigenvalues in the interval of interest, which suggests that the poles should be chosen close to or in the interval. On the other hand, the choice of poles has an important impact on the convergence speed of the iterative method. For the solution of problems arising from vibrations, the two criteria contradict each other, since fast convergence of the eigensolver requires poles to be in or close to the interval, whereas the iterative linear system solver becomes cheaper when the poles lie further away from the eigenvalues. In the paper, we propose a selection of poles inspired by the shifted Laplace preconditioner for the Helmholtz equation.   We show numerical experiments from finite element models of vibrations. We compare the shifted Laplace rational filter with rational filters based on quadrature rules for contour integration.|\n", "2503.21532": "|**2025-03-27**|**Multiscale geometrical Lagrangian statistics of heavy impurities in drift-wave turbulence**|Zetao Lin, Benjamin Kadoch, Saddrudin Benkadda et.al.|[2503.21532](http://arxiv.org/abs/2503.21532)|null|19 pages, 2 tables, 7 figures|We investigate the behavior of heavy impurities in edge plasma turbulence by analyzing their trajectories using the Hasegawa-Wakatani model. Through direct numerical simulations, we track ensembles of charged impurity particles over hundreds of eddy turnover times within statistically steady turbulent flows. Assuming that heavy impurities lag behind the flow, a novel derivation of relaxation time of heavy impurities is proposed. Our results reveal that heavy impurities can cluster within turbulence. We provide multiscale geometrical Lagrangian statistics of heavy impurities trajectories. To quantify directional changes, we analyze the scale-dependent curvature angle, along with the influence of the Stokes number on the mean curvature angles and the probability distribution function of curvature angles.|\n", "2503.21456": "|**2025-03-27**|**Mechanostat-type effective density correction for Carter-Hayes growth applied to topology optimization and its efficient interpolation for a target strain energy and volume fraction**|Luis Irastorza-Valera, Ricardo Larra\u00ednzar-Garijo, Javier Montoya-Ad\u00e1rraga et.al.|[2503.21456](http://arxiv.org/abs/2503.21456)|null||The need for optimized structures with good mechanical performance for the minimum weight is common in industry. Solid Isotropic Material with Penalization (SIMP) is a Topology Optimization (TO) method offering a trade-off between minimum compliance (i.e., maximum stiffness) and a fixed material amount for a given set of boundary conditions. Since TO is a non-convex problem, its gradient can be tuned by filtering the topology's contour, creating sharper material profiles without necessarily compromising optimality. However, despite simplifying the layout, some filters fail to address manufacturability concerns such as capillarity (thin tweaks as struts) generated by uncertain loading, vibration or fatigue.   A tailored density-based filtering strategy is offered to tackle this issue. Additionally, volume fraction is left unconstrained so material can be strategically replenished through a logarithmic rule acting on the updated compliance. In doing so, an interpolation space with three degrees of freedom (volume, compliance, minimum thickness) is created, yielding diverse topologies for the same boundary conditions and design values along different stages of evolving topological families with distinct features.   The optimization process is further accelerated by introducing the volume-compliance iterative scheme as a physical loss function in a Double Distance Neural Network (D$^2$NN), obtaining similar results to 2,000 steps worth of vanilla iteration within 500 training epochs. This proposal offers a novel topology optimization design space based on minimum strut thickness - via filtering - and topological families defined by minimum volume fraction and compliance. The methodology is tested on several examples with diverse loading and boundary conditions, obtaining similarly satisfactory results, and then boosted via Machine Learning, acting as a fast and cheap surrogate.|\n", "2503.21452": "|**2025-03-27**|**Numerical solution of locally loaded Volterra integral equations**|Vladislav Byankin, Aleksandr Tynda, Denis Sidorov et.al.|[2503.21452](http://arxiv.org/abs/2503.21452)|null|7 pages, 2 figures|Volterra's integral equations with local and nonlocal loads represent the novel class of integral equations that have attracted considerable attention in recent years. These equations are a generalisation of the classic Volterra integral equations, which were first introduced by Vito Volterra in the late 19th century. The loaded Volterra integral equations are characterised by the presence of a load which complicates the process of their theoretical and numerical study. Sometimes these equation are called the equations with ``frozen'' argument. The present work is devoted to the study of Volterra equations with locally loaded integral operators. The existence and uniquness theorems are proved. Among the main contributions is the collocation method for approximate solution of such equations based on the piecewise linear approximation. To confirm the convergence of the method, a number of numerical results for solving model problems are provided.|\n", "2503.21382": "|**2025-03-27**|**Limited Diffusion of Silicon in GaN: A DFT Study Supported by Experimental Evidence**|Karol Kawka, Pawel Kempisty, Akira Kusaba et.al.|[2503.21382](http://arxiv.org/abs/2503.21382)|null||Silicon (Si) is the primary donor dopant in gallium nitride (GaN), introduced through epitaxial growth or ion implantation. However, precise control over Si diffusion remains a critical challenge for high-performance device applications. This study investigates Si diffusion mechanisms in bulk GaN using first-principles density functional theory (DFT) calculations, supported by ultra-high-pressure annealing (UHPA) experiments. Vacancy-mediated diffusion pathways were analyzed using the SIESTA code, with minimum energy paths (MEPs) and activation barriers determined via the nudged elastic band (NEB) method. The results indicate that Si diffusion barriers vary with crystallographic direction, with the lowest barrier of 3.2 eV along [11-20] and the highest barrier of ~9.9 eV along [1-100], rendering diffusion in this direction highly improbable. Alternative diffusion mechanisms, including direct exchange and ring-like migration, exhibit prohibitively high barriers ($>$12 eV). Phonon calculations confirm that temperature-induced reductions in effective diffusion barriers are minimal. Experimental validation using SIMS analysis on Si-implanted GaN samples subjected to UHPA (1450{\\deg}C, 1 GPa) confirms negligible Si diffusion under these extreme conditions. These findings resolve inconsistencies in prior reports and establish that Si-doped GaN remains highly stable, ensuring reliable doping profiles for advanced electronic and optoelectronic applications.|\n", "2503.21361": "|**2025-03-27**|**Computing adjoint mismatch of linear maps**|Jonas Bresch, Dirk A. Lorenz, Felix Schneppe et.al.|[2503.21361](http://arxiv.org/abs/2503.21361)|null||This paper considers the problem of detecting adjoint mismatch for two linear maps. To clarify, this means that we aim to calculate the operator norm for the difference of two linear maps, where for one we only have a black-box implementation for the evaluation of the map, and for the other we only have a black-box for the evaluation of the adjoint map. We give two stochastic algorithms for which we prove the almost sure convergence to the operator norm. The algorithm is a random search method for a generalization of the Rayleigh quotient and uses optimal step sizes. Additionally, a convergence analysis is done for the corresponding singular vector and the respective eigenvalue equation.|\n", "2503.21318": "|**2025-03-27**|**Explicit error bounds and guaranteed convergence of the Koopman-Hill projection stability method for linear time-periodic dynamics**|Fabia Bayer, Remco I. Leine et.al.|[2503.21318](http://arxiv.org/abs/2503.21318)|null|preprint, 34 pages, 10 figures|The Koopman-Hill projection method is used to approximate the fundamental solution matrix of linear time-periodic ordinary differential equations, possibly stemming from linearization around a periodic solution of a nonlinear dynamical system. By expressing both the true fundamental solution and its approximation as series, we derive an upper bound for the approximation error that decays exponentially with the size of the Hill matrix. Exponential decay of the Fourier coefficients of the system dynamics is key to guarantee convergence. The paper also analyzes a subharmonic formulation that improves the convergence rate. Two numerical examples, including a Duffing oscillator, illustrate the theoretical findings.|\n", "2503.21252": "|**2025-03-27**|**Multi-fidelity Learning of Reduced Order Models for Parabolic PDE Constrained Optimization**|Benedikt Klein, Mario Ohlberger et.al.|[2503.21252](http://arxiv.org/abs/2503.21252)|null|36 pages, 5 figures|This article builds on the recently proposed RB-ML-ROM approach for parameterized parabolic PDEs and proposes a novel hierarchical Trust Region algorithm for solving parabolic PDE constrained optimization problems. Instead of using a traditional offline/online splitting approach for model order reduction, we adopt an active learning or enrichment strategy to construct a multi-fidelity hierarchy of reduced order models on-the-fly during the outer optimization loop. The multi-fidelity surrogate model consists of a full order model, a reduced order model and a machine learning model. The proposed hierarchical framework adaptively updates its hierarchy when querying parameters, utilizing a rigorous a posteriori error estimator in an error aware trust region framework. Numerical experiments are given to demonstrate the efficiency of the proposed approach.|\n", "2503.21234": "|**2025-03-27**|**Continuous Data Assimilation for the Navier-Stokes Equations with Nonlinear Slip Boundary Conditions**|W. C. Wu, H. Y. Dong, K. Wang et.al.|[2503.21234](http://arxiv.org/abs/2503.21234)|null||This paper focuses on continuous data assimilation (CDA) for the Navier-Stokes equations with nonlinear slip boundary conditions. CDA methods are typically employed to recover the original system when initial data or viscosity coefficients are unknown, by incorporating a feedback control term generated by observational data over a time period. In this study, based on a regularized form derived from the variational inequalities of the Navier-Stokes equations with nonlinear slip boundary conditions, we first investigate the classical CDA problem when initial data is absent. After establishing the existence, uniqueness and regularity of the solution, we prove its exponential convergence with respect to the time. Additionally, we extend the CDA to address the problem of missing viscosity coefficients and analyze its convergence order, too. Furthermore, utilizing the predictive capabilities of partial evolutionary tensor neural networks (pETNNs) for time-dependent problems, we propose a novel CDA by replacing observational data with predictions got by pETNNs. Compared with the classical CDA, the new one can achieve similar approximation accuracy but need much less computational cost. Some numerical experiments are presented, which not only validate the theoretical results, but also demonstrate the efficiency of the CDA.|\n", "2503.21201": "|**2025-03-27**|**Efficient Crystal Structure Prediction Using Genetic Algorithm and Universal Neural Network Potential**|Takuya Shibayama, Hideaki Imamura, Katsuhiko Nishimra et.al.|[2503.21201](http://arxiv.org/abs/2503.21201)|null||Crystal structure prediction (CSP) is crucial for identifying stable crystal structures in given systems and is a prerequisite for computational atomistic simulations. Recent advances in neural network potentials (NNPs) have reduced the computational cost of CSP. However, searching for stable crystal structures across the entire composition space in multicomponent systems remains a significant challenge. Here, we propose a novel genetic algorithm (GA) -based CSP method using a universal NNP. Our GA-based methods are designed to efficiently expand convex hull volumes while preserving the diversity of crystal structures. This approach draws inspiration from the similarity between convex hull updates and Pareto front evolution in multi-objective optimization. Our evaluation shows that the present method outperforms the symmetry-aware random structure generation, achieving a larger convex hull with fewer trials. We demonstrated that our approach, combined with the developed universal NNP (PFP), can accurately reproduce and explore phase diagrams obtained through DFT calculations; this indicates the validity of PFP across a wide range of crystal structures and element combinations. This study, which integrates a universal NNP with a GA-based CSP method, highlights the promise of these methods in materials discovery.|\n", "2503.21196": "|**2025-03-27**|**Reaction Dynamics of the H + HeH$^+$ $\\rightarrow$ He + H$_2^+$ System**|Meenu Upadhyay, Silvan K\u00e4ser, Jayakrushna Sahoo et.al.|[2503.21196](http://arxiv.org/abs/2503.21196)|null||The reaction dynamics for the H + HeH$^+$ $\\rightarrow$ He + H$_2^+$ reaction in its electronic ground state is investigated using two different representations of the potential energy surface (PES). The first uses a combined kernel and neural network representation of UCCSD(T) reference data whereas the second is a corrected PES (cR-PES) that eliminates an artificial barrier in the entrance channel appearing in its initial expansion based on full configuration interaction reference data. Despite the differences between the two PESs, both yield $k_{v=0,j=0} \\approx 2 \\times 10^{-9}$ cm$^3$/molecule/s at $T = 10$ K which is consistent with a $T-$independent Langevin rate $k_{\\rm L} = 2.1 \\times 10^{-9}$ cm$^3$/molecule/s but considerably larger than the only experimentally reported value $k_{\\rm ICR} = (9.1 \\pm 2.5) \\times 10^{-10}$ cm$^3$/molecule/s from ion cyclotron resonance experiments. Similarly, branching ratios for the reaction outcomes are comparable for the two PESs. However, when analysing less averaged properties such as initial state-selected $T-$dependent rate coefficients and final vibrational states of the H$_2^+$ product for low temperatures, the differences in the two PESs manifest themselves in the observables. Thus, depending on the property analyzed, accurate and globally valid representations of the PES are required, whereas more approximate and empirical construction schemes can be followed for state-averaged observables.|\n", "2503.21182": "|**2025-03-27**|**Optimal Transportation for the Far-field Reflector Problem**|Gang Bao, Yixuan Zhang et.al.|[2503.21182](http://arxiv.org/abs/2503.21182)|null||The inverse reflector problem aims to design a freeform reflecting surface that can direct the light from a specified source to produce the desired illumination in the target area, which is significant in the field of geometrical non-imaging optics. Mathematically, it can be formulated as an optimization problem, which is exactly the optimal transportation problem (OT) when the target is in the far field. The gradient of OT is governed by the generalized Monge-Amp`ere equation that models the far-field reflector system. Based on the gradient, this work presents a Sobolev gradient descent method implemented within a finite element framework to solve the corresponding OT. Convergence of the method is established and numerical examples are provided to demonstrate the effectiveness of the method.|\n", "2503.21176": "|**2025-03-27**|**GPU-Accelerated Charge-Equilibration for Shadow Molecular Dynamics in Python**|Mehmet Cagri Kaymak, Nicholas Lubbers, Christian F. A. Negre et.al.|[2503.21176](http://arxiv.org/abs/2503.21176)|null||With recent advancements in machine learning for interatomic potentials, Python has become the go-to programming language for exploring new ideas. While machine-learning potentials are often developed in Python-based frameworks, existing molecular dynamics software is predominantly written in lower-level languages. This disparity complicates the integration of machine learning potentials into these molecular dynamics libraries. Additionally, machine learning potentials typically focus on local features, often neglecting long-range electrostatics due to computational complexities. This is a key limitation as applications can require long-range electrostatics and even flexible charges to achieve the desired accuracy. Recent charge equilibration models can address these issues, but they require iterative solvers to assign relaxed flexible charges to the atoms. Conventional implementations also demand very tight convergence to achieve long-term stability, further increasing computational cost. In this work, we present a scalable Python implementation of a recently proposed shadow molecular dynamics scheme based on a charge equilibration model, which avoids the convergence problem while maintaining long-term energy stability and accuracy of observable properties. To deliver a functional and user-friendly Python-based library, we implemented an efficient neighbor list algorithm, Particle Mesh Ewald, and traditional Ewald summation techniques, leveraging the GPU-accelerated power of Triton and PyTorch. We integrated these approaches with the Python-based shadow molecular dynamics scheme, enabling fast charge equilibration for scalable machine learning potentials involving systems with hundreds of thousands of atoms.|\n", "2503.21103": "|**2025-03-27**|**Low Stein Discrepancy via Message-Passing Monte Carlo**|Nathan Kirk, T. Konstantin Rusch, Jakob Zech et.al.|[2503.21103](http://arxiv.org/abs/2503.21103)|null|8 pages, 2 figures, Accepted at the ICLR 2025 Workshop on Frontiers   in Probabilistic Inference|Message-Passing Monte Carlo (MPMC) was recently introduced as a novel low-discrepancy sampling approach leveraging tools from geometric deep learning. While originally designed for generating uniform point sets, we extend this framework to sample from general multivariate probability distributions with known probability density function. Our proposed method, Stein-Message-Passing Monte Carlo (Stein-MPMC), minimizes a kernelized Stein discrepancy, ensuring improved sample quality. Finally, we show that Stein-MPMC outperforms competing methods, such as Stein Variational Gradient Descent and (greedy) Stein Points, by achieving a lower Stein discrepancy.|\n", "2503.21078": "|**2025-03-27**|**Sub-ODEs Simplify Taylor Series Algorithms for Ordinary Differential Equations**|Nedialko S. Nedialkov, John D. Pryce et.al.|[2503.21078](http://arxiv.org/abs/2503.21078)|null|25 pages|A Taylor method for solving an ordinary differential equation initial-value problem $\\dot x = f(t,x)$, $x(t_0) = x_0$, computes the Taylor series (TS) of the solution at the current point, truncated to some order, and then advances to the next point by summing the TS with a suitable step size.   A standard ODE method (e.g. Runge-Kutta) treats function $f$ as a black box, but a Taylor solver requires $f$ to be preprocessed into a code-list of elementary operations that it interprets as operations on (truncated) TS.   The trade-off for this extra work includes arbitrary order, typically enabling much larger step sizes.   For a standard function, such as $\\exp$, this means evaluating $v(t)=\\exp(u(t))$, where $u(t),v(t)$ are TS.   The sub-ODE method applies the ODE $d v/d u=v$, obeyed by $v=\\exp(u)$, to in-line this operation as $\\dot v=v\\dot u$.   This gives economy of implementation: each function that satisfies a simple ODE goes into the \"Taylor library\" with a few lines of code--not needing a separate recurrence relation, which is the typical approach.   Mathematically, however, the use of sub-ODEs generally transforms the original ODE into a differential-algebraic system, making it nontrivial to ensure a sound system of recurrences for Taylor coefficients.   We prove that, regardless of how many sub-ODEs are incorporated into $f$, this approach guarantees a sound system.   We introduce our sub-ODE-based Matlab ODE solver and show that its performance compares favorably with solvers from the Matlab ODE suite.|\n", "2503.21037": "|**2025-03-26**|**Optimal Rejection-Free Path Sampling**|Gianmarco Lazzeri, Peter G. Bolhuis, Roberto Covino et.al.|[2503.21037](http://arxiv.org/abs/2503.21037)|null||We propose an efficient novel path sampling-based framework designed to accelerate the investigation of rare events in complex molecular systems. A key innovation is the shift from sampling restricted path ensemble distributions, as in transition path sampling, to directly sampling the distribution of shooting points. This allows for a rejection-free algorithm that samples the entire path ensemble efficiently. Optimal sampling is achieved by applying a selection bias that is the inverse of the free energy along a reaction coordinate. The optimal reaction coordinate, the committor, is iteratively constructed as a neural network using AI for Molecular Mechanism Discovery (AIMMD), concurrently with the free energy profile, which is obtained through reweighting the sampled path ensembles. We showcase our algorithm on theoretical and molecular bechnmarks, and demonstrate how it provides at the same time molecular mechanism, free energy, and rates at a moderate computational cost.|\n", "2503.22652": "|**2025-03-28**|**Residual-based Chebyshev filtered subspace iteration for sparse Hermitian eigenvalue problems tolerant to inexact matrix-vector products**|Nikhil Kodali, Kartick Ramakrishnan, Phani Motamarri et.al.|[2503.22652](http://arxiv.org/abs/2503.22652)|null|32 Pages, 12 Figures, 1 Table|Chebyshev Filtered Subspace Iteration (ChFSI) has been widely adopted for computing a small subset of extreme eigenvalues in large sparse matrices. This work introduces a residual-based reformulation of ChFSI, referred to as R-ChFSI, designed to accommodate inexact matrix-vector products while maintaining robust convergence properties. By reformulating the traditional Chebyshev recurrence to operate on residuals rather than eigenvector estimates, the R-ChFSI approach effectively suppresses the errors made in matrix-vector products, improving the convergence behaviour for both standard and generalized eigenproblems. This ability of R-ChFSI to be tolerant to inexact matrix-vector products allows one to incorporate approximate inverses for large-scale generalized eigenproblems, making the method particularly attractive where exact matrix factorizations or iterative methods become computationally expensive for evaluating inverses. It also allows us to compute the matrix-vector products in lower-precision arithmetic allowing us to leverage modern hardware accelerators. Through extensive benchmarking, we demonstrate that R-ChFSI achieves desired residual tolerances while leveraging low-precision arithmetic. For problems with millions of degrees of freedom and thousands of eigenvalues, R-ChFSI attains final residual norms in the range of 10$^{-12}$ to 10$^{-14}$, even with FP32 and TF32 arithmetic, significantly outperforming standard ChFSI in similar settings. In generalized eigenproblems, where approximate inverses are used, R-ChFSI achieves residual tolerances up to ten orders of magnitude lower, demonstrating its robustness to approximation errors. Finally, R-ChFSI provides a scalable and computationally efficient alternative for solving large-scale eigenproblems in high-performance computing environments.|\n", "2503.22649": "|**2025-03-28**|**Stochastic reduced-order Koopman model for turbulent flows**|Tianyi Chu, Oliver T. Schmidt et.al.|[2503.22649](http://arxiv.org/abs/2503.22649)|null||A stochastic data-driven reduced-order model applicable to a wide range of turbulent natural and engineering flows is presented. Combining ideas from Koopman theory and spectral model order reduction, the stochastic low-dimensional inflated convolutional Koopman model (SLICK) accurately forecasts short-time transient dynamics while preserving long-term statistical properties. A discrete Koopman operator is used to evolve convolutional coordinates that govern the temporal dynamics of spectral orthogonal modes, which in turn represent the energetically most salient large-scale coherent flow structures. Turbulence closure is achieved in two steps: first, by inflating the convolutional coordinates to incorporate nonlinear interactions between different scales, and second, by modeling the residual error as a stochastic source. An empirical dewhitening filter informed by the data is used to maintain the second-order flow statistics within the long-time limit. The model uncertainty is quantified through either Monte Carlo simulation or by directly propagating the model covariance matrix. The model is demonstrated on the Ginzburg-Landau equations, large-eddy simulation (LES) data of a turbulent jet, and particle image velocimetry (PIV) data of the flow over an open cavity. In all cases, the model is predictive over time horizons indicated by a detailed error analysis and integrates stably over arbitrary time horizons, generating realistic surrogate data.|\n", "2503.22646": "|**2025-03-28**|**Finding Unknown Unknowns using Cyber-Physical System Simulators (Extended Report)**|Semaan Douglas Wehbe, Stanley Bak et.al.|[2503.22646](http://arxiv.org/abs/2503.22646)|null||Simulation-based approaches are among the most practical means to search for safety violations, bugs, and other unexpected events in cyber-physical systems (CPS). Where existing approaches search for simulations violating a formal specification or maximizing a notion of coverage, in this work we propose a new goal for testing: to discover unknown rare behaviors by examining discrete mode sequences. We assume a CPS simulator outputs mode information, and strive to explore the sequences of modes produced by varying the initial state or time-varying uncertainties. We hypothesize that rare mode sequences are often the most interesting to a designer, and we develop two accelerated sampling algorithms that speed up the process of finding such sequences. We evaluate our approach on several benchmarks, ranging from synthetic examples to Simulink diagrams of a CPS, demonstrating in some cases a speedup of over 100x compared with a random sampling strategy.|\n", "2503.22631": "|**2025-03-28**|**Accelerating a restarted Krylov method for matrix functions with randomization**|Nicolas L. Guidotti, Per-Gunnar Martinsson, Juan A. Acebr\u00f3n et.al.|[2503.22631](http://arxiv.org/abs/2503.22631)|null||Many scientific applications require the evaluation of the action of the matrix function over a vector and the most common methods for this task are those based on the Krylov subspace. Since the orthogonalization cost and memory requirement can quickly become overwhelming as the basis grows, the Krylov method is often restarted after a few iterations. This paper proposes a new acceleration technique for restarted Krylov methods based on randomization. The numerical experiments show that the randomized method greatly outperforms the classical approach with the same level of accuracy. In fact, randomization can actually improve the convergence rate of restarted methods in some cases. The paper also compares the performance and stability of the randomized methods proposed so far for solving very large finite element problems, complementing the numerical analyses from previous studies.|\n", "2503.22621": "|**2025-03-28**|**Improved error estimates for low-regularity integrators using space-time bounds**|Maximilian Ruff et.al.|[2503.22621](http://arxiv.org/abs/2503.22621)|null|14 pages|We prove optimal convergence rates for certain low-regularity integrators applied to the one-dimensional periodic nonlinear Schr\\\"odinger and wave equations under the assumption of $H^1$ solutions. For the Schr\\\"odinger equation we analyze the exponential-type scheme proposed by Ostermann and Schratz in 2018, whereas in the wave case we treat the corrected Lie splitting proposed by Li, Schratz, and Zivcovich in 2023. We show that the integrators converge with their full order of one and two, respectively. In this situation only fractional convergence rates were previously known. The crucial ingredients in the proofs are known space-time bounds for the solutions to the corresponding linear problems. More precisely, in the Schr\\\"odinger case we use the $L^4$ Strichartz inequality, and for the wave equation a null form estimate. To our knowledge, this is the first time that a null form estimate is exploited in numerical analysis. We apply the estimates for continuous time, thus avoiding potential losses resulting from discrete-time estimates.|\n", "2503.22604": "|**2025-03-28**|**Enhanced Variational Quantum Kolmogorov-Arnold Network**|Hikaru Wakaura, Rahmat Mulyawan, Andriyan B. Suksmono et.al.|[2503.22604](http://arxiv.org/abs/2503.22604)|null|arXiv admin note: substantial text overlap with arXiv:2503.21336|The Kolmogorov-Arnold Network (KAN) is a novel multi-layer network model recognized for its efficiency in neuromorphic computing, where synapses between neurons are trained linearly. Computations in KAN are performed by generating a polynomial vector from the state vector and layer-wise trained synapses, enabling efficient processing. While KAN can be implemented on quantum computers using block encoding and Quantum Signal Processing, these methods require fault-tolerant quantum devices, making them impractical for current Noisy Intermediate-Scale Quantum (NISQ) hardware. We propose the Enhanced Variational Quantum Kolmogorov-Arnold Network (EVQKAN) to overcome this limitation, which emulates KAN through variational quantum algorithms. The EVQKAN ansatz employs a tiling technique to emulate layer matrices, leading to significantly higher accuracy compared to conventional Variational Quantum Kolmogorov-Arnold Network (VQKAN) and Quantum Neural Networks (QNN), even with a smaller number of layers. EVQKAN achieves superior performance with a single-layer architecture, whereas QNN and VQKAN typically struggle. Additionally, EVQKAN eliminates the need for Quantum Signal Processing, enhancing its robustness to noise and making it well-suited for practical deployment on NISQ-era quantum devices.|\n", "2503.22596": "|**2025-03-28**|**Pressure-temperature phase diagram calculations using polynomial machine learning potentials: A comprehensive study based on global structure prediction and self-consistent phonon calculations**|Hayato Wakai, Atsuto Seko, Isao Tanaka et.al.|[2503.22596](http://arxiv.org/abs/2503.22596)|null|REVTeX 4-2, 18 pages, 16 figures|Polynomial machine learning potentials (MLPs) based on polynomial rotational invariants have been systematically developed for various systems and applied to efficiently predict crystal structures. In this study, we propose a robust methodology founded on polynomial MLPs to comprehensively enumerate crystal structures under high-pressure conditions and to evaluate their phase stability at finite temperatures. The proposed approach involves constructing polynomial MLPs with high predictive accuracy across a broad range of pressures, conducting reliable global structure searches, and performing exhaustive self-consistent phonon calculations. We demonstrate the effectiveness of this approach by examining elemental silicon at pressures up to 100 GPa and temperatures up to 1000 K, revealing stable phases across these conditions. The framework established in this study offers a powerful strategy for predicting crystal structures and phase stability under high-pressure and finite-temperature conditions.|\n", "2503.22528": "|**2025-03-28**|**MixFunn: A Neural Network for Differential Equations with Improved Generalization and Interpretability**|Tiago de Souza Farias, Gubio Gomes de Lima, Jonas Maziero et.al.|[2503.22528](http://arxiv.org/abs/2503.22528)|**[link](https://github.com/tiago939/MixFunn)**|21 pages|We introduce MixFunn, a novel neural network architecture designed to solve differential equations with enhanced precision, interpretability, and generalization capability. The architecture comprises two key components: the mixed-function neuron, which integrates multiple parameterized nonlinear functions to improve representational flexibility, and the second-order neuron, which combines a linear transformation of its inputs with a quadratic term to capture cross-combinations of input variables. These features significantly enhance the expressive power of the network, enabling it to achieve comparable or superior results with drastically fewer parameters and a reduction of up to four orders of magnitude compared to conventional approaches. We applied MixFunn in a physics-informed setting to solve differential equations in classical mechanics, quantum mechanics, and fluid dynamics, demonstrating its effectiveness in achieving higher accuracy and improved generalization to regions outside the training domain relative to standard machine learning models. Furthermore, the architecture facilitates the extraction of interpretable analytical expressions, offering valuable insights into the underlying solutions.|\n", "2503.22481": "|**2025-03-28**|**Charge creation via quantum tunneling in one-dimensional Mott insulators: A numerical study of the extended Hubbard model**|Thomas Hansen, Lars Bojer Madsen, Yuta Murakami et.al.|[2503.22481](http://arxiv.org/abs/2503.22481)|null|14 pages including bibliography and the appendix (11 pages without   them), 8 figures in the main text and 1 in the appendix for a total of 9   figures, and 2 tables in the main text|Charge creation via quantum tunneling, i.e. dielectric breakdown, is one of the most fundamental and significant phenomena arising from strong light(field)-matter coupling. In this work, we conduct a systematic numerical analysis of quantum tunneling in one-dimensional Mott insulators described by the extended ($U$-$V$) Hubbard model. We discuss the applicability of the analytical formula for doublon-holon (DH) pair production, previously derived for the one-dimensional Hubbard model, which highlights the relationship between the tunneling threshold, the charge gap, and the correlation length. We test the formulas ability to predict both DH pair production and energy increase rate. Using tensor-network-based approaches, we demonstrate that the formula provides accurate predictions in the absence of excitonic states facilitated by the nearest-neighbor interaction $V$. However, when excitonic states emerge, the formula more accurately describes the rate of energy increase than the DH pair creation rate and in both cases gets improved by incorporating the exciton energy as the effective gap.|\n", "2503.22476": "|**2025-03-28**|**The 2D Materials Roadmap**|Wencai Ren, Peter B\u00f8ggild, Joan Redwing et.al.|[2503.22476](http://arxiv.org/abs/2503.22476)|null|104 pages, to be published in 2D Materials|Over the past two decades, 2D materials have rapidly evolved into a diverse and expanding family of material platforms. Many members of this materials class have demonstrated their potential to deliver transformative impact on fundamental research and technological applications across different fields. In this roadmap, we provide an overview of the key aspects of 2D material research and development, spanning synthesis, properties and commercial applications. We specifically present roadmaps for high impact 2D materials, including graphene and its derivatives, transition metal dichalcogenides, MXenes as well as their heterostructures and moir\\'e systems. The discussions are organized into thematic sections covering emerging research areas (e.g., twisted electronics, moir\\'e nano-optoelectronics, polaritronics, quantum photonics, and neuromorphic computing), breakthrough applications in key technologies (e.g., 2D transistors, energy storage, electrocatalysis, filtration and separation, thermal management, flexible electronics, sensing, electromagnetic interference shielding, and composites) and other important topics (computational discovery of novel materials, commercialization and standardization). This roadmap focuses on the current research landscape, future challenges and scientific and technological advances required to address, with the intent to provide useful references for promoting the development of 2D materials.|\n", "2503.22455": "|**2025-03-28**|**A high order multigrid-preconditioned immersed interface solver for the Poisson equation with boundary and interface conditions**|James Gabbard, Andrea Paris, Wim M. van Rees et.al.|[2503.22455](http://arxiv.org/abs/2503.22455)|null||This work presents a multigrid preconditioned high order immersed finite difference solver to accurately and efficiently solve the Poisson equation on complex 2D and 3D domains. The solver employs a low order Shortley-Weller multigrid method to precondition a high order matrix-free Krylov subspace solver. The matrix-free approach enables full compatibility with high order IIM discretizations of boundary and interface conditions, as well as high order wavelet-adapted multiresolution grids. Through verification and analysis on 2D domains, we demonstrate the ability of the algorithm to provide high order accurate results to Laplace and Poisson problems with Dirichlet, Neumann, and/or interface jump boundary conditions, all effectively preconditioned using the multigrid method. We further show that the proposed method is able to efficiently solve high order discretizations of Laplace and Poisson problems on complex 3D domains using thousands of compute cores and on multiresolution grids. To our knowledge, this work presents the largest problem sizes tackled with high order immersed methods applied to elliptic partial differential equations, and the first high order results on 3D multiresolution adaptive grids. Together, this work paves the way for employing high order immersed methods to a variety of 3D partial differential equations with boundary or inter-face conditions, including linear and non-linear elasticity problems, the incompressible Navier-Stokes equations, and fluid-structure interactions.|\n", "2503.22386": "|**2025-03-28**|**Spectral coefficient learning physics informed neural network for time-dependent fractional parametric differential problems**|S M Sivalingam, V Govindaraj, A. S. Hendy et.al.|[2503.22386](http://arxiv.org/abs/2503.22386)|null||The study of parametric differential equations plays a crucial role in weather forecasting and epidemiological modeling. These phenomena are better represented using fractional derivatives due to their inherent memory or hereditary effects. This paper introduces a novel scientific machine learning approach for solving parametric time-fractional differential equations by combining traditional spectral methods with neural networks. Instead of relying on automatic differentiation techniques, commonly used in traditional Physics-Informed Neural Networks (PINNs), we propose a more efficient global discretization method based on Legendre polynomials. This approach eliminates the need to simulate the parametric fractional differential equations across multiple parameter values. By applying the Legendre-Galerkin weak formulation to the differential equation, we construct a loss function for training the neural network. The trial solutions are represented as linear combinations of Legendre polynomials, with the coefficients learned by the neural network. The convergence of this method is theoretically established, and the theoretical results are validated through numerical experiments on several well-known differential equations.|\n", "2503.22372": "|**2025-03-28**|**A Morphotropic Phase Boundary in MA$_{1-x}$FA$_x$PbI$_3$: Linking Structure, Dynamics, and Electronic Properties**|Tobias Hainer, Erik Fransson, Sangita Dutta et.al.|[2503.22372](http://arxiv.org/abs/2503.22372)|null|11 pages, 6 figures|Understanding the phase behavior of mixed-cation halide perovskites is critical for optimizing their structural stability and optoelectronic performance. Here, we map the phase diagram of MA$_{1-x}$FA$_x$PbI$_3$ using a machine-learned interatomic potential in molecular dynamics simulations. We identify a morphotropic phase boundary (MPB) at approximately 27% FA content, delineating the transition between out-of-phase and in-phase octahedral tilt patterns. Phonon mode projections reveal that this transition coincides with a mode crossover composition, where the free energy landscapes of the M and R phonon modes become nearly degenerate. This results in nanoscale layered structures with alternating tilt patterns, suggesting minimal interface energy between competing phases. Our results provide a systematic and consistent description of this important system, complementing earlier partial and sometimes conflicting experimental assessments. Furthermore, density functional theory calculations show that band edge fluctuations peak near the MPB, indicating an enhancement of electron-phonon coupling and dynamic disorder effects. These findings establish a direct link between phonon dynamics, phase behavior, and electronic structure, providing a further composition-driven pathway for tailoring the optoelectronic properties of perovskite materials. By demonstrating that phonon overdamping serves as a hallmark of the MPB, our study offers new insights into the design principles for stable, high-performance perovskite solar cells.|\n", "2503.22360": "|**2025-03-28**|**Improvement of conformal maps combined with the Sinc approximation for derivatives over infinite intervals**|Tomoaki Okayama, Yuito Kuwashita, Ao Kondo et.al.|[2503.22360](http://arxiv.org/abs/2503.22360)|null|Keywords: Sinc approximation, single-exponential transformation,   numerical differentiation|F. Stenger proposed efficient approximation formulas for derivatives over infinite intervals. Those formulas were derived by the combination of the Sinc approximation and appropriate conformal maps. It has been shown that those formulas can attain root-exponential convergence. In this study, we enhance the convergence rate by improving the conformal maps employed in those formulas. We provide theoretical error analysis and numerical experiments that confirm the effectiveness of our new formulas.|\n", "2503.22335": "|**2025-03-28**|**A numerical Bernstein splines approach for nonlinear initial value problems with Hilfer fractional derivative**|Niels Goedegebure, Kateryna Marynets et.al.|[2503.22335](http://arxiv.org/abs/2503.22335)|null||The Hilfer fractional derivative interpolates the commonly used Riemann-Liouville and Caputo fractional derivative. In general, solutions to Hilfer fractional differential equations are singular for $t \\downarrow 0$ and are difficult to approximate with high numerical accuracy. We propose a numerical Bernstein splines technique to approximate solutions to generalized nonlinear initial values problems with Hilfer fractional derivatives. Convergent approximations are obtained using an efficient vectorized solution setup with few convergence requirements for a wide range of nonlinear fractional differential equations. We demonstrate efficiency of the developed method by applying it to the fractional Van der Pol oscillator, a system with applications in control systems and electronic circuits.|\n", "2503.22301": "|**2025-03-28**|**Approximation results on neural network operators of convolution type**|Asiye Arif, Tu\u011fba Yurdakadim et.al.|[2503.22301](http://arxiv.org/abs/2503.22301)|null|30 pages, no figures|In the present paper, we introduce three neural network operators of convolution type activated by symmetrized, deformed and parametrized B-generalized logistic function. We deal with the approximation properties of these operators to the identity by using modulus of continuity. Furthermore, we show that our operators preserve global smoothness and consider the iterated versions of them. Here, we find it is worthy to mention that these operators play important roles in neural network approximation since most of the basic network models are activated by logistic functions.|\n", "2503.22297": "|**2025-03-28**|**A posteriori error estimates for the finite element discretization of second-order PDEs set in unbounded domains**|T. Chaumont-Frelet et.al.|[2503.22297](http://arxiv.org/abs/2503.22297)|null||We consider second-order PDE problems set in unbounded domains and discretized by Lagrange finite elements on a finite mesh, thus introducing an artificial boundary in the discretization. Specifically, we consider the reaction diffusion equation as well as Helmholtz problems in waveguides with perfectly matched layers. The usual procedure to deal with such problems is to first consider a modeling error due to the introduction of the artificial boundary, and estimate the remaining discretization error with a standard a posteriori technique. A shortcoming of this method, however, is that it is typically hard to obtain sharp bounds on the modeling error. In this work, we propose a new technique that allows to control the whole error by an a posteriori error estimator. Specifically, we propose a flux-equilibrated estimator that is slightly modified to handle the truncation boundary. For the reaction diffusion equation, we obtain fully-computable guaranteed error bounds, and the estimator is locally efficient and polynomial-degree-robust provided that the elements touching the truncation boundary are not too refined. This last condition may be seen as an extension of the notion of shape-regularity of the mesh, and does not prevent the design of efficient adaptive algorithms. For the Helmholtz problem, as usual, these statements remain valid if the mesh is sufficiently refined. Our theoretical findings are completed with numerical examples which indicate that the estimator is suited to drive optimal adaptive mesh refinements.|\n", "2503.22286": "|**2025-03-28**|**Connecting Kaporin's condition number and the Bregman log determinant divergence**|Andreas A. Bock, Martin S. Andersen et.al.|[2503.22286](http://arxiv.org/abs/2503.22286)|null|14 pages|This paper presents some theoretical results relating the Bregman log determinant matrix divergence to Kaporin's condition number. These can be viewed as nearness measures between a preconditioner and a given matrix, and we show under which conditions these two functions coincide. We also give examples of constraint sets over which it is equivalent to minimise these two objectives. We focus on preconditioners that are the sum of a positive definite and low-rank matrix, which were developed in a previous work. These were constructed as minimisers of the aforementioned divergence, and we show that they are only a constant scaling from also minimising Kaporin's condition number. We highlight connections to information geometry and comment on future directions.|\n", "2503.22273": "|**2025-03-28**|**General form of the Gauss-Seidel equation to linearly approximate the Moore-Penrose pseudoinverse in random non-square systems and high order tensors**|Luis Saucedo-Mora, Luis Irastorza-Valera et.al.|[2503.22273](http://arxiv.org/abs/2503.22273)|null||The Gauss-Seidel method has been used for more than 100 years as the standard method for the solution of linear systems of equations under certain restrictions. This method, as well as Cramer and Jacobi, is widely used in education and engineering, but there is a theoretical gap when we want to solve less restricted systems, or even non-square or non-exact systems of equation. Here, the solution goes through the use of numerical systems, such as the minimization theories or the Moore-Penrose pseudoinverse. In this paper we fill this gap with a global analytical iterative formulation that is capable to reach the solutions obtained with the Moore-Penrose pseudoinverse and the minimization methodologies, but that analytically lies to the solutions of Gauss-Seidel, Jacobi, or Cramer when the system is simplified.|\n", "2503.22246": "|**2025-03-28**|**Lee-Yang zeros in heavy-quark QCD**|Masakiyo Kitazawa, Tatsuya Wada, Kazuyuki Kanaya et.al.|[2503.22246](http://arxiv.org/abs/2503.22246)|null|8 pages, 6 figures; contribution to the Proceedings of QCHSC24|We explore the distribution of Lee-Yang zeros around the critical point that appears in the heavy-quark region of QCD at nonzero temperature in lattice numerical simulations. With the aid of the hopping-parameter expansion that is well justified around the critical point in our setting, our numerical analysis is capable of analyzing the partition function for complex parameters with high accuracy. This enables precise analyses of the Lee-Yang zeros around the critical point. We study their finite-size scaling around the critical point. We also propose new methods to utilize the scaling behavior of the Lee-Yang zeros for fixing the location of the critical point.|\n", "2503.24286": "|**2025-03-31**|**Pyrometheus: Symbolic abstractions for XPU and automatically differentiated computation of combustion kinetics and thermodynamics**|Esteban Cisneros-Garibay, Henry Le Berre, Spencer H. Bryngelson et.al.|[2503.24286](http://arxiv.org/abs/2503.24286)|**[link](https://github.com/pyrometheus/pyrometheus)**|41 pages, 12 figures, 29 listings, 60 references|The cost of combustion simulations is often dominated by the evaluation of net production rates of chemical species and mixture thermodynamics (thermochemistry). Execution on computing accelerators (XPUs) like graphic processing units (GPUs) can greatly reduce this cost. However, established thermochemistry software is not readily portable to such devices or sacrifices valuable analytical forms that enable differentiation for sensitivity analysis and implicit time integration. Symbolic abstractions are developed with corresponding transformations that enable computation on accelerators and automatic differentiation by avoiding premature specification of detail. The software package Pyrometheus is introduced as an implementation of these abstractions and their transformations for combustion thermochemistry. The formulation facilitates code generation from the symbolic representation of a specific thermochemical mechanism in multiple target languages, including Python, C++, and Fortran. Computational concerns are separated: the generated code processes array-valued expressions but does not specify their semantics. These semantics are provided by compatible array libraries, such as NumPy, Pytato, and Google JAX. Thus, the generated code retains a symbolic representation of the thermochemistry, which translates to computation on accelerators and CPUs and automatic differentiation. The design and operation of these symbolic abstractions and their companion tool, Pyrometheus, are discussed throughout. Roofline demonstrations show that the computation of chemical source terms within MFC, a Fortran-based flow solver we link to Pyrometheus, is performant.|\n", "2503.24234": "|**2025-03-31**|**Beyond Gaussian Assumptions: A Nonlinear Generalization of Linear Inverse Modeling**|Justin Lien, Hiroyasu Ando et.al.|[2503.24234](http://arxiv.org/abs/2503.24234)|null||The Linear Inverse Model (LIM) is a class of data-driven methods that construct approximate linear stochastic models to represent complex observational data. The stochastic forcing can be modeled using either Gaussian white noise or Ornstein-Uhlenbeck colored noise; the corresponding models are called White-LIM and Colored-LIM, respectively. Although LIMs are widely applied in climate sciences, they inherently approximate observed distributions as Gaussian, limiting their ability to capture asymmetries.   In this study, we extend LIMs to incorporate nonlinear dynamics, introducing White-nLIM and Colored-nLIM which allow for a more flexible and accurate representation of complex dynamics from observations. The proposed methods not only account for the nonlinear nature of the underlying system but also effectively capture the skewness of the observed distribution. Moreover, we apply these methods to a lower-dimensional representation of ENSO and demonstrate that both White-nLIM and Colored-nLIM successfully capture its nonlinear characteristic.|\n", "2503.24232": "|**2025-03-31**|**Polynomial Inequalities and Optimal Stability of Numerical Integrators**|Luke Shaw et.al.|[2503.24232](http://arxiv.org/abs/2503.24232)|null||A numerical integrator for $\\dot{x}=f(x)$ is called \\emph{stable} if, when applied to the 1D Dahlquist test equation $\\dot{x}=\\lambda x,\\lambda\\in\\mathbb{C}$ with fixed timestep $h>0$, the numerical solution remains bounded as the number of steps tends to infinity. It is well known that no explicit integrator may remain stable beyond certain limits in $\\lambda$. Furthermore, these stability limits are only tight for certain specific integrators (different in each case), which may then be called `optimally stable'. Such optimal stability results are typically proven using sophisticated techniques from complex analysis, leading to rather abstruse proofs. In this article, we pursue an alternative approach, exploiting connections with the Bernstein and Markov brothers inequalities for polynomials. This simplifies the proofs greatly and offers a framework which unifies the diverse results that have been obtained.|\n", "2503.24208": "|**2025-03-31**|**Data-driven construction of a generalized kinetic collision operator from molecular dynamics**|Yue Zhao, William Burby, Andrew Christlieb et.al.|[2503.24208](http://arxiv.org/abs/2503.24208)|null||We introduce a data-driven approach to learn a generalized kinetic collision operator directly from molecular dynamics. Unlike the conventional (e.g., Landau) models, the present operator takes an anisotropic form that accounts for a second energy transfer arising from the collective interactions between the pair of collision particles and the environment. Numerical results show that preserving the broadly overlooked anisotropic nature of the collision energy transfer is crucial for predicting the plasma kinetics with non-negligible correlations, where the Landau model shows limitations.|\n", "2503.24195": "|**2025-03-31**|**Computational Orthodontic Force Simulation: A Review**|Waheed Ahmad, Jing Xiong, Zeyang Xia et.al.|[2503.24195](http://arxiv.org/abs/2503.24195)|null|19 pages, 4 figure, 1 table|In orthodontic treatment, the biological response of the tooth, periodontal ligament, and bone complex to orthodontic force is crucial in influencing treatment outcomes. The challenge lies in accurately measuring, estimating, and predicting these forces during clinical procedures. This review aims to fill the gap in the literature by systematically summarizing existing research on orthodontic force simulation, examining common loading techniques and technologies, and discussing the potential for refining the orthodontic force simulation process. The literature was comprehensively reviewed, with an emphasis on the exploration of the biological mechanism of tooth movement. Studies were categorized based on force-loading techniques for both fixed and invisible orthodontic appliances. Finite element (FE) analysis stands out as the predominant technique for orthodontic force simulation, with a significant focus on fixed orthodontics but limited emphasis on invisible orthodontics. Current orthodontic force simulations tend to be fragmented, often considering only the instantaneous response to applied forces. There exists an urgent demand for a sophisticated analytical simulation model. Such a model, possibly leveraging advanced technologies like deep learning, holds the promise of forecasting orthodontic treatment outcomes with heightened precision and efficiency.|\n", "2503.24131": "|**2025-03-31**|**A simple and general framework for the construction of exactly div-curl-grad compatible discontinuous Galerkin finite element schemes on unstructured simplex meshes**|R. Abgrall, M. Dumbser, P. H. Maire et.al.|[2503.24131](http://arxiv.org/abs/2503.24131)|null||We introduce a new family of discontinuous Galerkin (DG) finite element schemes for the discretization of first order systems of hyperbolic partial differential equations (PDE) on unstructured simplex meshes in two and three space dimensions that respect the two basic vector calculus identities exactly also at the discrete level, namely that the curl of the gradient is zero and that the divergence of the curl is zero. The key ingredient here is the construction of two compatible discrete nabla operators, a primary one and a dual one, both defined on general unstructured simplex meshes in multiple space dimensions. Our new schemes extend existing cell-centered finite volume methods based on corner fluxes to arbitrary high order of accuracy in space. An important feature of our new method is the fact that only two different discrete function spaces are needed to represent the numerical solution, and the choice of the appropriate function space for each variable is related to the origin and nature of the underlying PDE. The first class of variables is discretized at the aid of a discontinuous Galerkin approach, where the numerical solution is represented via piecewise polynomials of degree N and which are allowed to jump across element interfaces. This set of variables is related to those PDE which are mere consequences of the definitions, derived from some abstract scalar and vector potentials, and for which involutions like the divergence-free or the curl-free property must hold if satisfied by the initial data. The second class of variables is discretized via classical continuous Lagrange finite elements of approximation degree M=N+1 and is related to those PDE which can be derived as the Euler-Lagrange equations of an underlying variational principle.|\n", "2503.24074": "|**2025-03-31**|**Physics-informed neural networks for hidden boundary detection and flow field reconstruction**|Yongzheng Zhu, Weizheng Chen, Jian Deng et.al.|[2503.24074](http://arxiv.org/abs/2503.24074)|null|21 pages, 17 figures|Simultaneously detecting hidden solid boundaries and reconstructing flow fields from sparse observations poses a significant inverse challenge in fluid mechanics. This study presents a physics-informed neural network (PINN) framework designed to infer the presence, shape, and motion of static or moving solid boundaries within a flow field. By integrating a body fraction parameter into the governing equations, the model enforces no-slip/no-penetration boundary conditions in solid regions while preserving conservation laws of fluid dynamics. Using partial flow field data, the method simultaneously reconstructs the unknown flow field and infers the body fraction distribution, thereby revealing solid boundaries. The framework is validated across diverse scenarios, including incompressible Navier-Stokes and compressible Euler flows, such as steady flow past a fixed cylinder, an inline oscillating cylinder, and subsonic flow over an airfoil. The results demonstrate accurate detection of hidden boundaries, reconstruction of missing flow data, and estimation of trajectories and velocities of a moving body. Further analysis examines the effects of data sparsity, velocity-only measurements, and noise on inference accuracy. The proposed method exhibits robustness and versatility, highlighting its potential for applications when only limited experimental or numerical data are available.|\n", "2503.24069": "|**2025-03-31**|**Impact of Amplitude and Phase Damping Noise on Quantum Reinforcement Learning: Challenges and Opportunities**|Mar\u00eda Laura Olivera-Atencio, Lucas Lamata, Jes\u00fas Casado-Pascual et.al.|[2503.24069](http://arxiv.org/abs/2503.24069)|null|11 pages, 3 figures|Quantum machine learning (QML) is an emerging field with significant potential, yet it remains highly susceptible to noise, which poses a major challenge to its practical implementation. While various noise mitigation strategies have been proposed to enhance algorithmic performance, the impact of noise is not fully understood. In this work, we investigate the effects of amplitude and phase damping noise on a quantum reinforcement learning algorithm. Through analytical and numerical analysis, we assess how these noise sources influence the learning process and overall performance. Our findings contribute to a deeper understanding of the role of noise in quantum learning algorithms and suggest that, rather than being purely detrimental, unavoidable noise may present opportunities to enhance QML processes.|\n", "2503.24050": "|**2025-04-01**|**A Deep Learning Framework for the Electronic Structure of Water: Towards a Universal Model**|Xinyuan Liang, Renxi Liu, Mohan Chen et.al.|[2503.24050](http://arxiv.org/abs/2503.24050)|null||Accurately modeling the electronic structure of water across scales, from individual molecules to bulk liquid, remains a grand challenge. Traditional computational methods face a critical trade-off between computational cost and efficiency. We present an enhanced machine-learning Deep Kohn-Sham (DeePKS) method for improved electronic structure, DeePKS-ES, that overcomes this dilemma. By incorporating the Hamiltonian matrix and their eigenvalues and eigenvectors into the loss function, we establish a universal model for water systems, which can reproduce high-level hybrid functional (HSE06) electronic properties from inexpensive generalized gradient approximation (PBE) calculations. Validated across molecular clusters and liquid-phase simulations, our approach reliably predicts key electronic structure properties such as band gaps and density of states, as well as total energy and atomic forces. This work bridges quantum-mechanical precision with scalable computation, offering transformative opportunities for modeling aqueous systems in catalysis, climate science, and energy storage.|\n", "2503.24005": "|**2025-03-31**|**Attraction of a jerk**|Sini Peltonen, Laura Vasko, Inka Tenhunen et.al.|[2503.24005](http://arxiv.org/abs/2503.24005)|null|Prepared for April Fools' Day. 5 pages|Jerk plays a pivotal role in the thrilling experience of many amusemement park rides. In addition to exploring the physical aspect of jerks, we tackle the empirical observation of an attractive force between passengers in the popular attraction, the spinning teacups. By modeling the complex system of rotating platforms, we show that pseudotorques induced by changing acceleration lead to jerky movements and an attractive interaction among riders. Our numerical analysis confirms the empirical observations, highlighting the connection between attraction and jerks.|\n", "2503.24001": "|**2025-03-31**|**Convergence of a finite volume scheme for a model for ants**|Maria Bruna, Markus Schmidtchen, Oscar de Wit et.al.|[2503.24001](http://arxiv.org/abs/2503.24001)|null||We develop and analyse a finite volume scheme for a nonlocal active matter system known to exhibit a rich array of complex behaviours. The model under investigation was derived from a stochastic system of interacting particles describing a foraging ant colony coupled to pheromone dynamics. In this work, we prove that the unique numerical solution converges to the unique weak solution as the mesh size and the time step go to zero. We also show discrete long-time estimates, which prove that certain norms are preserved for all times, uniformly in the mesh size and time step. In particular, we prove higher regularity estimates which provide an analogue of continuum parabolic higher regularity estimates. Finally, we numerically study the rate of convergence of the scheme, and we provide examples of the existence of multiple metastable steady states.|\n", "2503.23969": "|**2025-03-31**|**Electronic structure of UGe$_2$ at ambient pressure: comparison with X-ray photoemission spectra**|M. Samsel-Czeka\u0142a, M. Werwi\u0144ski, A. Szajek et.al.|[2503.23969](http://arxiv.org/abs/2503.23969)|null||Based on experimental crystallographic data, electronic structure of UGe$_2$ have been calculated and compared with our results of X-ray photoelectron spectroscopy (XPS) measurements. We employed two different advanced full potential (FP) methods: FP-local-orbital (FPLO) and FP-linear augmented plane waves (Wien2k) codes for non-magnetic and ferromagnetic states. Starting from the local spin-density approximation (LSDA) or generalised gradient approximation (GGA), we verified either the orbital polarisation (OP) correction or the GGA+U approach for the U 5f-electrons, changing Coulomb-repulsion energies U in the range 0-4 eV. Satisfying agreement was achieved between experimental and our calculated magnetic moments using ab-initio LSDA+OP and non-ab-initio GGA+U approaches, the latter for realistic U values of 2-3 eV. We proved by the LSDA+OP approach an existence of the Fermi surface nesting vector along the a axis, possibly responsible for the triplet superconducting pairing. The calculated data reveal predominantly an itinerant U 5f-electron character of bands near the Fermi level, EF, with only small contributions from the U 6d and Ge 4p states. The experimental XPS spectrum of valence bands (VB) also contains the sharp main 5f-electron peak at EF, a wide hump (around -2 eV), and broad small peaks at higher energies. In the calculated XPS spectrum, the width of the main 5f-electron peak varies between 0.8 and 1.4 eV, depending on a method used in computations, but the hump remains unresolved. A newly observed asymmetric 1-eV satellite in the experimental 4f-core XPS spectrum together with known 3-eV and 7-eV satellites suggest dual behaviour of U-5f-electrons in UGe$_2$, the feature is inferred also from the VB studies.|\n", "2503.23900": "|**2025-03-31**|**Convergence of Calder\u00f3n residuals**|Ralf Hiptmair, Carolina Urz\u00faa-Torres, Anouk Wisse et.al.|[2503.23900](http://arxiv.org/abs/2503.23900)|null||In this paper, we describe a framework to compute expected convergence rates for residuals based on the Calder\\'on identities for general second order differential operators for which fundamental solutions are known. The idea is that these rates could be used to validate implementations of boundary integral operators and allow to test operators separately by choosing solutions where parts of the Calder\\'on identities vanish. Our estimates rely on simple vector norms, and thus avoid the use of hard-to-compute norms and the residual computation can be easily implemented in existing boundary element codes. We test the proposed Calder\\'on residuals as debugging tool by introducing artificial errors into the Galerkin matrices of some of the boundary integral operators for the Laplacian and time-harmonic Maxwell's equations. From this, we learn that our estimates are not sharp enough to always detect errors, but still provide a simple and useful debugging tool in many situations.|\n", "2503.23874": "|**2025-03-31**|**He-Mg compounds and helium-driven nonmetal transition in metallic magnesium**|Y. S. Huang, H. X. Song, Q. D. Hao et.al.|[2503.23874](http://arxiv.org/abs/2503.23874)|null|22 pages, 5 figures, with supporting materials|The polymorphism and mechanism of helium compounds is crucial for understanding the physical and chemical nature of He-bearing materials under pressures. Here, we predict two new types of He-bearing compounds, MgHe and MgnHe (n = 6, 8, 10, 15, 18), being formed above 750 GPa by unbiased ab initio structure search. An unexpected bandgap is opened up in MgHe at as low as around 200 GPa. This is the first case of noble gas driven metal-nonmetal transition in all elements. The same mechanism is demonstrated also being applicable to other metallic elements, and making beryllium transform into a non-metallic state, a triumph that is impossible otherwise. Furthermore, the stability of the simple cubic phase of Mg (Mg-sc) is greatly enhanced by mixing with He, which lowers the critical pressure of pure Mg-sc from about 1.1 TPa down to 750 GPa to form ordered substitutional alloying phase of MgnHe on a simple cubic lattice of Mg. This is the first report on Mg-based noble gas substitutional alloy, in sharp contrast to the conventional wisdom that He preferring interstitial sites. The observed striking influences of He demonstrate the rich physics and chemistry of He-bearing compounds under ultra-high pressures.|\n", "2503.23794": "|**2025-03-31**|**Force-Free Molecular Dynamics Through Autoregressive Equivariant Networks**|Fabian L. Thiemann, Thiago Resch\u00fctzegger, Massimiliano Esposito et.al.|[2503.23794](http://arxiv.org/abs/2503.23794)|**[link](https://github.com/ibm/trajcast)**|25 pages total (19 manuscript, 6 SI). 5 figures in manuscript, 3   figures and 2 tables in SI|Molecular dynamics (MD) simulations play a crucial role in scientific research. Yet their computational cost often limits the timescales and system sizes that can be explored. Most data-driven efforts have been focused on reducing the computational cost of accurate interatomic forces required for solving the equations of motion. Despite their success, however, these machine learning interatomic potentials (MLIPs) are still bound to small time-steps. In this work, we introduce TrajCast, a transferable and data-efficient framework based on autoregressive equivariant message passing networks that directly updates atomic positions and velocities lifting the constraints imposed by traditional numerical integration. We benchmark our framework across various systems, including a small molecule, crystalline material, and bulk liquid, demonstrating excellent agreement with reference MD simulations for structural, dynamical, and energetic properties. Depending on the system, TrajCast allows for forecast intervals up to $30\\times$ larger than traditional MD time-steps, generating over 15 ns of trajectory data per day for a solid with more than 4,000 atoms. By enabling efficient large-scale simulations over extended timescales, TrajCast can accelerate materials discovery and explore physical phenomena beyond the reach of traditional simulations and experiments. An open-source implementation of TrajCast is accessible under https://github.com/IBM/trajcast.|\n", "2503.23782": "|**2025-03-31**|**Distributional regression with reject option**|Ahmed Zaoui, Cl\u00e9ment Dombry et.al.|[2503.23782](http://arxiv.org/abs/2503.23782)|null||Selective prediction, where a model has the option to abstain from making a decision, is crucial for machine learning applications in which mistakes are costly. In this work, we focus on distributional regression and introduce a framework that enables the model to abstain from estimation in situations of high uncertainty. We refer to this approach as distributional regression with reject option, inspired by similar concepts in classification and regression with reject option. We study the scenario where the rejection rate is fixed. We derive a closed-form expression for the optimal rule, which relies on thresholding the entropy function of the Continuous Ranked Probability Score (CRPS). We propose a semi-supervised estimation procedure for the optimal rule, using two datasets: the first, labeled, is used to estimate both the conditional distribution function and the entropy function of the CRPS, while the second, unlabeled, is employed to calibrate the desired rejection rate. Notably, the control of the rejection rate is distribution-free. Under mild conditions, we show that our procedure is asymptotically as effective as the optimal rule, both in terms of error rate and rejection rate. Additionally, we establish rates of convergence for our approach based on distributional k-nearest neighbor. A numerical analysis on real-world datasets demonstrates the strong performance of our procedure|\n", "2503.23729": "|**2025-03-31**|**Integral regularization PINNs for evolution equations**|Xiaodong Feng, Haojiong Shangguan, Tao Tang et.al.|[2503.23729](http://arxiv.org/abs/2503.23729)|null||Evolution equations, including both ordinary differential equations (ODEs) and partial differential equations (PDEs), play a pivotal role in modeling dynamic systems. However, achieving accurate long-time integration for these equations remains a significant challenge. While physics-informed neural networks (PINNs) provide a mesh-free framework for solving PDEs, they often suffer from temporal error accumulation, which limits their effectiveness in capturing long-time behaviors. To alleviate this issue, we propose integral regularization PINNs (IR-PINNs), a novel approach that enhances temporal accuracy by incorporating an integral-based residual term into the loss function. This method divides the entire time interval into smaller sub-intervals and enforces constraints over these sub-intervals, thereby improving the resolution and correlation of temporal dynamics. Furthermore, IR-PINNs leverage adaptive sampling to dynamically refine the distribution of collocation points based on the evolving solution, ensuring higher accuracy in regions with sharp gradients or rapid variations. Numerical experiments on benchmark problems demonstrate that IR-PINNs outperform original PINNs and other state-of-the-art methods in capturing long-time behaviors, offering a robust and accurate solution for evolution equations.|\n", "2503.23728": "|**2025-03-31**|**Performing Path Integral Molecular Dynamics Using Artificial Intelligence Enhanced Molecular Simulation Framework**|Cheng Fan, Maodong Li, Sihao Yuan et.al.|[2503.23728](http://arxiv.org/abs/2503.23728)|null||This study employed an artificial intelligence-enhanced molecular simulation framework to enable efficient Path Integral Molecular Dynamics (PIMD) simulations. Owing to its modular architecture and high-throughput capabilities, the framework effectively mitigates the computational complexity and resource-intensive limitations associated with conventional PIMD approaches. By integrating machine learning force fields (MLFFs) into the framework, we rigorously tested its performance through two representative cases: a small-molecule reaction system (double proton transfer in formic acid dimer) and a bulk-phase transition system (water-ice phase transformation). Computational results demonstrate that the proposed framework achieves accelerated PIMD simulations while preserving quantum mechanical accuracy. These findings show that nuclear quantum effects can be captured for complex molecular systems, using relatively low computational cost.|\n", "2503.23716": "|**2025-03-31**|**On blowup solution in NLS equation under dispersion or nonlinearity management**|Jing Li, Cui Ning, Xiaofei Zhao et.al.|[2503.23716](http://arxiv.org/abs/2503.23716)|null||In this paper, we study the dispersion-managed nonlinear Schr\\\"odinger (DM-NLS) equation $$ i\\partial_t u(t,x)+\\gamma(t)\\Delta u(t,x)=|u(t,x)|^{\\frac4d}u(t,x),\\quad x\\in\\R^d, $$ and the nonlinearity-managed NLS (NM-NLS) equation: $$ i\\partial_t u(t,x)+\\Delta u(t,x)=\\gamma(t)|u(t,x)|^{\\frac4d}u(t,x), \\quad x\\in\\R^d, $$ where $\\gamma(t)$ is a periodic function which is equal to $-1$ when $t\\in (0,1]$ and is equal to $1$ when $t\\in (1,2]$. The two models share the feature that the focusing and defocusing effects convert periodically. For the classical focusing NLS, it is known that the initial data $$ u_0(x)=T^{-\\frac{d}{2}}\\fe^{i\\frac{|x|^2}{4T} -i\\frac{\\omega^2}{T}}Q_\\omega\\left(\\frac{x}{T}\\right) $$ leads to a blowup solution $$(T-t)^{-\\frac{d}{2}}\\fe^{i\\frac{|x|^2}{4(T-t)} -i\\frac{\\omega^2}{T-t}}Q_\\omega\\left(\\frac{x}{T-t}\\right), $$ so when $T\\leq1$, this is also a blowup solution for DM-NLS and NM-NLS which blows up in the first focusing layer.   For DM-NLS, we prove that when $T>1$, the initial data $u_0$ above does not lead to a finite-time blowup and the corresponding solution is globally well-posed. For NM-NLS, we prove the global well-posedness for $T\\in(1,2)$ and we construct solution that can blow up at any focusing layer. The theoretical studies are complemented by extensive numerical explorations towards understanding the stabilization effects in the two models and addressing their difference.|\n", "2503.23690": "|**2025-03-31**|**Effects of the delocalized charge distribution in trapped ion-atom collisions**|Ruiren Shi, Michael Drewsen, Jes\u00fas P\u00e9rez-R\u00edos et.al.|[2503.23690](http://arxiv.org/abs/2503.23690)|null||In the study of ion-atom interactions, the ion often remain trapped during the experiments. However, the effects of the trapping potential of the ion on ion-neutral interactions remain largely unexplored. Although trap-assisted ion-neutral complex formation has been experimentally studied and described by applying semiclassical theories where the ion is treated as a point charge particle, the potential effect of a delocalized charge distribution of a confined ion due to its quantum mechanical wavefunction has not been considered. To remedy this, in the present theoretical work we substitute the point charge of the ion with a delocalized charged distribution according to its motional ground state in the trap. Our results show that the trapping frequency and hence the spatial extension of the ion's ground-state wavefunction drastically affects the elastic and transport cross sections in interactions with neutral atoms. Stimulated by these results, we propose experimental procedures to verify the effects of the delocalize charge distribution in ion-atom interactions via measuring the heating rate of the ion due to the energy transfer in atomic collisions. Our novel approach brings new possibilities for investigating ion-neutral systems and, through them, new perspectives on ionic polarons and potentially a better understanding of trap-induced losses in ion-neutral experiments.|\n", "2504.02814": "|**2025-04-03**|**Convergence of the Markovian iteration for coupled FBSDEs via a differentiation approach**|Zhipeng Huang, Cornelis W. Oosterlee et.al.|[2504.02814](http://arxiv.org/abs/2504.02814)|null|28 pages, 2 figures|In this paper, we investigate the Markovian iteration method for solving coupled forward-backward stochastic differential equations (FBSDEs) featuring a fully coupled forward drift, meaning the drift term explicitly depends on both the forward and backward processes. An FBSDE system typically involves three stochastic processes: the forward process $X$, the backward process $Y$ representing the solution, and the $Z$ process corresponding to the scaled derivative of $Y$. Prior research by Bender and Zhang (2008) has established convergence results for iterative schemes dealing with $Y$-coupled FBSDEs. However, extending these results to equations with $Z$ coupling poses significant challenges, especially in uniformly controlling the Lipschitz constant of the decoupling fields across iterations and time steps within a fixed-point framework.   To overcome this issue, we propose a novel differentiation-based method for handling the $Z$ process. This approach enables improved management of the Lipschitz continuity of decoupling fields, facilitating the well-posedness of the discretized FBSDE system with fully coupled drift. We rigorously prove the convergence of our Markovian iteration method in this more complex setting. Finally, numerical experiments confirm our theoretical insights, showcasing the effectiveness and accuracy of the proposed methodology.|\n", "2504.02721": "|**2025-04-03**|**Phase transitions for interacting particle systems on random graphs**|Benedetta Bertoli, Grigorios A. Pavliotis, Niccol\u00f2 Zagli et.al.|[2504.02721](http://arxiv.org/abs/2504.02721)|null|28 pages, 4 figures|In this paper, we study weakly interacting diffusion processes on random graphs. Our main focus is on the properties of the mean-field limit and, in particular, on the nonuniqueness of stationary states. By extending classical bifurcation analysis to include multichromatic interaction potentials and random graph structures, we explicitly identify bifurcation points and relate them to the eigenvalues of the graphon integral operator. Furthermore, we characterize the resulting McKean-Vlasov PDE as a gradient flow with respect to a suitable metric. We combine these theoretical results with the spectral analysis of the linearized McKean-Vlasov operator and extensive numerical simulations to gain insight into the stability and long-term behaviour of stationary solutions. In addition, we provide strong evidence that (minus) the interaction energy of the interacting particle system serves as a natural order parameter. In particular, beyond the transition point and for multichromatic interactions, we observe an energy cascade that is strongly linked to the dynamical metastability of the system.|\n", "2504.02700": "|**2025-04-03**|**Centroidal Voronoi Tessellations as Electrostatic Equilibria: A Generalized Thomson Problem in Convex Domains**|Zachary Mullaghy et.al.|[2504.02700](http://arxiv.org/abs/2504.02700)|null||We present a variational framework in which Centroidal Voronoi Tessellations (CVTs) arise as local minimizers of a generalized electrostatic energy functional. By modeling interior point distributions in a convex domain as repelling charges balanced against a continuous boundary charge, we show that the resulting equilibrium configurations converge to CVT structures. We prove this by showing that CVTs minimize both the classical centroidal energy and the electrostatic potential, establishing a connection between geometric quantization and potential theory. Finally, we introduce a thermodynamic annealing scheme for global CVT optimization, rooted in Boltzmann statistics and random walk dynamics. By introducing a scheme for varying time steps (faster or slower cooling) we show that the set of minima of the centroid energy functional (and therefore the electrostatic potential) can be recovered. By recovering a set of generator locations corresponding to each minimum we can create a lattice continuation that allows for a customizable framework for individual minimum seeking.|\n", "2504.02672": "|**2025-04-03**|**Certified Model Order Reduction for parametric Hermitian eigenproblems**|Mattia Manucci, Benjamin Stamm, Zhuoyao Zeng et.al.|[2504.02672](http://arxiv.org/abs/2504.02672)|null||This article deals with the efficient and certified numerical approximation of the smallest eigenvalue and the associated eigenspace of a large-scale parametric Hermitian matrix. For this aim, we rely on projection-based model order reduction (MOR), i.e., we approximate the large-scale problem by projecting it onto a suitable subspace and reducing it to one of a much smaller dimension. Such a subspace is constructed by means of weak greedy-type strategies. After detailing the connections with the reduced basis method for source problems, we introduce a novel error estimate for the approximation error related to the eigenspace associated with the smallest eigenvalue. Since the difference between the second smallest and the smallest eigenvalue, the so-called spectral gap, is crucial for the reliability of the error estimate, we propose efficiently computable upper and lower bounds for higher eigenvalues and for the spectral gap, which enable the assembly of a subspace for the MOR approximation of the spectral gap. Based on that, a second subspace is then generated for the MOR approximation of the eigenspace associated with the smallest eigenvalue. We also provide efficiently computable conditions to ensure that the multiplicity of the smallest eigenvalue is fully captured in the reduced space. This work is motivated by a specific application: the repeated identifications of the states with minimal energy, the so-called ground states, of parametric quantum spin system models.|\n", "2504.02629": "|**2025-04-03**|**An efficient and energy-stable IMEX splitting scheme for dispersed multiphase flows**|Douglas Pacheco, Richard Schussnig et.al.|[2504.02629](http://arxiv.org/abs/2504.02629)|null||Volume-averaged Navier--Stokes equations are used in various applications to model systems with two or more interpenetrating phases. Each fluid obeys its own momentum and mass equations, and the phases are typically coupled via drag forces and a shared pressure. Monolithic solvers can therefore be very expensive and difficult to implement. On the other hand, designing robust splitting schemes requires making both pressure and drag forces explicit without sacrificing temporal stability. In this context, we derive a new first-order pressure-correction method based on the incompressibility of the mean velocity field, combined with an explicit treatment of the drag forces. Furthermore, the convective terms are linearised using extrapolated velocities, while the viscous terms are treated semi-implicitly. This gives us an implicit-explicit (IMEX) method that is very robust not only due to its unconditional energy stability, but also because it does not require any type of fixed-point iterations. Each time step involves only linear, scalar transport equations and a single Poisson problem as building blocks, thereby offering both efficiency and simplicity. We rigorously prove temporal stability without any time-step size restrictions, and the theory is confirmed through two-phase numerical examples.|\n", "2504.02513": "|**2025-04-03**|**Adaptive Bivariate Quarklet Tree Approximation via Anisotropic Tensor Quarklets**|Marc Hovemann et.al.|[2504.02513](http://arxiv.org/abs/2504.02513)|null||This paper deals with near-best approximation of a given bivariate function using elements of quarkonial tensor frames. For that purpose we apply anisotropic tensor products of the univariate B-spline quarklets introduced around 2017 by Dahlke, Keding and Raasch. We introduce the concept of bivariate quarklet trees and develop an adaptive algorithm which allows for generalized hp-approximation of a given bivariate function by selected frame elements. It is proved that this algorithm is near-best, which means that as long as some standard conditions concerning local errors are fulfilled it provides an approximation with an error close to that one of the best possible quarklet tree approximation. For this algorithm the complexity is investigated. Moreover, we use our techniques to approximate a bivariate test function with inverse-exponential rates of convergence. It can be expected that the results presented in this paper serve as important building block for the design of adaptive wavelet-hp-methods for solving PDEs in the bivariate setting with very good convergence properties.|\n", "2504.02488": "|**2025-04-03**|**A Behaviour and Disease Model of Testing and Isolation**|Matthew Ryan, Roslyn I. Hickson, Edward M. Hill et.al.|[2504.02488](http://arxiv.org/abs/2504.02488)|**[link](https://github.com/Matthew-Ryan1995/BaD_testing_and_isolation)**|22 pages, 10 figures|There has been interest in the interactions between infectious disease dynamics and behaviour for most of the history of mathematical epidemiology. This has included consideration of which mathematical models best capture each phenomenon, as well as their interaction, but typically in a manner that is agnostic to the exact behaviour in question. Here, we investigate interacting behaviour and disease dynamics specifically related to behaviours around testing and isolation. This epidemiological-behavioural interaction is of particular interest as, prospectively, it is well-placed to be informed by real-world data temporally monitoring test results and compliance with testing policy. To carry out our investigation we extend an existing \"behaviour and disease\" (BaD) model by incorporating the dynamics of symptomatic testing and isolation. We provide a dynamical systems analysis of the ordinary differential equations that define this model, providing theoretical results on its behaviour early in a new outbreak (particularly its basic reproduction number) and endemicity of the system (its steady states and associated stability criteria). We then supplement these findings with a numerical analysis to inform how temporal and cumulative outbreak metrics depend on the model parameter values for epidemic and endemic regimes. As the presented interdisciplinary modelling approach can accommodate further extensions (including, but not limited to, adding testing capacity, decay in behavioural effects and multiple pathogen variants), we hope that our work will encourage further modelling studies integrating specific measured behaviours and disease dynamics that may reduce the health and economic impacts of future epidemics.|\n", "2504.02475": "|**2025-04-03**|**Heat Conduction with Phase Change in Permafrost Modules of Vegetation Models**|David H\u00f6tten, Jenny Niebsch, Ronny Ramlau et.al.|[2504.02475](http://arxiv.org/abs/2504.02475)|null||We consider the problem of heat conduction with phase change, that is essential for permafrost modeling in Land Surface Models and Dynamic Global Vegetation Models. These models require minimal computational effort and an extremely robust solver for large-scale, long-term simulations. The weak enthalpy formulation of the Stefan problem is used as the mathematical model and a finite element method is employed for the discretization. Leveraging the piecewise affine structure of the nonlinear time-stepping equation system, we demonstrate that this system has a unique solution and provide a solver that is guaranteed to find this solution in a finite number of steps from any initial guess. Comparisons with the Neumann analytical solution and tests in the Lund-Potsdam-Jena managed Land vegetation model reveal that the new method does not introduce significantly higher computational costs than the widely used DECP method while providing greater accuracy. In particular, it avoids a known nonphysical artifact in the solution.|\n", "2504.02432": "|**2025-04-03**|**Robust Randomized Low-Rank Approximation with Row-Wise Outlier Detection**|Aidan Tiruvan et.al.|[2504.02432](http://arxiv.org/abs/2504.02432)|null|27 pages, 9 figures, preprint|Robust low-rank approximation under row-wise adversarial corruption can be achieved with a single pass, randomized procedure that detects and removes outlier rows by thresholding their projected norms. We propose a scalable, non-iterative algorithm that efficiently recovers the underlying low-rank structure in the presence of row-wise adversarial corruption. By first compressing the data with a Johnson Lindenstrauss projection, our approach preserves the geometry of clean rows while dramatically reducing dimensionality. Robust statistical techniques based on the median and median absolute deviation then enable precise identification and removal of outlier rows with abnormally high norms. The subsequent rank-k approximation achieves near-optimal error bounds with a one pass procedure that scales linearly with the number of observations. Empirical results confirm that combining random sketches with robust statistics yields efficient, accurate decompositions even in the presence of large fractions of corrupted rows.|\n", "2504.02422": "|**2025-04-04**|**Applying Space-Group Symmetry to Speed up Hybrid-Functional Calculations within the Framework of Numerical Atomic Orbitals**|Yu Cao, Min-Ye Zhang, Peize Lin et.al.|[2504.02422](http://arxiv.org/abs/2504.02422)|null||Building upon the efficient implementation of hybrid density functionals (HDFs) for large-scale periodic systems within the framework of numerical atomic orbital bases using the localized resolution of identity (RI) technique, we have developed an algorithm that exploits the space group symmetry in key operation steps of HDF calculations, leading to further improvements in two ways. First, the reduction of $\\mathbf{k}$-points in the Brillouin zone can reduce the number of Kohn-Sham equations to be solved. This necessitates the correct implementation of the rotation relation between the density matrices of equivalent $\\mathbf{k}$-points within the representation of atomic orbitals. Second, the reduction of the real-space sector can accelerate the construction of the exact-exchange part of the Hamiltonian in real space. We have implemented this algorithm in the ABACUS software interfaced with LibRI, and tested its performance for several types of crystal systems with different symmetries. The expected speed-up is achieved in both aspects: the time of solving the Kohn-Sham equations decreases in proportion with the reduction of $\\mathbf{k}$-points, while the construction of the Hamiltonian in real space is sped up by several times, with the degree of acceleration depending on the size and symmetry of the system.|\n", "2504.02413": "|**2025-04-03**|**Dislocation-density based crystal plasticity: stability and attractors in slip rate driven processes**|Jalal Smiri, O\u011fuz Umut Salman, Ioan R. Ionescu et.al.|[2504.02413](http://arxiv.org/abs/2504.02413)|null||Dislocation-density based crystal plasticity (CP) models are introduced to account for the microstructral changes throughout the deformation process, enabling more quantitative predictions of the deformation process compared to slip-system resistance-based plasticity models. In this work, we present a stability analysis of slip rate driven processes for some established dislocation density-based models, including the Kocks and Mecking (KM) model and its variants. Our analysis can be generalized to any type of dislocation density model, providing a broader framework for understanding the stability of such systems. Interestingly, we demonstrate that even size-independent models can exhibit size-dependent effects through variations in initial dislocation density. Notably, the initial dislocation density significantly influences material hardening or softening responses. To further explore these phenomena, we conduct numerical simulations of micro-pillar compression using an Eulerian crystal plasticity framework. Our results show that dislocation-density-based CP models effectively capture microstructural evolution in small-scale materials, offering critical insights for the design of miniaturized mechanical devices and advanced materials in nanotechnology.|\n", "2504.02367": "|**2025-04-03**|**CrystalFormer-RL: Reinforcement Fine-Tuning for Materials Design**|Zhendong Cao, Lei Wang et.al.|[2504.02367](http://arxiv.org/abs/2504.02367)|**[link](https://github.com/deepmodeling/crystalformer)**|8 pages, 6 figures|Reinforcement fine-tuning has instrumental enhanced the instruction-following and reasoning abilities of large language models. In this work, we explore the applications of reinforcement fine-tuning to the autoregressive transformer-based materials generative model CrystalFormer (arXiv:2403.15734) using discriminative machine learning models such as interatomic potentials and property prediction models. By optimizing reward signals-such as energy above the convex hull and material property figures of merit-reinforcement fine-tuning infuses knowledge from discriminative models into generative models. The resulting model, CrystalFormer-RL, shows enhanced stability in generated crystals and successfully discovers crystals with desirable yet conflicting material properties, such as substantial dielectric constant and band gap simultaneously. Notably, we observe that reinforcement fine-tuning enables not only the property-guided novel material design ability of generative pre-trained model but also unlocks property-driven material retrieval from the unsupervised pre-training dataset. Leveraging rewards from discriminative models to fine-tune materials generative models opens an exciting gateway to the synergies of the machine learning ecosystem for materials.|\n", "2504.02267": "|**2025-04-03**|**Third-Order Spontaneous Parametric Down Conversion in Dielectric Nonlinear Resonant Metasurfaces**|Miguel Y. Bacaoco, Kirill Koshelev, Alexander S. Solntsev et.al.|[2504.02267](http://arxiv.org/abs/2504.02267)|null||We propose a general scheme to investigate photon triplet generation (PTG) via third-order spontaneous parametric downconversion (TOSPDC) in $\\chi^{(3)}$ nonlinear structures. Our approach leverages the quantum-classical correspondence between TOSPDC and its reverse classical process, three-wave sum-frequency generation (TSFG), to efficiently estimate the PTG rate. We apply this framework to nonlinear metasurfaces supporting quasi-bound states in the continuum (qBICs) in the optical range. From numerical analysis of non-collinear TSFG with degenerate input waves at qBIC wavelengths, we predict wavelength-tunable three-photon emission with spatio-angular correlations. These findings establish a novel method for modelling TOSPDC and also highlight the potential of nonlinear resonant metasurfaces as compact free-space photon triplet sources with quantum state control.|\n", "2504.02245": "|**2025-04-03**|**Traffic Flow Data Completion and Anomaly Diagnosis via Sparse and Low-Rank Tensor Optimization**|Junxi Man, Yumin Lin, Xiaoyu Li et.al.|[2504.02245](http://arxiv.org/abs/2504.02245)|null||Spatiotemporal traffic time series, such as traffic speed data, collected from sensing systems are often incomplete, with considerable corruption and large amounts of missing values. A vast amount of data conceals implicit data structures, which poses significant challenges for data recovery issues, such as mining the potential spatio-temporal correlations of data and identifying abnormal data. In this paper, we propose a Tucker decomposition-based sparse low-rank high-order tensor optimization model (TSLTO) for data imputation and anomaly diagnosis. We decompose the traffic tensor data into low-rank and sparse tensors, and establish a sparse low-rank high-order tensor optimization model based on Tucker decomposition. By utilizing tools of non-smooth analysis for tensor functions, we explore the optimality conditions of the proposed tensor optimization model and design an ADMM optimization algorithm for solving the model. Finally, numerical experiments are conducted on both synthetic data and a real-world dataset: the urban traffic speed dataset of Guangzhou. Numerical comparisons with several representative existing algorithms demonstrate that our proposed approach achieves higher accuracy and efficiency in traffic flow data recovery and anomaly diagnosis tasks.|\n", "2504.02228": "|**2025-04-03**|**Stochastic positivity-preserving symplectic splitting methods for stochastic Lotka--Volterra predator-prey model**|Liying Zhang, Xinyue Kang, Lihai Ji et.al.|[2504.02228](http://arxiv.org/abs/2504.02228)|null||In this paper, we present two stochastic positive-preserving symplectic methods for the stochastic Lotka-Volterra predator-prey model driven by a multiplicative noise. To inherit the intrinsic characteristic of the original system, the stochastic Lie--Trotter splitting method and the stochastic Strang splitting method are introduced, which are proved to preserve the positivity of the numerical solution and possess the discrete stochastic symplectic conservation law as well. By deriving the uniform boundedness of the $p$-th moment of the numerical solution, we prove that the strong convergence orders of these two methods are both one in the $L^2(\\Omega)$-norm. Finally, we validate the theoretical results through two and four dimensional numerical examples.|\n", "2504.02226": "|**2025-04-03**|**Error analysis of the diffuse domain finite element method for second order parabolic equations**|Wenrui Hao, Lili Ju, Yuejin Xu et.al.|[2504.02226](http://arxiv.org/abs/2504.02226)|null||In this paper, we analyze the diffuse domain finite element method (DDFE) to solve a class of second-order parabolic partial differential equations defined in general irregular domains. The proposed method first applies the diffuse domain method (DDM) with a phase-field function to extend the target parabolic equation to a similar problem defined over a larger rectangular domain that contains the original physical domain. The transformed equation is then discretized by using the finite element method with continuous piecewise multilinear basis functions in space and the BDF2 scheme in time to produce a fully discrete numerical scheme. Based on the weighted Sobolev spaces, we prove the convergence of the DDM solution to the original solution as the interface thickness parameter goes to zero, with the corresponding approximation errors under the $L^2$ and $H^1$ norms. Furthermore, the optimal error estimate for the fully discrete DDFE scheme is also obtained under the $H^1$ norm. Various numerical experiments are finally carried out to validate the theoretical results and demonstrate the performance of the proposed method.|\n", "2504.02198": "|**2025-04-03**|**Error Analysis of Sampling Algorithms for Approximating Stochastic Optimal Control**|Anant A. Joshi, Amirhossein Taghvaei, Prashant G. Mehta et.al.|[2504.02198](http://arxiv.org/abs/2504.02198)|null||This paper is concerned with the error analysis of two types of sampling algorithms, namely model predictive path integral (MPPI) and an interacting particle system (\\IPS) algorithm, that have been proposed in the literature for numerical approximation of the stochastic optimal control. The analysis is presented through the lens of Gibbs variational principle. For an illustrative example of a single-stage stochastic optimal control problem, analytical expressions for approximation error and scaling laws, with respect to the state dimension and sample size, are derived. The analytical results are illustrated with numerical simulations.|\n", "2504.02183": "|**2025-04-03**|**Numerical Framework for Multimode Jaynes- and Tavis-Cummings Models Incorporating the Modified Langevin Noise Formalism: Non-Markovian Analysis of Atom-Field Interactions in Dissipative Electromagnetic Environments**|Hyunwoo Choi, Weng Cho Chew, Dong-Yeop Na et.al.|[2504.02183](http://arxiv.org/abs/2504.02183)|null||We present a novel numerical framework that integrates the modified Langevin noise formalism into the multimode Jaynes- and Tavis-Cummings models, enabling a first-principles, non-Markovian analysis of atom-field interactions in dissipative electromagnetic (EM) environments that account for both radiative losses and absorptive dissipation in lossy dielectric media (or satisfying general inhomogeneous causal media exhibiting both dispersion and absorption effects). In the modified Langevin noise formalism, the boundary- and medium-assisted (BA and MA) fields, which constitute a continuum set of EM modes in dissipative EM environments, are numerically obtained using the finite-element method (FEM). Specifically, BA field modes are extracted by solving plane-wave scattering problems, while MA field modes are determined through point-source radiation problems. These numerically obtained BA and MA field modes are then incorporated into the multimode Jaynes- and Tavis-Cummings models such that the coupling strength between atoms and BA-MA field modes can be calculated for the study of atom-field interactions in dissipative EM environments. The proposed methodology captures non-Markovian atomic dynamics that cannot be described by traditional quantum master equations under the Markovian approximation. To validate the accuracy of the proposed numerical framework, we present four numerical examples: (i) a two-level system (TLS) in a perfect electric conductor (PEC) half-space; (ii) dissipative cavity electrodynamics with two limiting cases approaching spontaneous emission in free space and ideal Rabi oscillations; (iii) super-radiance in TLS arrays; and (iv) entanglement sudden death of two TLSs inside dissipative cavities. The proposed methodology can serve as a ground-truth numerical simulator for studying atom-field interactions in general dissipative EM environments.|\n", "2504.02117": "|**2025-04-02**|**Vectorised Parallel in Time methods for low-order discretizations with application to Porous Media problems**|Christian Engwer, Alexander Schell, Nils-Arne Dreier et.al.|[2504.02117](http://arxiv.org/abs/2504.02117)|null||High order methods have shown great potential to overcome performance issues of simulations of partial differential equations (PDEs) on modern hardware, still many users stick to low-order, matrixbased simulations, in particular in porous media applications. Heterogeneous coefficients and low regularity of the solution are reasons not to employ high order discretizations. We present a new approach for the simulation of instationary PDEs that allows to partially mitigate the performance problems. By reformulating the original problem we derive a parallel in time time integrator that increases the arithmetic intensity and introduces additional structure into the problem. By this it helps accelerate matrix-based simulations on modern hardware architectures. Based on a system for multiple time steps we will formulate a matrix equation that can be solved using vectorised solvers like Block Krylov methods. The structure of this approach makes it applicable for a wide range of linear and nonlinear problems. In our numerical experiments we present some first results for three different PDEs, a linear convection-diffusion equation, a nonlinear diffusion-reaction equation and a realistic example based on the Richards' equation.|\n", "2504.02089": "|**2025-04-02**|**Perturbations and Phase Transitions in Swarm Optimization Algorithms**|Tom\u00e1\u0161 Vantuch, Ivan Zelinka, Andrew Adamatzky et.al.|[2504.02089](http://arxiv.org/abs/2504.02089)|null||Natural systems often exhibit chaotic behavior in their space-time evolution. Systems transiting between chaos and order manifest a potential to compute, as shown with cellular automata and artificial neural networks. We demonstrate that swarm optimization algorithms also exhibit transitions from chaos, analogous to a motion of gas molecules, when particles explore solution space disorderly, to order, when particles follow a leader, similar to molecules propagating along diffusion gradients in liquid solutions of reagents. We analyze these `phase-like' transitions in swarm optimization algorithms using recurrence quantification analysis and Lempel-Ziv complexity estimation. We demonstrate that converging iterations of the optimization algorithms are statistically different from non-converging ones in a view of applied chaos, complexity and predictability estimating indicators.   An identification of a key factor responsible for the intensity of their phase transition is the main contribution of this paper. We examined an optimization as a process with three variable factors -- an algorithm, number generator and optimization function. More than 9.000 executions of the optimization algorithm revealed that the nature of an applied algorithm itself is the main source of the phase transitions. Some of the algorithms exhibit larger transition-shifting behavior while others perform rather transition-steady computing. These findings might be important for future extensions of these algorithms.|\n", "2504.05300": "|**2025-04-07**|**Dimension-Free Convergence of Diffusion Models for Approximate Gaussian Mixtures**|Gen Li, Changxiao Cai, Yuting Wei et.al.|[2504.05300](http://arxiv.org/abs/2504.05300)|null||Diffusion models are distinguished by their exceptional generative performance, particularly in producing high-quality samples through iterative denoising. While current theory suggests that the number of denoising steps required for accurate sample generation should scale linearly with data dimension, this does not reflect the practical efficiency of widely used algorithms like Denoising Diffusion Probabilistic Models (DDPMs). This paper investigates the effectiveness of diffusion models in sampling from complex high-dimensional distributions that can be well-approximated by Gaussian Mixture Models (GMMs). For these distributions, our main result shows that DDPM takes at most $\\widetilde{O}(1/\\varepsilon)$ iterations to attain an $\\varepsilon$-accurate distribution in total variation (TV) distance, independent of both the ambient dimension $d$ and the number of components $K$, up to logarithmic factors. Furthermore, this result remains robust to score estimation errors. These findings highlight the remarkable effectiveness of diffusion models in high-dimensional settings given the universal approximation capability of GMMs, and provide theoretical insights into their practical success.|\n", "2504.05266": "|**2025-04-07**|**Differential forms: Lagrange interpolation, sampling and approximation on polynomial admissible integral k-meshes**|Ludovico Bruni Bruno, Federico Piazzon et.al.|[2504.05266](http://arxiv.org/abs/2504.05266)|null||In this work we address the problem of interpolating and approximating differential forms starting from data defined by integration. We show that many aspects of nodal interpolation can naturally be carried to this more general framework; in contrast, some of them require the introduction of geometric and measure theoretic hypotheses. After characterizing the norms of the operators involved, we introduce the concept of admissible integral k-mesh, which allows for the construction of robust approximation schemes, and is used to extract interpolation sets with high stability properties. To this end, the concepts of Fekete currents and Leja sequences of currents are formalized, and a numerical scheme for their approximation is proposed.|\n", "2504.05248": "|**2025-04-07**|**PINNverse: Accurate parameter estimation in differential equations from noisy data with constrained physics-informed neural networks**|Marius Almanst\u00f6tter, Roman Vetter, Dagmar Iber et.al.|[2504.05248](http://arxiv.org/abs/2504.05248)|null|17 pages, 5 figures|Parameter estimation for differential equations from measured data is an inverse problem prevalent across quantitative sciences. Physics-Informed Neural Networks (PINNs) have emerged as effective tools for solving such problems, especially with sparse measurements and incomplete system information. However, PINNs face convergence issues, stability problems, overfitting, and complex loss function design. Here we introduce PINNverse, a training paradigm that addresses these limitations by reformulating the learning process as a constrained differential optimization problem. This approach achieves a dynamic balance between data loss and differential equation residual loss during training while preventing overfitting. PINNverse combines the advantages of PINNs with the Modified Differential Method of Multipliers to enable convergence on any point on the Pareto front. We demonstrate robust and accurate parameter estimation from noisy data in four classical ODE and PDE models from physics and biology. Our method enables accurate parameter inference also when the forward problem is expensive to solve.|\n", "2504.05169": "|**2025-04-07**|**Machine learning interatomic potential can infer electrical response**|Peichen Zhong, Dongjin Kim, Daniel S. King et.al.|[2504.05169](http://arxiv.org/abs/2504.05169)|null||Modeling the response of material and chemical systems to electric fields remains a longstanding challenge. Machine learning interatomic potentials (MLIPs) offer an efficient and scalable alternative to quantum mechanical methods but do not by themselves incorporate electrical response. Here, we show that polarization and Born effective charge (BEC) tensors can be directly extracted from long-range MLIPs within the Latent Ewald Summation (LES) framework, solely by learning from energy and force data. Using this approach, we predict the infrared spectra of bulk water under zero or finite external electric fields, ionic conductivities of high-pressure superionic ice, and the phase transition and hysteresis in ferroelectric PbTiO$_3$ perovskite. This work thus extends the capability of MLIPs to predict electrical response--without training on charges or polarization or BECs--and enables accurate modeling of electric-field-driven processes in diverse systems at scale.|\n", "2504.05151": "|**2025-04-07**|**Error formulas for block rational Krylov approximations of matrix functions**|Stefano Massei, Leonardo Robol et.al.|[2504.05151](http://arxiv.org/abs/2504.05151)|null||This paper investigates explicit expressions for the error associated with the block rational Krylov approximation of matrix functions. Two formulas are proposed, both derived from characterizations of the block FOM residual. The first formula employs a block generalization of the residual polynomial, while the second leverages the block collinearity of the residuals. A posteriori error bounds based on the knowledge of spectral information of the argument are derived and tested on a set of examples. Notably, both error formulas and their corresponding upper bounds do not require the use of quadratures for their practical evaluation.|\n", "2504.05149": "|**2025-04-07**|**Fast Convolutions on $\\mathbb{Z}^2\\backslash SE(2)$ via Radial Translational Dependence and Classical FFT**|Arash Ghaani Farashahi, Gregory S. Chirikjian et.al.|[2504.05149](http://arxiv.org/abs/2504.05149)|null||Let $\\mathbb{Z}^2\\backslash SE(2)$ denote the right coset space of the subgroup consisting of translational isometries of the orthogonal lattice $\\mathbb{Z}^2$ in the non-Abelian group of planar motions $SE(2)$. This paper develops a fast and accurate numerical scheme for approximation of functions on $\\mathbb{Z}^2\\backslash SE(2)$. We address finite Fourier series of functions on the right coset space $\\mathbb{Z}^2\\backslash SE(2)$ using finite Fourier coefficients. The convergence/error analysis of finite Fourier coefficients are investigated. Conditions are established for the finite Fourier coefficients to converge to the Fourier coefficients. The matrix forms of the finite transforms are discussed. The implementation of the discrete method to compute numerical approximation of $SE(2)$-convolutions with functions which are radial in translations are considered. The paper is concluded by discussing capability of the numerical scheme to develop fast algorithms for approximating multiple convolutions with functions with are radial in translations.|\n", "2504.05124": "|**2025-04-07**|**Generators of $H^1(\u0393, \\partial \u0393^c)$ with $\\partial \u0393^c \\subset \\partial \u0393$ for Triangulated Surfaces $\u0393$: Construction and Classification of Global Loops**|Silvano Pitassi et.al.|[2504.05124](http://arxiv.org/abs/2504.05124)|null||Given a compact surface $\\Gamma$ embedded in $\\mathbb R^3$ with boundary $\\partial \\Gamma$, our goal is to construct a set of representatives for a basis of the relative cohomology group $H^1(\\Gamma, \\partial \\Gamma^c)$, where $\\Gamma^c$ is a specified subset of $\\partial \\Gamma$. To achieve this, we propose a novel graph-based algorithm with two key features: it is applicable to non-orientable surfaces, thereby generalizing previous approaches, and it has a worst-case time complexity that is linear in the number of edges of the mesh $\\mathcal{K}$ triangulating $\\Gamma$. Importantly, this algorithm serves as a critical pre-processing step to address the low-frequency breakdown encountered in boundary element discretizations of integral equation formulations.|\n", "2504.05066": "|**2025-04-07**|**Turing instability for nonlocal heterogeneous reaction-diffusion systems: A computer-assisted proof approach**|Maxime Breden, Maxime Payan, Cordula Reisch et.al.|[2504.05066](http://arxiv.org/abs/2504.05066)|null||This paper provides a computer-assisted proof for the Turing instability induced by heterogeneous nonlocality in reaction-diffusion systems. Due to the heterogeneity and nonlocality, the linear Fourier analysis gives rise to \\textit{strongly coupled} infinite differential systems. By introducing suitable changes of basis as well as the Gershgorin disks theorem for infinite matrices, we first show that all $N$-th Gershgorin disks lie completely on the left half-plane for sufficiently large $N$. For the remaining finitely many disks, a computer-assisted proof shows that if the intensity $\\delta$ of the nonlocal term is large enough, there is precisely one eigenvalue with positive real part, which proves the Turing instability. Moreover, by detailed study of this eigenvalue as a function of $\\delta$, we obtain a sharp threshold $\\delta^*$ which is the bifurcation point for Turing instability.|\n", "2504.05036": "|**2025-04-07**|**Hybrid Nitsche for distributed computing**|Tom Gustafsson, Antti Hannukainen, Vili Kohonen et.al.|[2504.05036](http://arxiv.org/abs/2504.05036)|null||We extend the distributed finite element method of [1], built upon model order reduction, to arbitrary polynomial degree using a hybrid Nitsche scheme. This new method considerably simplifies the transformation of the finite element system to the reduced basis for large problems. We prove that the error of the reduced Nitsche solution converges optimally with respect to the approximation order of the finite element spaces and linearly with respect to the dimension reduction parameter $\\epsilon$. Numerical tests with nontrivial tetrahedral meshes using second-degree polynomial bases support the theoretical results.|\n", "2504.05026": "|**2025-04-07**|**Multi-level Neural Networks for high-dimensional parametric obstacle problems**|Martin Eigel, Cosmas Hei\u00df, Janina E. Sch\u00fctte et.al.|[2504.05026](http://arxiv.org/abs/2504.05026)|null||A new method to solve computationally challenging (random) parametric obstacle problems is developed and analyzed, where the parameters can influence the related partial differential equation (PDE) and determine the position and surface structure of the obstacle. As governing equation, a stationary elliptic diffusion problem is assumed. The high-dimensional solution of the obstacle problem is approximated by a specifically constructed convolutional neural network (CNN). This novel algorithm is inspired by a finite element constrained multigrid algorithm to represent the parameter to solution map. This has two benefits: First, it allows for efficient practical computations since multi-level data is used as an explicit output of the NN thanks to an appropriate data preprocessing. This improves the efficacy of the training process and subsequently leads to small errors in the natural energy norm. Second, the comparison of the CNN to a multigrid algorithm provides means to carry out a complete a priori convergence and complexity analysis of the proposed NN architecture. Numerical experiments illustrate a state-of-the-art performance for this challenging problem.|\n", "2504.05022": "|**2025-04-07**|**Solving the fully nonlinear Monge-Amp\u00e8re equation using the Legendre-Kolmogorov-Arnold Network method**|Bingcheng Hu, Lixiang Jin, Zhaoxiang Li et.al.|[2504.05022](http://arxiv.org/abs/2504.05022)|null|20 pages, 12 figures|In this paper, we propose a novel neural network framework, the Legendre-Kolmogorov-Arnold Network (Legendre-KAN) method, designed to solve fully nonlinear Monge-Amp\\`ere equations with Dirichlet boundary conditions. The architecture leverages the orthogonality of Legendre polynomials as basis functions, significantly enhancing both convergence speed and solution accuracy compared to traditional methods. Furthermore, the Kolmogorov-Arnold representation theorem provides a strong theoretical foundation for the interpretability and optimization of the network. We demonstrate the effectiveness of the proposed method through numerical examples, involving both smooth and singular solutions in various dimensions. This work not only addresses the challenges of solving high-dimensional and singular Monge-Amp\\`ere equations but also highlights the potential of neural network-based approaches for complex partial differential equations. Additionally, the method is applied to the optimal transport problem in image mapping, showcasing its practical utility in geometric image transformation. This approach is expected to pave the way for further enhancement of KAN-based applications and numerical solutions of PDEs across a wide range of scientific and engineering fields.|\n", "2504.04989": "|**2025-04-07**|**Randomized block Krylov method for approximation of truncated tensor SVD**|Malihe Nobakht Kooshkghazi, Salman Ahmadi-Asl, Andre L. F. de Almeida et.al.|[2504.04989](http://arxiv.org/abs/2504.04989)|null||This paper is devoted to studying the application of the block Krylov subspace method for approximation of the truncated tensor SVD (T-SVD). The theoretical results of the proposed randomized approach are presented. Several experimental experiments using synthetics and real-world data are conducted to verify the efficiency and feasibility of the proposed randomized approach, and the numerical results show that the proposed method provides promising results. Applications of the proposed approach to data completion and data compression are presented.|\n", "2504.04951": "|**2025-04-07**|**Anisotropic space-time goal-oriented error control and mesh adaptivity for convection-diffusion-reaction equations**|M. Bause, M. Bruchh\u00e4user, B. Endtmayer et.al.|[2504.04951](http://arxiv.org/abs/2504.04951)|null||We present an anisotropic goal-oriented error estimator based on the Dual Weighted Residual (DWR) method for time-dependent convection-diffusion-reaction (CDR) equations. Using anisotropic interpolation operators the estimator is elementwise separated with respect to the single directions in space and time leading to adaptive, anisotropic mesh refinement in a natural way. To prevent spurious oscillations the streamline upwind Petrov-Galerkin (SUPG) method is applied to stabilize the underlying system in the case of high P\\'{e}clet numbers. Efficiency and robustness of the underlying algorithm are demonstrated for different goal functionals. The directional error indicators quantify anisotropy of the solution with respect to the goal, and produce meshes that efficiently capture sharp layers. Numerical examples show the superiority of the proposed approach over isotropic adaptive and global mesh refinement using established benchmarks for convection-dominated transport.|\n", "2504.04947": "|**2025-04-07**|**Phase transitions in swarm optimization algorithms**|Tom\u00e1\u0161 Vantuch, Ivan Zelinka, Andrew Adamatzky et.al.|[2504.04947](http://arxiv.org/abs/2504.04947)|null||Natural systems often exhibit chaotic behavior in their space-time evolution. Systems transiting between chaos and order manifest a potential to compute, as shown with cellular automata and artificial neural networks. We demonstrate that swarms optimisation algorithms also exhibit transitions from chaos, analogous to motion of gas molecules, when particles explore solution space disorderly, to order, when particles follow a leader, similar to molecules propagating along diffusion gradients in liquid solutions of reagents. We analyse these `phase-like' transitions in swarm optimization algorithms using recurrence quantification analysis and Lempel-Ziv complexity estimation. We demonstrate that converging and non-converging iterations of the optimization algorithms are statistically different in a view of applied chaos, complexity and predictability estimating indicators.|\n", "2504.04941": "|**2025-04-07**|**The Kratos Framework for Heterogeneous Astrophysical Simulations: Ray Tracing, Reacting Flow and Thermochemistry**|Lile Wang et.al.|[2504.04941](http://arxiv.org/abs/2504.04941)|null|15 pages, 7 figures, submitted to ApJS|Thermochemistry, ray-tracing radiation, and radiation-matter interactions are important processes which are computationally difficult to model in astrophysical simulations, addressed by introducing novel algorithms optimized for heterogeneous architectures in the Kratos framework. Key innovations include a stoichiometry-compatible reconstruction scheme for consistent chemical species advection, which ensures element conservation while avoiding matrix inversions, and a LU decomposition method specifically designed for multi-thread parallelization in order to solve stiff thermochemical ordinary differential equations with high efficiency. The framework also implements efficient ray-tracing techniques for radiation transport for radiation-matter interactions. Various verification tests, spanning from chemical advection, combustion, Str\\\"omgren spheres, and detonation dynamics, are conducted to demonstrate the accuracy and robustness of Kratos, with results closely matching semi-analytic solutions and benchmarks such as Cantera and the Shock and Detonation Toolbox. The modular design and performance optimizations position it as a versatile tool for studying coupled microphysical processes in the diverse environments of contemporary astrophysical studies.|\n", "2504.04929": "|**2025-04-07**|**The Linearized Vlasov-Maxwell System as a Hamiltonian System**|Dominik Bell, Martin Campos Pinto, Stefan Possanner et.al.|[2504.04929](http://arxiv.org/abs/2504.04929)|null||We present a Hamiltonian formulation for the linearized Vlasov-Maxwell system with a Maxwellian background distribution function. We discuss the geometric properties of the model at the continuous level, and how to discretize the model in the GEMPIC framework [1]. This method allows us to keep the structure of the system at the semi-discrete level. To integrate the model in time, we employ a Poisson splitting and discuss how to integrate each subsystem separately. We test the model against the full Vlasov-Maxwell model with a control variate method for noise reduction; the two chosen test-cases are the weak Landau damping and the Bernstein waves. Both test-cases exhibit the same physical properties for short simulations but our model enjoys better long-time stability and energy conservation due to its geometric construction. The model is implemented in the open-source Python library STRUPHY [2, 3].|\n", "2504.04912": "|**2025-04-07**|**An iterative process for the feasibility-seeking problem with sets that are unions of convex sets**|Yair Censor, Alexander J. Zaslavski et.al.|[2504.04912](http://arxiv.org/abs/2504.04912)|null|Accepted for publication in: Communications in Optimization Theory|In this paper we deal with the feasibility-seeking problem for unions of convex sets (UCS) sets and propose an iterative process for its solution. Renewed interest in this problem stems from the fact that it was recently discovered to serve as a modeling approach in fields of applications and from the ongoing recent research efforts to handle non-convexity in feasibility-seeking.|\n", "2504.04887": "|**2025-04-07**|**A mechanism for growth of topological entropy and global changes of the shape of chaotic attractors**|Daniel Wilczak, Sergio Serrano, Roberto Barrio et.al.|[2504.04887](http://arxiv.org/abs/2504.04887)|null||The theoretical and numerical understanding of the key concept of topological entropy is an important problem in dynamical systems. Most studies have been carried out on maps (discrete-time systems). We analyse a scenario of global changes of the structure of an attractor in continuous-time systems leading to an unbounded growth of the topological entropy of the underlying dynamical system. As an example, we consider the classical Roessler system. We show that for an explicit range of parameters a chaotic attractor exists. We also prove the existence of a sequence of bifurcations leading to the growth of the topological entropy. The proofs are computer-aided.|\n", "2504.04859": "|**2025-04-07**|**Block BDDC/FETI-DP Preconditioners for Three-Field mixed finite element Discretizations of Biot's consolidation model**|Hanyu Chu, Luca Franco Pavarino, Stefano Zampini et.al.|[2504.04859](http://arxiv.org/abs/2504.04859)|null||In this paper, we construct and analyze a block dual-primal preconditioner for Biot's consolidation model approximated by three-field mixed finite elements based on a displacement, pressure, and total pressure formulation. The domain is decomposed into nonoverlapping subdomains, and the continuity of the displacement component across the subdomain interface is enforced by introducing a Lagrange multiplier. After eliminating all displacement variables and the independent subdomain interior components of pressure and total pressure, the problem is reduced to a symmetric positive definite linear system for the subdomain interface pressure, total pressure, and the Lagrange multiplier. This reduced system is solved by a preconditioned conjugate gradient method, with a block dual-primal preconditioner using a Balancing Domain Decomposition by Constraints (BDDC) preconditioner for both the interface total pressure block and the interface pressure blocks, as well as a Finite Element Tearing and Interconnecting-Dual Primal (FETI-DP) preconditioner for the Lagrange multiplier block. By analyzing the conditioning of the preconditioned subsystem associated with the interface pressure and total pressure components, we obtain a condition number bound of the preconditioned system, which is scalable in the number of subdomains, poly-logarithmic in the ratio of subdomain and mesh sizes, and robust with respect to the parameters of the model. Extensive numerical experiments confirm the theoretical result of the proposed algorithm.|\n", "2504.04852": "|**2025-04-07**|**An invariant-region-preserving scheme for a convection-reaction-Cahn-Hilliard multiphase model of biofilm growth in slow sand filters**|Julio Careaga, Stefan Diehl, Jaime Manr\u00edquez et.al.|[2504.04852](http://arxiv.org/abs/2504.04852)|null|32 pages, 5 images|A multidimensional model of biofilm growth present in the supernatant water of a Slow Sand Filter is derived. The multiphase model, consisting of solid and liquid phases, is written as a convection-reaction system with a Cahn-Hilliard-type equation with degenerate mobility coupled to a Stokes-flow equation for the mixture velocity. An upwind discontinuous Galerkin approach is used to approximate the convection-reaction equations, whereas an $H^1$-conforming primal formulation is proposed for the Stokes system. By means of a splitting procedure due to the reaction terms, an invariant-region principle is shown for the concentration unknowns, namely non-negativity for all phases and an upper bound for the total concentration of the solid phases. Numerical examples with reduced biofilm reactions are presented to illustrate the performance of the model and numerical scheme.|\n", "2504.07058": "|**2025-04-09**|**Physics informed neural network for forward and inverse modeling of low grade brain tumors**|K. Murari, P. Roul, S. Sundar et.al.|[2504.07058](http://arxiv.org/abs/2504.07058)|null||A low grade tumor is a slow growing tumor with a lower likelihood of spreading compared to high grade tumors. Mathematical modeling using partial differential equations (PDEs) plays a crucial role in describing tumor behavior, growth and progression. This study employs the Burgess and extended Fisher Kolmogorov equations to model low-grade brain tumors. We utilize Physics Informed Neural Networks (PINNs) based algorithm to develop an automated numerical solver for these models and explore their application in solving forward and inverse problems in brain tumor modeling. The study aims to demonstrate that the PINN based algorithms serve as advanced methodologies for modeling brain tumor dynamics by integrating deep learning with physics-informed principles. Additionally, we establish generalized error bounds in terms of training and quadrature errors. The convergence and stability of the neural network are derived for both models. Numerical tests confirm the accuracy and efficiency of the algorithms in both linear and nonlinear cases. Additionally, a statistical analysis of the numerical results is presented.|\n", "2504.06998": "|**2025-04-09**|**A Krylov projection algorithm for large symmetric matrices with dense spectra**|Vladimir Druskin J\u00f6rn Zimmerling et.al.|[2504.06998](http://arxiv.org/abs/2504.06998)|null|Block Lanczos, Quadrature, Transfer function, Kre\\u{i}n-Nudelman,   Hermite-Pad\\'e|We consider the approximation of $B^T (A+sI)^{-1} B$ for large s.p.d. $A\\in\\mathbb{R}^{n\\times n}$ with dense spectrum and $B\\in\\mathbb{R}^{n\\times p}$, $p\\ll n$. We target the computations of Multiple-Input Multiple-Output (MIMO) transfer functions for large-scale discretizations of problems with continuous spectral measures, such as linear time-invariant (LTI) PDEs on unbounded domains. Traditional Krylov methods, such as the Lanczos or CG algorithm, are known to be optimal for the computation of $(A+sI)^{-1}B$ with real positive $s$, resulting in an adaptation to the distinctively discrete and nonuniform spectra. However, the adaptation is damped for matrices with dense spectra. It was demonstrated in [Zimmerling, Druskin, Simoncini, Journal of Scientific Computing 103(1), 5 (2025)] that averaging Gau{\\ss} and Gau\\ss -Radau quadratures computed using the block-Lanczos method significantly reduces approximation errors for such problems. Here, we introduce an adaptive Kre\\u{i}n-Nudelman extension to the (block) Lanczos recursions, allowing further acceleration at negligible $o(n)$ cost. Similar to the Gau\\ss -Radau quadrature, a low-rank modification is applied to the (block) Lanczos matrix. However, unlike the Gau\\ss -Radau quadrature, this modification depends on $\\sqrt{s}$ and can be considered in the framework of the Hermite-Pad\\'e approximants, which are known to be efficient for problems with branch-cuts, that can be good approximations to dense spectral intervals. Numerical results for large-scale discretizations of heat-diffusion and quasi-magnetostatic Maxwell's operators in unbounded domains confirm the efficiency of the proposed approach.|\n", "2504.06993": "|**2025-04-10**|**Screening of material defects using universal machine-learning interatomic potentials**|Ethan Berger, Mohammad Bagheri, Hannu-Pekka Komsa et.al.|[2504.06993](http://arxiv.org/abs/2504.06993)|null||Finding new materials with previously unknown atomic structure or materials with optimal set of properties for a specific application greatly benefits from computational modeling. Recently, such screening has been dramatically accelerated by the invent of universal machine-learning interatomic potentials that offer first principles accuracy at orders of magnitude lower computational cost. Their application to the screening of defects with desired properties or to finding new stable compounds with high density of defects, however, has not been explored. Here, we show that the universal machine-learning interatomic potentials have reached sufficient accuracy to enable large-scale screening of defective materials. We carried out vacancy calculations for 86 259 materials in the Materials Project database and analyzed the formation energies in terms of oxidation numbers. We further demonstrate the application of these models for finding new materials at or below the convex hull of known materials and for simulated etching of low-dimensional materials.|\n", "2504.06951": "|**2025-04-09**|**GLT hidden structures in mean-field quantum spin systems**|Christiaan J. F. van de Ven, Muhammad Faisal Khan, S. Serra-Capizzano et.al.|[2504.06951](http://arxiv.org/abs/2504.06951)|null|20 pages, 6 figures|This work explores structured matrix sequences arising in mean-field quantum spin systems. We express these sequences within the framework of generalized locally Toeplitz (GLT) $*$-algebras, leveraging the fact that each GLT matrix sequence has a unique GLT symbol. This symbol characterizes both the asymptotic singular value distribution and, for Hermitian or quasi-Hermitian sequences, the asymptotic spectral distribution. Specifically, we analyze two cases of real symmetric matrix sequences stemming from mean-field quantum spin systems and determine their associated distributions using GLT theory. Our study concludes with visualizations and numerical tests that validate the theoretical findings, followed by a discussion of open problems and future directions.|\n", "2504.06938": "|**2025-04-09**|**On the Compressibility of Integral Operators in Anisotropic Wavelet Coordinates**|Helmut Harbrecht, Remo von Rickenbach et.al.|[2504.06938](http://arxiv.org/abs/2504.06938)|null||The present article is concerned with the s*-compressibility of classical boundary integral operators in anisotropic wavelet coordinates. Having the s*-compressibility at hand, one can design adaptive wavelet algorithms which are asymptotically optimal, meaning that any target accuracy can be achieved at a computational expense that stays proportional to the number of degrees of freedom (within the setting determined by an underlying wavelet basis) that would ideally be necessary for realising that target accuracy if full knowledge about the unknown solution were given. As we consider here anisotropic wavelet coordinates, we can achieve higher convergence rates compared to the standard, isotropic setting. Especially, edge singularities of anisotropic nature can be resolved.|\n", "2504.06912": "|**2025-04-09**|**A modified-residue prescription to calculate dynamical correlation functions**|Igor Benek-Lins, Jonathan Discenza, Saurabh Maiti et.al.|[2504.06912](http://arxiv.org/abs/2504.06912)|null|12 pages, 8 figures|One of the challenges in using numerical methods to address many-body problems is the multi-dimensional integration over poles. More often that not, one needs such integrations to be evaluated as a function of an external variable. An example would be calculating dynamical correlations functions that are used to model response functions, where the external variable is the frequency. The standard numerical techniques rely on building an adaptive mesh, using special points in the Brillouin zone or using advanced smearing techniques. Most of these techniques, however, suffer when the grid is coarse. Here we propose that, if one knows the nature of the singularity in the integrand, one can define a residue and use it to faithfully estimate the integral and reproduce all the resulting singular features even with a coarse grid. We demonstrate the effectiveness of the method for different scenarios of calculating correlation functions with different resulting singular features, for calculating collective modes and densities of states. We also present a quantitative analysis of the error and show that this method can be widely applicable.|\n", "2504.06889": "|**2025-04-09**|**Mixed-Precision in High-Order Methods: the Impact of Floating-Point Precision on the ADER-DG Algorithm**|Marc Marot-Lassauzaie, Michael Bader et.al.|[2504.06889](http://arxiv.org/abs/2504.06889)|null||We present a mixed-precision implementation of the high-order discontinuous Galerkin method with ADER time stepping (ADER-DG) for solving hyperbolic systems of partial differential equations (PDEs) in the hyperbolic PDE engine ExaHyPE. The implementation provides a simple API extension for specifying the numerical precision for individual kernels, and thus allows for testing the effect of low and mixed precision on the accuracy of the solution. To showcase this, we study the impact of precision on the overall convergence order and actual accuracy of the method as achieved for four common hyperbolic PDE systems and five relevant scenarios that feature an analytic solution. For all scenarios, we also assess how sensitive each kernel of the ADER-DG algorithm is to using double, single or even half precision. This addresses the question where thoughtful adoption of mixed precision can mitigate hurtful effects of low precision on the overall simulation.|\n", "2504.06837": "|**2025-04-09**|**Discrete-to-continuum limit for nonlinear reaction-diffusion systems via EDP convergence for gradient systems**|Georg Heinze, Alexander Mielke, Artur Stephan et.al.|[2504.06837](http://arxiv.org/abs/2504.06837)|null||We investigate the convergence of spatial discretizations for reaction-diffusion systems with mass-action law satisfying a detailed balance condition. Considering systems on the d-dimensional torus, we construct appropriate space-discrete processes and show convergence not only on the level of solutions, but also on the level of the gradient systems governing the evolutions. As an important step, we prove chain rule inequalities for the reaction-diffusion systems as well as their discretizations, featuring a non-convex dissipation functional. The convergence is then obtained with variational methods by building on the recently introduced notion of gradient systems in continuity equation format.|\n", "2504.06809": "|**2025-04-09**|**Modeling and analysis methods for early detection of leakage points in gas transmission systems**|Ilgar Aliyev et.al.|[2504.06809](http://arxiv.org/abs/2504.06809)|null|16 pages, 1 figures, Graph 5, 14th International Conference on   Control, Modelling, Computing and Applications (CMCA 2025)|Early detection of leaks in gas transmission systems is crucial for ensuring uninterrupted gas supply, enhancing operational efficiency, and minimizing environmental and economic risks. This study aims to develop an analytical method for accurately identifying leak locations in gas pipelines based on unsteady gas flow dynamics. A novel approach is proposed that utilizes pressure variations at the inlet and outlet points to determine the minimum fixation time (t = t1) required for real-time leak detection. Through mathematical modeling and numerical analysis, the study demonstrates that the ratio of pressure drops at different points along the pipeline can be effectively used to pinpoint leakage locations. The results indicate that the proposed method significantly improves detection accuracy and response time, making it a viable solution for integration into gas pipeline monitoring and control systems.|\n", "2504.06782": "|**2025-04-09**|**Probabilistic Grading and Classification System for End-of-Life Building Components Toward Circular Economy Loop**|Yiping Meng, Sergio Cavalaro, Mohamed Osmani et.al.|[2504.06782](http://arxiv.org/abs/2504.06782)|null|23 pages, 12 figures, 8tables|The longevity and viability of construction components in a circular economy demand a robust, data-informed framework for reuse decision-making. This paper introduces a multi-level grading and classification system that combines Bayesian probabilistic modeling with scenario-based performance thresholds to assess the reusability of end-of-life modular components. By grading components across a five-tier scale, the system supports strategic decisions for reuse, up-use, or down-use, ensuring alignment with engineering standards and sustainability objectives. The model's development is grounded in empirical data from precast concrete wall panels, and its explainability is enhanced through decision tree logic and Sankey visualizations that trace the influence of contextual scenarios on classification outcomes. MGCS addresses the environmental, economic, and operational challenges of EoL management--reducing material waste, optimizing value recovery, and improving workflow efficiency. Through dynamic feature weighting and transparent reasoning, the system offers a practical yet rigorous pathway to embed circular thinking into construction industry practices.|\n", "2504.06774": "|**2025-04-09**|**Hybrid machine learning models based on physical patterns to accelerate CFD simulations: a short guide on autoregressive models**|Arindam Sengupta, Rodrigo Abad\u00eda-Heredia, Ashton Hetherington et.al.|[2504.06774](http://arxiv.org/abs/2504.06774)|null||Accurate modeling of the complex dynamics of fluid flows is a fundamental challenge in computational physics and engineering. This study presents an innovative integration of High-Order Singular Value Decomposition (HOSVD) with Long Short-Term Memory (LSTM) architectures to address the complexities of reduced-order modeling (ROM) in fluid dynamics. HOSVD improves the dimensionality reduction process by preserving multidimensional structures, surpassing the limitations of Singular Value Decomposition (SVD). The methodology is tested across numerical and experimental data sets, including two- and three-dimensional (2D and 3D) cylinder wake flows, spanning both laminar and turbulent regimes. The emphasis is also on exploring how the depth and complexity of LSTM architectures contribute to improving predictive performance. Simpler architectures with a single dense layer effectively capture the periodic dynamics, demonstrating the network's ability to model non-linearities and chaotic dynamics. The addition of extra layers provides higher accuracy at minimal computational cost. These additional layers enable the network to expand its representational capacity, improving the prediction accuracy and reliability. The results demonstrate that HOSVD outperforms SVD in all tested scenarios, as evidenced by using different error metrics. Efficient mode truncation by HOSVD-based models enables the capture of complex temporal patterns, offering reliable predictions even in challenging, noise-influenced data sets. The findings underscore the adaptability and robustness of HOSVD-LSTM architectures, offering a scalable framework for modeling fluid dynamics.|\n", "2504.06763": "|**2025-04-09**|**Convergence of a continuous Galerkin method for the Biot-Allard poroelasticity system**|Jakob S. Stokke, Markus Bause, Florin A. Radu et.al.|[2504.06763](http://arxiv.org/abs/2504.06763)|null||We study a space-time finite element method for a system of poromechanics with memory effects that are modeled by a convolution integral. In the literature, the system is referred to as the Biot-Allard model. We recast the model as a first-order system in time, where the memory effects are transformed into an auxiliary differential equation. This allows for a computationally efficient numerical scheme. The system is discretized by continuous Galerkin methods in time and equal-order finite element methods in space. An optimal order error estimate is proved for the norm of the first-order energy of the unknowns of the system. The estimate is confirmed by numerical experiments.|\n", "2504.06621": "|**2025-04-09**|**Computation of shape Taylor expansions**|Gang Bao, Jun Lai, Haoran Ma et.al.|[2504.06621](http://arxiv.org/abs/2504.06621)|null||Shape derivative is an important analytical tool for studying scattering problems involving perturbations in scatterers. Many applications, including inverse scattering, optimal design, and uncertainty quantification, are based on shape derivatives. However, computing high order shape derivatives is challenging due to the complexity of shape calculus. This work introduces a comprehensive method for computing shape Taylor expansions in two dimensions using recurrence formulas. The approach is developed under sound-soft, sound-hard, impedance, and transmission boundary conditions. Additionally, we apply the shape Taylor expansion to uncertainty quantification in wave scattering, enabling high order moment estimation for the scattered field under random boundary perturbations. Numerical examples are provided to illustrate the effectiveness of the shape Taylor expansion in achieving high order approximations.|\n", "2504.06603": "|**2025-04-09**|**Asymptotic Variance in the Central Limit Theorem for Multilevel Markovian Stochastic Approximation**|Ajay Jasra, Abylay Zhumekenov et.al.|[2504.06603](http://arxiv.org/abs/2504.06603)|null||In this note we consider the finite-dimensional parameter estimation problem associated to inverse problems. In such scenarios, one seeks to maximize the marginal likelihood associated to a Bayesian model. This latter model is connected to the solution of partial or ordinary differential equation. As such, there are two primary difficulties in maximizing the marginal likelihood (i) that the solution of differential equation is not always analytically tractable and (ii) neither is the marginal likelihood. Typically (i) is dealt with using a numerical solution of the differential equation, leading to a numerical bias and (ii) has been well studied in the literature using, for instance, Markovian stochastic approximation. It is well-known that to reduce the computational effort to obtain the maximal value of the parameter, one can use a hierarchy of solutions of the differential equation and combine with stochastic gradient methods. Several approaches do exactly this. In this paper we consider the asymptotic variance in the central limit theorem, associated to known estimates and find bounds on the asymptotic variance in terms of the precision of the solution of the differential equation. The significance of these bounds are the that they provide missing theoretical guidelines on how to set simulation parameters; that is, these appear to be the first mathematical results which help to run the methods efficiently in practice.|\n", "2504.06583": "|**2025-04-09**|**Aplicando diferencias finitas para resolver ecuaciones y sistemas de ecuaciones diferenciales parciales sobre dominios planos irregulares simplemente conexos y no conexos**|Miriam Sosa-D\u00edaz, Faustino Sanchez-Garduno et.al.|[2504.06583](http://arxiv.org/abs/2504.06583)|null|In Spanish language, in preparation for journal submission|Using exhaustion method and finite differences a new method to solve system of partial differential equations and is presented. This method allows design algorithm to solve linear and nonlinear systems in irregular domains. Applying this method to solve linear and nonlinear problems with prescribed conditions Dirichlet over two-dimensional irregular domains are analyzed.|\n", "2504.06475": "|**2025-04-08**|**Successive randomized compression: A randomized algorithm for the compressed MPO-MPS product**|Chris Cama\u00f1o, Ethan N. Epperly, Joel A. Tropp et.al.|[2504.06475](http://arxiv.org/abs/2504.06475)|null|29 pages, 5 figures|Tensor networks like matrix product states (MPSs) and matrix product operators (MPOs) are powerful tools for representing exponentially large states and operators, with applications in quantum many-body physics, machine learning, numerical analysis, and other areas. In these applications, computing a compressed representation of the MPO--MPS product is a fundamental computational primitive. For this operation, this paper introduces a new single-pass, randomized algorithm, called successive randomized compression (SRC), that improves on existing approaches in speed or in accuracy. The performance of the new algorithm is evaluated on synthetic problems and unitary time evolution problems for quantum spin systems.|\n", "2504.06458": "|**2025-04-08**|**Solving Power System Problems using Adiabatic Quantum Computing**|Zeynab Kaseb, Matthias Moller, Peter Palensky et.al.|[2504.06458](http://arxiv.org/abs/2504.06458)|null|3 pages, 3 figures|This letter proposes a novel combinatorial optimization framework that reformulates existing power system problems into a format executable on quantum annealers. The proposed framework accommodates both normal and complex numbers and enables efficient handling of large-scale problems, thus ensuring broad applicability across power system problems. As a proof of concept, we demonstrate its applicability in two classical problems: (i) power system parameter identification, where we estimate the admittance matrix given voltage and current measurements, and (ii) power flow analysis, where we reformulate the nonlinear equations governing active and reactive power balance. The results show that the proposed framework effectively and efficiently solves both linear and nonlinear power system problems, and thus offers significant advantages in scenarios where traditional solvers face challenges, such as ill-conditioned systems and fault conditions.|\n", "2504.06425": "|**2025-04-08**|**Neural Network Enhanced Polyconvexification of Isotropic Energy Densities in Computational Mechanics**|Lo\u00efc Balazi, Timo Neumeier, Malte A. Peter et.al.|[2504.06425](http://arxiv.org/abs/2504.06425)|null|24 pages, 14 figures|We present a neural network approach for fast evaluation of parameter-dependent polyconvex envelopes, which are crucial in computational mechanics. Our method uses a neural network architecture that inherently encodes polyconvexity in the main variable by combining a feature extraction layer that computes the minors function on the signed singular value characterisation of isotropic energy densities with a partially input convex neural network (PICNN). Polyconvex underestimation is weakly enforced by penalisation during training, as are the symmetries of the function. As a guiding example, we focus on a well-known isotropic damage problem, reformulated in terms of signed singular values, and apply a splitting approach to reduce the dimensionality of the parameter space, thereby making training more tractable. Numerical experiments show that the networks achieve sufficient accuracy for engineering applications while providing high compression and significant speed-up over traditional polyconvexification schemes. Most importantly, the network adapts to varying physical or material parameters, enabling real-time polyconvexification in large-scale computational mechanics scenarios.|\n", "2504.06388": "|**2025-04-08**|**Elimination of spurious oscillations on photoemission spectra**|Mart\u00edn Barlari, Diego G. Arb\u00f3, Mar\u00eda Silvia Gravielle et.al.|[2504.06388](http://arxiv.org/abs/2504.06388)|null|23 pages, 11 figures|We present a method for accurately computing transition probabilities in one-dimensional photoionization problems. Our approach involves solving the time-dependent Schr\\\"odinger equation and projecting its solution onto scattering states that satisfy the correct incoming or outgoing boundary conditions. Conventionally, the photoelectron emission spectrum is obtained by projecting the time-evolved wavefunction onto the stationary continuum eigenstates of the unperturbed, time-independent Hamiltonian. However, when the spatial potential is symmetric, both the initial bound state and the final continuum states exhibit well-defined parity. The propagated wavefunction retains structural features of the initial bound state, including its parity. As a result, changes in the parity of the continuum states can introduce substantial variations in the projections, leading to spurious oscillations in the computed electron emission spectrum. Our method circumvents this issue by employing scattering states without defined parity. Furthermore, it enables the calculation of directional emission, making it possible to study emission asymmetries. To illustrate the capabilities of our scattering projection method, we analyze the partial differential photoionization probabilities of Al(111) metallic surfaces under short laser pulses at grazing incidence.|\n", "2504.06378": "|**2025-04-08**|**On Mixed-Precision Iterative Methods and Analysis for Nearly Completely Decomposable Markov Processes**|Vasileios Kalantzis, Mark S. Squillante, Chai Wah Wu et.al.|[2504.06378](http://arxiv.org/abs/2504.06378)|null|30 pages, 3 figures|In this paper we consider the problem of computing the stationary distribution of nearly completely decomposable Markov processes, a well-established area in the classical theory of Markov processes with broad applications in the design, modeling, analysis and optimization of computer systems. We design general classes of algorithmic solution approaches that exploit forms of mixed-precision computation to significantly reduce computation times and that exploit forms of iterative approximate methods to mitigate the impact of inaccurate computations, further reduce computation times, and ensure convergence. Then we derive a mathematical analysis of our general algorithmic approaches that establishes theoretical results on approximation errors, convergence behaviors, and other algorithmic properties. Numerical experiments demonstrate that our general algorithmic approaches provide significant improvements in computation times over the most efficient existing numerical methods.|\n", "2504.07893": "|**2025-04-10**|**Molecular excited state in the interaction quench dynamics of two different atoms in a two-dimensional anisotropic trap**|I. S. Ishmukhamedov, A. S. Ishmukhamedov, Zh. E. Jalankuzov et.al.|[2504.07893](http://arxiv.org/abs/2504.07893)|null|This preprint has not undergone peer review (when applicable) or any   post-submission improvements or corrections. The Version of Record of this   article is published in The European Physical Journal Plus, and is available   online at https://doi.org/10.1140/epjp/s13360-024-04864-2|We explore the interaction quench dynamics of two atoms with different masses and subject to different trapping potentials. Notably, under such anisotropic conditions, the nonequilibrium dynamics can lead to the occupation of molecular excited states. We consider cases of quenching from attractive to repulsive interaction and vice versa, analyzing the impact of the pre- and postquench states. The analysis of overlap integrals for the both states reveals a significant contribution from the molecular excited state. Moreover, the overlap with the prequench states might serve as an indicator of when this excited state may emerge. Additionally, we calculate the energy spectrum for the lowest levels in the both isotropic and anisotropic harmonic traps. Throughout our study, we use a Gaussian-shaped finite-range interaction potential.|\n", "2504.07850": "|**2025-04-10**|**Probabilistic Multi-Criteria Decision-Making for Circularity Performance of Modern Methods of Construction Products**|Yiping Meng, Sergio Cavalaro, Frozan Dizaye Mohamed Osmani et.al.|[2504.07850](http://arxiv.org/abs/2504.07850)|null|37 pages,30 figures,4 tables|The construction industry faces increasingly more significant pressure to reduce resource consumption, minimise waste, and enhance environmental performance. Towards the transition to a circular economy in the construction industry, one of the challenges is the lack of a standardised assessment framework and methods to measure circularity at the product level. To support a more sustainable and circular construction industry through robust and enhanced scenario analysis, this paper integrates probabilistic analysis into the coupled assessment framework; this research addresses uncertainties associated with multiple criteria and diverse stakeholders in the construction industry to enable more robust decision-making support on both circularity and sustainability performance. By demonstrating the application in three real-world MMC products, the proposed framework offers a novel approach to simultaneously assess the circularity and sustainability of MMC products with robustness and objectiveness.|\n", "2504.07835": "|**2025-04-10**|**Pychop: Emulating Low-Precision Arithmetic in Numerical Methods and Neural Networks**|Erin Carson, Xinye Chen et.al.|[2504.07835](http://arxiv.org/abs/2504.07835)|null||Motivated by the growing demand for low-precision arithmetic in computational science, we exploit lower-precision emulation in Python -- widely regarded as the dominant programming language for numerical analysis and machine learning. Low-precision training has revolutionized deep learning by enabling more efficient computation and reduced memory and energy consumption while maintaining model fidelity. To better enable numerical experimentation with and exploration of low precision computation, we developed the Pychop library, which supports customizable floating-point formats and a comprehensive set of rounding modes in Python, allowing users to benefit from fast, low-precision emulation in numerous applications. Pychop also introduces interfaces for both PyTorch and JAX, enabling efficient low-precision emulation on GPUs for neural network training and inference with unparalleled flexibility.   In this paper, we offer a comprehensive exposition of the design, implementation, validation, and practical application of Pychop, establishing it as a foundational tool for advancing efficient mixed-precision algorithms. Furthermore, we present empirical results on low-precision emulation for image classification and object detection using published datasets, illustrating the sensitivity of the use of low precision and offering valuable insights into its impact. Pychop enables in-depth investigations into the effects of numerical precision, facilitates the development of novel hardware accelerators, and integrates seamlessly into existing deep learning workflows. Software and experimental code are publicly available at https://github.com/inEXASCALE/pychop.|\n", "2504.07809": "|**2025-04-10**|**A Riemannian Gradient Descent Method for the Least Squares Inverse Eigenvalue Problem**|Alban Bloor Riley, Marcus Webb, Michael L. Baker et.al.|[2504.07809](http://arxiv.org/abs/2504.07809)|null||We address an algorithm for the least squares fitting of a subset of the eigenvalues of an unknown Hermitian matrix lying an an affine subspace, called the Lift and Projection (LP) method, due to Chen and Chu (SIAM Journal on Numerical Analysis, 33 (1996), pp.2417-2430). The LP method iteratively `lifts' the current iterate onto the spectral constraint manifold then 'projects' onto the solution's affine subspace. We prove that this is equivalent to a Riemannian Gradient Descent with respect to a natural Riemannian metric. This insight allows us to derive a more efficient implementation, analyse more precisely its global convergence properties, and naturally append additional constraints to the problem. We provide several numerical experiments to demonstrate the improvement in computation time, which can be more than an order of magnitude if the eigenvalue constraints are on the smallest eigenvalues, the largest eigenvalues, or the eigenvalues closest to a given number. These experiments include an inverse eigenvalue problem arising in Inelastic Neutron Scattering of Manganese-6, which requires the least squares fitting of 16 experimentally observed eigenvalues of a $32400\\times32400$ sparse matrix from a 5-dimensional subspace of spin Hamiltonian matrices.|\n", "2504.07796": "|**2025-04-10**|**Numerical solution by shape optimization method to an inverse shape problem in multi-dimensional advection-diffusion problem with space dependent coefficients**|Elmehdi Cherrat, Lekbir Afraites, Julius Fergy Tiongson Rabago et.al.|[2504.07796](http://arxiv.org/abs/2504.07796)|null||This work focuses on numerically solving a shape identification problem related to advection-diffusion processes with space-dependent coefficients using shape optimization techniques. Two boundary-type cost functionals are considered, and their corresponding variations with respect to shapes are derived using the adjoint method, employing the chain rule approach. This involves firstly utilizing the material derivative of the state system and secondly using its shape derivative. Subsequently, an alternating direction method of multipliers (ADMM) combined with the Sobolev-gradient-descent algorithm is applied to stably solve the shape reconstruction problem. Numerical experiments in two and three dimensions are conducted to demonstrate the feasibility of the methods.|\n", "2504.07712": "|**2025-04-10**|**On the instabilities of naive FEM discretizations for PDEs with sign-changing coefficients**|Martin Halla, Florian Oberender et.al.|[2504.07712](http://arxiv.org/abs/2504.07712)|null||We consider a scalar diffusion equation with a sign-changing coefficient in its principle part. The well-posedness of such problems has already been studied extensively provided that the contrast of the coefficient is non-critical. Furthermore, many different approaches have been proposed to construct stable discretizations thereof, because naive finite element discretizations are expected to be non-reliable in general. However, no explicit example proving the actual instability is known and numerical experiments often do not manifest instabilities in a conclusive manner. To this end we construct an explicit example with a broad family of meshes for which we prove that the corresponding naive finite element discretizations are unstable. On the other hand, we also provide a broad family of (non-symmetric) meshes for which we prove that the discretizations are stable. Together, these two findings explain the results observed in numerical experiments.|\n", "2504.07637": "|**2025-04-10**|**Global approximation to the Boys functions for vectorized computation**|Dimitri N. Laikov et.al.|[2504.07637](http://arxiv.org/abs/2504.07637)|null|Boys, Boys, Boys. I'm looking for a good time|A fast approximation to the Boys functions (related to the lower incomplete gamma function of half-integer parameter) by a single closed-form analytical expression for all argument values have been developed and tested. Besides the exponential function needed anyway for downward recursion, it uses a small number of addition, multiplication, division, and square root operations, and thus is straightforward to vectorize.|\n", "2504.07580": "|**2025-04-10**|**A computational study of low precision incomplete Cholesky factorization preconditioners for sparse linear least-squares problems**|Jennifer Scott, Miroslav T\u016fma et.al.|[2504.07580](http://arxiv.org/abs/2504.07580)|null|25 pages, 5 figures, 11 tables|Our interest lies in the robust and efficient solution of large sparse linear least-squares problems. In recent years, hardware developments have led to a surge in interest in exploiting mixed precision arithmetic within numerical linear algebra algorithms to take advantage of potential savings in memory requirements, runtime and energy use, whilst still achieving the requested accuracy. We explore employing mixed precision when solving least-squares problems, focusing on the practicalities of developing robust approaches using low precision incomplete Cholesky factorization preconditioners. Key penalties associated with lower precision include a loss of reliability and less accuracy in the computed solution. Through experiments involving problems from practical applications, we study computing incomplete Cholesky factorizations of the normal matrix using low precision and using the factors to precondition LSQR using mixed precision. We investigate level-based and memory-limited incomplete factorization preconditioners. We find that the former are not effective for least-squares problems while the latter can provide high-quality preconditioners. In particular, half precision arithmetic can be considered if high accuracy is not required in the solution or the memory for the incomplete factors is very restricted; otherwise, single precision can be used, and double precision accuracy recovered while reducing memory consumption, even for ill-conditioned problems.|\n", "2504.07568": "|**2025-04-10**|**Ground State Energy of Helium Using a Four-Qubit Photonic Processor with the Variational Quantum Eigensolver (VQE)**|Badie Ghavami, Forouzan Mirmasoudi et.al.|[2504.07568](http://arxiv.org/abs/2504.07568)|null|7 pages, 3 figures, 1 table|To understand the properties and interactions of materials, and determining the ground state energies is one of the important challenges in quantum chemistry, materials science, and quantum mechanics, where quantum computing can play an important role for studying the properties of materials. In this study, we have explored the quantum processor application to compute the Helium (He) molecule ground state energy which utilizes the Variational Quantum Eigensolver (VQE) algorithm. In here, we have implemented VQE on a state-of-the-art quantum processor, optimizing a parameterized quantum circuit to minimize the energy expectation value of the He molecule's Hamiltonian on the four qubits processor. The obtained results of this work show a significant improvement in accuracy compared to classical computational methods, such as Hartree-Fock and density functional theory, which demonstrate the compute potential of quantum algorithms in quantum many-body problems. Thus, these results demonstrate the advantages of quantum computing in achieving high accuracy in simulations of molecular and material properties, and pave the way for future applications in more complex systems. This work highlights the potential of quantum processors in the fields of quantum chemistry, computational physics, and data science.|\n", "2504.07558": "|**2025-04-10**|**Atomic structure analysis of PL5 in silicon carbide with single-spin spectroscopy**|Yu Chen, Qi Zhang, Mingzhe Liu et.al.|[2504.07558](http://arxiv.org/abs/2504.07558)|null|6 pages, 5 figures|Divacancy (VV) spin defects in 4H polytype of silicon carbide (4H-SiC) are emerging candidates for quantum information processing and quantum sensing. Among these defects, PL5 and PL6 stand out due to their superior charge stability and optically detected magnetic resonance (ODMR) properties at room temperature. However, their atomic structures remain unresolved, with ongoing controversy regarding their potential association with stacking faults. Previous measurements relying on spin ensemble detection are insufficient to draw definitive conclusions. In this study, we conduct correlative imaging of stacking faults and PL5-6 at single-defect level, conclusively demonstrating that PL5-6 are not associated with stacking faults. Further investigation of PL5 through single-spin ODMR spectroscopy allows us to determine its six spatial orientations, as well as to measure the orientation of its transverse anisotropy spin splitting (E) and the statistical distribution of hyperfine splitting. These results and ab initio calculations suggest that PL5 should be VsiVc(hk) divacancy coupled with a nearby antisite atom (VVA). The structure resolution of PL5 starts the first step toward its controllable fabrication, paving the way for various applications.|\n", "2504.07520": "|**2025-04-10**|**Stability and Convergence of Strang Splitting Method for the Allen-Cahn Equation with Homogeneous Neumann Boundary Condition**|Chaoyu Quan, Zhijun Tan, Yanyao Wu et.al.|[2504.07520](http://arxiv.org/abs/2504.07520)|null||The Strang splitting method has been widely used to solve nonlinear reaction-diffusion equations, with most theoretical convergence analysis assuming periodic boundary conditions. However, such analysis presents additional challenges for the case of homogeneous Neumann boundary condition. In this work the Strang splitting method with variable time steps is investigated for solving the Allen--Cahn equation with homogeneous Neumann boundary conditions. Uniform $H^k$-norm stability is established under the assumption that the initial condition $u^0$ belongs to the Sobolev space $H^k(\\Omega)$ with integer $k\\ge 0$, using the Gagliardo--Nirenberg interpolation inequality and the Sobolev embedding inequality. Furthermore, rigorous convergence analysis is provided in the $H^k$-norm for initial conditions $u^0 \\in H^{k+6}(\\Omega)$, based on the uniform stability. Several numerical experiments are conducted to verify the theoretical results, demonstrating the effectiveness of the proposed method.|\n", "2504.07508": "|**2025-04-10**|**Parton Distribution Functions in the Schwinger model from Tensor Network States**|Mari Carmen Ban\u0169ls, Krzysztof Cichy, C. -J. David Lin et.al.|[2504.07508](http://arxiv.org/abs/2504.07508)|null|14 pages, 9 figures|Parton distribution functions (PDFs) describe the inner, non-perturbative structure of hadrons. Their computation involves matrix elements with a Wilson line along a direction on the light cone, posing significant challenges in Euclidean lattice calculations, where the time direction is not directly accessible. We propose implementing the light-front Wilson line within the Hamiltonian formalism using tensor network techniques. The approach is demonstrated in the massive Schwinger model (quantum electrodynamics in 1+1 dimensions), a toy model that shares key features with quantum chromodynamics. We present accurate continuum results for the fermion PDF of the vector meson at varying fermion masses, obtained from first principle calculations directly in Minkowski space. Our strategy also provides a useful path for quantum simulations and quantum computing.|\n", "2504.07391": "|**2025-04-10**|**High-order discretization errors for the Caputo derivative in H\u00f6lder spaces**|Xiangyi Peng, Lisen Ding, Dongling Wang et.al.|[2504.07391](http://arxiv.org/abs/2504.07391)|null||Building upon the recent work of Teso and Plociniczak (2025) regarding L1 discretization errors for the Caputo derivative in H\\\"{o}lder spaces, this study extends the analysis to higher-order discretization errors within the same functional framework. We first investigate truncation errors for the L2 and L1-2 methods, which approximate the Caputo derivative via piecewise quadratic interpolation. Then we generalize the results to arbitrary high-order discretization. Theoretical analyses reveal a unified error structure across all schemes: the convergence order equals the difference between the smoothness degree of the function space and the fractional derivative order, i.e., order of error = degree of smoothness - order of the derivative. Numerical experiments validate these theoretical findings.|\n", "2504.07388": "|**2025-04-10**|**Min-Max Optimisation for Nonconvex-Nonconcave Functions Using a Random Zeroth-Order Extragradient Algorithm**|Amir Ali Farzin, Yuen Man Pun, Philipp Braun et.al.|[2504.07388](http://arxiv.org/abs/2504.07388)|null||This study explores the performance of the random Gaussian smoothing Zeroth-Order ExtraGradient (ZO-EG) scheme considering min-max optimisation problems with possibly NonConvex-NonConcave (NC-NC) objective functions. We consider both unconstrained and constrained, differentiable and non-differentiable settings. We discuss the min-max problem from the point of view of variational inequalities. For the unconstrained problem, we establish the convergence of the ZO-EG algorithm to the neighbourhood of an $\\epsilon$-stationary point of the NC-NC objective function, whose radius can be controlled under a variance reduction scheme, along with its complexity. For the constrained problem, we introduce the new notion of proximal variational inequalities and give examples of functions satisfying this property. Moreover, we prove analogous results to the unconstrained case for the constrained problem. For the non-differentiable case, we prove the convergence of the ZO-EG algorithm to a neighbourhood of an $\\epsilon$-stationary point of the smoothed version of the objective function, where the radius of the neighbourhood can be controlled, which can be related to the ($\\delta,\\epsilon$)-Goldstein stationary point of the original objective function.|\n", "2504.07377": "|**2025-04-10**|**Euler-Lagrange study of Microbubble-Laden Turbulent Flow over Superhydrophobic surfaces**|Byeong-Cheon Kim, Kyoungsik Chang, Sang-Wook Lee et.al.|[2504.07377](http://arxiv.org/abs/2504.07377)|null|28 pages, 9 figures|For slow-speed ships, underwater vehicles, and pipe transportation systems, viscous resistance accounts for a large proportion of the total energy losses. As such, various technologies have been developed to reduce viscous resistance and enhance energy efficiency in these applications. Air injection and surface treatment are two representative drag reduction techniques. Additionally, efforts to combine multiple drag-reduction techniques have been the subject of extensive research. In this study, the synergistic effects of integrating microbubble injection and superhydrophobic Surface(SHS) drag reduction approaches were analyzed. A 2-way coupling Euler-Lagrange approach was used alongside direct numerical simulation, based on the spectral element method, to investigate the synergistic effects of applying two separate drag reduction methods. Three types of SHS were investigated in our simulations; post type, transverse ridge type, and ridge type. The drag reduction performances and flow characteristics of the various configurations, with and without microbubble injection, were compared in a turbulent horizontal channel flow with $Re_{\\tau}=180$. The results of these tests showed that, combining post-type SHS with microbubbles was the most effective, producing a synergistic drag reduction effect. However, combining microbubble injection with ridge-type SHS increased drag relative to ridge-type SHS alone, showing the importance of carefully selecting wall type for the best possible performance.|\n", "2504.07295": "|**2025-04-09**|**Advanced measurement techniques in quantum Monte Carlo: The permutation matrix representation approach**|Nic Ezzell, Itay Hen et.al.|[2504.07295](http://arxiv.org/abs/2504.07295)|**[link](https://github.com/naezzell/PMRQMC_fidsus)**|33 pages, 3 figures, 2 tables|In a typical finite temperature quantum Monte Carlo (QMC) simulation, estimators for simple static observables such as specific heat and magnetization are known. With a great deal of system-specific manual labor, one can sometimes also derive more complicated non-local or even dynamic observable estimators. Within the permutation matrix representation (PMR) flavor of QMC, however, we show that one can derive formal estimators for arbitrary static observables. We also derive exact, explicit estimators for general imaginary-time correlation functions and non-trivial integrated susceptibilities thereof. We demonstrate the practical versatility of our method by estimating various non-local, random observables for the transverse-field Ising model on a square lattice.|\n", "2504.07269": "|**2025-04-09**|**A Space-Time Continuous Galerkin Finite Element Method for Linear Schr\u00f6dinger Equations**|Marco Zank et.al.|[2504.07269](http://arxiv.org/abs/2504.07269)|null|8 pages|We introduce a space-time finite element method for the linear time-dependent Schr\\\"odinger equation with Dirichlet conditions in a bounded Lipschitz domain. The proposed discretization scheme is based on a space-time variational formulation of the time-dependent Schr\\\"odinger equation. In particular, the space-time method is conforming and is of Galerkin-type, i.e., trial and test spaces are equal. We consider a tensor-product approach with respect to time and space, using piecewise polynomial, continuous trial and test functions. In this case, we state the global linear system and efficient direct space-time solvers based on exploiting the Kronecker structure of the global system matrix. This leads to the Bartels-Stewart method and the fast diagonalization method. Both methods result in solving a sequence of spatial subproblems. In particular, the fast diagonalization method allows for solving the spatial subproblems in parallel, i.e., a time parallelization is possible. Numerical examples for a two-dimensional spatial domain illustrate convergence in space-time norms and show the potential of the proposed solvers.|\n", "2504.07218": "|**2025-04-09**|**Numerical analysis of three-dimensional magnetohydrodynamic effects in an inductively coupled plasma wind tunnel**|Sanjeev Kumar, Alessandro Munafo, Daniel J Bodony et.al.|[2504.07218](http://arxiv.org/abs/2504.07218)|null|36 pages, 22 figures|This paper introduces a three-dimensional model for the 350kW Plasmatron X inductively coupled plasma facility at the University of Illinois Urbana-Champaign, designed for testing high-temperature materials. Simulations of the facility have been performed using a three-dimensional, multiphysics computational framework, which reveals pronounced three-dimensional characteristics within the facility. The analysis of the plasma and electromagnetic field in the torch region reveals the influence of the helical coils, which cause a non-axisymmetric distribution of the plasma discharge. Additionally, simulations of the torch-chamber configuration at two operating pressures have been conducted to examine the impact of plasma asymmetry in the torch on jet characteristics in the chamber. The results indicate an unsteady, three-dimensional behavior of the plasma jet at high pressure. Spectral Proper Orthogonal Decomposition (SPOD) has been performed on the unsteady flow field to identify the dominant modes and their associated frequencies. At low pressure, a steady, supersonic, nearly axisymmetric plasma jet forms with consistent flow properties, such as temperature and velocity. However, strong non-equilibrium effects at low pressures lead to substantial deviations in species concentrations from axial symmetry despite having an almost axisymmetric distribution for quantities such as velocity and temperatures.|\n", "2504.08730": "|**2025-04-11**|**Dimension reduction for derivative-informed operator learning: An analysis of approximation errors**|Dingcheng Luo, Thomas O'Leary-Roseberry, Peng Chen et.al.|[2504.08730](http://arxiv.org/abs/2504.08730)|null||We study the derivative-informed learning of nonlinear operators between infinite-dimensional separable Hilbert spaces by neural networks. Such operators can arise from the solution of partial differential equations (PDEs), and are used in many simulation-based outer-loop tasks in science and engineering, such as PDE-constrained optimization, Bayesian inverse problems, and optimal experimental design. In these settings, the neural network approximations can be used as surrogate models to accelerate the solution of the outer-loop tasks. However, since outer-loop tasks in infinite dimensions often require knowledge of the underlying geometry, the approximation accuracy of the operator's derivatives can also significantly impact the performance of the surrogate model. Motivated by this, we analyze the approximation errors of neural operators in Sobolev norms over infinite-dimensional Gaussian input measures. We focus on the reduced basis neural operator (RBNO), which uses linear encoders and decoders defined on dominant input/output subspaces spanned by reduced sets of orthonormal bases. To this end, we study two methods for generating the bases; principal component analysis (PCA) and derivative-informed subspaces (DIS), which use the dominant eigenvectors of the covariance of the data or the derivatives as the reduced bases, respectively. We then derive bounds for errors arising from both the dimension reduction and the latent neural network approximation, including the sampling errors associated with the empirical estimation of the PCA/DIS. Our analysis is validated on numerical experiments with elliptic PDEs, where our results show that bases informed by the map (i.e., DIS or output PCA) yield accurate reconstructions and generalization errors for both the operator and its derivatives, while input PCA may underperform unless ranks and training sample sizes are sufficiently large.|\n", "2504.08608": "|**2025-04-11**|**Discretization Error Analysis of a High Order Unfitted Space-Time Method for moving domain problems**|Fabian Heimann, Christoph Lehrenfeld, Janosch Preu\u00df et.al.|[2504.08608](http://arxiv.org/abs/2504.08608)|null||We present a numerical analysis of a higher order unfitted space-time Finite Element method applied to a convection-diffusion model problem posed on a moving bulk domain. The method uses isoparametric space-time mappings for the geometry approximation of level set domains and has been presented and investigated computationally in [Heimann, Lehrenfeld, Preu{\\ss}, SIAM J. Sci. Comp. 45(2), 2023, B139 - B165]. Recently, in [Heimann, Lehrenfeld, IMA J. Numer. Anal., 2025] error bounds for the geometry approximation have been proven. In this paper we prove stability and accuracy including the influence of the geometry approximation.|\n", "2504.08544": "|**2025-04-11**|**Slicing the Gaussian Mixture Wasserstein Distance**|Moritz Piening, Robert Beinert et.al.|[2504.08544](http://arxiv.org/abs/2504.08544)|null||Gaussian mixture models (GMMs) are widely used in machine learning for tasks such as clustering, classification, image reconstruction, and generative modeling. A key challenge in working with GMMs is defining a computationally efficient and geometrically meaningful metric. The mixture Wasserstein (MW) distance adapts the Wasserstein metric to GMMs and has been applied in various domains, including domain adaptation, dataset comparison, and reinforcement learning. However, its high computational cost -- arising from repeated Wasserstein distance computations involving matrix square root estimations and an expensive linear program -- limits its scalability to high-dimensional and large-scale problems. To address this, we propose multiple novel slicing-based approximations to the MW distance that significantly reduce computational complexity while preserving key optimal transport properties. From a theoretical viewpoint, we establish several weak and strong equivalences between the introduced metrics, and show the relations to the original MW distance and the well-established sliced Wasserstein distance. Furthermore, we validate the effectiveness of our approach through numerical experiments, demonstrating computational efficiency and applications in clustering, perceptual image comparison, and GMM minimization|\n", "2504.08461": "|**2025-04-11**|**Astrophysical constraints on the simulation hypothesis for this Universe: why it is (nearly) impossible that we live in a simulation**|F. Vazza et.al.|[2504.08461](http://arxiv.org/abs/2504.08461)|null|17 pages, 4 figures. Frontiers in Physics, in press|We assess how physically realistic the ''simulation hypothesis'' for this Universe is, based on physical constraints arising from the link between information and energy, and on known astrophysical constraints. We investigate three cases: the simulation of the entire visible Universe, the simulation of Earth only, or a low resolution simulation of Earth, compatible with high-energy neutrino observations. In all cases, the amounts of energy or power required by any version of the simulation hypothesis are entirely incompatible with physics, or (literally) astronomically large, even in the lowest resolution case. Only universes with very different physical properties can produce some version of this Universe as a simulation. On the other hand, our results show that it is just impossible that this Universe is simulated by a universe sharing the same properties, regardless of technological advancements of the far future.|\n", "2504.08450": "|**2025-04-11**|**Well-Posedness of Discretizations for Fractional Elasto-Plasticity**|Michael Feischl, David Niederkofler, Barbara Wohlmuth et.al.|[2504.08450](http://arxiv.org/abs/2504.08450)|null||We consider a fractional plasticity model based on linear isotropic and kinematic hardening as well as a standard von-Mises yield function, where the flow rule is replaced by a Riesz--Caputo fractional derivative. The resulting mathematical model is typically non-local and non-smooth. Our numerical algorithm is based on the well-known radial return mapping and exploits that the kernel is finitely supported. We propose explicit and implicit discretizations of the model and show the well-posedness of the explicit in time discretization in combination with a standard finite element approach in space. Our numerical results in 2D and 3D illustrate the performance of the algorithm and the influence of the fractional parameter.|\n", "2504.08434": "|**2025-04-11**|**Elasticity of bidisperse attractive particle systems**|Yaqi Zhao, Antoine Sanner, Luca Michel et.al.|[2504.08434](http://arxiv.org/abs/2504.08434)|null|20 pages, 5 figures|Bidisperse particle systems are common in both natural and engineered materials, and it is known to influence packing, flow, and stability. However, their direct effect on elastic properties, particularly in systems with attractive interactions, remains poorly understood. Gaining insight into this relationship is important for designing soft particle-based materials with desired mechanical response. In this work, we study how particle size ratio and composition affect the shear modulus of attractive particle systems. Using coarse-grained molecular simulations, we analyze systems composed of two particle sizes at fixed total packing fraction and find that the shear modulus increases systematically with bidispersity. To explain this behavior, we develop two asymptotic models following limiting cases: one where a percolated network of large particles is stiffened by small particles, and another where a small-particle network is modified by embedded large particles. Both models yield closed-form expressions that capture the qualitative trends observed in simulations, including the dependence of shear modulus on size ratio and relative volume fraction. Our results demonstrate that bidispersity can enhance elastic stiffness through microstructural effects, independently of overall density, offering a simple strategy to design particle-based materials with tunable mechanical properties.|\n", "2504.08382": "|**2025-04-11**|**An posteriori error estimator for discontinuous Galerkin discretisations of convection-diffusion problems with application to Earth's mantle convection simulations**|Tiffany Barry, Andrea Cangiani, Samuel P. Cox et.al.|[2504.08382](http://arxiv.org/abs/2504.08382)|null||We present new aposteriori error estimates for the interior penalty discontinuous Galerkin method applied to non-stationary convection-diffusion equations. The focus is on strongly convection-dominated problems without zeroth-order reaction terms, which leads to the absence of positive L^2-like components. An important specific example is the energy/temperature equation of the Boussinesq system arising from the modelling of mantle convection of the Earth. The key mathematical challenge of mitigating the effects of exponential factors with respect to the final time, arising from the use of Gronwall-type arguments, is addressed by an exponential fitting technique. The latter results to a new class of aposteriori error estimates for the stationary problem, which are valid in cases of convection and reaction coefficient combinations not covered by the existing literature. This new class of estimators is combined with an elliptic reconstruction technique to derive new respective estimates for the non-stationary problem, exhibiting reduced dependence on Gronwall-type exponents and, thus, offer more accurate estimation for longer time intervals. We showcase the superior performance of the new class of aposteriori error estimators in driving mesh adaptivity in Earth's mantle convection simulations, in a setting where the energy/temperature equation is discretised by the discontinuous Galerkin method, coupled with the Taylor-Hood finite element for the momentum and mass conservation equations. We exploit the community code ASPECT, to present numerical examples showing the effectivity of the proposed approach.|\n", "2504.08346": "|**2025-04-11**|**Stochastic surfing turbulent vorticity**|Ziqi Wang, Xander M. de Wit, Roberto Benzi et.al.|[2504.08346](http://arxiv.org/abs/2504.08346)|null|8 pages, 5 figures|The chaotic dynamics of small-scale vorticity plays a key role in understanding and controlling turbulence, with direct implications for energy transfer, mixing, and coherent structure evolution. However, measuring or controlling its dynamics remains a major conceptual and experimental challenge due to its transient and chaotic nature. Here we use a combination of experiments, theory and simulations to show that small magnetic particles of different densities, exploring flow regions of distinct vorticity statistics, can act as effective probes for measuring and forcing turbulence at its smallest scale. The interplay between the magnetic torque, from an externally controllable magnetic field, and hydrodynamic stresses, from small-scale turbulent vorticity, reveals an extremely rich phenomenology. Notably, we present the first observation of stochastic resonance for particles in turbulence: turbulent fluctuations, effectively acting as noise, counterintuitively enhance the particle rotational response to external forcing. We identify a pronounced resonant peak in particle rotational phase-lag when the applied magnetic field matches the characteristic intensity of small-scale vortices. Furthermore, we uncover a novel symmetry-breaking mechanism: an oscillating magnetic field with zero-mean angular velocity remarkably induces net particle rotation in turbulence with zero-mean vorticity, as turbulent fluctuations aid the particle in \"surfing\" the magnetic field. Our findings offer insights into flexible particle manipulation in complex flows and open up a novel magnetic resonance-based approach for measuring vorticity: magnetic particles as probes emit detectable magnetic fields, enabling turbulence quantification even under optically-inaccessible conditions.|\n", "2504.08342": "|**2025-04-11**|**An Efficient Integrator Scheme for Sampling the (Quantum) Isobaric-Isothermal Ensemble in (Path Integral) Molecular Dynamics Simulations**|Weihao Liang, Sihan Wang, Cong Wang et.al.|[2504.08342](http://arxiv.org/abs/2504.08342)|null||Because most chemical or biological experiments are performed under conditions of controlled pressure and temperature, it is important to simulate the isobaric-isothermal ensemble at the atomic level to reveal the microscopic mechanism. By extending our configuration sampling protocol for the canonical ensemble, we propose a unified middle scheme to sample the coordinate (configuration) and volume distribution and thereby are able to accurately simulate either classical or quantum isobaric-isothermal processes. Various barostats and thermostats can be employed in the unified middle scheme for simulating real molecular systems with or without holonomic constraints. In particular, we demonstrate the recommended middle scheme by employing the Martyna-Tuckerman-Tobias-Klein barostat and stochastic cell-rescaling barostat, with the Langevin thermostat, in molecular simulation packages (DL_POLY, Amber, Gromacs, etc.). Benchmark numerical tests show that, without additional numerical effort, the middle scheme is competent in increasing the time interval by a factor of 5~10 to achieve the same accuracy of converged results for most thermodynamic properties in (path integral) molecular dynamics simulations.|\n", "2504.08341": "|**2025-04-11**|**Deep learning-based moment closure for multi-phase computation of semiclassical limit of the Schr\u00f6dinger equation**|Jin Woo Jang, Jae Yong Lee, Liu Liu et.al.|[2504.08341](http://arxiv.org/abs/2504.08341)|null|27 pages, 11 figures|We present a deep learning approach for computing multi-phase solutions to the semiclassical limit of the Schr\\\"odinger equation. Traditional methods require deriving a multi-phase ansatz to close the moment system of the Liouville equation, a process that is often computationally intensive and impractical. Our method offers an efficient alternative by introducing a novel two-stage neural network framework to close the $2N\\times 2N$ moment system, where $N$ represents the number of phases in the solution ansatz. In the first stage, we train neural networks to learn the mapping between higher-order moments and lower-order moments (along with their derivatives). The second stage incorporates physics-informed neural networks (PINNs), where we substitute the learned higher-order moments to systematically close the system. We provide theoretical guarantees for the convergence of both the loss functions and the neural network approximations. Numerical experiments demonstrate the effectiveness of our method for one- and two-dimensional problems with various phase numbers $N$ in the multi-phase solutions. The results confirm the accuracy and computational efficiency of the proposed approach compared to conventional techniques.|\n", "2504.08262": "|**2025-04-11**|**A General DoF and Pattern Analyzing Scheme for Electromagnetic Information Theory**|Zhongzhichao Wan, Jieao Zhu, Yongli Yan et.al.|[2504.08262](http://arxiv.org/abs/2504.08262)|null|In this paper, we provide a general DoF and pattern analyzing scheme   for EIT|Electromagnetic information theory (EIT) is one of the emerging topics for 6G communication due to its potential to reveal the performance limit of wireless communication systems. For EIT, one of the most important research directions is degree of freedom (DoF) analysis. Existing research works on DoF analysis for EIT focus on asymptotic conclusions of DoF, which do not well fit the practical wireless communication systems with finite spatial regions and finite frequency bandwidth. In this paper, we use the theoretical analyzing tools from Slepian concentration problem and extend them to three-dimensional space domain and four-dimensional space-time domain under electromagnetic constraints. Then we provide asymptotic DoF conclusions and non-asymptotic DoF analyzing scheme, which suits practical scenarios better, under different scenarios like three-dimensional antenna array. Moreover, we theoretically prove that the channel DoF is upper bounded by the proposed DoF of electromagnetic fields. Finally, we use numerical analysis to provide some insights about the optimal spatial sampling interval of the antenna array, the DoF of three-dimensional antenna array, the impact of unequal antenna spacing, the orthogonal space-time patterns, etc.|\n", "2504.08250": "|**2025-04-11**|**Nonadiabatic Field: A Conceptually Novel Approach for Nonadiabatic Quantum Molecular Dynamics**|Baihua Wu, Bingqi Li, Xin He et.al.|[2504.08250](http://arxiv.org/abs/2504.08250)|null||Reliable trajectory-based nonadiabatic quantum dynamics methods at the atomic level are critical for understanding many important processes in real systems. The paper reports latest progress of nonadiabatic field (NaF), a conceptually novel approach for nonadiabatic quantum dynamics with independent trajectories. Substantially different from the mainstreams of Ehrenfest-like dynamics and surface hopping methods, the nuclear force in NaF involves the nonadiabatic force arising from the nonadiabatic coupling between different electronic states, in addition to the adiabatic force contributed by a single adiabatic electronic state. NaF is capable of faithfully describing the interplay between electronic and nuclear motion in a broad regime, which covers where the relevant electronic states keep coupled in a wide range or all the time and where the bifurcation characteristic of nuclear motion is essential. NaF is derived from the exact generalized phase space formulation with coordinate-momentum variables, where constraint phase space (CPS) is employed for discrete electronic-state degrees of freedom. We propose efficient integrators for the equations of motion of NaF in both adiabatic and diabatic representations. Since the formalism in the CPS formulation is not unique, NaF can in principle be implemented with various phase space representations of the time correlation function (TCF) for the time-dependent property. They are applied to a suite of representative gas-phase and condensed-phase benchmark models where numerically exact results are available for comparison. It is shown that NaF is relatively insensitive to the phase space representation of the electronic TCF and will be a potential tool for practical and reliable simulations of the quantum mechanical behavior of both electronic and nuclear dynamics of nonadiabatic transition processes in real systems.|\n", "2504.08223": "|**2025-04-11**|**Stochastic Momentum ADMM for nonconvex and nonsmooth optimization with application to PnP algorithm**|Kangkang Deng, Shuchang Zhang, Boyu Wang et.al.|[2504.08223](http://arxiv.org/abs/2504.08223)|null|28 Pages|This paper introduces a single-loop Stochastic Momentum Alternating Direction Method of Multipliers (SMADMM) for tackling a class of nonconvex and nonsmooth optimization problems. We establish that SMADMM achieves an optimal oracle complexity of $\\mathcal{O}(\\epsilon^{-\\frac{3}{2}})$ in the online setting, where only stochastic first-order oracle, is available. In particular, SMADMM requires only $\\mathcal{O}(1)$ stochastic gradient evaluations per iteration and avoids the need for restarting with large batch gradient estimates. This is the first stochastic ADMM method achieving optimal oracle complexity for nonconvex and nonsmooth problems, requiring $\\mathcal{O}(1)$ batch size. Furthermore, we extend our method by integrating it with plug-and-play (PnP) priors, resulting in the PnP-SMADMM algorithm. Numerical experiments on classification, CT image reconstruction and phase retrieve demonstrate the practical effectiveness of our approach and validate the theoretical findings.|\n", "2504.08170": "|**2025-04-10**|**Efficient measurement of neutral-atom qubits with matched filters**|Robert M. Kent, Linipun Phuttitarn, Chaithanya Naik Mude et.al.|[2504.08170](http://arxiv.org/abs/2504.08170)|null||Quantum computers require high-fidelity measurement of many qubits to achieve a quantum advantage. Traditional approaches suffer from readout crosstalk for a neutral-atom quantum processor with a tightly spaced array. Although classical machine learning algorithms based on convolutional neural networks can improve fidelity, they are computationally expensive, making it difficult to scale them to large qubit counts. We present two simpler and scalable machine learning algorithms that realize matched filters for the readout problem. One is a local model that focuses on a single qubit, and the other uses information from neighboring qubits in the array to prevent crosstalk among the qubits. We demonstrate error reductions of up to 32% and 43% for the site and array models, respectively, compared to a conventional Gaussian threshold approach. Additionally, our array model uses two orders of magnitude fewer trainable parameters and four orders of magnitude fewer multiplications and nonlinear function evaluations than a recent convolutional neural network approach, with only a minor (3.5%) increase in error across different readout times. Another strength of our approach is its physical interpretability: the learned filter can be visualized to provide insights into experimental imperfections. We also show that a convolutional neural network model for improved can be pruned to have 70x and 4000x fewer parameters, respectively, while maintaining similar errors. Our work shows that simple machine learning approaches can achieve high-fidelity qubit measurements while remaining scalable to systems with larger qubit counts.|\n", "2504.08157": "|**2025-04-10**|**A GPU-accelerated simulation of rapid intensification of a tropical cyclone with observed heating**|Soonpil Kang, Francis X. Giraldo, Seth Camp et.al.|[2504.08157](http://arxiv.org/abs/2504.08157)|null|10 pages, 12 figures|This paper presents a limited-area atmospheric simulation of a tropical cyclone accelerated using GPUs. The OpenACC directive-based programming model is used to port the atmospheric model to the GPU. The GPU implementation of the main functions and kernels is discussed. The GPU-accelerated code produces high-fidelity simulations of a realistic tropical cyclone forced by observational latent heating. Performance tests show that the GPU-accelerated code yields energy-efficient simulations and scales well in both the strong and weak limit.|\n", "2504.08155": "|**2025-04-10**|**Benchmarking and contrasting exchange-correlation functional differences in response to static correlation in unrestricted Kohn-Sham and a hybrid 1-electron reduced density matrix functional theory**|Daniel Gibney, Jan-Niklas Boyn et.al.|[2504.08155](http://arxiv.org/abs/2504.08155)|null||A hybrid Kohn-Sham Density Functional Theory (KS-DFT) and 1-electron Reduced Density Matrix Functional Theory (1-RDMFT) has recently been developed to describe strongly correlated systems at mean-field computational cost. This approach relies on combining a Reduced Density Matrix Functional to capture strong correlation effects with existing exchange correlation (XC) functionals to capture the remaining dynamical correlation effects. In this work, we systematically benchmark the performance of nearly 200 different XC functionals available within LibXC in this DFA 1-RDMFT framework, contrasting it with their performance in unrestricted KS-DFT. We identify optimal XC functionals for use within DFA 1-RDMFT and elucidate fundamental trends in the response of different XC functionals to strong correlation in both DFA 1-RDMFT and UKS-DFT.|\n", "2504.08136": "|**2025-04-10**|**A physics informed neural network approach to simulating ice dynamics governed by the shallow ice approximation**|Kapil Chawla, William Holmes et.al.|[2504.08136](http://arxiv.org/abs/2504.08136)|null||In this article we develop a Physics Informed Neural Network (PINN) approach to simulate ice sheet dynamics governed by the Shallow Ice Approximation. This problem takes the form of a time-dependent parabolic obstacle problem. Prior work has used this approach to address the stationary obstacle problem and here we extend it to the time dependent problem. Through comprehensive 1D and 2D simulations, we validate the model's effectiveness in capturing complex free-boundary conditions. By merging traditional mathematical modeling with cutting-edge deep learning methods, this approach provides a scalable and robust solution for predicting temporal variations in ice thickness. To illustrate this approach in a real world setting, we simulate the dynamics of the Devon Ice Cap, incorporating aerogeophysical data from 2000 and 2018.|\n", "2504.08096": "|**2025-04-10**|**Cellular Development Follows the Path of Minimum Action**|Rohola Zandie, Farhan Khodaee, Yufan Xia et.al.|[2504.08096](http://arxiv.org/abs/2504.08096)|null||Cellular development follows a stochastic yet rule-governed trajectory, though the underlying principles remain elusive. Here, we propose that cellular development follows paths of least action, aligning with foundational physical laws that govern dynamic systems across nature. We introduce a computational framework that takes advantage of the deep connection between the principle of least action and maximum entropy to model developmental processes using Transformers architecture. This approach enables precise quantification of entropy production, information flow curvature, and local irreversibility for developmental asymmetry in single-cell RNA sequence data. Within this unified framework, we provide interpretable metrics: entropy to capture exploration-exploitation trade-offs, curvature to assess plasticity-elasticity dynamics, and entropy production to characterize dedifferentiation and transdifferentiation. We validate our method across both single-cell and embryonic development datasets, demonstrating its ability to reveal hidden thermodynamic and informational constraints shaping cellular fate decisions.|\n", "2504.11435": "|**2025-04-15**|**Robust Containment Queries over Collections of Trimmed NURBS Surfaces via Generalized Winding Numbers**|Jacob Spainhour, Kenneth Weiss et.al.|[2504.11435](http://arxiv.org/abs/2504.11435)|null|20 Pages, 18 Figures, 2 Tables|Efficient and accurate evaluation of containment queries for regions bound by trimmed NURBS surfaces is important in many graphics and engineering applications. However, the algebraic complexity of surface-surface intersections makes gaps and overlaps between surfaces difficult to avoid for in-the-wild surface models. By considering this problem through the lens of the generalized winding number (GWN), a mathematical construction that is indifferent to the arrangement of surfaces in the shape, we can define a containment query that is robust to model watertightness. Applying contemporary techniques for the 3D GWN on arbitrary curved surfaces would require some form of geometric discretization, potentially inducing containment misclassifications near boundary components. In contrast, our proposed method computes an accurate GWN directly on the curved geometry of the input model. We accomplish this using a novel reformulation of the relevant surface integral using Stokes' theorem, which in turn permits an efficient adaptive quadrature calculation on the boundary and trimming curves of the model. While this is sufficient for \"far-field\" query points that are distant from the surface, we augment this approach for \"near-field\" query points (i.e., within a bounding box) and even those coincident to the surface patches via a strategy that directly identifies and accounts for the jump discontinuity in the scalar field. We demonstrate that our method of evaluating the GWN field is robust to complex trimming geometry in a CAD model, and is accurate up to arbitrary precision at arbitrary distances from the surface. Furthermore, the derived containment query is robust to non-watertightness while respecting all curved features of the input shape.|\n", "2504.11433": "|**2025-04-15**|**Predicting Wave Dynamics using Deep Learning with Multistep Integration Inspired Attention and Physics-Based Loss Decomposition**|Indu Kant Deo, Rajeev K. Jaiman et.al.|[2504.11433](http://arxiv.org/abs/2504.11433)|null|30 pages, 14 figures|In this paper, we present a physics-based deep learning framework for data-driven prediction of wave propagation in fluid media. The proposed approach, termed Multistep Integration-Inspired Attention (MI2A), combines a denoising-based convolutional autoencoder for reduced latent representation with an attention-based recurrent neural network with long-short-term memory cells for time evolution of reduced coordinates. This proposed architecture draws inspiration from classical linear multistep methods to enhance stability and long-horizon accuracy in latent-time integration. Despite the efficiency of hybrid neural architectures in modeling wave dynamics, autoregressive predictions are often prone to accumulating phase and amplitude errors over time. To mitigate this issue within the MI2A framework, we introduce a novel loss decomposition strategy that explicitly separates the training loss function into distinct phase and amplitude components. We assess the performance of MI2A against two baseline reduced-order models trained with standard mean-squared error loss: a sequence-to-sequence recurrent neural network and a variant using Luong-style attention. To demonstrate the effectiveness of the MI2A model, we consider three benchmark wave propagation problems of increasing complexity, namely one-dimensional linear convection, the nonlinear viscous Burgers equation, and the two-dimensional Saint-Venant shallow water system. Our results demonstrate that the MI2A framework significantly improves the accuracy and stability of long-term predictions, accurately preserving wave amplitude and phase characteristics. Compared to the standard long-short term memory and attention-based models, MI2A-based deep learning exhibits superior generalization and temporal accuracy, making it a promising tool for real-time wave modeling.|\n", "2504.11425": "|**2025-04-15**|**MINDS: The very low-mass star and brown dwarf sample -- Hidden water in carbon-dominated protoplanetary disks**|Aditya M. Arabhavi, Inga Kamp, Ewine F. van Dishoeck et.al.|[2504.11425](http://arxiv.org/abs/2504.11425)|null|Accepted for publication in Astrophysical Journal Letters, 21 pages,   24 figures|Infrared observations of the inner disks around very low-mass stars (VLMS, $<$0.3$\\,M_{\\odot}$) have revealed a carbon-rich gas composition in the terrestrial planet-forming regions. Contrary to the typically water-rich T Tauri disk spectra, only two disks around VLMS have been observed to be water-rich among more than ten VLMS disks observed so far with JWST/MIRI. In this letter, we systematically search for the presence of water and other oxygen-bearing molecules in the JWST/MIRI spectra of ten VLMS disks from the MIRI mid-INfrared Disk Survey (MINDS). In addition to the two previously reported detections of water emission in this VLMS sample, we detect water emission in the spectra of three other sources and tentatively in one source, and we provide strong evidence for water emission in the remaining disks in the MINDS sample, most of which have bright emission from carbon-bearing molecules. We show that the $\\rm C_2H_2$ emission is much stronger than that of water for sources with low luminosities, and the hydrocarbons outshine the water emission in such conditions. We propose that the appearance of water-rich vs. hydrocarbon-rich spectra is related to the location of the water reservoir in the disk relative to the main hydrocarbon reservoir. Our findings indicate that the terrestrial planet forming regions in VLMS disks have high carbon-to-oxygen ratios (C/O$>$1), but can still harbor ample water similar to those in the T Tauri disks.|\n", "2504.11397": "|**2025-04-15**|**MLPs and KANs for data-driven learning in physical problems: A performance comparison**|Raghav Pant, Sikan Li, Xingjian Li et.al.|[2504.11397](http://arxiv.org/abs/2504.11397)|null|30 pages, 18 figures, 8 tables|There is increasing interest in solving partial differential equations (PDEs) by casting them as machine learning problems. Recently, there has been a spike in exploring Kolmogorov-Arnold Networks (KANs) as an alternative to traditional neural networks represented by Multi-Layer Perceptrons (MLPs). While showing promise, their performance advantages in physics-based problems remain largely unexplored. Several critical questions persist: Can KANs capture complex physical dynamics and under what conditions might they outperform traditional architectures? In this work, we present a comparative study of KANs and MLPs for learning physical systems governed by PDEs. We assess their performance when applied in deep operator networks (DeepONet) and graph network-based simulators (GNS), and test them on physical problems that vary significantly in scale and complexity. Drawing inspiration from the Kolmogorov Representation Theorem, we examine the behavior of KANs and MLPs across shallow and deep network architectures. Our results reveal that although KANs do not consistently outperform MLPs when configured as deep neural networks, they demonstrate superior expressiveness in shallow network settings, significantly outpacing MLPs in accuracy over our test cases. This suggests that KANs are a promising choice, offering a balance of efficiency and accuracy in applications involving physical systems.|\n", "2504.11350": "|**2025-04-15**|**Adaptive Compressible Smoothed Particle Hydrodynamics**|Navaneet Villodi, Prabhu Ramachandran et.al.|[2504.11350](http://arxiv.org/abs/2504.11350)|null|60 pages, 21 figures|Modulating the number of particles in a region is key to accurately capturing the nuances in compressible flows with Smoothed Particle Hydrodynamics (SPH). This paper details the implementation of a volume-based adaptive refinement and derefinement procedure, incorporating state-of-the-art features such as automatic local adaptivity and solution adaptivity. A shock-aware particle shifting procedure is introduced to regularize the particle distribution while preserving the integrity of shocks. To our knowledge, this is the first demonstration of shock-based solution adaptivity and shock-aware particle shifting in the literature. A wide variety of test problems, which involve flow in and around boundaries, are employed to highlight the utility of these adaptivity features in improving the results and in making simulations faster. For instance, the adaptive resolution procedure is shown to deliver an order of magnitude speedup. We also demonstrate the effectiveness of the adaptivity procedure in resolving existing issues like errors due to interaction with differently spaced ghost particles at boundaries, formation of spot-like structures due to particle clumping, and poorly resolved low-density regions. In essence, the adaptivity technique presented in this paper is positioned as a powerful tool for simulating compressible flows with enhanced accuracy and efficiency.|\n", "2504.11339": "|**2025-04-15**|**Optimal and Scalable Augmented Lagrangian preconditioners for Fictitious Domain problems**|Michele Benzi, Marco Feder, Luca Heltai et.al.|[2504.11339](http://arxiv.org/abs/2504.11339)|null||We present optimal and scalable preconditioning techniques to solve linear systems of equations with a block two-by-two and three-by-three structure arising from fictitious domain problems and from finite element discretizations of immersed boundary methods. In particular, we propose two augmented Lagrangian-based preconditioners to accelerate the convergence of iterative solvers for these two classes of linear. We consider two relevant examples to illustrate the performance of these preconditioners when used in conjunction with flexible GMRES: the Poisson and the Stokes fictitious domain problems. A spectral analysis is established for both exact and inexact versions of these preconditioners. We show the effectiveness of the proposed approach and the robustness of our preconditioning strategy through extensive numerical tests in both two and three dimensions.|\n", "2504.11333": "|**2025-04-15**|**Implicit dual time-stepping positivity-preserving entropy-stable schemes for the compressible Navier-Stokes equations**|Mohammed Sayyari, Nail K. Yamaleev et.al.|[2504.11333](http://arxiv.org/abs/2504.11333)|null||We generalize the explicit high-order positivity-preserving entropy-stable spectral collocation schemes developed in [30, 34] for the three-dimensional (3D) compressible Navier Stokes equations to a time implicit formulation. The time derivative terms are discretized by using the first- and second-order implicit backward difference formulas (BDF1 and BDF2) that are well suited for solving steady-state and time-dependent viscous flows at high Reynolds numbers, respectively. The nonlinear system of discrete equations at each physical timestep is solved by using a dual time-stepping technique. The proposed scheme is provably entropy-stable and positivity-preserving and provides unconditional stability properties in the physical time. Numerical results demonstrating accuracy and positivity-preserving properties of the new dual time-stepping scheme are presented for supersonic viscous flows with strong shock waves and contact discontinuities.|\n", "2504.11292": "|**2025-04-15**|**Optimal finite element approximations of monotone semilinear elliptic PDE with subcritical nonlinearities**|Florian Spicher, Thomas P. Wihler et.al.|[2504.11292](http://arxiv.org/abs/2504.11292)|null||We study iterative finite element approximations for the numerical approximation of semilinear elliptic boundary value problems with monotone nonlinear reactions of subcritical growth. The focus of our contribution is on an optimal a priori error estimate for a contractive Picard type iteration scheme on meshes that are locally refined towards possible corner singularities in polygonal domains. Our analysis involves, in particular, an elliptic regularity result in weighted Sobolev spaces and the use of the Trudinger inequality, which is instrumental in dealing with subcritically growing nonlinearities. A series of numerical experiments confirm the accuracy and efficiency of our method.|\n", "2504.11291": "|**2025-04-15**|**Policy heterogeneity improves collective olfactory search in 3-D turbulence**|Lorenzo Piro, Robin A. Heinonen, Maurizio Carbone et.al.|[2504.11291](http://arxiv.org/abs/2504.11291)|null||We investigate the role of policy heterogeneity in enhancing the olfactory search capabilities of cooperative agent swarms operating in complex, real-world turbulent environments. Using odor fields from direct numerical simulations of the Navier-Stokes equations, we demonstrate that heterogeneous groups, with exploratory and exploitative agents, consistently outperform homogeneous swarms where the exploration-exploitation tradeoff is managed at the individual level. Our results reveal that policy diversity enables the group to reach the odor source more efficiently by mitigating the detrimental effects of spatial correlations in the signal. These findings provide new insights into collective search behavior in biological systems and offer promising strategies for the design of robust, bioinspired search algorithms in engineered systems.|\n", "2504.11287": "|**2025-04-15**|**On kinetic energy localization in fluid flow**|Damian \u015anie\u017cek et.al.|[2504.11287](http://arxiv.org/abs/2504.11287)|null||This works focuses on participation number -- a parameter that allows to quantitatively asses the level of kinetic energy localization. The author presents a clear way of deriving participation number in a continuous case without making any assumptions about the system, fluid or flow regime. Moreover, a method of computing participation number in discretized cases is discussed and verified against well known analytical solutions using three methods, in which one was used previously in research on fluid flow through porous media. A robust formula, that works for both uniform and nonuniform discretization grids is presented.|\n", "2504.11236": "|**2025-04-15**|**Influence of a Xenon interlayer on dissociative electron attachment to deuterated methane on a platinum substrate**|Norhan Omar, Pierre Cloutier, Christophe Ramseyer et.al.|[2504.11236](http://arxiv.org/abs/2504.11236)|null||We investigate the impact of intercalating a xenon layer between a thin condensed CD4 film of two monolayers (ML) and a platinum surface on the dissociative electron attachment (DEA). The observed desorption results are compared with density functional theory (DFT) calculations, which reveal the binding energies of various anionic and neutral species as a function of the xenon film thickness on the Pt (111) substrate. The theoretical results suggest that 6 ML of xenon are sufficient to diminish the surface effect, enabling physisorbed anionic fragments to desorb from the CD4 film. In contrast, 20 ML (approximately 10 nm) are experimentally necessary to achieve saturation in the desorption of D-. In addition, the presence of xenon layers enables the coupling of resonance states with Xe excited states, thereby inhibiting the electrons from returning to the metal. Aside from reducing surface interactions, the xenon interlayer significantly enhances DEA to CD4.|\n", "2504.11213": "|**2025-04-15**|**Characterizing High Schmidt Number Witnesses in Arbitrary Dimensions System**|Liang Xiong, Nung-sing Sze et.al.|[2504.11213](http://arxiv.org/abs/2504.11213)|null||A profound comprehension of quantum entanglement is crucial for the progression of quantum technologies. The degree of entanglement can be assessed by enumerating the entangled degrees of freedom, leading to the determination of a parameter known as the Schmidt number. In this paper, we develop an efficient analytical tool for characterizing high Schmidt number witnesses for bipartite quantum states in arbitrary dimensions. Our methods not only offer viable mathematical methods for constructing high-dimensional Schmidt number witnesses in theory but also simplify the quantification of entanglement and dimensionality. Most notably, we develop high-dimensional Schmidt number witnesses within arbitrary-dimensional systems, with our Schmidt witness coefficients relying solely on the operator Schmidt coefficient. Subsequently, we demonstrate our theoretical advancements and computational superiority by constructing Schmidt number witnesses in arbitrary dimensional bipartite quantum systems with Schmidt numbers four and five.|\n", "2504.11212": "|**2025-04-15**|**SDFs from Unoriented Point Clouds using Neural Variational Heat Distances**|Samuel Weidemaier, Florine Hartwig, Josua Sassen et.al.|[2504.11212](http://arxiv.org/abs/2504.11212)|null|14 pages, 16 figures, 4 tables|We propose a novel variational approach for computing neural Signed Distance Fields (SDF) from unoriented point clouds. To this end, we replace the commonly used eikonal equation with the heat method, carrying over to the neural domain what has long been standard practice for computing distances on discrete surfaces. This yields two convex optimization problems for whose solution we employ neural networks: We first compute a neural approximation of the gradients of the unsigned distance field through a small time step of heat flow with weighted point cloud densities as initial data. Then we use it to compute a neural approximation of the SDF. We prove that the underlying variational problems are well-posed. Through numerical experiments, we demonstrate that our method provides state-of-the-art surface reconstruction and consistent SDF gradients. Furthermore, we show in a proof-of-concept that it is accurate enough for solving a PDE on the zero-level set.|\n", "2504.11181": "|**2025-04-15**|**A Quantum-Inspired Algorithm for Wave Simulation Using Tensor Networks**|Kevin Lively, Vittorio Pagni, Gonzalo Camacho et.al.|[2504.11181](http://arxiv.org/abs/2504.11181)|null|12 pages, 7 figures|We present an efficient classical algorithm based on the construction of a unitary quantum circuit for simulating the Isotropic Wave Equation (IWE) in one, two, or three dimensions. Using an analogy with the massless Dirac equation, second order time and space derivatives in the IWE are reduced to first order, resulting in a Schr\\\"odinger equation of motion. Exact diagonalization of the unitary circuit in combination with Tensor Networks allows simulation of the wave equation with a resolution of $10^{13}$ grid points on a laptop. A method for encoding arbitrary analytical functions into diagonal Matrix Product Operators is employed to prepare and evolve a Matrix Product State (MPS) encoding the solution. Since the method relies on the Quantum Fourier Transform, which has been shown to generate small entanglement when applied to arbitrary MPSs, simulating the evolution of initial conditions with sufficiently low bond dimensions to high accuracy becomes highly efficient, up to the cost of Trotterized propagation and sampling of the wavefunction. We conclude by discussing possible extensions of the approach for carrying out Tensor Network simulations of other partial differential equations such as Maxwell's equations.|\n", "2504.11167": "|**2025-04-15**|**Low-Rank SPIKE Framework for Solving Large Sparse Linear Systems with Applications**|Braegan S. Spring, Eric Polizzi, Ahmed H. Sameh et.al.|[2504.11167](http://arxiv.org/abs/2504.11167)|null|26 pages|The SPIKE family of linear system solvers provides parallelism using a block tridiagonal partitioning. Typically SPIKE-based solvers are applied to banded systems, resulting in structured off-diagonal blocks with non-zeros elements restricted to relatively small submatrices comprising the band of the original matrix. In this work, a low-rank SVD based approximation of the off-diagonal blocks is investigated. This produces a representation which more effectively handles matrices with large, sparse bands. A set of flexible distributed solvers, the LR-SPIKE variants, are implemented. There are applicable to a wide range of applications -- from use as a \"black-box\" preconditioner which straightforwardly improves upon the classic Block Jacobi preconditioner, to use as a specialized \"approximate direct solver.\" An investigation of the effectiveness of the new preconditioners for a selection of SuiteSparse matrices is performed, particularly focusing on matrices derived from 3D finite element simulations. In addition, the SPIKE approximate linear system solvers are also paired with the FEAST eigenvalue solver, where they are shown to be particularly effective due to the former's rapid convergence, and the latter's acceptance of loose linear system solver convergence, resulting in a combination which requires very few solver iterations.|\n", "2504.11140": "|**2025-04-15**|**An Unsupervised Network Architecture Search Method for Solving Partial Differential Equations**|Qing Li, Jingrun Chen et.al.|[2504.11140](http://arxiv.org/abs/2504.11140)|null||Solving partial differential equations (PDEs) has been indispensable in scientific and engineering applications. Recently, deep learning methods have been widely used to solve high-dimensional problems, one of which is the physics-informed neural network (PINN). Typically, a deep learning method has three main components: a neural network, a loss function, and an optimizer. While the construction of the loss function is rooted in the definition of solution space, how to choose a optimal neural network is somewhat ad hoc, leaving much room for improvement. In the framework of PINN, we propose an unsupervised network architecture search method for solving PDEs, termed PINN-DARTS, which applies the differentiable architecture search (DARTS) to find the optimal network architecture structure in a given set of neural networks. In this set, the number of layers and the number of neurons in each layer can change. In the searching phase, both network and architecture parameters are updated simultaneously, so the running time is close to that of PINN with a pre-determined network structure. Unlike available works, our approach is unsupervised and purely based on the PDE residual without any prior usage of solutions. PINN-DARTS outputs the optimal network structure as well as the associated numerical solution. The performance of PINN-DARTS is verified on several benchmark PDEs, including elliptic, parabolic, wave, and Burgers' equations. Compared to traditional architecture search methods, PINN-DARTS achieves significantly higher architectural accuracy. Another interesting observation is that both the solution complexity and the PDE type have a prominent impact on the optimal network architecture. Our study suggests that architectures with uneven widths from layer to layer may have superior performance across different solution complexities and different PDE types.|\n", "2504.11096": "|**2025-04-15**|**A fully variational numerical method for structural topology optimization based on a Cahn-Hilliard model**|Edmund Bell-Navas, David Portillo, Ignacio Romero et.al.|[2504.11096](http://arxiv.org/abs/2504.11096)|null||We formulate a novel numerical method suitable for the solution of topology optimization problems in solid mechanics. The most salient feature of the new approach is that the space and time discrete equations of the numerical method can be obtained as the optimality conditions of a single incremental potential. The governing equations define a gradient flow of the mass in the domain that maximizes the stiffness of the proposed solid, while exactly preserving the mass of the allocated material. Moreover, we propose a change of variables in the model equations that constrains the value of the density within admissible bounds and a continuation strategy that speeds up the evolution of the flow. The proposed strategy results in a robust and efficient topology optimization method that is exactly mass-preserving, does not employ Lagrange multipliers, and is fully variational.|\n", "2504.11074": "|**2025-04-16**|**Dynamical errors in machine learning forecasts**|Zhou Fang, Gianmarco Mengaldo et.al.|[2504.11074](http://arxiv.org/abs/2504.11074)|null||In machine learning forecasting, standard error metrics such as mean absolute error (MAE) and mean squared error (MSE) quantify discrepancies between predictions and target values. However, these metrics do not directly evaluate the physical and/or dynamical consistency of forecasts, an increasingly critical concern in scientific and engineering applications.   Indeed, a fundamental yet often overlooked question is whether machine learning forecasts preserve the dynamical behavior of the underlying system. Addressing this issue is essential for assessing the fidelity of machine learning models and identifying potential failure modes, particularly in applications where maintaining correct dynamical behavior is crucial.   In this work, we investigate the relationship between standard forecasting error metrics, such as MAE and MSE, and the dynamical properties of the underlying system. To achieve this goal, we use two recently developed dynamical indices: the instantaneous dimension ($d$), and the inverse persistence ($\\theta$). Our results indicate that larger forecast errors -- e.g., higher MSE -- tend to occur in states with higher $d$ (higher complexity) and higher $\\theta$ (lower persistence). To further assess dynamical consistency, we propose error metrics based on the dynamical indices that measure the discrepancy of the forecasted $d$ and $\\theta$ versus their correct values. Leveraging these dynamical indices-based metrics, we analyze direct and recursive forecasting strategies for three canonical datasets -- Lorenz, Kuramoto-Sivashinsky equation, and Kolmogorov flow -- as well as a real-world weather forecasting task. Our findings reveal substantial distortions in dynamical properties in ML forecasts, especially for long forecast lead times or long recursive simulations, providing complementary information on ML forecast fidelity that can be used to improve ML models.|\n", "2504.11056": "|**2025-04-15**|**A study of troubled-cell indicators applied to finite volume methods using a novel monotonicity parameter**|R. Shivananda Rao, M. Ramakrishna et.al.|[2504.11056](http://arxiv.org/abs/2504.11056)|null||We adapt a troubled-cell indicator developed for discontinuous Galerkin (DG) methods to the finite volume method (FVM) framework for solving hyperbolic conservation laws. This indicator depends solely on the cell-average data of the target cell and its immediate neighbours. Once the troubled-cells are identified, we apply the limiter only in these cells instead of applying in all computational cells. We introduce a novel technique to quantify the quality of the solution in the neighbourhood of the shock by defining a monotonicity parameter $\\mu$. Numerical results from various two-dimensional simulations on the hyperbolic systems of Euler equations using a finite volume solver employing MUSCL reconstruction validate the performance of the troubled-cell indicator and the approach of limiting only in the troubled-cells. These results show that limiting only in the troubled-cells is preferable to limiting everywhere as it improves convergence without compromising on the solution accuracy.|\n", "2504.11006": "|**2025-04-15**|**A Navier-Stokes-Peridynamics hybrid algorithm for the coupling of compressible flows and fracturing materials**|Mingshuo Han, Shiwei Hu, Tianbai Xiao et.al.|[2504.11006](http://arxiv.org/abs/2504.11006)|null|25 pages, 17 figures, 3 tables|Modeling and simulation of fluid-structure interactions are crucial to the success of aerospace engineering. This work addresses a novel hybrid algorithm that models the close coupling between compressible flows and deformable materials using a mesoscopic approach. Specifically, the high-speed flows are described by the gas-kinetic scheme, which is a robust Navier-Stokes alternative solver built on the molecular kinetic theory. The deformation, damage, and fracture of materials are depicted using the bond-based peridynamics, which serves as coarse-grained molecular dynamics to construct non-local extensions of classical continuum mechanics. The evolution of fluids and materials are closely coupled using the ghost-cell immersed boundary method. Within each time step, the solutions of flow and solid fields are updated simultaneously, and physics-driven boundary conditions are exchanged for each other via ghost cells. Extensive numerical experiments, including crack propagation in a pre-cracked plate, subsonic flow around the NACA0012 airfoil, supersonic flow around the circular cylinder, and shock wave impacting on the elastic panel, are performed to validate the algorithm. The simulation results demonstrate the unique advantages of current hybrid algorithm in solving fracture propagation induced by high-speed flows.|\n", "2504.15201": "|**2025-04-21**|**Phase-separated lipid vesicles: continuum modeling, simulation, and validation**|Maxim Olshanskii, Annalisa Quaini et.al.|[2504.15201](http://arxiv.org/abs/2504.15201)|null||The paper presents a complete research cycle comprising continuum-based modeling, computational framework development, and validation setup to predict phase separation and surface hydrodynamics in lipid bilayer membranes. We starting with an overview of the key physical characteristics of lipid bilayers, including their composition, mechanical properties, and thermodynamics, and then discuss continuum models of multi-component bilayers. The most complex model is a Navier--Stokes--Cahn--Hilliard (NSCH) type system, describing the coupling of incompressible surface fluid dynamics with phase-field dynamics on arbitrarily curved geometries. It is discretized using trace finite element methods, which offer geometric flexibility and stability in representing surface PDEs. Numerical studies are conducted to examine physical features such as coarsening rates and interfacial dynamics. The computational results obtained from the NSCH model are compared against experimental data for membrane compositions with distinct phase behaviors, demonstrating that including both phase-field models and surface hydrodynamics is essential to accurately reproduce domain evolution observed in epi-fluorescence microscopy. Lastly, we extend the model to incorporate external forces that enable the simulation of vesicles containing cationic lipids, used to enhance membrane fusion.|\n", "2504.15177": "|**2025-04-21**|**An $rp$-adaptive method for accurate resolution of shock-dominated viscous flow based on implicit shock tracking**|Huijing Dong, Masayuki Yano, Tianci Huang et.al.|[2504.15177](http://arxiv.org/abs/2504.15177)|null|43 pages, 35 figures,|This work introduces an optimization-based $rp$-adaptive numerical method to approximate solutions of viscous, shock-dominated flows using implicit shock tracking and a high-order discontinuous Galerkin discretization on traditionally coarse grids without nonlinear stabilization (e.g., artificial viscosity or limiting). The proposed method adapts implicit shock tracking methods, originally developed to align mesh faces with solution discontinuities, to compress elements into viscous shocks and boundary layers, functioning as a novel approach to aggressive $r$-adaptation. This form of $r$-adaptation is achieved naturally as the minimizer of the enriched residual with respect to the discrete flow variables and coordinates of the nodes of the grid. Several innovations to the shock tracking optimization solver are proposed to ensure sufficient mesh compression at viscous features to render stabilization unnecessary, including residual weighting, step constraints and modifications, and viscosity-based continuation. Finally, $p$-adaptivity is used to locally increase the polynomial degree with three clear benefits: (1) lessens the mesh compression requirements near shock waves and boundary layers, (2) reduces the error in regions where $r$-adaptivity is not sufficient with the given grid topology, and (3) reduces computational cost by performing a majority of the $r$-adaptivity iterations on the coarsest discretization. A series of numerical experiments show the proposed method effectively resolves viscous, shock-dominated flows, including accurate prediction of heat flux profiles produced by hypersonic flow over a cylinder, and compares favorably in terms of accuracy per degree of freedom to $h$-adaptation with a high-order discretization.|\n", "2504.15173": "|**2025-04-21**|**Poroelastic flow across a permeable interface: a Hamilton's principle approach and its finite element implementation**|Francesco Costanzo, Mohammad Jannesari, Beatrice Ghitti et.al.|[2504.15173](http://arxiv.org/abs/2504.15173)|null||We consider fluid flow across a permeable interface within a deformable porous medium. We use mixture theory. The mixture's constituents are assumed to be incompressible in their pure form. We use Hamilton's principle to obtain the governing equations, and we propose a corresponding finite element implementation. The filtration velocity and the pore pressure are allowed to be discontinuous across the interface while some control of these discontinuities is built into the interfacial constitutive behavior. To facilitate the practical implementation of the formulation in a finite element scheme, we introduce a Lagrange multiplier field over the interface for the explicit enforcement of the jump condition of the balance of mass. Our formulation appears to recover some basic results from the literature. The novelty of the work is the formulation of an approach that can accommodate specific constitutive assumptions pertaining to the behavior of the interface that do not necessarily imply the continuity of the filtration velocity and/or of the pore pressure across it.|\n", "2504.15151": "|**2025-04-21**|**Artificial compressibility method for the incompressible Navier-Stokes equations with variable density**|Cappanera Loic, Giordano Salvatore et.al.|[2504.15151](http://arxiv.org/abs/2504.15151)|null||We introduce a novel artificial compressibility technique to approximate the incompressible Navier-Stokes equations with variable fluid properties such as density and dynamical viscosity. The proposed scheme used the couple pressure and momentum, equal to the density times the velocity, as primary unknowns. It also involves an adequate treatment of the diffusive operator such that treating the nonlinear convective term explicitly leads to a scheme with time independent stiffness matrices that is suitable for pseudo-spectral methods. The stability and temporal convergence of the semi-implicit version of the scheme is established under the hypothesis that the density is approximated with a method that conserves the minimum-maximum principle. Numerical illustrations confirm that both the semi-implicit and explicit scheme are stable and converge with order one under classic CFL condition. Moreover, the proposed scheme is shown to perform better than a momentum based pressure projection method, previously introduced by one of the authors, on setups involving gravitational waves and immiscible multi-fluids in a cylinder.|\n", "2504.15110": "|**2025-04-21**|**Kolmogorov-Arnold Networks: Approximation and Learning Guarantees for Functions and their Derivatives**|Anastasis Kratsios, Takashi Furuya et.al.|[2504.15110](http://arxiv.org/abs/2504.15110)|null||Inspired by the Kolmogorov-Arnold superposition theorem, Kolmogorov-Arnold Networks (KANs) have recently emerged as an improved backbone for most deep learning frameworks, promising more adaptivity than their multilayer perception (MLP) predecessor by allowing for trainable spline-based activation functions. In this paper, we probe the theoretical foundations of the KAN architecture by showing that it can optimally approximate any Besov function in $B^{s}_{p,q}(\\mathcal{X})$ on a bounded open, or even fractal, domain $\\mathcal{X}$ in $\\mathbb{R}^d$ at the optimal approximation rate with respect to any weaker Besov norm $B^{\\alpha}_{p,q}(\\mathcal{X})$; where $\\alpha < s$. We complement our approximation guarantee with a dimension-free estimate on the sample complexity of a residual KAN model when learning a function of Besov regularity from $N$ i.i.d. noiseless samples. Our KAN architecture incorporates contemporary deep learning wisdom by leveraging residual/skip connections between layers.|\n", "2504.15100": "|**2025-04-21**|**Application of Sensitivity Analysis Methods for Studying Neural Network Models**|Jiaxuan Miao, Sergey Matveev et.al.|[2504.15100](http://arxiv.org/abs/2504.15100)|null|11 pages, 16 figures, 32 references|This study demonstrates the capabilities of several methods for analyzing the sensitivity of neural networks to perturbations of the input data and interpreting their underlying mechanisms. The investigated approaches include the Sobol global sensitivity analysis, the local sensitivity method for input pixel perturbations and the activation maximization technique. As examples, in this study we consider a small feedforward neural network for analyzing an open tabular dataset of clinical diabetes data, as well as two classical convolutional architectures, VGG-16 and ResNet-18, which are widely used in image processing and classification. Utilization of the global sensitivity analysis allows us to identify the leading input parameters of the chosen tiny neural network and reduce their number without significant loss of the accuracy. As far as global sensitivity analysis is not applicable to larger models we try the local sensitivity analysis and activation maximization method in application to the convolutional neural networks. These methods show interesting patterns for the convolutional models solving the image classification problem. All in all, we compare the results of the activation maximization method with popular Grad-CAM technique in the context of ultrasound data analysis.|\n", "2504.15073": "|**2025-04-21**|**Hermitian Quaternion Toeplitz Matrices by Quaternion-valued Generating Functions**|Xue-lei Lin, Michael K. Ng, Junjun Pan et.al.|[2504.15073](http://arxiv.org/abs/2504.15073)|null||In this paper, we study Hermitian quaternion Toeplitz matrices generated by quaternion-valued functions. We show that such generating function must be the sum of a real-valued function and an odd function with imaginary component. This setting is different from the case of Hermitian complex Toeplitz matrices generated by real-valued functions only. By using of 2-by-2 block complex representation of quaternion matrices, we give a quaternion version of Grenander-Szeg\\\"{o} theorem stating the distribution of eigenvalues of Hermitian quaternion Toeplitz matrices in terms of its generating function. As an application, we investigate Strang's circulant preconditioners for Hermitian quaternion Toeplitz linear systems arising from quaternion signal processing. We show that Strang's circulant preconditioners can be diagionalized by discrete quaternion Fourier transform matrices whereas general quaternion circulant matrices cannot be diagonalized by them. Also we verify the theoretical and numerical convergence results of Strang's circulant preconditioned conjugate gradient method for solving Hermitian quaternion Toeplitz systems.|\n", "2504.14978": "|**2025-04-21**|**clusttraj: a solvent-informed clustering tool for molecular modeling**|Rafael Bicudo Ribeiro, Henrique Musseli Cezar et.al.|[2504.14978](http://arxiv.org/abs/2504.14978)|null||Clustering techniques are consolidated as a powerful strategy for analyzing the extensive data generated from molecular modeling. In particular, some tools have been developed to cluster configurations from classical simulations with a standard focus on individual units, ranging from small molecules to complex proteins. Since the standard approach includes computing the Root Mean Square Deviation (RMSD) of atomic positions, accounting for the permutation between atoms is crucial for optimizing the clustering procedure in the presence of identical molecules. To address this issue, we present the clusttraj program, a solvent-informed clustering package that fixes inflated RMSD values by finding the optimal pairing between configurations. The program combines reordering schemes with the Kabsch algorithm to minimize the RMSD of molecular configurations before running a hierarchical clustering protocol. By considering evaluation metrics, one can determine the ideal threshold in an automated fashion and compare the different linkage schemes available. The program capabilities are exemplified by considering solute-solvent systems ranging from pure water clusters to a solvated protein or a small solute in different solvents. As a result, we investigate the dependence on different parameters, such as the system size and reordering method, and also the representativeness of the cluster medoids for the characterization of optical properties. clusttraj is implemented as a Python library and can be employed to cluster generic ensembles of molecular configurations that go beyond solute-solvent systems.|\n", "2504.14939": "|**2025-04-21**|**Full Discretization of Stochastic Semilinear Schr\u00f6dinger equation driven by multiplicative Wiener noise**|Suprio Bhar, Mrinmay Biswas, Mangala Prasad et.al.|[2504.14939](http://arxiv.org/abs/2504.14939)|null|27 pages, Comments are welcome|In this article, we have analyzed the full discretization of the Stochastic semilinear Schr\\\"{o}dinger equation in a bounded convex polygonal domain driven by multiplicative Wiener noise. We use the finite element method for spatial discretization and the stochastic trigonometric method for time discretization and derive a strong convergence rate with respect to both parameters (temporal and spatial). Numerical experiments have also been performed to support theoretical bounds.|\n", "2504.14930": "|**2025-04-21**|**Kernel-learning parameter prediction and evaluation in algebraic multigrid method for several PDEs**|Juan Zhang, Junyue Luo, Fangfang Zhang et.al.|[2504.14930](http://arxiv.org/abs/2504.14930)|null||This paper explores the application of kernel learning methods for parameter prediction and evaluation in the Algebraic Multigrid Method (AMG), focusing on several Partial Differential Equation (PDE) problems. AMG is an efficient iterative solver for large-scale sparse linear systems, particularly those derived from elliptic and parabolic PDE discretizations. However, its performance heavily relies on numerous parameters, which are often set empirically and are highly sensitive to AMG's effectiveness. Traditional parameter optimization methods are either computationally expensive or lack theoretical support. To address this, we propose a Gaussian Process Regression (GPR)-based strategy to optimize AMG parameters and introduce evaluation metrics to assess their effectiveness. Trained on small-scale datasets, GPR predicts nearly optimal parameters, bypassing the time-consuming parameter sweeping process. We also use kernel learning techniques to build a kernel function library and determine the optimal kernel function through linear combination, enhancing prediction accuracy. In numerical experiments, we tested typical PDEs such as the constant-coefficient Poisson equation, variable-coefficient Poisson equation, diffusion equation, and Helmholtz equation. Results show that GPR-predicted parameters match grid search results in iteration counts while significantly reducing computational time. A comprehensive analysis using metrics like mean squared error, prediction interval coverage, and Bayesian information criterion confirms GPR's efficiency and reliability. These findings validate GPR's effectiveness in AMG parameter optimization and provide theoretical support for AMG's practical application.|\n", "2504.14790": "|**2025-04-21**|**Enhanced Data-driven Topology Design Methodology with Multi-level Mesh and Correlation-based Mutation for Stress-related Multi-objective Optimization**|Jun Yang, Shintaro Yamasaki et.al.|[2504.14790](http://arxiv.org/abs/2504.14790)|null|23 pages, 22 figures|Topology optimization (TO) serves as a widely applied structural design approach to tackle various engineering problems. Nevertheless, sensitivity-based TO methods usually struggle with solving strongly nonlinear optimization problems. By leveraging high capacity of deep generative model, which is an influential machine learning technique, the sensitivity-free data-driven topology design (DDTD) methodology is regarded as an effective means of overcoming these issues. The DDTD methodology depends on initial dataset with a certain regularity, making its results highly sensitive to initial dataset quality. This limits its effectiveness and generalizability, especially for optimization problems without priori information. In this research, we proposed a multi-level mesh DDTD-based method with correlation-based mutation module to escape from the limitation of the quality of the initial dataset on the results and enhance computational efficiency. The core is to employ a correlation-based mutation module to assign new geometric features with physical meaning to the generated data, while utilizing a multi-level mesh strategy to progressively enhance the refinement of the structural representation, thus avoiding the maintenance of a high degree-of-freedom (DOF) representation throughout the iterative process. The proposed multi-level mesh DDTD-based method can be driven by a low quality initial dataset without the need for time-consuming construction of a specific dataset, thus significantly increasing generality and reducing application difficulty, while further lowering computational cost of DDTD methodology. Various comparison experiments with the traditional sensitivity-based TO methods on stress-related strongly nonlinear problems demonstrate the generality and effectiveness of the proposed method.|\n", "2504.14768": "|**2025-04-20**|**A note on unshifted lattice rules for high-dimensional integration in weighted unanchored Sobolev spaces**|Takashi Goda et.al.|[2504.14768](http://arxiv.org/abs/2504.14768)|null|6 pages|This short article studies a deterministic quasi-Monte Carlo lattice rule in weighted unanchored Sobolev spaces of smoothness $1$. Building on the error analysis by Kazashi and Sloan, we prove the existence of unshifted rank-1 lattice rules that achieve a worst-case error of $O(n^{-1/4}(\\log n)^{1/2})$, with the implied constant independent of the dimension, under certain summability conditions on the weights. Although this convergence rate is inferior to the one achievable for the shifted-averaged root mean squared worst-case error, the result does not rely on random shifting or transformation and holds unconditionally without any conjecture, as assumed by Kazashi and Sloan.|\n", "2504.14722": "|**2025-04-20**|**Path sampling challenges in large biomolecular systems: RETIS and REPPTIS for ABL-imatinib kinetics**|Wouter Vervust, Daniel T. Zhang, Enrico Riccardi et.al.|[2504.14722](http://arxiv.org/abs/2504.14722)|null|Preprint|Predicting the kinetics of drug-protein interactions is crucial for understanding drug efficacy, particularly in personalized medicine, where protein mutations can significantly alter drug residence times. This study applies Replica Exchange Transition Interface Sampling (RETIS) and its Partial Path variant (REPPTIS) to investigate the dissociation kinetics of imatinib from Abelson nonreceptor tyrosine kinase (ABL) and mutants relevant to chronic myeloid leukemia therapy. These path-sampling methods offer a bias-free alternative to conventional approaches requiring qualitative predefined reaction coordinates. Nevertheless, the complex free-energy landscape of ABL-imatinib dissociation presents significant challenges. Multiple metastable states and orthogonal barriers lead to parallel unbinding pathways, complicating convergence in TIS-based methods. Despite employing computational efficiency strategies such as asynchronous replica exchange, full convergence remained elusive. This work provides a critical assessment of path sampling in high-dimensional biological systems, discussing the need for enhanced initialization strategies, advanced Monte Carlo path generation moves, and machine learning-derived reaction coordinates to improve kinetic predictions of drug dissociation with minimal prior knowledge.|\n", "2504.14721": "|**2025-04-20**|**Data-driven model order reduction for T-Product-Based dynamical systems**|Shenghan Mei, Ziqin He, Yidan Mei et.al.|[2504.14721](http://arxiv.org/abs/2504.14721)|null|12 pages, 1 figure|Model order reduction plays a crucial role in simplifying complex systems while preserving their essential dynamic characteristics, making it an invaluable tool in a wide range of applications, including robotic systems, signal processing, and fluid dynamics. However, traditional model order reduction techniques like balanced truncation are not designed to handle tensor data directly and instead require unfolding the data, which may lead to the loss of important higher-order structural information. In this article, we introduce a novel framework for data-driven model order reduction of T-product-based dynamical systems (TPDSs), which are often used to capture the evolution of third-order tensor data such as images and videos through the T-product. Specifically, we develop advanced T-product-based techniques, including T-balanced truncation, T-balanced proper orthogonal decomposition, and the T-eigensystem realization algorithm for input-output TPDSs by leveraging the unique properties of T-singular value decomposition. We demonstrate that these techniques offer significant memory and computational savings while achieving reduction errors that are comparable to those of conventional methods. The effectiveness of the proposed framework is further validated through synthetic and real-world examples.|\n", "2504.14498": "|**2025-04-20**|**Assessing the Performance of Mixed-Precision ILU(0)-Preconditioned Multiple-Precision Real and Complex Krylov Subspace Methods**|Tomonori Kouya et.al.|[2504.14498](http://arxiv.org/abs/2504.14498)|null||Krylov subspace methods are linear solvers based on matrix-vector multiplications and vector operations. While easily parallelizable, they are sensitive to rounding errors and may experience convergence issues. ILU(0), an incomplete LU factorization with zero fill-in, is a well-known preconditioning technique that enhances convergence for sparse matrices. In this paper, we implement a double-precision and multiple-precision ILU(0) preconditioner, compatible with product-type Krylov subspace methods, and evaluate its performance.|\n", "2504.14479": "|**2025-04-20**|**Stochastic Norton Dynamics: An Alternative Approach for the Computation of Transport Coefficients in Dissipative Particle Dynamics**|Xinyi Wu, Xiaocheng Shang et.al.|[2504.14479](http://arxiv.org/abs/2504.14479)|null||We study a novel alternative approach for the computation of transport coefficients at mesoscales. While standard nonequilibrium molecular dynamics (NEMD) approaches fix the forcing and measure the average induced flux in the system driven out of equilibrium, the so-called ``stochastic Norton dynamics'' instead fixes the value of the flux and measures the average magnitude of the forcing needed to induce it. We extend recent results obtained in Langevin dynamics to consider the generalisation of the stochastic Norton dynamics in the popular dissipative particle dynamics (DPD) at mesoscales, important for a wide range of complex fluids and soft matter applications. We demonstrate that the responses profiles for both the NEMD and stochastic Norton dynamics approaches coincide in both linear and nonlinear regimes, indicating that the stochastic Norton dynamics can indeed act as an alternative approach for the computation of transport coefficients, including the mobility and the shear viscosity, as the NEMD dynamics. In addition, based on the linear response of the DPD system with small perturbations, we derive a closed-form expression for the shear viscosity, and numerically validate its effectiveness with various types of external forces. Moreover, our numerical experiments demonstrate that the stochastic Norton dynamics approach clearly outperforms the NEMD dynamics in controlling the asymptotic variance, a key metric to measure the associated computational costs, particularly in the high friction limit.|\n", "2504.14473": "|**2025-04-20**|**Invariance-embedded Machine Learning Sub-grid-scale Stress Models for Meso-scale Hurricane Boundary Layer Flow Simulation I: Model Development and $\\textit{a priori}$ Studies**|Md Badrul Hasan, Meilin Yu, Tim Oates et.al.|[2504.14473](http://arxiv.org/abs/2504.14473)|null||This study develops invariance-embedded machine learning sub-grid-scale (SGS) stress models admitting turbulence kinetic energy (TKE) backscatter towards more accurate large eddy simulation (LES) of meso-scale turbulent hurricane boundary layer flows. The new machine learning SGS model consists of two parts: a classification model used to distinguish regions with either strong energy cascade or energy backscatter from those with mild TKE transfer and a regression model used to calculate SGS stresses in regions with strong TKE transfer. To ease model implementation in computational fluid dynamics (CFD) solvers, the Smagorinsky model with a signed coefficient $C_s$, where a positive value indicates energy cascade while a negative one indicates energy backscatter, is employed as the carrier of the machine learning model. To improve its robustness and generality, both physical invariance and geometric invariance features of turbulent flows are embedded into the model input for classification and regression, and the signed Smagorinsky model coefficient is used as the output of the regression model. Different machine-learning methods and input setups have been used to test the classification model's performance. The F1-scores, which measure balanced precision and recall of a model, of the classification models with physical and geometric invariance embedded can be improved by about $17\\%$ over those without considering geometric invariance. Regression models based on ensemble neural networks have demonstrated superior performance in predicting the signed Smagorinsky model coefficient, exceeding that of the dynamic Smagorinsky model in $\\textit{a priori}$ tests.|\n", "2504.14422": "|**2025-04-19**|**Optimal Lattice Boltzmann Closures through Multi-Agent Reinforcement Learning**|Paul Fischer, Sebastian Kaltenbach, Sergey Litvinov et.al.|[2504.14422](http://arxiv.org/abs/2504.14422)|null||The Lattice Boltzmann method (LBM) offers a powerful and versatile approach to simulating diverse hydrodynamic phenomena, spanning microfluidics to aerodynamics. The vast range of spatiotemporal scales inherent in these systems currently renders full resolution impractical, necessitating the development of effective closure models for under-resolved simulations. Under-resolved LBMs are unstable, and while there is a number of important efforts to stabilize them, they often face limitations in generalizing across scales and physical systems. We present a novel, data-driven, multiagent reinforcement learning (MARL) approach that drastically improves stability and accuracy of coarse-grained LBM simulations. The proposed method uses a convolutional neural network to dynamically control the local relaxation parameter for the LB across the simulation grid. The LB-MARL framework is showcased in turbulent Kolmogorov flows. We find that the MARL closures stabilize the simulations and recover the energy spectra of significantly more expensive fully resolved simulations while maintaining computational efficiency. The learned closure model can be transferred to flow scenarios unseen during training and has improved robustness and spectral accuracy compared to traditional LBM models. We believe that MARL closures open new frontiers for efficient and accurate simulations of a multitude of complex problems not accessible to present-day LB methods alone.|\n", "2504.14405": "|**2025-04-19**|**Sensitivity-aware rock physics enhanced digital shadow for underground-energy storage monitoring**|Abhinav Prakash Gahlot, Huseyin Tuna Erdinc, Felix J. Herrmann et.al.|[2504.14405](http://arxiv.org/abs/2504.14405)|null||Underground energy storage, which includes storage of hydrogen, compressed air, and CO2, requires careful monitoring to track potential leakage pathways, a situation where time-lapse seismic imaging alone may be inadequate. A recently developed Digital Shadow (DS) enhances forecasting using machine learning and Bayesian inference, yet their accuracy depends on assumed rock physics models, the mismatch of which can lead to unreliable predictions for the reservoir's state (saturation/pressure). Augmenting DS training with multiple rock physics models mitigates errors but averages over uncertainties, obscuring their sources. To address this challenge, we introduce context-aware sensitivity analysis inspired by amortized Bayesian inference, allowing the DS to learn explicit dependencies between seismic data, the reservoir state, e.g., CO2 saturation, and rock physics models. At inference time, this approach allows for real-time ''what if'' scenario testing rather than relying on costly retraining, thereby enhancing interpretability and decision-making for safer, more reliable underground storage.|\n", "2504.14343": "|**2025-04-19**|**Numerical analysis of a particle system for the calibrated Heston-type local stochastic volatility model**|Christoph Reisinger, Maria Olympia Tsianni et.al.|[2504.14343](http://arxiv.org/abs/2504.14343)|null||We analyse a Monte Carlo particle method for the simulation of the calibrated Heston-type local stochastic volatility (H-LSV) model. The common application of a kernel estimator for a conditional expectation in the calibration condition results in a McKean-Vlasov (MV) stochastic differential equation (SDE) with non-standard coefficients. The primary challenges lie in certain mean-field terms in the drift and diffusion coefficients and the $1/2$-H\\\"{o}lder regularity of the diffusion coefficient. We establish the well-posedness of this equation for a fixed but arbitrarily small bandwidth of the kernel estimator. Moreover, we prove a strong propagation of chaos result, ensuring convergence of the particle system under a condition on the Feller ratio and up to a critical time. For the numerical simulation, we employ an Euler-Maruyama scheme for the log-spot process and a full truncation Euler scheme for the CIR volatility process. Under certain conditions on the inputs and the Feller ratio, we prove strong convergence of the Euler-Maruyama scheme with rate $1/2$ in time, up to a logarithmic factor. Numerical experiments illustrate the convergence of the discretisation scheme and validate the propagation of chaos in practice.|\n"}}