{"Machine Learning in Finance": {"2503.08697": "|**2025-03-06**|**Matrix H-theory approach to stock market fluctuations**|Luan M. T. de Moraes, Ant\u00f4nio M. S. Macedo, Raydonal Ospina et.al.|[2503.08697](http://arxiv.org/abs/2503.08697)|null|26 pages, 10 figures. Published on Physical Review E|We introduce matrix H theory, a framework for analyzing collective behavior arising from multivariate stochastic processes with hierarchical structure. The theory models the joint distribution of the multiple variables (the measured signal) as a compound of a large-scale multivariate distribution with the distribution of a slowly fluctuating background. The background is characterized by a hierarchical stochastic evolution of internal degrees of freedom, representing the correlations between stocks at different time scales. As in its univariate version, the matrix H-theory formalism also has two universality classes: Wishart and inverse Wishart, enabling a concise description of both the background and the signal probability distributions in terms of Meijer G-functions with matrix argument. Empirical analysis of daily returns of stocks within the S&P500 demonstrates the effectiveness of matrix H theory in describing fluctuations in stock markets. These findings contribute to a deeper understanding of multivariate hierarchical processes and offer potential for developing more informed portfolio strategies in financial markets.|\n", "2503.08696": "|**2025-03-05**|**Multimodal Stock Price Prediction: A Case Study of the Russian Securities Market**|Kasymkhan Khubiev, Mikhail Semenov et.al.|[2503.08696](http://arxiv.org/abs/2503.08696)|null|NSCF-2024, PROGRAM SYSTEMS: THEORY AND APPLICATIONS|Classical asset price forecasting methods primarily rely on numerical data, such as price time series, trading volumes, limit order book data, and technical analysis indicators. However, the news flow plays a significant role in price formation, making the development of multimodal approaches that combine textual and numerical data for improved prediction accuracy highly relevant. This paper addresses the problem of forecasting financial asset prices using the multimodal approach that combines candlestick time series and textual news flow data. A unique dataset was collected for the study, which includes time series for 176 Russian stocks traded on the Moscow Exchange and 79,555 financial news articles in Russian. For processing textual data, pre-trained models RuBERT and Vikhr-Qwen2.5-0.5b-Instruct (a large language model) were used, while time series and vectorized text data were processed using an LSTM recurrent neural network. The experiments compared models based on a single modality (time series only) and two modalities, as well as various methods for aggregating text vector representations. Prediction quality was estimated using two key metrics: Accuracy (direction of price movement prediction: up or down) and Mean Absolute Percentage Error (MAPE), which measures the deviation of the predicted price from the true price. The experiments showed that incorporating textual modality reduced the MAPE value by 55%. The resulting multimodal dataset holds value for the further adaptation of language models in the financial sector. Future research directions include optimizing textual modality parameters, such as the time window, sentiment, and chronological order of news messages.|\n", "2503.03612": "|**2025-03-10**|**Large language models in finance : what is financial sentiment?**|Kemal Kirtac, Guido Germano et.al.|[2503.03612](http://arxiv.org/abs/2503.03612)|null||Financial sentiment has become a crucial yet complex concept in finance, increasingly used in market forecasting and investment strategies. Despite its growing importance, there remains a need to define and understand what financial sentiment truly represents and how it can be effectively measured. We explore the nature of financial sentiment and investigate how large language models (LLMs) contribute to its estimation. We trace the evolution of sentiment measurement in finance, from market-based and lexicon-based methods to advanced natural language processing techniques. The emergence of LLMs has significantly enhanced sentiment analysis, providing deeper contextual understanding and greater accuracy in extracting sentiment from financial text. We examine how BERT-based models, such as RoBERTa and FinBERT, are optimized for structured sentiment classification, while GPT-based models, including GPT-4, OPT, and LLaMA, excel in financial text generation and real-time sentiment interpretation. A comparative analysis of bidirectional and autoregressive transformer architectures highlights their respective roles in investor sentiment analysis, algorithmic trading, and financial decision-making. By exploring what financial sentiment is and how it is estimated within LLMs, we provide insights into the growing role of AI-driven sentiment analysis in finance.|\n", "2503.02680": "|**2025-03-04**|**VWAP Execution with Signature-Enhanced Transformers: A Multi-Asset Learning Approach**|Remi Genet et.al.|[2503.02680](http://arxiv.org/abs/2503.02680)|**[link](https://github.com/remigenet/DynamicVWAPTransformer)**||In this paper I propose a novel approach to Volume Weighted Average Price (VWAP) execution that addresses two key practical challenges: the need for asset-specific model training and the capture of complex temporal dependencies. Building upon my recent work in dynamic VWAP execution arXiv:2502.18177, I demonstrate that a single neural network trained across multiple assets can achieve performance comparable to or better than traditional asset-specific models. The proposed architecture combines a transformer-based design inspired by arXiv:2406.02486 with path signatures for capturing geometric features of price-volume trajectories, as in arXiv:2406.17890. The empirical analysis, conducted on hourly cryptocurrency trading data from 80 trading pairs, shows that the globally-fitted model with signature features (GFT-Sig) achieves superior performance in both absolute and quadratic VWAP loss metrics compared to asset-specific approaches. Notably, these improvements persist for out-of-sample assets, demonstrating the model's ability to generalize across different market conditions. The results suggest that combining global parameter sharing with signature-based feature extraction provides a scalable and robust approach to VWAP execution, offering significant practical advantages over traditional asset-specific implementations.|\n", "2503.02518": "|**2025-03-04**|**Extrapolating the long-term seasonal component of electricity prices for forecasting in the day-ahead market**|Katarzyna Ch\u0119\u0107, Bartosz Uniejewski, Rafa\u0142 Weron et.al.|[2503.02518](http://arxiv.org/abs/2503.02518)|null||Recent studies provide evidence that decomposing the electricity price into the long-term seasonal component (LTSC) and the remaining part, predicting both separately, and then combining their forecasts can bring significant accuracy gains in day-ahead electricity price forecasting. However, not much attention has been paid to predicting the LTSC, and the last 24 hourly values of the estimated pattern are typically copied for the target day. To address this gap, we introduce a novel approach which extracts the trend-seasonal pattern from a price series extrapolated using price forecasts for the next 24 hours. We assess it using two 5-year long test periods from the German and Spanish power markets, covering the Covid-19 pandemic, the 2021/2022 energy crisis, and the war in Ukraine. Considering parsimonious autoregressive and LASSO-estimated models, we find that improvements in predictive accuracy range from 3\\% to 15\\% in terms of the root mean squared error and exceed 1\\% in terms of profits from a realistic trading strategy involving day-ahead bidding and battery storage.|\n", "2503.08693": "|**2025-03-02**|**Liquidity-adjusted Return and Volatility, and Autoregressive Models**|Qi Deng, Zhong-guo Zhou et.al.|[2503.08693](http://arxiv.org/abs/2503.08693)|null||We construct liquidity-adjusted return and volatility using purposely designed liquidity metrics (liquidity jump and liquidity diffusion) that incorporate additional liquidity information. Based on these measures, we introduce a liquidity-adjusted ARMA-GARCH framework to address the limitations of traditional ARMA-GARCH models, which are not effectively in modeling illiquid assets with high liquidity variability, such as cryptocurrencies. We demonstrate that the liquidity-adjusted model improves model fit for cryptocurrencies, with greater volatility sensitivity to past shocks and reduced volatility persistence of erratic past volatility. Our model is validated by the empirical evidence that the liquidity-adjusted mean-variance (LAMV) portfolios outperform the traditional mean-variance (TMV) portfolios.|\n", "2503.00603": "|**2025-03-01**|**Understanding the Commodity Futures Term Structure Through Signatures**|Hari P. Krishnan, Stephan Sturm et.al.|[2503.00603](http://arxiv.org/abs/2503.00603)|null|19 pages, 1 figure|Signature methods have been widely and effectively used as a tool for feature extraction in statistical learning methods, notably in mathematical finance. They lack, however, interpretability: in the general case, it is unclear why signatures actually work. The present article aims to address this issue directly, by introducing and developing the concept of signature perturbations. In particular, we construct a regular perturbation of the signature of the term structure of log prices for various commodities, in terms of the convenience yield. Our perturbation expansion and rigorous convergence estimates help explain the success of signature-based classification of commodities markets according to their term structure, with the volatility of the convenience yield as the major discriminant.|\n", "2502.20978": "|**2025-03-04**|**Using quantile time series and historical simulation to forecast financial risk multiple steps ahead**|Richard Gerlach, Antonio Naimoli, Giuseppe Storti et.al.|[2502.20978](http://arxiv.org/abs/2502.20978)|null||A method for quantile-based, semi-parametric historical simulation estimation of multiple step ahead Value-at-Risk (VaR) and Expected Shortfall (ES) models is developed. It uses the quantile loss function, analogous to how the quasi-likelihood is employed by standard historical simulation methods. The returns data are scaled by the estimated quantile series, then resampling is employed to estimate the forecast distribution one and multiple steps ahead, allowing tail risk forecasting. The proposed method is applicable to any data or model where the relationship between VaR and ES does not change over time and can be extended to allow a measurement equation incorporating realized measures, thus including Realized GARCH and Realized CAViaR type models. Its finite sample properties, and its comparison with existing historical simulation methods, are evaluated via a simulation study. A forecasting study assesses the relative accuracy of the 1% and 2.5% VaR and ES one-day-ahead and ten-day-ahead forecasting results for the proposed class of models compared to several competitors.|\n", "2503.08692": "|**2025-02-27**|**Detecting Crypto Pump-and-Dump Schemes: A Thresholding-Based Approach to Handling Market Noise**|Mahya Karbalaii et.al.|[2503.08692](http://arxiv.org/abs/2503.08692)|null||We propose a simple yet robust unsupervised model to detect pump-and-dump events on tokens listed on the Poloniex Exchange platform. By combining threshold-based criteria with exponentially weighted moving averages (EWMA) and volatility measures, our approach effectively distinguishes genuine anomalies from minor trading fluctuations, even for tokens with low liquidity and prolonged inactivity. These characteristics present a unique challenge, as standard anomaly-detection methods often over-flag negligible volume spikes. Our framework overcomes this issue by tailoring both price and volume thresholds to the specific trading patterns observed, resulting in a model that balances high true-positive detection with minimal noise.|\n", "2502.19305": "|**2025-02-26**|**Corporate Fraud Detection in Rich-yet-Noisy Financial Graph**|Shiqi Wang, Zhibo Zhang, Libing Fang et.al.|[2502.19305](http://arxiv.org/abs/2502.19305)|**[link](https://github.com/wangskyGit/KeHGN-R)**||Corporate fraud detection aims to automatically recognize companies that conduct wrongful activities such as fraudulent financial statements or illegal insider trading. Previous learning-based methods fail to effectively integrate rich interactions in the company network. To close this gap, we collect 18-year financial records in China to form three graph datasets with fraud labels. We analyze the characteristics of the financial graphs, highlighting two pronounced issues: (1) information overload: the dominance of (noisy) non-company nodes over company nodes hinders the message-passing process in Graph Convolution Networks (GCN); and (2) hidden fraud: there exists a large percentage of possible undetected violations in the collected data. The hidden fraud problem will introduce noisy labels in the training dataset and compromise fraud detection results. To handle such challenges, we propose a novel graph-based method, namely, Knowledge-enhanced GCN with Robust Two-stage Learning (${\\rm KeGCN}_{R}$), which leverages Knowledge Graph Embeddings to mitigate the information overload and effectively learns rich representations. The proposed model adopts a two-stage learning method to enhance robustness against hidden frauds. Extensive experimental results not only confirm the importance of interactions but also show the superiority of ${\\rm KeGCN}_{R}$ over a number of strong baselines in terms of fraud detection effectiveness and robustness.|\n", "2502.18177": "|**2025-02-25**|**Recurrent Neural Networks for Dynamic VWAP Execution: Adaptive Trading Strategies with Temporal Kolmogorov-Arnold Networks**|Remi Genet et.al.|[2502.18177](http://arxiv.org/abs/2502.18177)|**[link](https://github.com/remigenet/deepdynamicvwap)**||The execution of Volume Weighted Average Price (VWAP) orders remains a critical challenge in modern financial markets, particularly as trading volumes and market complexity continue to increase. In my previous work arXiv:2502.13722, I introduced a novel deep learning approach that demonstrated significant improvements over traditional VWAP execution methods by directly optimizing the execution problem rather than relying on volume curve predictions. However, that model was static because it employed the fully linear approach described in arXiv:2410.21448, which is not designed for dynamic adjustment. This paper extends that foundation by developing a dynamic neural VWAP framework that adapts to evolving market conditions in real time. We introduce two key innovations: first, the integration of recurrent neural networks to capture complex temporal dependencies in market dynamics, and second, a sophisticated dynamic adjustment mechanism that continuously optimizes execution decisions based on market feedback. The empirical analysis, conducted across five major cryptocurrency markets, demonstrates that this dynamic approach achieves substantial improvements over both traditional methods and our previous static implementation, with execution performance gains of 10 to 15% in liquid markets and consistent outperformance across varying conditions. These results suggest that adaptive neural architectures can effectively address the challenges of modern VWAP execution while maintaining computational efficiency suitable for practical deployment.|\n", "2502.17967": "|**2025-02-25**|**LLM Knows Geometry Better than Algebra: Numerical Understanding of LLM-Based Agents in A Trading Arena**|Tianmi Ma, Jiawei Du, Wenxin Huang et.al.|[2502.17967](http://arxiv.org/abs/2502.17967)|**[link](https://github.com/wekjsdvnm/agent-trading-arena)**||Recent advancements in large language models (LLMs) have significantly improved performance in natural language processing tasks. However, their ability to generalize to dynamic, unseen tasks, particularly in numerical reasoning, remains a challenge. Existing benchmarks mainly evaluate LLMs on problems with predefined optimal solutions, which may not align with real-world scenarios where clear answers are absent. To bridge this gap, we design the Agent Trading Arena, a virtual numerical game simulating complex economic systems through zero-sum games, where agents invest in stock portfolios. Our experiments reveal that LLMs, including GPT-4o, struggle with algebraic reasoning when dealing with plain-text stock data, often focusing on local details rather than global trends. In contrast, LLMs perform significantly better with geometric reasoning when presented with visual data, such as scatter plots or K-line charts, suggesting that visual representations enhance numerical reasoning. This capability is further improved by incorporating the reflection module, which aids in the analysis and interpretation of complex data. We validate our findings on NASDAQ Stock dataset, where LLMs demonstrate stronger reasoning with visual data compared to text. Our code and data are publicly available at https://github.com/wekjsdvnm/Agent-Trading-Arena.git.|\n", "2502.17044": "|**2025-02-24**|**A data-driven econo-financial stress-testing framework to estimate the effect of supply chain networks on financial systemic risk**|Jan Fialkowski, Christian Diem, Andr\u00e1s Borsos et.al.|[2502.17044](http://arxiv.org/abs/2502.17044)|**[link](https://github.com/JanFialkowski/FSRI_Plus)**||Supply chain disruptions constitute an often underestimated risk for financial stability. As in financial networks, systemic risks in production networks arises when the local failure of one firm impacts the production of others and might trigger cascading disruptions that affect significant parts of the economy. Here, we study how systemic risk in production networks translates into financial systemic risk through a mechanism where supply chain contagion leads to correlated bank-firm loan defaults. We propose a financial stress-testing framework for micro- and macro-prudential applications that features a national firm level supply chain network in combination with interbank network layers. The model is calibrated by using a unique data set including about 1 million firm-level supply links, practically all bank-firm loans, and all interbank loans in a small European economy. As a showcase we implement a real COVID-19 shock scenario on the firm level. This model allows us to study how the disruption dynamics in the real economy can lead to interbank solvency contagion dynamics. We estimate to what extent this amplifies financial systemic risk. We discuss the relative importance of these contagion channels and find an increase of interbank contagion by 70% when production network contagion is present. We then examine the financial systemic risk firms bring to banks and find an increase of up to 28% in the presence of the interbank contagion channel. This framework is the first financial systemic risk model to take agent-level dynamics of the production network and shocks of the real economy into account which opens a path for directly, and event-driven understanding of the dynamical interaction between the real economy and financial systems.|\n", "2502.16023": "|**2025-02-22**|**Contrastive Similarity Learning for Market Forecasting: The ContraSim Framework**|Nicholas Vinden, Raeid Saqur, Zining Zhu et.al.|[2502.16023](http://arxiv.org/abs/2502.16023)|null|8 pages, 3 appendices|We introduce the Contrastive Similarity Space Embedding Algorithm (ContraSim), a novel framework for uncovering the global semantic relationships between daily financial headlines and market movements. ContraSim operates in two key stages: (I) Weighted Headline Augmentation, which generates augmented financial headlines along with a semantic fine-grained similarity score, and (II) Weighted Self-Supervised Contrastive Learning (WSSCL), an extended version of classical self-supervised contrastive learning that uses the similarity metric to create a refined weighted embedding space. This embedding space clusters semantically similar headlines together, facilitating deeper market insights. Empirical results demonstrate that integrating ContraSim features into financial forecasting tasks improves classification accuracy from WSJ headlines by 7%. Moreover, leveraging an information density analysis, we find that the similarity spaces constructed by ContraSim intrinsically cluster days with homogeneous market movement directions, indicating that ContraSim captures market dynamics independent of ground truth labels. Additionally, ContraSim enables the identification of historical news days that closely resemble the headlines of the current day, providing analysts with actionable insights to predict market trends by referencing analogous past events.|\n", "2502.15611": "|**2025-02-21**|**Network topology of the Euro Area interbank market**|Ilias Aarab, Thomas Gottron et.al.|[2502.15611](http://arxiv.org/abs/2502.15611)|null|This is the preprint version of the paper published in: Aarab, I.,   Gottron, T. (2024). Network Topology of the Euro Area Interbank Market. In:   Mingione, M., Vichi, M., Zaccaria, G. (eds) *High-quality and Timely   Statistics*. CESS 2022. Studies in Theoretical and Applied Statistics.   Springer, Cham. https://doi.org/10.1007/978-3-031-63630-1_1|The rapidly increasing availability of large amounts of granular financial data, paired with the advances of big data related technologies induces the need of suitable analytics that can represent and extract meaningful information from such data. In this paper we propose a multi-layer network approach to distill the Euro Area (EA) banking system in different distinct layers. Each layer of the network represents a specific type of financial relationship between banks, based on various sources of EA granular data collections. The resulting multi-layer network allows one to describe, analyze and compare the topology and structure of EA banks from different perspectives, eventually yielding a more complete picture of the financial market. This granular information representation has the potential to enable researchers and practitioners to better apprehend financial system dynamics as well as to support financial policies to manage and monitor financial risk from a more holistic point of view.|\n", "2502.15458": "|**2025-02-21**|**Clustered Network Connectedness: A New Measurement Framework with Application to Global Equity Markets**|Bastien Buchwalter, Francis X. Diebold, Kamil Yilmaz et.al.|[2502.15458](http://arxiv.org/abs/2502.15458)|null||Network connections, both across and within markets, are central in countless economic contexts. In recent decades, a large literature has developed and applied flexible methods for measuring network connectedness and its evolution, based on variance decompositions from vector autoregressions (VARs), as in Diebold and Yilmaz (2014). Those VARs are, however, typically identified using full orthogonalization (Sims, 1980), or no orthogonalization (Koop, Pesaran, and Potter, 1996; Pesaran and Shin, 1998), which, although useful, are special and extreme cases of a more general framework that we develop in this paper. In particular, we allow network nodes to be connected in \"clusters\", such as asset classes, industries, regions, etc., where shocks are orthogonal across clusters (Sims style orthogonalized identification) but correlated within clusters (Koop-Pesaran-Potter-Shin style generalized identification), so that the ordering of network nodes is relevant across clusters but irrelevant within clusters. After developing the clustered connectedness framework, we apply it in a detailed empirical exploration of sixteen country equity markets spanning three global regions.|\n", "2502.15853": "|**2025-02-21**|**Multi-Agent Stock Prediction Systems: Machine Learning Models, Simulations, and Real-Time Trading Strategies**|Daksh Dave, Gauransh Sawhney, Vikhyat Chauhan et.al.|[2502.15853](http://arxiv.org/abs/2502.15853)|null||This paper presents a comprehensive study on stock price prediction, leveragingadvanced machine learning (ML) and deep learning (DL) techniques to improve financial forecasting accuracy. The research evaluates the performance of various recurrent neural network (RNN) architectures, including Long Short-Term Memory (LSTM) networks, Gated Recurrent Units (GRU), and attention-based models. These models are assessed for their ability to capture complex temporal dependencies inherent in stock market data. Our findings show that attention-based models outperform other architectures, achieving the highest accuracy by capturing both short and long-term dependencies. This study contributes valuable insights into AI-driven financial forecasting, offering practical guidance for developing more accurate and efficient trading systems.|\n", "2502.14479": "|**2025-02-20**|**Modelling the term-structure of default risk under IFRS 9 within a multistate regression framework**|Arno Botha, Tanja Verster, Roland Breedt et.al.|[2502.14479](http://arxiv.org/abs/2502.14479)|null|33 pages, 8192 words, 12 figures|The lifetime behaviour of loans is notoriously difficult to model, which can compromise a bank's financial reserves against future losses, if modelled poorly. Therefore, we present a data-driven comparative study amongst three techniques in modelling a series of default risk estimates over the lifetime of each loan, i.e., its term-structure. The behaviour of loans can be described using a nonstationary and time-dependent semi-Markov model, though we model its elements using a multistate regression-based approach. As such, the transition probabilities are explicitly modelled as a function of a rich set of input variables, including macroeconomic and loan-level inputs. Our modelling techniques are deliberately chosen in ascending order of complexity: 1) a Markov chain; 2) beta regression; and 3) multinomial logistic regression. Using residential mortgage data, our results show that each successive model outperforms the previous, likely as a result of greater sophistication. This finding required devising a novel suite of simple model diagnostics, which can itself be reused in assessing sampling representativeness and the performance of other modelling techniques. These contributions surely advance the current practice within banking when conducting multistate modelling. Consequently, we believe that the estimation of loss reserves will be more timeous and accurate under IFRS 9.|\n", "2502.14431": "|**2025-02-20**|**Causality Analysis of COVID-19 Induced Crashes in Stock and Commodity Markets: A Topological Perspective**|Buddha Nath Sharma, Anish Rai, SR Luwang et.al.|[2502.14431](http://arxiv.org/abs/2502.14431)|null||The paper presents a comprehensive causality analysis of the US stock and commodity markets during the COVID-19 crash. The dynamics of different sectors are also compared. We use Topological Data Analysis (TDA) on multidimensional time-series to identify crashes in stock and commodity markets. The Wasserstein Distance WD shows distinct spikes signaling the crash for both stock and commodity markets. We then compare the persistence diagrams of stock and commodity markets using the WD metric. A significant spike in the $WD$ between stock and commodity markets is observed during the crisis, suggesting significant topological differences between the markets. Similar spikes are observed between the sectors of the US market as well. Spikes obtained may be due to either a difference in the magnitude of crashes in the two markets (or sectors), or from the temporal lag between the two markets suggesting information flow. We study the Granger-causality between stock and commodity markets and also between different sectors. The results show a bidirectional Granger-causality between commodity and stock during the crash period, demonstrating the greater interdependence of financial markets during the crash. However, the overall analysis shows that the causal direction is from stock to commodity. A pairwise Granger-causal analysis between US sectors is also conducted. There is a significant increase in the interdependence between the sectors during the crash period. TDA combined with Granger-causality effectively analyzes the interdependence and sensitivity of different markets and sectors.|\n", "2502.15822": "|**2025-02-20**|**Financial fraud detection system based on improved random forest and gradient boosting machine (GBM)**|Tianzuo Hu et.al.|[2502.15822](http://arxiv.org/abs/2502.15822)|null||This paper proposes a financial fraud detection system based on improved Random Forest (RF) and Gradient Boosting Machine (GBM). Specifically, the system introduces a novel model architecture called GBM-SSRF (Gradient Boosting Machine with Simplified and Strengthened Random Forest), which cleverly combines the powerful optimization capabilities of the gradient boosting machine (GBM) with improved randomization. The computational efficiency and feature extraction capabilities of the Simplified and Strengthened Random Forest (SSRF) forest significantly improve the performance of financial fraud detection. Although the traditional random forest model has good classification capabilities, it has high computational complexity when faced with large-scale data and has certain limitations in feature selection. As a commonly used ensemble learning method, the GBM model has significant advantages in optimizing performance and handling nonlinear problems. However, GBM takes a long time to train and is prone to overfitting problems when data samples are unbalanced. In response to these limitations, this paper optimizes the random forest based on the structure, reducing the computational complexity and improving the feature selection ability through the structural simplification and enhancement of the random forest. In addition, the optimized random forest is embedded into the GBM framework, and the model can maintain efficiency and stability with the help of GBM's gradient optimization capability. Experiments show that the GBM-SSRF model not only has good performance, but also has good robustness and generalization capabilities, providing an efficient and reliable solution for financial fraud detection.|\n"}, "Deep Learning in Finance": {"2503.10285": "|**2025-03-13**|**Unifying monitoring and modelling of water concentration levels in surface waters**|Peter B Sorensen, Anders Nielsen, Peter E Holm et.al.|[2503.10285](http://arxiv.org/abs/2503.10285)|null|41 pages, 11 figures, Developed to support the Danish EPA|Accurate prediction of expected concentrations is essential for effective catchment management, requiring both extensive monitoring and advanced modeling techniques. However, due to limitations in the equation solving capacity, the integration of monitoring and modeling has been suffering suboptimal statistical approaches. This limitation results in models that can only partially leverage monitoring data, thus being an obstacle for realistic uncertainty assessments by overlooking critical correlations between both measurements and model parameters. This study presents a novel solution that integrates catchment monitoring and a unified hieratical statistical catchment modeling that employs a log-normal distribution for residuals within a left-censored likelihood function to address measurements below detection limits. This enables the estimation of concentrations within sub-catchments in conjunction with a source/fate sub-catchment model and monitoring data. This approach is possible due to a model builder R package denoted RTMB. The proposed approach introduces a statistical paradigm based on a hierarchical structure, capable of accommodating heterogeneous sampling across various sampling locations and the authors suggest that this also will encourage further refinement of other existing modeling platforms within the scientific community to improve synergy with monitoring programs. The application of the method is demonstrated through an analysis of nickel concentrations in Danish surface waters.|\n", "2503.10032": "|**2025-03-13**|**A Neumann-Neumann Acceleration with Coarse Space for Domain Decomposition of Extreme Learning Machines**|Chang-Ock Lee, Byungeun Ryoo et.al.|[2503.10032](http://arxiv.org/abs/2503.10032)|null|21 pages, 6 figures, 6 tables|Extreme learning machines (ELMs), which preset hidden layer parameters and solve for last layer coefficients via a least squares method, can typically solve partial differential equations faster and more accurately than Physics Informed Neural Networks. However, they remain computationally expensive when high accuracy requires large least squares problems to be solved. Domain decomposition methods (DDMs) for ELMs have allowed parallel computation to reduce training times of large systems. This paper constructs a coarse space for ELMs, which enables further acceleration of their training. By partitioning interface variables into coarse and non-coarse variables, selective elimination introduces a Schur complement system on the non-coarse variables with the coarse problem embedded. Key to the performance of the proposed method is a Neumann-Neumann acceleration that utilizes the coarse space. Numerical experiments demonstrate significant speedup compared to a previous DDM method for ELMs.|\n", "2503.09409": "|**2025-03-12**|**AI-based Framework for Robust Model-Based Connector Mating in Robotic Wire Harness Installation**|Claudius Kienle, Benjamin Alt, Finn Schneider et.al.|[2503.09409](http://arxiv.org/abs/2503.09409)|null|6 pages, 6 figures, 4 tables, submitted to the 2025 IEEE 21st   International Conference on Automation Science and Engineering|Despite the widespread adoption of industrial robots in automotive assembly, wire harness installation remains a largely manual process, as it requires precise and flexible manipulation. To address this challenge, we design a novel AI-based framework that automates cable connector mating by integrating force control with deep visuotactile learning. Our system optimizes search-and-insertion strategies using first-order optimization over a multimodal transformer architecture trained on visual, tactile, and proprioceptive data. Additionally, we design a novel automated data collection and optimization pipeline that minimizes the need for machine learning expertise. The framework optimizes robot programs that run natively on standard industrial controllers, permitting human experts to audit and certify them. Experimental validations on a center console assembly task demonstrate significant improvements in cycle times and robustness compared to conventional robot programming approaches. Videos are available under https://claudius-kienle.github.io/AppMuTT.|\n", "2503.09345": "|**2025-03-12**|**Large-scale Thermo-Mechanical Simulation of Laser Beam Welding Using High-Performance Computing: A Qualitative Reproduction of Experimental Results**|Tommaso Bevilacqua, Andrey Gumenyuk, Niloufar Habibi et.al.|[2503.09345](http://arxiv.org/abs/2503.09345)|null||Laser beam welding is a non-contact joining technique that has gained significant importance in the course of the increasing degree of automation in industrial manufacturing. This process has established itself as a suitable joining tool for metallic materials due to its non-contact processing, short cycle times, and small heat-affected zones. One potential problem, however, is the formation of solidification cracks, which particularly affects alloys with a pronounced melting range. Since solidification cracking is influenced by both temperature and strain rate, precise measurement technologies are of crucial importance. For this purpose, as an experimental setup, a Controlled Tensile Weldability (CTW) test combined with a local deformation measurement technique is used.   The aim of the present work is the development of computational methods and software tools to numerically simulate the CTW. The numerical results are compared with those obtained from the experimental CTW. In this study, an austenitic stainless steel sheet is selected. A thermo-elastoplastic material behavior with temperature-dependent material parameters is assumed. The time-dependent problem is first discretized in time and then the resulting nonlinear problem is linearized with Newton's method. For the discretization in space, finite elements are used. In order to obtain a sufficiently accurate solution, a large number of finite elements has to be used. In each Newton step, this yields a large linear system of equations that has to be solved. Therefore, a highly parallel scalable solver framework, based on the software library PETSc, was used to solve this computationally challenging problem on a high-performance computing architecture. Finally, the experimental results and the numerical simulations are compared, showing to be qualitatively in good agreement.|\n", "2503.09655": "|**2025-03-12**|**A Deep Reinforcement Learning Approach to Automated Stock Trading, using xLSTM Networks**|Faezeh Sarlakifar, Mohammadreza Mohammadzadeh Asl, Sajjad Rezvani Khaledi et.al.|[2503.09655](http://arxiv.org/abs/2503.09655)|null||Traditional Long Short-Term Memory (LSTM) networks are effective for handling sequential data but have limitations such as gradient vanishing and difficulty in capturing long-term dependencies, which can impact their performance in dynamic and risky environments like stock trading. To address these limitations, this study explores the usage of the newly introduced Extended Long Short Term Memory (xLSTM) network in combination with a deep reinforcement learning (DRL) approach for automated stock trading. Our proposed method utilizes xLSTM networks in both actor and critic components, enabling effective handling of time series data and dynamic market environments. Proximal Policy Optimization (PPO), with its ability to balance exploration and exploitation, is employed to optimize the trading strategy. Experiments were conducted using financial data from major tech companies over a comprehensive timeline, demonstrating that the xLSTM-based model outperforms LSTM-based methods in key trading evaluation metrics, including cumulative return, average profitability per trade, maximum earning rate, maximum pullback, and Sharpe ratio. These findings mark the potential of xLSTM for enhancing DRL-based stock trading systems.|\n", "2503.09198": "|**2025-03-12**|**A 3d particle visualization system for temperature management**|Benoit Lange, Nancy Rodriguez, William Puech et.al.|[2503.09198](http://arxiv.org/abs/2503.09198)|null||This paper deals with a 3D visualization technique proposed to analyze and manage energy efficiency from a data center. Data are extracted from sensors located in the IBM Green Data Center in Montpellier France. These sensors measure different information such as hygrometry, pressure and temperature. We want to visualize in real-time the large among of data produced by these sensors. A visualization engine has been designed, based on particles system and a client server paradigm. In order to solve performance problems, a Level Of Detail solution has been developed. These methods are based on the earlier work introduced by J. Clark in 1976. In this paper we introduce a particle method used for this work and subsequently we explain different simplification methods we have applied to improve our solution.|\n", "2503.09647": "|**2025-03-12**|**Leveraging LLMS for Top-Down Sector Allocation In Automated Trading**|Ryan Quek Wei Heng, Edoardo Vittori, Keane Ong et.al.|[2503.09647](http://arxiv.org/abs/2503.09647)|null||This paper introduces a methodology leveraging Large Language Models (LLMs) for sector-level portfolio allocation through systematic analysis of macroeconomic conditions and market sentiment. Our framework emphasizes top-down sector allocation by processing multiple data streams simultaneously, including policy documents, economic indicators, and sentiment patterns. Empirical results demonstrate superior risk-adjusted returns compared to traditional cross momentum strategies, achieving a Sharpe ratio of 2.51 and portfolio return of 8.79% versus -0.61 and -1.39% respectively. These results suggest that LLM-based systematic macro analysis presents a viable approach for enhancing automated portfolio allocation decisions at the sector level.|\n", "2503.08953": "|**2025-03-11**|**Capturing Lifecycle System Degradation in Digital Twin Model Updating**|Yifan Tang, Mostafa Rahmani Dehaghani, G. Gary Wang et.al.|[2503.08953](http://arxiv.org/abs/2503.08953)|null|32 pages, 25 figures|Digital twin (DT) has emerged as a powerful tool to facilitate monitoring, control, and other decision-making tasks in real-world engineering systems. Online update methods have been proposed to update DT models. Considering the degradation behavior in the system lifecycle, these methods fail to enable DT models to predict the system responses affected by the system degradation over time. To alleviate this problem, degradation models of measurable parameters have been integrated into DT construction. However, identifying the degradation parameters relies on prior knowledge of the system and expensive experiments. To mitigate those limitations, this paper proposes a lifelong update method for DT models to capture the effects of system degradation on system responses without any prior knowledge and expensive offline experiments on the system. The core idea in the work is to represent the system degradation during the lifecycle as the dynamic changes of DT configurations (i.e., model parameters with a fixed model structure) at all degradation stages. During the lifelong update process, an Autoencoder is adopted to reconstruct the model parameters of all hidden layers simultaneously, so that the latent features taking into account the dependencies among hidden layers are obtained for each degradation stage. The dynamic behavior of latent features among successive degradation stages is then captured by a long short-term memory model, which enables prediction of the latent feature at any unseen stage. Based on the predicted latent features, the model configuration at future degradation stage is reconstructed to determine the new DT model, which predicts the system responses affected by the degradation at the same stage. The test results on two engineering datasets demonstrate that the proposed update method could capture effects of system degradation on system responses during the lifecycle.|\n", "2503.08904": "|**2025-03-11**|**Towards Efficient Parametric State Estimation in Circulating Fuel Reactors with Shallow Recurrent Decoder Networks**|Stefano Riva, Carolina Introini, J. Nathan Kutz et.al.|[2503.08904](http://arxiv.org/abs/2503.08904)|**[link](https://github.com/ermete-lab/nushred)**|arXiv admin note: text overlap with arXiv:2409.12550|The recent developments in data-driven methods have paved the way to new methodologies to provide accurate state reconstruction of engineering systems; nuclear reactors represent particularly challenging applications for this task due to the complexity of the strongly coupled physics involved and the extremely harsh and hostile environments, especially for new technologies such as Generation-IV reactors. Data-driven techniques can combine different sources of information, including computational proxy models and local noisy measurements on the system, to robustly estimate the state. This work leverages the novel Shallow Recurrent Decoder architecture to infer the entire state vector (including neutron fluxes, precursors concentrations, temperature, pressure and velocity) of a reactor from three out-of-core time-series neutron flux measurements alone. In particular, this work extends the standard architecture to treat parametric time-series data, ensuring the possibility of investigating different accidental scenarios and showing the capabilities of this approach to provide an accurate state estimation in various operating conditions. This paper considers as a test case the Molten Salt Fast Reactor (MSFR), a Generation-IV reactor concept, characterised by strong coupling between the neutronics and the thermal hydraulics due to the liquid nature of the fuel. The promising results of this work are further strengthened by the possibility of quantifying the uncertainty associated with the state estimation, due to the considerably low training cost. The accurate reconstruction of every characteristic field in real-time makes this approach suitable for monitoring and control purposes in the framework of a reactor digital twin.|\n", "2503.08283": "|**2025-03-11**|**Nonlinear optimals and their role in sustaining turbulence in channel flow**|Dario Klingenberg, Rich R. Kerswell et.al.|[2503.08283](http://arxiv.org/abs/2503.08283)|null||We investigate the energy transfer from the mean profile to velocity fluctuations in channel flow by calculating nonlinear optimal disturbances,i.e. the initial condition of a given finite energy that achieves the highest possible energy growth during a given fixed time horizon. It is found that for a large range of time horizons and initial disturbance energies, the nonlinear optimal exhibits streak spacing and amplitude consistent with DNS at least at Re_tau = 180, which suggests that they isolate the relevant physical mechanisms that sustain turbulence. Moreover, the time horizon necessary for a nonlinear disturbance to outperform a linear optimal is consistent with previous DNS-based estimates using eddy turnover time, which offers a new perspective on how some turbulent time scales are determined.|\n", "2503.08163": "|**2025-03-11**|**XAI4Extremes: An interpretable machine learning framework for understanding extreme-weather precursors under climate change**|Jiawen Wei, Aniruddha Bora, Vivek Oommen et.al.|[2503.08163](http://arxiv.org/abs/2503.08163)|null||Extreme weather events are increasing in frequency and intensity due to climate change. This, in turn, is exacting a significant toll in communities worldwide. While prediction skills are increasing with advances in numerical weather prediction and artificial intelligence tools, extreme weather still present challenges. More specifically, identifying the precursors of such extreme weather events and how these precursors may evolve under climate change remain unclear. In this paper, we propose to use post-hoc interpretability methods to construct relevance weather maps that show the key extreme-weather precursors identified by deep learning models. We then compare this machine view with existing domain knowledge to understand whether deep learning models identified patterns in data that may enrich our understanding of extreme-weather precursors. We finally bin these relevant maps into different multi-year time periods to understand the role that climate change is having on these precursors. The experiments are carried out on Indochina heatwaves, but the methodology can be readily extended to other extreme weather events worldwide.|\n", "2503.07834": "|**2025-03-10**|**Network Analysis of Uniswap: Centralization and Fragility in the Decentralized Exchange Market**|Tao Yan, Claudio J. Tessone et.al.|[2503.07834](http://arxiv.org/abs/2503.07834)|null||The Uniswap is a Decentralized Exchange (DEX) protocol that facilitates automatic token exchange without the need for traditional order books. Every pair of tokens forms a liquidity pool on Uniswap, and each token can be paired with any other token to create liquidity pools. This characteristic motivates us to employ a complex network approach to analyze the features of the Uniswap market. This research presents a comprehensive analysis of the Uniswap network using complex network methods. The network on October 31, 2023, is built to observe its recent features, showcasing both scale-free and core-periphery properties. By employing node and edge-betweenness metrics, we detect the most important tokens and liquidity pools. Additionally, we construct daily networks spanning from the beginning of Uniswap V2 on May 5, 2020, until October 31, 2023, and our findings demonstrate that the network becomes increasingly fragile over time. Furthermore, we conduct a robustness analysis by simulating the deletion of nodes to estimate the impact of some extreme events such as the Terra collapse. The results indicate that the Uniswap network exhibits robustness, yet it is notably fragile when deleting tokens with high betweenness centrality. This finding highlights that, despite being a decentralized exchange, Uniswap exhibits significant centralization tendencies in terms of token network connectivity and the distribution of TVL across nodes (tokens) and edges (liquidity pools).|\n", "2503.07462": "|**2025-03-10**|**Simultaneous Energy Harvesting and Bearing Fault Detection using Piezoelectric Cantilevers**|P. Peralta-Braz, M. M. Alamdari, C. T. Chou et.al.|[2503.07462](http://arxiv.org/abs/2503.07462)|null||Bearings are critical components in industrial machinery, yet their vulnerability to faults often leads to costly breakdowns. Conventional fault detection methods depend on continuous, high-frequency vibration sensing, digitising, and wireless transmission to the cloud-an approach that significantly drains the limited energy reserves of battery-powered sensors, accelerating their depletion and increasing maintenance costs. This work proposes a fundamentally different approach: rather than using instantaneous vibration data, we employ piezoelectric energy harvesters (PEHs) tuned to specific frequencies and leverage the cumulative harvested energy over time as the key diagnostic feature. By directly utilising the energy generated from the machinery's vibrations, we eliminate the need for frequent analog-to-digital conversions and data transmission, thereby reducing energy consumption at the sensor node and extending its operational lifetime. To validate this approach, we use a numerical PEH model and publicly available acceleration datasets, examining various PEH designs with different natural frequencies. We also consider the influence of the classification algorithm, the number of devices, and the observation window duration. The results demonstrate that the harvested energy reliably indicates bearing faults across a range of conditions and severities. By converting vibration energy into both a power source and a diagnostic feature, our solution offers a more sustainable, low-maintenance strategy for fault detection in smart machinery.|\n", "2503.07440": "|**2025-03-10**|**Early signs of stuck pipe detection based on Crossformer**|Bo Cao, Yu Song, Jin Yang et.al.|[2503.07440](http://arxiv.org/abs/2503.07440)|null|33 pages,9 figure|Stuck pipe incidents are one of the major challenges in drilling engineering,leading to massive time loss and additional costs.To address the limitations of insufficient long sequence modeling capability,the difficulty in accurately establishing warning threshold,and the lack of model interpretability in existing methods,we utilize Crossformer for early signs of detection indicating potential stuck events in order to provide guidance for on-site drilling engineers and prevent stuck pipe incidents.The sliding window technique is integrated into Crossformer to allow it to output and display longer outputs,the improved Crossformer model is trained using normal time series drilling data to generate predictions for various parameters at each time step.The relative reconstruction error of model is regard as the risk of stuck pipe,thereby considering data that the model can't predict as anomalies,which represent the early signs of stuck pipe incidents.The multi-step prediction capability of Crossformer and relative reconstruction error are combined to assess stuck pipe risk at each time step in advance.We partition the reconstruction error into modeling error and error due to anomalous data fluctuations,furthermore,the dynamic warning threshold and warning time for stuck pipe incidents are determined using the probability density function of reconstruction errors from normal drilling data.The results indicate that our method can effectively detect early signs of stuck pipe incidents during the drilling process.Crossformer exhibits superior modeling and predictive capabilities compared with other deep learning models.Transformer-based models with multi-step prediction capability are more suitable for stuck pipe prediction compared to the current single-step prediction models.|\n", "2503.07684": "|**2025-03-10**|**What is missing from existing Lithium-Sulfur models to capture coin-cell behaviour?**|Miss. Elizabeth Olisa Monica Marinescu et.al.|[2503.07684](http://arxiv.org/abs/2503.07684)|null|27 pages, 7 figures, conferences presented: ModVal 2025, ECS 2025|Lithium-sulfur (Li-S) batteries offer a promising alternative to current lithium-ion (Li-ion) batteries, with a high theoretical energy density, improved safety and high abundance, low cost of materials. For Li-S to reach commercial application, it is essential to understand how the behaviour scales between cell formats; new material development is predominately completed at coin-cell level, whilst pouch-cells will be used for commercial applications. Differences such as reduced electrolyte-to-sulfur (E/S) ratios and increased geometric size at larger cell formats contribute to the behavioural differences, in terms of achievable capacity, cyclability and potential degradation mechanisms.   This work focuses on the steps required to capture and test coin-cell behaviour, building upon the existing models within the literature, which predominately focus on pouch-cells. The areas investigated throughout this study, to improve the capability of the model in terms of scaling ability and causality of predictions, include the cathode surface area, precipitation dynamics and C-rate dependence.|\n", "2503.07231": "|**2025-03-10**|**An Analytics-Driven Approach to Enhancing Supply Chain Visibility with Graph Neural Networks and Federated Learning**|Ge Zheng, Alexandra Brintrup et.al.|[2503.07231](http://arxiv.org/abs/2503.07231)|null|15 pages, 5 figures, 5 tables, submitted to a journal|In today's globalised trade, supply chains form complex networks spanning multiple organisations and even countries, making them highly vulnerable to disruptions. These vulnerabilities, highlighted by recent global crises, underscore the urgent need for improved visibility and resilience of the supply chain. However, data-sharing limitations often hinder the achievement of comprehensive visibility between organisations or countries due to privacy, security, and regulatory concerns. Moreover, most existing research studies focused on individual firm- or product-level networks, overlooking the multifaceted interactions among diverse entities that characterise real-world supply chains, thus limiting a holistic understanding of supply chain dynamics. To address these challenges, we propose a novel approach that integrates Federated Learning (FL) and Graph Convolutional Neural Networks (GCNs) to enhance supply chain visibility through relationship prediction in supply chain knowledge graphs. FL enables collaborative model training across countries by facilitating information sharing without requiring raw data exchange, ensuring compliance with privacy regulations and maintaining data security. GCNs empower the framework to capture intricate relational patterns within knowledge graphs, enabling accurate link prediction to uncover hidden connections and provide comprehensive insights into supply chain networks. Experimental results validate the effectiveness of the proposed approach, demonstrating its ability to accurately predict relationships within country-level supply chain knowledge graphs. This enhanced visibility supports actionable insights, facilitates proactive risk management, and contributes to the development of resilient and adaptive supply chain strategies, ensuring that supply chains are better equipped to navigate the complexities of the global economy.|\n", "2503.07150": "|**2025-03-10**|**Simulating programmable morphing of shape memory polymer beam systems with complex geometry and topology**|Giulio Ferri, Enzo Marino et.al.|[2503.07150](http://arxiv.org/abs/2503.07150)|null||We propose a novel approach to the analysis of programmable geometrically exact shear deformable beam systems made of shape memory polymers. The proposed method combines the viscoelastic Generalized Maxwell model with the Williams, Landel and Ferry relaxation principle, enabling the reproduction of the shape memory effect of structural systems featuring complex geometry and topology. Very high efficiency is pursued by discretizing the differential problem in space through the isogeometric collocation (IGA-C) method. The method, in addition to the desirable attributes of isogeometric analysis (IGA), such as exactness of the geometric reconstruction of complex shapes and high-order accuracy, circumvents the need for numerical integration since it discretizes the problem in the strong form. Other distinguishing features of the proposed formulation are: i) ${\\rm SO}(3)$-consistency for the linearization of the problem and for the time stepping; ii) minimal (finite) rotation parametrization, that means only three rotational unknowns are used; iii) no additional unknowns are needed to account for the rate-dependent material compared to the purely elastic case. Through different numerical applications involving challenging initial geometries, we show that the proposed formulation possesses all the sought attributes in terms of programmability of complex systems, geometric flexibility, and high order accuracy.|\n", "2503.06926": "|**2025-03-10**|**Effect of Selection Format on LLM Performance**|Yuchen Han, Yucheng Wu, Jeffrey Willard et.al.|[2503.06926](http://arxiv.org/abs/2503.06926)|null||This paper investigates a critical aspect of large language model (LLM) performance: the optimal formatting of classification task options in prompts. Through an extensive experimental study, we compared two selection formats -- bullet points and plain English -- to determine their impact on model performance. Our findings suggest that presenting options via bullet points generally yields better results, although there are some exceptions. Furthermore, our research highlights the need for continued exploration of option formatting to drive further improvements in model performance.|\n", "2503.06769": "|**2025-03-09**|**Modular Photobioreactor Fa\u00e7ade Systems for Sustainable Architecture: Design, Fabrication, and Real-Time Monitoring**|Xiujin Liu et.al.|[2503.06769](http://arxiv.org/abs/2503.06769)|null|21 pages, 22 figures, 3 tables|This paper proposes an innovative solution to the growing issue of greenhouse gas emissions: a closed photobioreactor (PBR) fa\\c{c}ade system to mitigate greenhouse gas (GHG) concentrations. With digital fabrication technology, this study explores the transition from traditional, single function building facades to multifunctional, integrated building systems. It introduces a photobioreactor (PBR) fa\\c{c}ade system to mitigate greenhouse gas (GHG) concentrations while addressing the challenge of large-scale prefabricated components transportation. This research introduces a novel approach by designing the fa\\c{c}ade system as modular, user-friendly and transportation-friendly bricks, enabling the creation of a user-customized and self-assembled photobioreactor (PBR) system. The single module in the system is proposed to be \"neutralization bricks\", which embedded with algae and equipped with an air circulation system, facilitating the photobioreactor (PBR)'s functionality. A connection system between modules allows for easy assembly by users, while a limited variety of brick styles ensures modularity in manufacturing without sacrificing customization and diversity. The system is also equipped with an advanced microalgae status detection algorithm, which allows users to monitor the condition of the microalgae using monocular camera. This functionality ensures timely alerts and notifications for users to replace the algae, thereby optimizing the operational efficiency and sustainability of the algae cultivation process.|\n", "2503.06663": "|**2025-03-09**|**Energy-Adaptive Checkpoint-Free Intermittent Inference for Low Power Energy Harvesting Systems**|Sahidul Islam, Wei Wei, Jishnu Banarjee et.al.|[2503.06663](http://arxiv.org/abs/2503.06663)|null||Deep neural network (DNN) inference in energy harvesting (EH) devices poses significant challenges due to resource constraints and frequent power interruptions. These power losses not only increase end-to-end latency, but also compromise inference consistency and accuracy, as existing checkpointing and restore mechanisms are prone to errors. Consequently, the quality of service (QoS) for DNN inference on EH devices is severely impacted. In this paper, we propose an energy-adaptive DNN inference mechanism capable of dynamically transitioning the model into a low-power mode by reducing computational complexity when harvested energy is limited. This approach ensures that end-to-end latency requirements are met. Additionally, to address the limitations of error-prone checkpoint-and-restore mechanisms, we introduce a checkpoint-free intermittent inference framework that ensures consistent, progress-preserving DNN inference during power failures in energy-harvesting systems.|\n"}, "Reinforcement Learning in Finance": {"2305.07466": "|**2023-04-29**|**Systematic Review on Reinforcement Learning in the Field of Fintech**|Nadeem Malibari, Iyad Katib, Rashid Mehmood et.al.|[2305.07466](http://arxiv.org/abs/2305.07466)|null|31 pages, 15 figures, 7 tables|Applications of Reinforcement Learning in the Finance Technology (Fintech) have acquired a lot of admiration lately. Undoubtedly Reinforcement Learning, through its vast competence and proficiency, has aided remarkable results in the field of Fintech. The objective of this systematic survey is to perform an exploratory study on a correlation between reinforcement learning and Fintech to highlight the prediction accuracy, complexity, scalability, risks, profitability and performance. Major uses of reinforcement learning in finance or Fintech include portfolio optimization, credit risk reduction, investment capital management, profit maximization, effective recommendation systems, and better price setting strategies. Several studies have addressed the actual contribution of reinforcement learning to the performance of financial institutions. The latest studies included in this survey are publications from 2018 onward. The survey is conducted using PRISMA technique which focuses on the reporting of reviews and is based on a checklist and four-phase flow diagram. The conducted survey indicates that the performance of RL-based strategies in Fintech fields proves to perform considerably better than other state-of-the-art algorithms. The present work discusses the use of reinforcement learning algorithms in diverse decision-making challenges in Fintech and concludes that the organizations dealing with finance can benefit greatly from Robo-advising, smart order channelling, market making, hedging and options pricing, portfolio optimization, and optimal execution.|\n", "2206.14267": "|**2022-06-28**|**Applications of Reinforcement Learning in Finance -- Trading with a Double Deep Q-Network**|Frensi Zejnullahu, Maurice Moser, Joerg Osterrieder et.al.|[2206.14267](http://arxiv.org/abs/2206.14267)|null||This paper presents a Double Deep Q-Network algorithm for trading single assets, namely the E-mini S&P 500 continuous futures contract. We use a proven setup as the foundation for our environment with multiple extensions. The features of our trading agent are constantly being expanded to include additional assets such as commodities, resulting in four models. We also respond to environmental conditions, including costs and crises. Our trading agent is first trained for a specific time period and tested on new data and compared with the long-and-hold strategy as a benchmark (market). We analyze the differences between the various models and the in-sample/out-of-sample performance with respect to the environment. The experimental results show that the trading agent follows an appropriate behavior. It can adjust its policy to different circumstances, such as more extensive use of the neutral position when trading costs are present. Furthermore, the net asset value exceeded that of the benchmark, and the agent outperformed the market in the test set. We provide initial insights into the behavior of an agent in a financial domain using a DDQN algorithm. The results of this study can be used for further development.|\n", "2112.04553": "|**2023-02-28**|**Recent Advances in Reinforcement Learning in Finance**|Ben Hambly, Renyuan Xu, Huining Yang et.al.|[2112.04553](http://arxiv.org/abs/2112.04553)|null|60 pages, 1 figure|The rapid changes in the finance industry due to the increasing amount of data have revolutionized the techniques on data processing and data analysis and brought new theoretical and computational challenges. In contrast to classical stochastic control theory and other analytical approaches for solving financial decision-making problems that heavily reply on model assumptions, new developments from reinforcement learning (RL) are able to make full use of the large amount of financial data with fewer model assumptions and to improve decisions in complex financial environments. This survey paper aims to review the recent developments and use of RL approaches in finance. We give an introduction to Markov decision processes, which is the setting for many of the commonly used RL approaches. Various algorithms are then introduced with a focus on value and policy based methods that do not require any model assumptions. Connections are made with neural networks to extend the framework to encompass deep RL algorithms. Our survey concludes by discussing the application of these RL algorithms in a variety of decision-making problems in finance, including optimal execution, portfolio optimization, option pricing and hedging, market making, smart order routing, and robo-advising.|\n"}, "Time Series Forecasting": {"2503.10198": "|**2025-03-13**|**Deep Learning for Time Series Forecasting: A Survey**|Xiangjie Kong, Zhenghao Chen, Weiyao Liu et.al.|[2503.10198](http://arxiv.org/abs/2503.10198)|null||Time series forecasting (TSF) has long been a crucial task in both industry and daily life. Most classical statistical models may have certain limitations when applied to practical scenarios in fields such as energy, healthcare, traffic, meteorology, and economics, especially when high accuracy is required. With the continuous development of deep learning, numerous new models have emerged in the field of time series forecasting in recent years. However, existing surveys have not provided a unified summary of the wide range of model architectures in this field, nor have they given detailed summaries of works in feature extraction and datasets. To address this gap, in this review, we comprehensively study the previous works and summarize the general paradigms of Deep Time Series Forecasting (DTSF) in terms of model architectures. Besides, we take an innovative approach by focusing on the composition of time series and systematically explain important feature extraction methods. Additionally, we provide an overall compilation of datasets from various domains in existing works. Finally, we systematically emphasize the significant challenges faced and future research directions in this field.|\n", "2503.09791": "|**2025-03-12**|**Minimal Time Series Transformer**|Joni-Kristian K\u00e4m\u00e4r\u00e4inen et.al.|[2503.09791](http://arxiv.org/abs/2503.09791)|null|8 pages, 8 figures|Transformer is the state-of-the-art model for many natural language processing, computer vision, and audio analysis problems. Transformer effectively combines information from the past input and output samples in auto-regressive manner so that each sample becomes aware of all inputs and outputs. In sequence-to-sequence (Seq2Seq) modeling, the transformer processed samples become effective in predicting the next output. Time series forecasting is a Seq2Seq problem. The original architecture is defined for discrete input and output sequence tokens, but to adopt it for time series, the model must be adapted for continuous data. This work introduces minimal adaptations to make the original transformer architecture suitable for continuous value time series data.|\n", "2503.09656": "|**2025-03-12**|**LLM-PS: Empowering Large Language Models for Time Series Forecasting with Temporal Patterns and Semantics**|Jialiang Tang, Shuo Chen, Chen Gong et.al.|[2503.09656](http://arxiv.org/abs/2503.09656)|null||Time Series Forecasting (TSF) is critical in many real-world domains like financial planning and health monitoring. Recent studies have revealed that Large Language Models (LLMs), with their powerful in-contextual modeling capabilities, hold significant potential for TSF. However, existing LLM-based methods usually perform suboptimally because they neglect the inherent characteristics of time series data. Unlike the textual data used in LLM pre-training, the time series data is semantically sparse and comprises distinctive temporal patterns. To address this problem, we propose LLM-PS to empower the LLM for TSF by learning the fundamental \\textit{Patterns} and meaningful \\textit{Semantics} from time series data. Our LLM-PS incorporates a new multi-scale convolutional neural network adept at capturing both short-term fluctuations and long-term trends within the time series. Meanwhile, we introduce a time-to-text module for extracting valuable semantics across continuous time intervals rather than isolated time points. By integrating these patterns and semantics, LLM-PS effectively models temporal dependencies, enabling a deep comprehension of time series and delivering accurate forecasts. Intensive experimental results demonstrate that LLM-PS achieves state-of-the-art performance in both short- and long-term forecasting tasks, as well as in few- and zero-shot settings.|\n", "2503.08473": "|**2025-03-11**|**Data Driven Decision Making with Time Series and Spatio-temporal Data**|Bin Yang, Yuxuan Liang, Chenjuan Guo et.al.|[2503.08473](http://arxiv.org/abs/2503.08473)|null|This paper is accepted by ICDE 2025|Time series data captures properties that change over time. Such data occurs widely, ranging from the scientific and medical domains to the industrial and environmental domains. When the properties in time series exhibit spatial variations, we often call the data spatio-temporal. As part of the continued digitalization of processes throughout society, increasingly large volumes of time series and spatio-temporal data are available. In this tutorial, we focus on data-driven decision making with such data, e.g., enabling greener and more efficient transportation based on traffic time series forecasting. The tutorial adopts the holistic paradigm of \"data-governance-analytics-decision.\" We first introduce the data foundation of time series and spatio-temporal data, which is often heterogeneous. Next, we discuss data governance methods that aim to improve data quality. We then cover data analytics, focusing on five desired characteristics: automation, robustness, generality, explainability, and resource efficiency. We finally cover data-driven decision making strategies and briefly discuss promising research directions. We hope that the tutorial will serve as a primary resource for researchers and practitioners who are interested in value creation from time series and spatio-temporal data.|\n", "2503.08328": "|**2025-03-11**|**MFRS: A Multi-Frequency Reference Series Approach to Scalable and Accurate Time-Series Forecasting**|Liang Yu, Lai Tu, Xiang Bai et.al.|[2503.08328](http://arxiv.org/abs/2503.08328)|null||Multivariate time-series forecasting holds immense value across diverse applications, requiring methods to effectively capture complex temporal and inter-variable dynamics. A key challenge lies in uncovering the intrinsic patterns that govern predictability, beyond conventional designs, focusing on network architectures to explore latent relationships or temporal dependencies. Inspired by signal decomposition, this paper posits that time series predictability is derived from periodic characteristics at different frequencies. Consequently, we propose a novel time series forecasting method based on multi-frequency reference series correlation analysis. Through spectral analysis on long-term training data, we identify dominant spectral components and their harmonics to design base-pattern reference series. Unlike signal decomposition, which represents the original series as a linear combination of basis signals, our method uses a transformer model to compute cross-attention between the original series and reference series, capturing essential features for forecasting. Experiments on major open and synthetic datasets show state-of-the-art performance. Furthermore, by focusing on attention with a small number of reference series rather than pairwise variable attention, our method ensures scalability and broad applicability. The source code is available at: https://github.com/yuliang555/MFRS|\n", "2503.08271": "|**2025-03-11**|**LangTime: A Language-Guided Unified Model for Time Series Forecasting with Proximal Policy Optimization**|Wenzhe Niu, Zongxia Xie, Yanru Sun et.al.|[2503.08271](http://arxiv.org/abs/2503.08271)|null||Recent research has shown an increasing interest in utilizing pre-trained large language models (LLMs) for a variety of time series applications. However, there are three main challenges when using LLMs as foundational models for time series forecasting: (1) Cross-domain generalization. (2) Cross-modality alignment. (3) Error accumulation in autoregressive frameworks. To address these challenges, we proposed LangTime, a language-guided unified model for time series forecasting that incorporates cross-domain pre-training with reinforcement learning-based fine-tuning. Specifically, LangTime constructs Temporal Comprehension Prompts (TCPs), which include dataset-wise and channel-wise instructions, to facilitate domain adaptation and condense time series into a single token, enabling LLMs to understand better and align temporal data. To improve autoregressive forecasting, we introduce TimePPO, a reinforcement learning-based fine-tuning algorithm. TimePPO mitigates error accumulation by leveraging a multidimensional rewards function tailored for time series and a repeat-based value estimation strategy. Extensive experiments demonstrate that LangTime achieves state-of-the-art cross-domain forecasting performance, while TimePPO fine-tuning effectively enhances the stability and accuracy of autoregressive forecasting.|\n", "2503.06928": "|**2025-03-10**|**FinTSBridge: A New Evaluation Suite for Real-world Financial Prediction with Advanced Time Series Models**|Yanlong Wang, Jian Xu, Tiantian Gao et.al.|[2503.06928](http://arxiv.org/abs/2503.06928)|null|ICLR 2025 Workshop Advances in Financial AI|Despite the growing attention to time series forecasting in recent years, many studies have proposed various solutions to address the challenges encountered in time series prediction, aiming to improve forecasting performance. However, effectively applying these time series forecasting models to the field of financial asset pricing remains a challenging issue. There is still a need for a bridge to connect cutting-edge time series forecasting models with financial asset pricing. To bridge this gap, we have undertaken the following efforts: 1) We constructed three datasets from the financial domain; 2) We selected over ten time series forecasting models from recent studies and validated their performance in financial time series; 3) We developed new metrics, msIC and msIR, in addition to MSE and MAE, to showcase the time series correlation captured by the models; 4) We designed financial-specific tasks for these three datasets and assessed the practical performance and application potential of these forecasting models in important financial problems. We hope the developed new evaluation suite, FinTSBridge, can provide valuable insights into the effectiveness and robustness of advanced forecasting models in finanical domains.|\n", "2503.06867": "|**2025-03-10**|**Enhancing Time Series Forecasting via Logic-Inspired Regularization**|Jianqi Zhang, Jingyao Wang, Xingchen Shen et.al.|[2503.06867](http://arxiv.org/abs/2503.06867)|null||Time series forecasting (TSF) plays a crucial role in many applications. Transformer-based methods are one of the mainstream techniques for TSF. Existing methods treat all token dependencies equally. However, we find that the effectiveness of token dependencies varies across different forecasting scenarios, and existing methods ignore these differences, which affects their performance. This raises two issues: (1) What are effective token dependencies? (2) How can we learn effective dependencies? From a logical perspective, we align Transformer-based TSF methods with the logical framework and define effective token dependencies as those that ensure the tokens as atomic formulas (Issue 1). We then align the learning process of Transformer methods with the process of obtaining atomic formulas in logic, which inspires us to design a method for learning these effective dependencies (Issue 2). Specifically, we propose Attention Logic Regularization (Attn-L-Reg), a plug-and-play method that guides the model to use fewer but more effective dependencies by making the attention map sparse, thereby ensuring the tokens as atomic formulas and improving prediction performance. Extensive experiments and theoretical analysis confirm the effectiveness of Attn-L-Reg.|\n", "2503.06216": "|**2025-03-08**|**A Novel Distributed PV Power Forecasting Approach Based on Time-LLM**|Huapeng Lin, Miao Yu et.al.|[2503.06216](http://arxiv.org/abs/2503.06216)|null|23 pages, 8 figures|Distributed photovoltaic (DPV) systems are essential for advancing renewable energy applications and achieving energy independence. Accurate DPV power forecasting can optimize power system planning and scheduling while significantly reducing energy loss, thus enhancing overall system efficiency and reliability. However, solar energy's intermittent nature and DPV systems' spatial distribution create significant forecasting challenges. Traditional methods often rely on costly external data, such as numerical weather prediction (NWP) and satellite images, which are difficult to scale for smaller DPV systems. To tackle this issue, this study has introduced an advanced large language model (LLM)-based time series forecasting framework Time-LLM to improve the DPV power forecasting accuracy and generalization ability. By reprogramming, the framework aligns historical power data with natural language modalities, facilitating efficient modeling of time-series data. Then Qwen2.5-3B model is integrated as the backbone LLM to process input data by leveraging its pattern recognition and inference abilities, achieving a balance between efficiency and performance. Finally, by using a flatten and linear projection layer, the LLM's high-dimensional output is transformed into the final forecasts. Experimental results indicate that Time-LLM outperforms leading recent advanced time series forecasting models, such as Transformer-based methods and MLP-based models, achieving superior accuracy in both short-term and long-term forecasting. Time-LLM also demonstrates exceptional adaptability in few-shot and zero-shot learning scenarios. To the best of the authors' knowledge, this study is the first attempt to explore the application of LLMs to DPV power forecasting, which can offer a scalable solution that eliminates reliance on costly external data sources and improve real-world forecasting accuracy.|\n", "2503.06079": "|**2025-03-08**|**Fixing the Pitfalls of Probabilistic Time-Series Forecasting Evaluation by Kernel Quadrature**|Masaki Adachi, Masahiro Fujisawa, Michael A Osborne et.al.|[2503.06079](http://arxiv.org/abs/2503.06079)|null|11 pages, 6 figures|Despite the significance of probabilistic time-series forecasting models, their evaluation metrics often involve intractable integrations. The most widely used metric, the continuous ranked probability score (CRPS), is a strictly proper scoring function; however, its computation requires approximation. We found that popular CRPS estimators--specifically, the quantile-based estimator implemented in the widely used GluonTS library and the probability-weighted moment approximation--both exhibit inherent estimation biases. These biases lead to crude approximations, resulting in improper rankings of forecasting model performance when CRPS values are close. To address this issue, we introduced a kernel quadrature approach that leverages an unbiased CRPS estimator and employs cubature construction for scalable computation. Empirically, our approach consistently outperforms the two widely used CRPS estimators.|\n", "2503.05108": "|**2025-03-07**|**TS-LIF: A Temporal Segment Spiking Neuron Network for Time Series Forecasting**|Shibo Feng, Wanjin Feng, Xingyu Gao et.al.|[2503.05108](http://arxiv.org/abs/2503.05108)|null||Spiking Neural Networks (SNNs) offer a promising, biologically inspired approach for processing spatiotemporal data, particularly for time series forecasting. However, conventional neuron models like the Leaky Integrate-and-Fire (LIF) struggle to capture long-term dependencies and effectively process multi-scale temporal dynamics. To overcome these limitations, we introduce the Temporal Segment Leaky Integrate-and-Fire (TS-LIF) model, featuring a novel dual-compartment architecture. The dendritic and somatic compartments specialize in capturing distinct frequency components, providing functional heterogeneity that enhances the neuron's ability to process both low- and high-frequency information. Furthermore, the newly introduced direct somatic current injection reduces information loss during intra-neuronal transmission, while dendritic spike generation improves multi-scale information extraction. We provide a theoretical stability analysis of the TS-LIF model and explain how each compartment contributes to distinct frequency response characteristics. Experimental results show that TS-LIF outperforms traditional SNNs in time series forecasting, demonstrating better accuracy and robustness, even with missing data. TS-LIF advances the application of SNNs in time-series forecasting, providing a biologically inspired approach that captures complex temporal dynamics and offers potential for practical implementation in diverse forecasting scenarios. The source code is available at https://github.com/kkking-kk/TS-LIF.|\n", "2503.04956": "|**2025-03-06**|**Boltzmann convolutions and Welford mean-variance layers with an application to time series forecasting and classification**|Daniel Andrew Coulson, Martin T. Wells et.al.|[2503.04956](http://arxiv.org/abs/2503.04956)|null|40 pages, 7 figures, 11 tables|In this paper we propose a novel problem called the ForeClassing problem where the loss of a classification decision is only observed at a future time point after the classification decision has to be made. To solve this problem, we propose an approximately Bayesian deep neural network architecture called ForeClassNet for time series forecasting and classification. This network architecture forces the network to consider possible future realizations of the time series, by forecasting future time points and their likelihood of occurring, before making its final classification decision. To facilitate this, we introduce two novel neural network layers, Welford mean-variance layers and Boltzmann convolutional layers. Welford mean-variance layers allow networks to iteratively update their estimates of the mean and variance for the forecasted time points for each inputted time series to the network through successive forward passes, which the model can then consider in combination with a learned representation of the observed realizations of the time series for its classification decision. Boltzmann convolutional layers are linear combinations of approximately Bayesian convolutional layers with different filter lengths, allowing the model to learn multitemporal resolution representations of the input time series, and which resolutions to focus on within a given Boltzmann convolutional layer through a Boltzmann distribution. Through several simulation scenarios and two real world applications we demonstrate ForeClassNet achieves superior performance compared with current state of the art methods including a near 30% improvement in test set accuracy in our financial example compared to the second best performing model.|\n", "2503.07649": "|**2025-03-06**|**TS-RAG: Retrieval-Augmented Generation based Time Series Foundation Models are Stronger Zero-Shot Forecaster**|Kanghui Ning, Zijie Pan, Yu Liu et.al.|[2503.07649](http://arxiv.org/abs/2503.07649)|null||Recently, Large Language Models (LLMs) and Foundation Models (FMs) have become prevalent for time series forecasting tasks. However, fine-tuning large language models (LLMs) for forecasting enables the adaptation to specific domains but may not generalize well across diverse, unseen datasets. Meanwhile, existing time series foundation models (TSFMs) lack inherent mechanisms for domain adaptation and suffer from limited interpretability, making them suboptimal for zero-shot forecasting. To this end, we present TS-RAG, a retrieval-augmented generation based time series forecasting framework that enhances the generalization capability and interpretability of TSFMs. Specifically, TS-RAG leverages pre-trained time series encoders to retrieve semantically relevant time series segments from a dedicated knowledge database, incorporating contextual patterns for the given time series query. Next, we develop a learnable Mixture-of-Experts (MoE)-based augmentation module, which dynamically fuses retrieved time series patterns with the TSFM's representation of the input query, improving forecasting accuracy without requiring task-specific fine-tuning. Thorough empirical studies on seven public benchmark datasets demonstrate that TS-RAG achieves state-of-the-art zero-shot forecasting performance, outperforming TSFMs by up to 6.51% across diverse domains and showcasing desired interpretability.|\n", "2503.04218": "|**2025-03-06**|**Hedging with Sparse Reward Reinforcement Learning**|Yiheng Ding, Gangnan Yuan, Dewei Zuo et.al.|[2503.04218](http://arxiv.org/abs/2503.04218)|null||Derivatives, as a critical class of financial instruments, isolate and trade the price attributes of risk assets such as stocks, commodities, and indices, aiding risk management and enhancing market efficiency. However, traditional hedging models, constrained by assumptions such as continuous trading and zero transaction costs, fail to satisfy risk control requirements in complex and uncertain real-world markets.   With advances in computing technology and deep learning, data-driven trading strategies are becoming increasingly prevalent. This thesis proposes a derivatives hedging framework integrating deep learning and reinforcement learning. The framework comprises a probabilistic forecasting model and a hedging agent, enabling market probability prediction, derivative pricing, and hedging.   Specifically, we design a spatiotemporal attention-based probabilistic financial time series forecasting Transformer to address the scarcity of derivatives hedging data. A low-rank attention mechanism compresses high-dimensional assets into a low-dimensional latent space, capturing nonlinear asset relationships. The Transformer models sequential dependencies within this latent space, improving market probability forecasts and constructing an online training environment for downstream hedging tasks.   Additionally, we incorporate generalized geometric Brownian motion to develop a risk-neutral pricing approach for derivatives. We model derivatives hedging as a reinforcement learning problem with sparse rewards and propose a behavior cloning-based recurrent proximal policy optimization (BC-RPPO) algorithm. This pretraining-finetuning framework significantly enhances the hedging agent's performance. Numerical experiments in the U.S. and Chinese financial markets demonstrate our method's superiority over traditional approaches.|\n", "2503.04118": "|**2025-03-06**|**TimeFound: A Foundation Model for Time Series Forecasting**|Congxi Xiao, Jingbo Zhou, Yixiong Xiao et.al.|[2503.04118](http://arxiv.org/abs/2503.04118)|null||We present TimeFound, an encoder-decoder transformer-based time series foundation model for out-of-the-box zero-shot forecasting. To handle time series data from various domains, TimeFound employs a multi-resolution patching strategy to capture complex temporal patterns at multiple scales. We pre-train our model with two sizes (200M and 710M parameters) on a large time-series corpus comprising both real-world and synthetic datasets. Over a collection of unseen datasets across diverse domains and forecasting horizons, our empirical evaluations suggest that TimeFound can achieve superior or competitive zero-shot forecasting performance, compared to state-of-the-art time series foundation models.|\n", "2503.03729": "|**2025-03-05**|**Graph-Augmented LSTM for Forecasting Sparse Anomalies in Graph-Structured Time Series**|Sneh Pillai et.al.|[2503.03729](http://arxiv.org/abs/2503.03729)|null|12 pages|Detecting anomalies in time series data is a critical task across many domains. The challenge intensifies when anomalies are sparse and the data are multivariate with relational dependencies across sensors or nodes. Traditional univariate anomaly detectors struggle to capture such cross-node dependencies, particularly in sparse anomaly settings. To address this, we propose a graph-augmented time series forecasting approach that explicitly integrates the graph of relationships among time series into an LSTM forecasting model. This enables the model to detect rare anomalies that might otherwise go unnoticed in purely univariate approaches. We evaluate the approach on two benchmark datasets - the Yahoo Webscope S5 anomaly dataset and the METR-LA traffic sensor network - and compare the performance of the Graph-Augmented LSTM against LSTM-only, ARIMA, and Prophet baselines. Results demonstrate that the graph-augmented model achieves significantly higher precision and recall, improving F1-score by up to 10% over the best baseline|\n", "2503.03594": "|**2025-03-09**|**Small but Mighty: Enhancing Time Series Forecasting with Lightweight LLMs**|Haoran Fan, Bin Li, Yixuan Weng et.al.|[2503.03594](http://arxiv.org/abs/2503.03594)|null|20 pages, 10 figures|While LLMs have demonstrated remarkable potential in time series forecasting, their practical deployment remains constrained by excessive computational demands and memory footprints. Existing LLM-based approaches typically suffer from three critical limitations: Inefficient parameter utilization in handling numerical time series patterns; Modality misalignment between continuous temporal signals and discrete text embeddings; and Inflexibility for real-time expert knowledge integration. We present SMETimes, the first systematic investigation of sub-3B parameter SLMs for efficient and accurate time series forecasting. Our approach centers on three key innovations: A statistically-enhanced prompting mechanism that bridges numerical time series with textual semantics through descriptive statistical features; A adaptive fusion embedding architecture that aligns temporal patterns with language model token spaces through learnable parameters; And a dynamic mixture-of-experts framework enabled by SLMs' computational efficiency, adaptively combining base predictions with domain-specific models. Extensive evaluations across seven benchmark datasets demonstrate that our 3B-parameter SLM achieves state-of-the-art performance on five primary datasets while maintaining 3.8x faster training and 5.2x lower memory consumption compared to 7B-parameter LLM baselines. Notably, the proposed model exhibits better learning capabilities, achieving 12.3% lower MSE than conventional LLM. Ablation studies validate that our statistical prompting and cross-modal fusion modules respectively contribute 15.7% and 18.2% error reduction in long-horizon forecasting tasks. By redefining the efficiency-accuracy trade-off landscape, this work establishes SLMs as viable alternatives to resource-intensive LLMs for practical time series forecasting. Code and models are available at https://github.com/xiyan1234567/SMETimes.|\n", "2503.02836": "|**2025-03-04**|**SeqFusion: Sequential Fusion of Pre-Trained Models for Zero-Shot Time-Series Forecasting**|Ting-Ji Huang, Xu-Yang Chen, Han-Jia Ye et.al.|[2503.02836](http://arxiv.org/abs/2503.02836)|**[link](https://github.com/Tingji2419/SeqFusion)**||Unlike traditional time-series forecasting methods that require extensive in-task data for training, zero-shot forecasting can directly predict future values given a target time series without additional training data. Current zero-shot approaches primarily rely on pre-trained generalized models, with their performance often depending on the variety and relevance of the pre-training data, which can raise privacy concerns. Instead of collecting diverse pre-training data, we introduce SeqFusion in this work, a novel framework that collects and fuses diverse pre-trained models (PTMs) sequentially for zero-shot forecasting. Based on the specific temporal characteristics of the target time series, SeqFusion selects the most suitable PTMs from a batch of pre-collected PTMs, performs sequential predictions, and fuses all the predictions while using minimal data to protect privacy. Each of these PTMs specializes in different temporal patterns and forecasting tasks, allowing SeqFusion to select by measuring distances in a shared representation space of the target time series with each PTM. Experiments demonstrate that SeqFusion achieves competitive accuracy in zero-shot forecasting compared to state-of-the-art methods.|\n", "2503.02609": "|**2025-03-04**|**Lightweight Channel-wise Dynamic Fusion Model: Non-stationary Time Series Forecasting via Entropy Analysis**|Tianyu Jia, Zongxia Xie, Yanru Sun et.al.|[2503.02609](http://arxiv.org/abs/2503.02609)|null||Non-stationarity is an intrinsic property of real-world time series and plays a crucial role in time series forecasting. Previous studies primarily adopt instance normalization to attenuate the non-stationarity of original series for better predictability. However, instance normalization that directly removes the inherent non-stationarity can lead to three issues: (1) disrupting global temporal dependencies, (2) ignoring channel-specific differences, and (3) producing over-smoothed predictions. To address these issues, we theoretically demonstrate that variance can be a valid and interpretable proxy for quantifying non-stationarity of time series. Based on the analysis, we propose a novel lightweight \\textit{C}hannel-wise \\textit{D}ynamic \\textit{F}usion \\textit{M}odel (\\textit{CDFM}), which selectively and dynamically recovers intrinsic non-stationarity of the original series, while keeping the predictability of normalized series. First, we design a Dual-Predictor Module, which involves two branches: a Time Stationary Predictor for capturing stable patterns and a Time Non-stationary Predictor for modeling global dynamics patterns. Second, we propose a Fusion Weight Learner to dynamically characterize the intrinsic non-stationary information across different samples based on variance. Finally, we introduce a Channel Selector to selectively recover non-stationary information from specific channels by evaluating their non-stationarity, similarity, and distribution consistency, enabling the model to capture relevant dynamic features and avoid overfitting. Comprehensive experiments on seven time series datasets demonstrate the superiority and generalization capabilities of CDFM.|\n", "2503.01157": "|**2025-03-03**|**Unify and Anchor: A Context-Aware Transformer for Cross-Domain Time Series Forecasting**|Xiaobin Hong, Jiawen Zhang, Wenzhong Li et.al.|[2503.01157](http://arxiv.org/abs/2503.01157)|null|20 pages, 12 figures, 8 tables, conference under review|The rise of foundation models has revolutionized natural language processing and computer vision, yet their best practices to time series forecasting remains underexplored. Existing time series foundation models often adopt methodologies from these fields without addressing the unique characteristics of time series data. In this paper, we identify two key challenges in cross-domain time series forecasting: the complexity of temporal patterns and semantic misalignment. To tackle these issues, we propose the ``Unify and Anchor\" transfer paradigm, which disentangles frequency components for a unified perspective and incorporates external context as domain anchors for guided adaptation. Based on this framework, we introduce ContexTST, a Transformer-based model that employs a time series coordinator for structured representation and the Transformer blocks with a context-informed mixture-of-experts mechanism for effective cross-domain generalization. Extensive experiments demonstrate that ContexTST advances state-of-the-art forecasting performance while achieving strong zero-shot transferability across diverse domains.|\n"}, "Numerical Analysis": {"2503.10612": "|**2025-03-13**|**Approximation technique for preserving the minimum principle on the entropy for the compressible Euler Equations**|Bennett Clayton, Eric J. Tovar et.al.|[2503.10612](http://arxiv.org/abs/2503.10612)|null||This paper is concerned with constructing an invariant-domain preserving approximation technique for the compressible Euler equations that preserves the minimum principle on the physical entropy. We show that any numerical method that can be written as a convex combination of good auxiliary states will satisfy the minimum principle on the physical entropy provided the equation of state satisfies some mild assumptions. Furthermore, we derive a wave speed estimate in an extended Riemann problem necessary for constructing the auxiliary states with desired properties. Finally, we numerically illustrate the proposed methodology.|\n", "2503.10607": "|**2025-03-13**|**Utilizing discrete variable representations for decoherence-accurate numerical simulation of superconducting circuits**|Brittany Richman, C. J. Lobb, Jacob M. Taylor et.al.|[2503.10607](http://arxiv.org/abs/2503.10607)|null|26 pages, 14 figures|Given the prevalence of superconducting platforms for uses in quantum computing and quantum sensing, the simulation of quantum superconducting circuits has become increasingly important for identifying system characteristics and modeling their relevant dynamics. Various numerical tools and software packages have been developed with this purpose in mind, typically utilizing the harmonic oscillator basis or the charge basis to represent a Hamiltonian. In this work, we instead consider the use of discrete variable representations (DVRs) to model superconducting circuits. In particular, we use `sinc DVRs' of both charge number and phase to approximate the eigenenergies of several prototypical examples, exploring their use and effectiveness in the numerical analysis of superconducting circuits. We find that not only are these DVRs capable of achieving decoherence-accurate simulation, i.e., accuracy at the resolution of experiments subject to decay, decoherence, and dephasing, they also demonstrate improvements in efficiency with smaller basis sizes and better convergence over standard approaches, showing that DVRs are an advantageous alternative for representing superconducting circuits.|\n", "2503.10562": "|**2025-03-13**|**Discontinuous Galerkin discretization of conservative dynamical low-rank approximation schemes for the Vlasov-Poisson equation**|Andr\u00e9 Uschmajew, Andreas Zeiser et.al.|[2503.10562](http://arxiv.org/abs/2503.10562)|null||A numerical dynamical low-rank approximation (DLRA) scheme for the solution of the Vlasov-Poisson equation is presented. Based on the formulation of the DLRA equations as Friedrichs' systems in a continuous setting, it combines recently proposed conservative DLRA methods with a discontinuous Galerkin discretization. The resulting scheme is shown to ensure mass and momentum conservation at the discrete level. In addition, a new formulation of the conservative integrator is proposed which facilitates a projector splitting integrator. Numerical experiments validate our approach in one- and two-dimensional simulations of Landau damping. As a demonstration of feasibility, it is also shown that the rank-adaptive unconventional integrator can be combined with mesh adaptivity.|\n", "2503.10552": "|**2025-03-13**|**Mathematical and numerical methods for understanding immune cell motion during wound healing**|Giulia Lupi, Seol Ah Park, Martin Ambroz et.al.|[2503.10552](http://arxiv.org/abs/2503.10552)|null||In this paper, we propose a new workflow to analyze macrophage motion during wound healing. These immune cells are attracted to the wound after an injury and they move showing both directional and random motion. Thus, first, we smooth the trajectories and we separate the random from the directional parts of the motion. The smoothing model is based on curve evolution where the curve motion is influenced by the smoothing term and the attracting term. Once we obtain the random sub-trajectories, we analyze them using the mean squared displacement to characterize the type of diffusion. Finally, we compute the velocities on the smoothed trajectories and use them as sparse samples to reconstruct the wound attractant field. To do that, we consider a minimization problem for the vector components and lengths, which leads to solving the Laplace equation with Dirichlet conditions for the sparse samples and zero Neumann boundary conditions on the domain boundary.|\n", "2503.10492": "|**2025-03-13**|**Meta-learning characteristics and dynamics of quantum systems**|Lucas Schorling, Pranav Vaidhyanathan, Jonas Schuff et.al.|[2503.10492](http://arxiv.org/abs/2503.10492)|null|6+1 pages, 4 figures. L. Schorling and P. Vaidhyanathan contributed   equally to this work|While machine learning holds great promise for quantum technologies, most current methods focus on predicting or controlling a specific quantum system. Meta-learning approaches, however, can adapt to new systems for which little data is available, by leveraging knowledge obtained from previous data associated with similar systems. In this paper, we meta-learn dynamics and characteristics of closed and open two-level systems, as well as the Heisenberg model. Based on experimental data of a Loss-DiVincenzo spin-qubit hosted in a Ge/Si core/shell nanowire for different gate voltage configurations, we predict qubit characteristics i.e. $g$-factor and Rabi frequency using meta-learning. The algorithm we introduce improves upon previous state-of-the-art meta-learning methods for physics-based systems by introducing novel techniques such as adaptive learning rates and a global optimizer for improved robustness and increased computational efficiency. We benchmark our method against other meta-learning methods, a vanilla transformer, and a multilayer perceptron, and demonstrate improved performance.|\n", "2503.10487": "|**2025-03-13**|**Sediment Concentration Estimation via Multiscale Inverse Problem and Stochastic Homogenization**|Jiwei Li, Lingyun Qiu, Zhongjing Wang et.al.|[2503.10487](http://arxiv.org/abs/2503.10487)|null|20 pages, 7 figures, submitted to Archive for Rational Mechanics and   Analysis|In this work, we contribute to the broader understanding of inverse problems by introducing a versatile multiscale modeling framework tailored to the challenges of sediment concentration estimation. Specifically, we propose a novel approach for sediment concentration measurement in water flow, modeled as a multiscale inverse medium problem. To address the multiscale nature of the sediment distribution, we treat it as an inhomogeneous random field and use the homogenization theory in deriving the effective medium model. The inverse problem is formulated as the reconstruction of the effective medium model, specifically, the sediment concentration, from partial boundary measurements. Additionally, we develop numerical algorithms to improve the efficiency and accuracy of solving this inverse problem. Our numerical experiments demonstrate the effectiveness of the proposed model and methods in producing accurate sediment concentration estimates, offering new insights into sediment concentration measurement in complex environments.|\n", "2503.10478": "|**2025-03-13**|**Multiscale simulation of interacting turbulent and rarefied gas flows in the DSMC framework**|Liyan Luo, Songyan Tian, Lei Wu et.al.|[2503.10478](http://arxiv.org/abs/2503.10478)|null||A multiscale stochastic-deterministic coupling method is proposed to investigate the complex interactions between turbulent and rarefied gas flows within a unified framework. This method intermittently integrates the general synthetic iterative scheme with the shear stress transport turbulence model into the direct simulation Monte Carlo (DSMC) approach, enabling the simulation of gas flows across the free-molecular, transition, slip, and turbulent regimes. First, the macroscopic synthetic equations, derived directly from DSMC, are coupled with the turbulence model to establish a constitutive relation that incorporates not only turbulent and laminar transport coefficients but also higher-order terms accounting for rarefaction effects. Second, the macroscopic properties, statistically sampled over specific time intervals in DSMC, along with the turbulent properties provided by the turbulence model, serve as initial conditions for solving the macroscopic synthetic equations. Finally, the simulation particles in DSMC are updated based on the macroscopic properties obtained from the synthetic equations. Numerical simulations demonstrate that the proposed method asymptotically converges to either the turbulence model or DSMC results, adaptively adjusting to different flow regimes. Then, this coupling method is applied to simulate an opposing jet surrounded by hypersonic rarefied gas flows, revealing significant variations in surface properties due to the interplay of turbulent and rarefied effects. This study presents an efficient methodology for simulating the complex interplay between rarefied and turbulent flows, establishing a foundational framework for investigating the coupled effects of turbulence, hypersonic conditions, and chemical reactions in rarefied gas dynamics in the future.|\n", "2503.10453": "|**2025-03-13**|**Proceedings of the WAVES 2024 Conference**|Laurent Gizon et.al.|[2503.10453](http://arxiv.org/abs/2503.10453)|null||Proceedings of 16th International Conference on Mathematical and Numerical Aspects of Wave Propagation held at the Harnack House, Berlin, Germany, 30 June - 5 July, 2024.|\n", "2503.10402": "|**2025-03-13**|**Efficient and stable derivative-free Steffensen algorithm for root finding**|Alexandre Wagemakers, Vipul Periwal et.al.|[2503.10402](http://arxiv.org/abs/2503.10402)|null||We explore a family of numerical methods, based on the Steffensen divided difference iterative algorithm, that do not evaluate the derivative of the objective functions. The family of methods achieves second-order convergence with two function evaluations per iteration with marginal additional computational cost. An important side benefit of the method is the improvement in stability for different initial conditions compared to the vanilla Steffensen method. We present numerical results for scalar functions, fields, and scalar fields. This family of methods outperforms the Steffensen method with respect to standard quantitative metrics in most cases.|\n", "2503.10314": "|**2025-03-13**|**A rotation-based geometrically nonlinear spectral Reissner--Mindlin shell element**|Nima Azizi, Wolfgang Dornisch et.al.|[2503.10314](http://arxiv.org/abs/2503.10314)|null||In this paper, we propose a geometrically nonlinear spectral shell element based on Reissner--Mindlin kinematics using a rotation-based formulation with additive update of the discrete nodal rotation vector. The formulation is provided in matrix notation in detail. The use of a director vector, as opposed to multi-parameter shell models, significantly reduces the computational cost by minimizing the number of degrees of freedom. Additionally, we highlight the advantages of the spectral element method (SEM) in combination with Gauss-Lobatto-Legendre quadrature regarding the computational costs to generate the element stiffness matrix. To assess the performance of the new formulation for large deformation analysis, we compare it to three other numerical methods. One of these methods is a non-isoparametric SEM shell using the geometry definition of isogeometric analysis (IGA), while the other two are IGA shell formulations which differ in the rotation interpolation. All formulations base on Rodrigues' rotation tensor. Through the solution of various challenging numerical examples, it is demonstrated that although IGA benefits from an exact geometric representation, its influence on solution accuracy is less significant than that of shape function characteristics and rotational formulations. Furthermore, we show that the proposed SEM shell, despite its simpler rotational formulation, can produce results comparable to the most accurate and complex version of IGA. Finally, we discuss the optimal SEM strategy, emphasizing the effectiveness of employing coarser meshes with higher-order elements.|\n", "2503.10279": "|**2025-03-13**|**Numerically robust Gaussian state estimation with singular observation noise**|Nicholas Kr\u00e4mer, Filip Tronarp et.al.|[2503.10279](http://arxiv.org/abs/2503.10279)|null||This article proposes numerically robust algorithms for Gaussian state estimation with singular observation noise. Our approach combines a series of basis changes with Bayes' rule, transforming the singular estimation problem into a nonsingular one with reduced state dimension. In addition to ensuring low runtime and numerical stability, our proposal facilitates marginal-likelihood computations and Gauss-Markov representations of the posterior process. We analyse the proposed method's computational savings and numerical robustness and validate our findings in a series of simulations.|\n", "2503.10263": "|**2025-03-13**|**KARL -- A Monte Carlo model for atomic and molecular processes in the tritium atmosphere of the KATRIN experiment**|Christian Sendlinger, Jonas Kellerer, Felix Spanier et.al.|[2503.10263](http://arxiv.org/abs/2503.10263)|null|accepted for publication in Computer Physics Communications, 60   pages, 28 figures|A new parallelized simulation code is presented, which uses a Monte Carlo method to determine particle spectra in the KATRIN source. Reaction chains are generated from the decay of tritium within the source. The code includes all relevant processes: elastic scattering, ionization, excitation (electric, vibrational, rotational), recombination and various clustering processes. The main emphasis of the code is the calculation of particle spectra and particle densities and currents at specific points within the source. It features a new technique to determine these quantities. It also calculates target fields for the interaction of particles with each other as it is needed for recombination processes. The code has been designed for the KATRIN experiment but is easily adapt-able for other tritium based experiments like Project 8. Geometry and background tritium gas flow can be given as user input. The code is parallelized using MPI and writes output using HDF5. Input to the simulation is read from a JSON description.|\n", "2503.10251": "|**2025-03-13**|**Numerical Error Analysis of Large Language Models**|Stanislav Budzinskiy, Wenyi Fang, Longbin Zeng et.al.|[2503.10251](http://arxiv.org/abs/2503.10251)|null||Large language models based on transformer architectures have become integral to state-of-the-art natural language processing applications. However, their training remains computationally expensive and exhibits instabilities, some of which are expected to be caused by finite-precision computations. We provide a theoretical analysis of the impact of round-off errors within the forward pass of a transformer architecture which yields fundamental bounds for these effects. In addition, we conduct a series of numerical experiments which demonstrate the practical relevance of our bounds. Our results yield concrete guidelines for choosing hyperparameters that mitigate round-off errors, leading to more robust and stable inference.|\n", "2503.10244": "|**2025-03-13**|**Thermal Management of Lithium-Ion Batteries: A Comparative Study of Phase Change Materials and Air-Cooling Systems Equipped with Fins**|Masoumeh Karimi Kisomi et.al.|[2503.10244](http://arxiv.org/abs/2503.10244)|null||Lithium-ion batteries are extensively utilized as the primary power source for electric vehicles due to their high energy density, environmental friendliness and lightweight nature. However, their performance and safety are highly dependent on operating temperature. Therefore, a battery thermal management system (BTMS) is essential to ensure the reliable operation and safety of electric vehicles. This study presents a battery thermal management system incorporating phase change material (PCM) and air cooling in a cylindrical lithium-ion cell with fins to enhance heat dissipation. The effects of each system on maximum and minimum temperature, and temperature uniformity along the battery cell are analyzed. Additionally, the impact of fins in both systems is evaluated against a finless cell. A numerical analysis utilizing ANSYS software and the finite volume method (FVM) is performed to evaluate the cooling performance of the systems. The results show that PCM reduces both the maximum and minimum temperatures compared to the air cooling system due to the phase change mechanism. In the finless battery case, the maximum temperature decreases from 316 K to 304 K when using PCM instead of the air cooling system. Also, in the same fin-based battery, the minimum temperature decreases from 307 K to 302 K by using PCM instead of the air cooling system, leading to improved temperature stability. The results indicate that, in general, the fins help reduce the maximum cell temperature when compared to the case without fins in both cases. Using rectangular fins reduces the maximum temperature by approximately 3% compared to a finless battery in the air cooling system. Additionally, the presence of fins reduces the temperature difference along the battery, ensuring a more uniform temperature distribution, such that, in the PCM system with rectangular fins, the temperature difference remains below 1 K.|\n", "2503.10221": "|**2025-03-13**|**New More Efficient A-WENO Schemes**|Shaoshuai Chu, Alexander Kurganov, Ruixiao Xin et.al.|[2503.10221](http://arxiv.org/abs/2503.10221)|null||We develop new more efficient A-WENO schemes for both hyperbolic systems of conservation laws and nonconservative hyperbolic systems. The new schemes are a very simple modification of the existing A-WENO schemes: They are obtained by a more efficient evaluation of the high-order correction terms. We conduct several numerical experiments to demonstrate the performance of the introduced schemes.|\n", "2503.10199": "|**2025-03-13**|**Optimal Estimation and Uncertainty Quantification for Stochastic Inverse Problems via Variational Bayesian Methods**|Ruibiao Song, Liying Zhang et.al.|[2503.10199](http://arxiv.org/abs/2503.10199)|null||The Bayesian inversion method demonstrates significant potential for solving inverse problems, enabling both point estimation and uncertainty quantification. However, Bayesian maximum a posteriori (MAP) estimation may become unstable when handling data from diverse distributions (e.g., solutions of stochastic partial differential equations (SPDEs)). Additionally, Monte Carlo sampling methods are computationally expensive. To address these challenges, we propose a novel two-stage optimization method based on optimal control theory and variational Bayesian methods. This method not only achieves stable solutions for stochastic inverse problems but also efficiently quantifies the uncertainty of the solutions. In the first stage, we introduce a new weighting formulation to ensure the stability of the Bayesian MAP estimation. In the second stage, we derive the necessary condition to efficiently quantify the uncertainty of the solutions, by combining the new weighting formula with variational inference. Furthermore, we establish an error estimation theorem that relates the exact solution to the optimally estimated solution under different amounts of observed data. Finally, the efficiency of the proposed method is demonstrated through numerical examples.|\n", "2503.10196": "|**2025-03-13**|**A filtered Lie splitting method for the Zakharov system with low regularity estimates**|Lun Ji, Hang Li, Chunmei Su et.al.|[2503.10196](http://arxiv.org/abs/2503.10196)|null||In this paper, we present an error estimate for the filtered Lie splitting scheme applied to the Zakharov system, characterized by solutions exhibiting very low regularity across all dimensions. Our findings are derived from the application of multilinear estimates established within the framework of discrete Bourgain spaces. Specifically, we demonstrate that when the solution $(E,z,z_t) \\in H^{s+r+1/2}\\times H^{s+r}\\times H^{s+r-1}$, the error in $H^{r+1/2}\\times H^{r}\\times H^{r-1}$ is $\\mathcal{O}(\\tau^{s/2})$ for $s\\in(0,2]$, where $r=\\max(0,\\frac d2-1)$. To the best of our knowledge, this represents the first explicit error estimate for the splitting method based on the original Zakharov system, as well as the first instance where low regularity error estimates for coupled equations have been considered within the Bourgain framework. Furthermore, numerical experiments confirm the validity of our theoretical results.|\n", "2503.10194": "|**2025-03-13**|**Surrogate modeling of resonant behavior in scattering problems through adaptive rational approximation and sketching**|Davide Pradovera, Ralf Hiptmair, Ilaria Perugia et.al.|[2503.10194](http://arxiv.org/abs/2503.10194)|null||This paper describes novel algorithms for the identification of (almost-)resonant behavior in scattering problems. Our methods, relying on rational approximation, aim at building surrogate models of what we call \"field amplification\", defined as the norm of the solution operator of the scattering problem, which we express through boundary-integral equations. To provide our techniques with theoretical foundations, we first derive results linking the field amplification to the spectral properties of the operator that defines the scattering problem. Such results are then used to justify the use of rational approximation in the surrogate-modeling task. Some of our proposed methods apply rational approximation in a \"standard\" way, building a rational approximant for either the solution operator directly or, in the interest of computational efficiency, for a randomly \"sketched\" version of it. Our other \"hybrid\" approaches are more innovative, combining rational-approximation-assisted root-finding with approximation using radial basis functions. Three key features of our methods are that (i) they are agnostic of the strategy used to discretize the scattering problem, (ii) they do not require any computations involving non-real wavenumbers, and (iii) they can adjust to different settings through the use of adaptive sampling strategies. We carry out some numerical experiments involving 2D scatterers to compare our approaches. In our tests, two of our approaches (one standard, one hybrid) emerge as the best performers, with one or the other being preferable, depending on whether emphasis is placed on accuracy or efficiency.|\n", "2503.10179": "|**2025-03-13**|**Highly efficient norm preserving numerical schemes for micromagnetic energy minimization based on SAV method**|Jiajun Zhan, Lei Yang, Jiayun He et.al.|[2503.10179](http://arxiv.org/abs/2503.10179)|null||In this paper, two efficient and magnetization norm preserving numerical schemes based on the scalar auxiliary variable (SAV) method are developed for calculating the ground state in micromagnetic structures. The first SAV scheme is based on the original SAV method for the gradient flow model, while the second scheme features an updated scalar auxiliary variable to better align with the associated energy. To address the challenging constraint of pointwise constant magnetization length, an implicit projection method is designed, and verified by both SAV schemes. Both proposed SAV schemes partially preserve energy dissipation and exhibit exceptional efficiency, requiring two linear systems with constant coefficients to be solved. The computational efficiency is further enhanced by applying the Discrete Cosine Transform during the solving process. Numerical experiments demonstrate that our SAV schemes outperform commonly used numerical methods in terms of both efficiency and stability.|\n", "2503.10172": "|**2025-03-13**|**On convergence of greedy block nonlinear Kaczmarz methods with momentum**|Naiyu Jiang, Wendi Bao, Lili Xing et.al.|[2503.10172](http://arxiv.org/abs/2503.10172)|null||In this paper, for solving nonlinear systems we propose two pseudoinverse-free greedy block methods with momentum by combining the residual-based weighted nonlinear Kaczmarz and heavy ball methods. Without the full column rank assumptions on Jacobi matrices of nonlinear systems, we provide a thorough convergence analysis, and derive upper bounds for the convergence rates of the new methods. Numerical experiments demonstrate that the proposed methods with momentum are much more effective than the existing ones.|\n", "2503.13388": "|**2025-03-17**|**A mathematical model for a universal digital quantum computer with an application to the Grover-Rudolph algorithm**|Antonio Falc\u00f3, Daniela Falc\u00f3--Pomares, Hermann G. Matthies et.al.|[2503.13388](http://arxiv.org/abs/2503.13388)|null||In this work, we develop a novel mathematical framework for universal digital quantum computation using algebraic probability theory. We rigorously define quantum circuits as finite sequences of elementary quantum gates and establish their role in implementing unitary transformations. A key result demonstrates that every unitary matrix in \\(\\mathrm{U}(N)\\) can be expressed as a product of elementary quantum gates, leading to the concept of a universal dictionary for quantum computation. We apply this framework to the construction of quantum circuits that encode probability distributions, focusing on the Grover-Rudolph algorithm. By leveraging controlled quantum gates and rotation matrices, we design a quantum circuit that approximates a given probability density function. Numerical simulations, conducted using Qiskit, confirm the theoretical predictions and validate the effectiveness of our approach. These results provide a rigorous foundation for quantum circuit synthesis within an algebraic probability framework and offer new insights into the encoding of probability distributions in quantum algorithms. Potential applications include quantum machine learning, circuit optimization, and experimental implementations on real quantum hardware.|\n", "2503.13364": "|**2025-03-17**|**Demonstration of a Tunable Non-Hermitian Nonlinear Microwave Dimer**|Juan S. Salcedo-Gallo, Michiel Burgelman, Vincent P. Flynn et.al.|[2503.13364](http://arxiv.org/abs/2503.13364)|null|10 pages, 4 figures, 73 references|Achieving and controlling non-reciprocity in engineered photonic structures is of fundamental interest in science and engineering. Here, we introduce a tunable, non-Hermitian, nonlinear microwave dimer designed to precisely implement phase-non-reciprocal hopping dynamics between two spatially separated cavities at room temperature. Our system incorporates simple components such as three-dimensional microwave cavities, unidirectional amplifiers, digital attenuators, and a digital phase shifter. By dividing the energy transfer into forward and backward paths, our platform enables precise control over the amplitude and phase of the propagating signals in each direction. Through a combination of theoretical and numerical analysis, we model the dynamics of the system under different operating conditions, including a parameter regime where the gain not only compensates for but significantly exceeds the inherent loss. Our model quantitatively reproduces the observed weak-drive transmission spectra, the amplitude and frequency of self-sustained limit cycles, and the synchronization effect between the limit cycle and an external microwave tone. Our results may have implications in areas ranging from sensing and synthetic photonic materials to neuromorphic computing and quantum networks, while providing new insight into the interplay between non-Hermitian and nonlinear dynamics.|\n", "2503.13354": "|**2025-03-17**|**Parameter-free structure-texture image decomposition by unrolling**|Laura Girometti, Jean-Fran\u00e7ois Aujol, Antoine Guennec et.al.|[2503.13354](http://arxiv.org/abs/2503.13354)|null|To be published in Conference Proceedings: Scale Space and   Variational Method in Computer Vision, 2025|In this work, we propose a parameter-free and efficient method to tackle the structure-texture image decomposition problem. In particular, we present a neural network LPR-NET based on the unrolling of the Low Patch Rank model. On the one hand, this allows us to automatically learn parameters from data, and on the other hand to be computationally faster while obtaining qualitatively similar results compared to traditional iterative model-based methods. Moreover, despite being trained on synthetic images, numerical experiments show the ability of our network to generalize well when applied to natural images.|\n", "2503.13311": "|**2025-03-17**|**Numerical Hopf-Lax formulae for Hamilton-Jacobi equations on unstructured geometries**|Simone Cacace, Roberto Ferretti, Giulia Tatafiore et.al.|[2503.13311](http://arxiv.org/abs/2503.13311)|null||We consider a scheme of Semi-Lagrangian (SL) type for the numerical solution of Hamilton-Jacobi (HJ) equation on unstructured triangular grids. As it is well known, SL schemes are not well suited for unstructured grids, due to the cost of the point location phase; this drawback is augmented by the need for repeated minimization. In this work, we propose a scheme that works only on the basis of node values and connectivity of the grid. In a first version, we obtain a monotone scheme; then, applying a quadratic refinement to the numerical solution, we improve accuracy at the price of some extra computational cost. The scheme can be applied to both time-dependent and stationary HJ equations; in the latter case, we also study the construction of a fast policy iteration solver. We perform a theoretical analysis of the two versions, and validate them with an extensive set of examples, both in the time-dependent and in the stationary case.|\n", "2503.13298": "|**2025-03-17**|**From Few-Shot Optimal Control to Few-Shot Learning**|Roman Chertovskih, Nikolay Pogodaev, Maxim Staritsyn et.al.|[2503.13298](http://arxiv.org/abs/2503.13298)|null|6 pages|We present an approach to solving unconstrained nonlinear optimal control problems for a broad class of dynamical systems. This approach involves lifting the nonlinear problem to a linear ``super-problem'' on a dual Banach space, followed by a non-standard ``exact'' variational analysis, -- culminating in a descent method that achieves rapid convergence with minimal iterations. We investigate the applicability of this framework to mean-field control and discuss its perspectives for the analysis of information propagation in self-interacting neural networks.|\n", "2503.13284": "|**2025-03-17**|**Bayesian identification of material parameters in viscoelastic structures as an inverse problem in a semigroup setting**|Rebecca Rothermel, Thomas Schuster et.al.|[2503.13284](http://arxiv.org/abs/2503.13284)|null|32 pages, 7 figures, 17 tables|The article considers the nonlinear inverse problem of identifying the material parameters in viscoelastic structures based on a generalized Maxwell model. The aim is to reconstruct the model parameters from stress data acquired from a relaxation experiment, where the number of Maxwell elements, and thus the number of material parameters themselves, are assumed to be unknown. This implies that the forward operator acts on a Cartesian product of a semigroup (of integers) and a Hilbert space and demands for an extension of existing regularization theory. We develop a stable reconstruction procedure by applying Bayesian inversion to this setting. We use an appropriate binomial prior which takes the integer setting for the number of Maxwell elements into account and at the same time computes the underlying material parameters. We extend the regularization theory for inverse problems to this special setup and prove existence, stability and convergence of the computed solution. The theoretical results are evaluated by extensive numerical tests.|\n", "2503.13248": "|**2025-03-17**|**Neural network-based Godunov corrections for approximate Riemann solvers using bi-fidelity learning**|Akshay Thakur, Matthew J. Zahr et.al.|[2503.13248](http://arxiv.org/abs/2503.13248)|null|22 pages, 16 figures|The Riemann problem is fundamental in the computational modeling of hyperbolic partial differential equations, enabling the development of stable and accurate upwind schemes. While exact solvers provide robust upwinding fluxes, their high computational cost necessitates approximate solvers. Although approximate solvers achieve accuracy in many scenarios, they produce inaccurate solutions in certain cases. To overcome this limitation, we propose constructing neural network-based surrogate models, trained using supervised learning, designed to map interior and exterior conservative state variables to the corresponding exact flux. Specifically, we propose two distinct approaches: one utilizing a vanilla neural network and the other employing a bi-fidelity neural network. The performance of the proposed approaches is demonstrated through applications to one-dimensional and two-dimensional partial differential equations, showcasing their robustness and accuracy.|\n", "2503.13235": "|**2025-03-17**|**Emergent B2 chemical orderings in the AlTiVNb and AlTiCrMo refractory high-entropy superalloys studied via first-principles theory and atomistic modelling**|Christopher D. Woodgate, Hubert J. Naguszewski, David Redka et.al.|[2503.13235](http://arxiv.org/abs/2503.13235)|null|18 pages, 9 figures|We study the thermodynamics and phase stability of the AlTiVNb and AlTiCrMo refractory high-entropy superalloys using a combination of \\textit{ab initio} electronic structure theory -- namely a concentration wave analysis -- and atomistic Monte Carlo simulations. Our multiscale approach is suitable both for examining atomic short-range order in the solid solution, as well as for studying the emergence of long-range crystallographic order with decreasing temperature. In both alloys considered in this work, in alignment with experimental observations, we predict a B2 (CsCl) chemical ordering emerging at high temperatures, which is driven primarily by Al and Ti, with other elements expressing weaker site preferences. The predicted B2 ordering temperature for AlTiVNb is higher than that for AlTiCrMo. These chemical orderings are discussed in terms of the alloys' electronic structure, with hybridisation between the $sp$ states of Al and the $d$ states of the transition metals understood to play an important role. Within our modelling, the chemically ordered B2 phases for both alloys have an increased predicted residual resistivity compared to the A2 (disordered bcc) phases. These increased resistivity values are understood to originate in a reduction in the electronic density of states at the Fermi level, in conjunction with qualitative changes to the alloys' smeared-out Fermi surfaces. These results highlight the close connections between composition, structure, and physical properties in this technologically relevant class of materials.|\n", "2503.13193": "|**2025-03-17**|**The deep multi-FBSDE method: a robust deep learning method for coupled FBSDEs**|Kristoffer Andersson, Adam Andersson, Cornelis W. Oosterlee et.al.|[2503.13193](http://arxiv.org/abs/2503.13193)|null|18 pages, 11 figures|We introduce the deep multi-FBSDE method for robust approximation of coupled forward-backward stochastic differential equations (FBSDEs), focusing on cases where the deep BSDE method of Han, Jentzen, and E (2018) fails to converge. To overcome the convergence issues, we consider a family of FBSDEs that are equivalent to the original problem in the sense that they satisfy the same associated partial differential equation (PDE). Our algorithm proceeds in two phases: first, we approximate the initial condition for the FBSDE family, and second, we approximate the original FBSDE using the initial condition approximated in the first phase. Numerical experiments show that our method converges even when the standard deep BSDE method does not.|\n", "2503.13170": "|**2025-03-17**|**A~posteriori error analysis for optimization with PDE constraints**|Fernando Gaspoz, Christian Kreuzer, Andreas Veeser et.al.|[2503.13170](http://arxiv.org/abs/2503.13170)|null||We consider finite element solutions to optimization problems, where the state depends on the possibly constrained control through a linear partial differential equation. Basing upon a reduced and rescaled optimality system, we derive a posteriori bounds capturing the approximation of the state, the adjoint state, the control and the observation. The upper and lower bounds show a gap, which grows with decreasing cost or Tikhonov regularization parameter. This growth is mitigated compared to previous results and can be countered by refinement if control and observation involve compact operators. Numerical results illustrate these properties for model problems with distributed and boundary control.|\n", "2503.13126": "|**2025-03-17**|**Error analysis of the Strang splitting for the 3D semilinear wave equation with finite-energy data**|Maximilian Ruff et.al.|[2503.13126](http://arxiv.org/abs/2503.13126)|null|39 pages|We study a variant of the Strang splitting for the time integration of the semilinear wave equation under the finite-energy condition on the torus $\\mathbb{T}^3$. In the case of a cubic nonlinearity, we show almost second-order convergence in $L^2$ and almost first-order convergence in $H^1$. If the nonlinearity has a quartic form instead, we show an analogous convergence result with an order reduced by 1/2. To our knowledge these are the best convergence results available for the 3D cubic and quartic wave equations under the finite-energy condition. Our approach relies on continuous- and discrete-time Strichartz estimates. We also make use of the integration and summation by parts formulas to exploit cancellations in the error terms. Moreover, error bounds for a full discretization using the Fourier pseudo-spectral method in space are given. Finally, we discuss a numerical example indicating the sharpness of our theoretical results.|\n", "2503.13094": "|**2025-03-17**|**Preserving invariant domains and strong approximation of stochastic differential equations**|Utku Erdogan, Gabriel Lord et.al.|[2503.13094](http://arxiv.org/abs/2503.13094)|null||In this paper, we develop numerical methods for solving Stochastic Differential Equations (SDEs) with solutions that evolve within a hypercube $D$ in $\\mathbb{R}^d$. Our approach is based on a convex combination of two numerical flows, both of which are constructed from positivity preserving methods. The strong convergence of the Euler version of the method is proven to be of order $\\tfrac{1}{2}$, and numerical examples are provided to demonstrate that, in some cases, first-order convergence is observed in practice. We compare the Euler and Milstein versions of these new methods to existing domain preservation methods in the literature and observe our methods are robust, more widely applicable and that the error constant is in most cases superior.|\n", "2503.13093": "|**2025-03-17**|**Localized Dynamic Mode Decomposition with Temporally Adaptive Partitioning**|Qiuqi Li, Chang Liu, Yifei Yang et.al.|[2503.13093](http://arxiv.org/abs/2503.13093)|null|24 pages, 15 figures, 3 tables|Dynamic Mode Decomposition (DMD) is a widely used data-driven algorithm for predicting the future states of dynamical systems. However, its standard formulation often struggles with poor long-term predictive accuracy. To address this limitation, we propose a localized DMD framework that improves prediction performance by integrating DMD's strong short-term forecasting capabilities with time-domain decomposition techniques. Our approach segments the time domain of the dynamical system, independently constructing snapshot matrices and performing localized predictions within each segment. We first introduce a localized DMD method with predefined partitioning, which is simple to implement, and then extend it to an adaptive partitioning strategy that enhances prediction accuracy, robustness, and generalizability. Furthermore, we conduct an error analysis that provides the upper bound of the local and global truncation error for our method. To demonstrate the effectiveness of our approach, we apply it to four benchmark problems: Burgers' equation, the Allen-Cahn equation, the nonlinear Schrodinger equation, and Maxwell's equations. Numerical results show that our method significantly improves both predictive accuracy and computational efficiency.|\n", "2503.13032": "|**2025-03-17**|**Miniaturization-Oriented Design of Spline-Parameterized UWB Antenna for In-Door Positioning Applications**|Adrian Bekasiewicz, Tom Dhaene, Ivo Couckuyt et.al.|[2503.13032](http://arxiv.org/abs/2503.13032)|null||Design of ultra-wideband antennas for in-door localization applications is a challenging task that involves development of geometry that ensures appropriate balance between the size and performance. In this work, a topologically-flexible monopole has been generated using a stratified framework which embeds a gradient-based trust-region (TR) optimization algorithm in a meta-loop that gradually increases structure dimensionality. The optimization has been performed using a composite objective function that maintains acceptable size/performance trade-off. The final design features a reflection below -10 dB within the UWB spectrum and a small footprint of only 182 mm2. The considered method has been benchmarked against a standard TR-based routine executed directly on a multi-dimensional representation of the antenna model.|\n", "2503.13029": "|**2025-03-17**|**Specification-Oriented Automatic Design of Topologically Agnostic Antenna Structure**|Adrian Bekasiewicz, Mariusz Dzwonkowski, Tom Dhaene et.al.|[2503.13029](http://arxiv.org/abs/2503.13029)|null||Design of antennas for modern applications is a challenging task that combines cognition-driven development of topology intertwined with tuning of its parameters using rigorous numerical optimization. However, the process can be streamlined by neglecting the engineering insight in favor of automatic de-termination of structure geometry. In this work, a specification-oriented design of topologically agnostic antenna is considered. The radiator is developed using a bi-stage algorithm that involves min-max classification of randomly-generated topologies followed by local tuning of the promising designs using a trust-region optimization applied to a feature-based representation of the structure frequency response. The automatically generated antenna is characterized by -10 dB bandwidth of over 600 MHz w.r.t. the center frequency of 6.5 GHz and a dual-lobe radiation pattern. The obtained performance figures make the radiator of use for in-door positioning applications. The design method has been favorably compared against the frequency-based trust-region optimization.|\n", "2503.13027": "|**2025-03-17**|**Giant energy density nitride dielectrics enabled by a paraelectric-metaparaelectric phase transition**|Zhijie Liu, Xingyue Ma, Lan Chen et.al.|[2503.13027](http://arxiv.org/abs/2503.13027)|null|18 pages, 5 figures|Electrostatic dielectric capacitors are foundational to advance the electronics and electric power devices due to their ultrafast charging/discharging capability and high-power density. However, the low energy density limits the potential for next generation devices in terms of miniaturization and integration. We propose a strategy that relies on inducing a field-driven phase transition that we denote paraelectric-metaparaelectric, which yields an ultrahigh energy density in III-nitrides. III-nitride compounds (Al, Sc, B)N with certain cation concentrations possess a nonpolar hexagonal ground phase which could transform into a polar wurtzite phase under a very large electric field, which is denoted as metaparaelectric with nearly null hysteresis P-E loop. This paraelectric-metaparaelectric transition leads to a polarization saturation at large electric field. The corresponding P-E loop displays a giant energy density of 308 J/cm$^3$ with high efficiency nearly 100%. The proposed paraelectric-metaparaelectric phase transition strategy in nitrides opens an avenue to design of next generation high performance dielectrics.|\n", "2503.13019": "|**2025-03-17**|**TR-Based Antenna Design with Forward FD: the Effects of Step Size on the Optimization Performance**|Adrian Bekasiewicz, Slawomir Koziel, Tom Dhaene et.al.|[2503.13019](http://arxiv.org/abs/2503.13019)|null||Numerical methods are important tools for design of modern antennas. Trust-region (TR) methods coupled with data-efficient surrogates based on finite differentiation (FD) represent a popular class of antenna design algorithms. However, TR performance is subject to FD setup, which is normally determined a priori based on rules-of-thumb. In this work, the effect of FD perturbations on the performance of TR-based design is evaluated on a case study basis concerning a total of 80 optimizations of a planar antenna structure. The obtained results demonstrate that, for the considered radiator, the performance of the final designs obtained using different FD setups may vary by as much as 18 dB (and by over 4 dB on average). At the same time, the a priori perturbations in a range between 1.5% and 3% (w.r.t. the initial design) seem to be suitable for maintaining (relatively) consistent and high-quality results.|\n", "2503.13015": "|**2025-03-17**|**High-performance and reliable probabilistic Ising machine based on simulated quantum annealing**|Eleonora Raimondo, Esteban Garz\u00f3n, Yixin Shao et.al.|[2503.13015](http://arxiv.org/abs/2503.13015)|null||Probabilistic computing with pbits is emerging as a computational paradigm for machine learning and for facing combinatorial optimization problems (COPs) with the so-called probabilistic Ising machines (PIMs). From a hardware point of view, the key elements that characterize a PIM are the random number generation, the nonlinearity, the network of coupled pbits, and the energy minimization algorithm. Regarding the latter, in this work we show that PIMs using the simulated quantum annealing (SQA) schedule exhibit better performance as compared to simulated annealing and parallel tempering in solving a number of COPs, such as maximum satisfiability problems, planted Ising problem, and travelling salesman problem. Additionally, we design and simulate the architecture of a fully connected CMOS based PIM able to run the SQA algorithm having a spin-update time of 8 ns with a power consumption of 0.22 mW. Our results also show that SQA increases the reliability and the scalability of PIMs by compensating for device variability at an algorithmic level enabling the development of their implementation combining CMOS with different technologies such as spintronics. This work shows that the characteristics of the SQA are hardware agnostic and can be applied in the co-design of any hybrid analog digital Ising machine implementation. Our results open a promising direction for the implementation of a new generation of reliable and scalable PIMs.|\n", "2503.13007": "|**2025-03-17**|**On the characteristic structure of the adjoint Euler equations with application to supersonic flows**|Carlos Lozano, Jorge Ponsin et.al.|[2503.13007](http://arxiv.org/abs/2503.13007)|null|23 pages|We review the characteristic structure of the two-dimensional adjoint Euler equations. We derive the compatibility and jump conditions along characteristics and show that the characteristic information can be used to obtain exact predictions for the adjoint variables in certain supersonic flows.|\n", "2503.12949": "|**2025-03-17**|**Tunable topological protection in Rydberg lattices via a novel quantum Monte Carlo approach**|Pranay Patil, Owen Benton et.al.|[2503.12949](http://arxiv.org/abs/2503.12949)|null|15 pages, 17 figures|Rydberg atom arrays have recently been conjectured to host $Z_2$ quantum spin liquids (QSLs) in certain parameter regimes. Due to the strong interactions between these atoms, it is not possible to analytically study these systems, and one must resort to Monte Carlo sampling of the path integral to reach definite conclusions. We use a tailored update, specifically designed to target the low energy excitations of the QSL. This allows us to reliably simulate Rydberg atoms on a triangular lattice in the proposed QSL regime. We identify a correlated paramagnetic phase at low temperatures which hosts topological protection similar to a $Z_2$ spin liquid up to a length scale tuned by Hamiltonian parameters. However, this correlated paramagnet seems to be continuously connected to the trivial paramagnetic regime and thus does not seem to be a true QSL. This result indicates the feasibility of Rydberg atom arrays to act as topological qubits.|\n", "2503.15460": "|**2025-03-19**|**pyTTN: An Open Source Toolbox for Open and Closed System Quantum Dynamics Simulations Using Tree Tensor Networks**|Lachlan P Lindoy, Daniel Rodrigo-Albert, Yannic Rath et.al.|[2503.15460](http://arxiv.org/abs/2503.15460)|**[link](https://gitlab.npl.co.uk/qsm/pyttn/)**|31 pages, 25 figures (main manuscript); 7 pages, 3 figures   (supplemental information)|We present the Python Tree Tensor Network package (pyTTN) for the evaluation of dynamical properties of closed and open quantum systems that makes use of Tree Tensor Network (TTN), or equivalently the multi-layer multiconfiguration time-dependent Hartree (ML-MCTDH), based representations of wavefunctions. This package includes several features allowing for easy setup of zero- and finite-temperature calculations for general Hamiltonians using single and multi-set TTN ans\\\"atze with an adaptive bond dimension through the use of subspace expansion techniques. All core features are implemented in C++ with Python bindings provided to simplify the use of this package. In addition to these core features, pyTTN provides several tools for setting up efficient simulation of open quantum system dynamics, including the use of the TTN ansatz to represent the auxiliary density operator space for the simulation of the Hierarchical Equation of Motion (HEOM) method and generalised pseudomode methods; furthermore we demonstrate that the two approaches are equivalent up to a non-unitary normal mode transformation acting on the pseudomode degrees of freedom. We present a set of applications of the package, starting with the widely used benchmark case of the photo-excitation dynamics of 24 mode pyrazine, following which we consider a more challenging model describing the exciton dynamics at the interface of a $n$-oligothiophene donor-C$_{60}$ fullerene acceptor system. Finally, we consider applications to open quantum systems, including the spin-boson model, a set of extended dissipative spin models, and an Anderson impurity model. By combining ease of use, an efficient implementation, as well as an extendable design allowing for the addition of future extensions, pyTTN can be integrated in a wide range of computational modelling software.|\n", "2503.15441": "|**2025-03-19**|**A discontinuity-capturing neural network with categorical embedding and its application to anisotropic elliptic interface problems**|Wei-Fan Hu, Te-Sheng Lin, Ming-Chih Lai et.al.|[2503.15441](http://arxiv.org/abs/2503.15441)|null||In this paper, we propose a discontinuity-capturing shallow neural network with categorical embedding to represent piecewise smooth functions. The network comprises three hidden layers, a discontinuity-capturing layer, a categorical embedding layer, and a fully-connected layer. Under such a design, we show that a piecewise smooth function, even with a large number of pieces, can be approximated by a single neural network with high prediction accuracy. We then leverage the proposed network model to solve anisotropic elliptic interface problems. The network is trained by minimizing the mean squared error loss of the system. Our results show that, despite its simple and shallow structure, the proposed neural network model exhibits comparable efficiency and accuracy to traditional grid-based numerical methods.|\n", "2503.15397": "|**2025-03-19**|**A High Order IMEX Method for Generalized Korteweg de-Vries Equations**|Seth Gerberding et.al.|[2503.15397](http://arxiv.org/abs/2503.15397)|null||In this paper, we introduce a high order space-time approximation of generalized Korteweg de-Vries equations. More specifically, the method uses continuous $H^1$-conforming finite elements for the spatial approximation and implicit-explicit methods for the temporal approximation. The method is high order in both space, provably stable, and mass-conservative. The scheme is formulated, its properties are proven, and numerical simulations are provided to illustrate the proposed methodology.|\n", "2503.15391": "|**2025-03-19**|**Modeling crystal defects using defect-informed neural networks**|Ziduo Yang, Xiaoqing Liu, Xiuying Zhang et.al.|[2503.15391](http://arxiv.org/abs/2503.15391)|null||Machine learning has revolutionized the study of crystalline materials for enabling rapid predictions and discovery. However, most AI-for-Materials research to date has focused on ideal crystals, whereas real-world materials inevitably contain defects that play a critical role in modern functional technologies. The defects and dopants break geometric symmetry and increase interaction complexity, posing particular challenges for traditional ML models. Addressing these challenges requires models that are able to capture sparse defect-driven effects in crystals while maintaining adaptability and precision. Here, we introduce Defect-Informed Equivariant Graph Neural Network (DefiNet), a model specifically designed to accurately capture defect-related interactions and geometric configurations in point-defect structures. Trained on 14,866 defect structures, DefiNet achieves highly accurate structural predictions in a single step, avoiding the time-consuming iterative processes in modern ML relaxation models and possible error accumulation from iteration. We further validates DefiNet's accuracy by using density functional theory (DFT) relaxation on DefiNet-predicted structures. For most defect structures, regardless of defect complexity or system size, only 3 ionic steps are required to reach the DFT-level ground state. Finally, comparisons with scanning transmission electron microscopy (STEM) images confirm DefiNet's scalability and extrapolation beyond point defects, positioning it as a groundbreaking tool for defect-focused materials research.|\n", "2503.15278": "|**2025-03-20**|**Symplectic integration of guiding-center equations in canonical coordinates for general toroidal fields**|Christopher G. Albert, Georg S. Grassler, Sergei V. Kasilov et.al.|[2503.15278](http://arxiv.org/abs/2503.15278)|null||Symplectic integrators with long-term preservation of integrals of motion are introduced for the guiding-center model of plasma particles in toroidal magnetic fields of general topology. An efficient transformation to canonical coordinates from cylindrical and flux-like coordinates is discussed and applied using one component of the magnetic vector potential as a spatial coordinate. This choice is efficient in both, theoretical and numerical developments and marks a generalization of magnetic flux coordinates. The transformation enables the application of conventional symplectic integration schemes formulated in canonical coordinates, as well as variational integrators on the guiding-center system, without requiring magnetic flux coordinates. Symplectic properties and superior efficiency of the implicit midpoint scheme compared to conventional non-symplectic methods are demonstrated on perturbed tokamak fields with magnetic islands and stochastic regions. The presented results mark a crucial step towards gyrokinetic models that conserve physical invariants.|\n", "2503.15258": "|**2025-03-19**|**A Lie algebra view of matrix splittings**|Michele Benzi, Milo Viviani et.al.|[2503.15258](http://arxiv.org/abs/2503.15258)|null|26 pages|In this paper we use some basic facts from the theory of (matrix) Lie groups and algebras to show that many of the classical matrix splittings used to construct stationary iterative methods and preconditioniers for Krylov subspace methods can be interpreted as linearizations of matrix factorizations. Moreover, we show that new matrix splittings are obtained when we specialize these splittings to some of the classical matrix groups and their Lie and Jordan algebras. As an example, we derive structured generalizations of the HSS (Hermitian/skew-Hermitian) iteration, and provide sufficient conditions for their convergence.|\n", "2503.15188": "|**2025-03-19**|**Convergence analysis of SPH method on irregular particle distributions for the Poisson equation**|Zhonghua Qiao, Yifan Wei et.al.|[2503.15188](http://arxiv.org/abs/2503.15188)|null||The accuracy of particle approximation in Smoothed Particle Hydrodynamics (SPH) method decreases due to irregular particle distributions, especially for second-order derivatives. This study aims to enhance the accuracy of SPH method and analyze its convergence with irregular particle distributions. By establishing regularity conditions for particle distributions, we ensure that the local truncation error of traditional SPH formulations, including first and second derivatives, achieves second-order accuracy. Our proposed method, the volume reconstruction SPH method, guarantees these regularity conditions while preserving the discrete maximum principle. Benefiting from the discrete maximum principle, we conduct a rigorous global error analysis in the $L^\\infty$-norm for the Poisson equation with variable coefficients, achieving second-order convergence. Numerical examples are presented to validate the theoretical findings.|\n", "2503.15187": "|**2025-03-19**|**Thermal Enskog-Vlasov Lattice Boltzmann model with phase separation**|Sergiu Busuioc, Victor Sofonea et.al.|[2503.15187](http://arxiv.org/abs/2503.15187)|null|29 pages, 9 figures|An Enskog-Vlasov finite-difference Lattice Boltzmann (EV-FDLB) for liquid-vapor systems with variable temperature is introduced. The model involves both the simplified Enskog collision operator and the self-consistent force field which accounts for the long-range interaction between the fluid particles. Full-range Gauss-Hermite quadratures were used for the discretization of the momentum space. The numerical solutions of the Enskog-Vlasov equation obtained employing the EV-FDLB model and the Direct Simulation Monte Carlo (DSMC)-like particle method (PM) are compared. Reasonable agreement is found between the two approaches when simulating the liquid-vapor phase separation and the liquid slab evaporation.|\n", "2503.15149": "|**2025-03-19**|**Machine learning surrogate models of many-body dispersion interactions in polymer melts**|Zhaoxiang Shen, Ra\u00fal I. Sosa, Jakub Lengiewicz et.al.|[2503.15149](http://arxiv.org/abs/2503.15149)|null||Accurate prediction of many-body dispersion (MBD) interactions is essential for understanding the van der Waals forces that govern the behavior of many complex molecular systems. However, the high computational cost of MBD calculations limits their direct application in large-scale simulations. In this work, we introduce a machine learning surrogate model specifically designed to predict MBD forces in polymer melts, a system that demands accurate MBD description and offers structural advantages for machine learning approaches. Our model is based on a trimmed SchNet architecture that selectively retains the most relevant atomic connections and incorporates trainable radial basis functions for geometric encoding. We validate our surrogate model on datasets from polyethylene, polypropylene, and polyvinyl chloride melts, demonstrating high predictive accuracy and robust generalization across diverse polymer systems. In addition, the model captures key physical features, such as the characteristic decay behavior of MBD interactions, providing valuable insights for optimizing cutoff strategies. Characterized by high computational efficiency, our surrogate model enables practical incorporation of MBD effects into large-scale molecular simulations.|\n", "2503.15125": "|**2025-03-19**|**A Spectral Approach to Optimal Control of the Fokker-Planck Equation**|Dante Kalise, Lucas M. Moschen, Grigorios A. Pavliotis et.al.|[2503.15125](http://arxiv.org/abs/2503.15125)|null|6 pages, 4 figures|In this paper, we present a spectral optimal control framework for Fokker-Planck equations based on the standard ground state transformation that maps the Fokker-Planck operator to a Schrodinger operator. Our primary objective is to accelerate convergence toward the (unique) steady state. To fulfill this objective, a gradient-based iterative algorithm with Pontryagin's maximum principle and Barzilai-Borwein update is developed to compute time-dependent controls. Numerical experiments on two-dimensional ill-conditioned normal distributions and double-well potentials demonstrate that our approach effectively targets slow-decaying modes, thus increasing the spectral gap.|\n", "2503.15121": "|**2025-03-19**|**Analytic adjoint solution for incompressible potential flows**|Carlos Lozano, Jorge Ponsin et.al.|[2503.15121](http://arxiv.org/abs/2503.15121)|null|20 pages|We obtain the analytic adjoint solution for two-dimensional (2D) incompressible potential flow for a cost function measuring aerodynamic force using the connection of the adjoint approach to Green's functions and also by establishing and exploiting its relation to the adjoint incompressible Euler equations. By comparison with the analytic solution, it is shown that the naive approach based on solving Laplace's equation for the adjoint variables can be ill-defined. The analysis of the boundary behavior of the analytic solution is used to discuss the proper formulation of the adjoint problem as well as the mechanism for incorporating the Kutta condition in the adjoint formulation|\n", "2503.15105": "|**2025-03-19**|**Control, Optimal Transport and Neural Differential Equations in Supervised Learning**|Minh-Nhat Phung, Minh-Binh Tran et.al.|[2503.15105](http://arxiv.org/abs/2503.15105)|null||From the perspective of control theory, neural differential equations (neural ODEs) have become an important tool for supervised learning. In the fundamental work of Ruiz-Balet and Zuazua (SIAM REVIEW 2023), the authors pose an open problem regarding the connection between control theory, optimal transport theory, and neural differential equations. More precisely, they inquire how one can quantify the closeness of the optimal flows in neural transport equations to the true dynamic optimal transport. In this work, we propose a construction of neural differential equations that converge to the true dynamic optimal transport in the limit, providing a significant step in solving the formerly mentioned open problem.|\n", "2503.15080": "|**2025-03-19**|**Exponentially Tilted Thermodynamic Maps (expTM): Predicting Phase Transitions Across Temperature, Pressure, and Chemical Potential**|Suemin Lee, Ruiyu Wang, Lukas Herron et.al.|[2503.15080](http://arxiv.org/abs/2503.15080)|null||Predicting and characterizing phase transitions is crucial for understanding generic physical phenomena such as crystallization, protein folding and others. However, directly observing phase transitions is not always easy, and often one has limited observations far from the phase boundary and measured under some specific thermodynamic conditions. In this study, we propose a statistical physics and Generative AI driven framework that can take such limited information to generate samples of different phases under arbitrary thermodynamic conditions, which we name Exponentially Tilted Thermodynamic Maps (expTM). The central idea is to map collected data into a tractable simple prior expressed as an exponentially tilted Gaussian. We demonstrate how the variance and mean of the prior can be correlated with pairs of thermodynamic control variables, including temperature, pressure, and chemical potential. This gives us the ability to generate thermodynamically correct samples under any values of the control variables. To demonstrate the practical applicability of this approach, we use expTM to sample the lattice gas models with the Grand Canonical ensemble, capturing phase transitions under varying chemical potentials and temperatures. We further demonstrate how expTM can model the isothermal-isobaric ensemble, with which we predict different phases of CO2 under varying pressure conditions. Both examples are trained on very limited data far from the phase boundary. These results establish expTM as a robust tool for understanding phase transitions across diverse thermodynamic conditions requiring only a small number of observations.|\n", "2503.15077": "|**2025-03-19**|**Efficient forward and inverse uncertainty quantification for dynamical systems based on dimension reduction and Kriging surrogate modeling in functional space**|Zhouzhou Song, Weiyun Xu, Marcos A. Valdebenito et.al.|[2503.15077](http://arxiv.org/abs/2503.15077)|null||Surrogate models are extensively employed for forward and inverse uncertainty quantification in complex, computation-intensive engineering problems. Nonetheless, constructing high-accuracy surrogate models for complex dynamical systems with limited training samples continues to be a challenge, as capturing the variability in high-dimensional dynamical system responses with a small training set is inherently difficult. This study introduces an efficient Kriging modeling framework based on functional dimension reduction (KFDR) for conducting forward and inverse uncertainty quantification in dynamical systems. By treating the responses of dynamical systems as functions of time, the proposed KFDR method first projects these responses onto a functional space spanned by a set of predefined basis functions, which can deal with noisy data by adding a roughness regularization term. A few key latent functions are then identified by solving the functional eigenequation, mapping the time-variant responses into a low-dimensional latent functional space. Subsequently, Kriging surrogate models with noise terms are constructed in the latent space. With an inverse mapping established from the latent space to the original output space, the proposed approach enables accurate and efficient predictions for dynamical systems. Finally, the surrogate model derived from KFDR is directly utilized for efficient forward and inverse uncertainty quantification of the dynamical system. Through three numerical examples, the proposed method demonstrates its ability to construct highly accurate surrogate models and perform uncertainty quantification for dynamical systems accurately and efficiently.|\n", "2503.15051": "|**2025-03-19**|**Ab initio study of pressure-induced phase transition, band gaps and X-ray photoemission valence band spectra of YVO$_4$**|M. Werwi\u0144ski, J. Kaczkowski, P. Le\u015bniak et.al.|[2503.15051](http://arxiv.org/abs/2503.15051)|null||High-pressure induced structural transition from zircon-type phase into scheelite-type phase in YVO$_4$ is studied using ab initio calculations. Several structures with compressed volumes are evaluated, where for every considered volume the c/a ratio and atomic positions are optimised. The transition pressure and transition volume change are calculated. The reports on YVO$_4$ electronic structure and electronic band gap behaviour are followed by results of X-Ray photoemission spectra XPS calculations. Most of our theoretical predictions are compared with experimental results taken from literature.|\n", "2503.14978": "|**2025-03-19**|**Inferring diffusivity from killed diffusion**|Richard Nickl, Fanny Seizilles et.al.|[2503.14978](http://arxiv.org/abs/2503.14978)|null|30 pages, 4 figures|We consider diffusion of independent molecules in an insulated Euclidean domain with unknown diffusivity parameter. At a random time and position, the molecules may bind and stop diffusing in dependence of a given `binding potential'. The binding process can be modeled by an additive random functional corresponding to the canonical construction of a `killed' diffusion Markov process. We study the problem of conducting inference on the infinite-dimensional diffusion parameter from a histogram plot of the `killing' positions of the process. We show first that these positions follow a Poisson point process whose intensity measure is determined by the solution of a certain Schr\\\"odinger equation. The inference problem can then be re-cast as a non-linear inverse problem for this PDE, which we show to be consistently solvable in a Bayesian way under natural conditions on the initial state of the diffusion, provided the binding potential is not too `aggressive'. In the course of our proofs we obtain novel posterior contraction rate results for high-dimensional Poisson count data that are of independent interest. A numerical illustration of the algorithm by standard MCMC methods is also provided.|\n", "2503.14972": "|**2025-03-19**|**Effect of Gd and Co content on electrochemical and electronic properties of La$_{1.5}$Mg$_{0.5}$Ni$_7$ alloys: a combined experimental and first-principles study**|Miros\u0142aw Werwi\u0144ski, Andrzej Szajek, Agnieszka Marczy\u0144ska et.al.|[2503.14972](http://arxiv.org/abs/2503.14972)|null||In this work, we investigate the effect of Gd and Co substitutions on the electrochemical and electronic properties of La$_{1.5}$Mg$_{0.5}$Ni$_7$ alloy. Two series of La$_{1.5-x}$Gd$_x$Mg$_{0.5}$Ni$_7$ ($x$ = 0.0, 0.25, 1.0) and La$_{1.5}$Mg$_{0.5}$Ni$_{7-y}$Co$_y$ ($y$ = 0.0, 0.5, 1.5) alloys are produced using mechanical alloying technique. The X-ray diffraction indicates multiphase character of the samples, with the majority of hexagonal Ce$_2$Ni$_7$-type and rhombohedral Gd$_2$Co$_7$-type structures of (La,Mg)$_2$Ni$_7$ phase. Partial substitutions of La by Gd or Ni by Co in La$_{1.5}$Mg$_{0.5}$Ni$_7$ phase result in increase of cycle stability of the metal hydride (MH$_x$) electrodes. All considered alloys reach the maximum discharge capacity after three charging-discharging cycles. Two optimal compositions, in respect of electrochemical properties, are subsequently investigated by the X-ray photoelectron spectroscopy (XPS). The experimental analysis of the valence band is further extended by the density functional theory (DFT) calculations. We also discuss the effects of alloying and site preference of dopants on the position of the van Hove-type singularity, as observed in the electronic densities of states (DOS) in proximity of the Fermi level.|\n", "2503.14952": "|**2025-03-19**|**Effect of substitution La by Mg on electrochemical and electronic properties in La$_{2-x}$Mg$_x$Ni$_7$ alloys: a combined experimental and ab initio studies**|Miros\u0142aw Werwi\u0144ski, Andrzej Szajek, Agnieszka Marczy\u0144ska et.al.|[2503.14952](http://arxiv.org/abs/2503.14952)|null||La-Mg-Ni-based alloys are promising negative electrode materials for 3rd generation of Ni-MH$_x$ batteries. In this work, we investigate the effect of Mg substitution on the electrochemical and electronic properties of La$_{2-x}$Mg$_x$Ni$_7$ materials. The mechanical alloying technique is used to produce a series of La$_{2-x}$Mg$_x$Ni$_7$ alloys ($x$ = 0.00, 0.25, 0.50 and 0.75). The X-ray diffraction measurements indicate multiphase character of the samples with majority (La,Mg)$_2$Ni$_7$ phases of hexagonal Ce$_2$Ni$_7$-type and rhombohedral Gd$_2$Co$_7$-type. Electrochemical measurements show how the maximum discharge capacity ($C_{max}$) increases with Mg concentration and that reach the highest value of 304 mAh/g for La$_{1.5}$Mg$_{0.5}$Ni$_7$ ($x$ = 0.5). The experimental efforts are followed by the density functional theory (DFT) calculations performed with the full-potential local-orbital minimum-basis scheme (FPLO). To simulate chemical disorder, we use the coherent potential approximation (CPA). The calculations are focused on the La$_{1.5}$Mg$_{0.5}$Ni$_7$ composition with the highest measured value of $C_{max}$. Additionally, several other structures are considered as reference points. We find that hexagonal and rhombohedral structures of La$_2$Ni$_7$ have almost identical total energies, which is in a good agreement with a coexistence of both phases in the samples. The calculated site preferences of Mg in both Ce$_2$Ni$_7$-type and Gd$_2$Co$_7$-type La$_{1.5}$Mg$_{0.5}$Ni$_7$ phases are consistent with the previous experimental data. Furthermore, the valence band of the nanocrystalline La$_{1.5}$Mg$_{0.5}$Ni$_7$ sample is investigated by X-ray photoelectron spectroscopy (XPS). The experimental XPS are interpreted based on the corresponding spectra calculated with DFT.|\n", "2503.14947": "|**2025-03-19**|**Image Restoration Models with Optimal Transport and Total Variation Regularization**|Weijia Huang, Zhongyi Huang, Wenli Yang et.al.|[2503.14947](http://arxiv.org/abs/2503.14947)|null||In this paper, we propose image restoration models using optimal transport (OT) and total variation regularization. We present theoretical results of the proposed models based on the relations between the dual Lipschitz norm from OT and the G-norm introduced by Yves Meyer. We design a numerical method based on the Primal-Dual Hybrid Gradient (PDHG) algorithm for the Wasserstain distance and the augmented Lagrangian method (ALM) for the total variation, and the convergence analysis of the proposed numerical method is established. We also consider replacing the total variation in our model by one of its modifications developed in \\cite{zhu}, with the aim of suppressing the stair-casing effect and preserving image contrasts. Numerical experiments demonstrate the features of the proposed models.|\n", "2503.14913": "|**2025-03-19**|**A PINN-enriched finite element method for linear elliptic problems**|Xiao Chen, Yixin Luo, Jingrun Chen et.al.|[2503.14913](http://arxiv.org/abs/2503.14913)|null||In this paper, we propose a hybrid method that combines finite element method (FEM) and physics-informed neural network (PINN) for solving linear elliptic problems. This method contains three steps: (1) train a PINN and obtain an approximate solution $u_{\\theta}$; (2) enrich the finite element space with $u_{\\theta}$; (3) obtain the final solution by FEM in the enriched space. In the second step, the enriched space is constructed by addition $v + u_{\\theta}$ or multiplication $v \\cdot u_{\\theta}$, where $v$ belongs to the standard finite element space. We conduct the convergence analysis for the proposed method. Compared to the standard FEM, the same convergence order is obtained and higher accuracy can be achieved when solution derivatives are well approximated in PINN. Numerical examples from one dimension to three dimensions verify these theoretical results. For some examples, the accuracy of the proposed method can be reduced by a couple of orders of magnitude compared to the standard FEM.|\n", "2503.16388": "|**2025-03-20**|**A Mixed-FEM approximation with uniform conservation of the exponential stability for a class of anisotropic port-Hamiltonian system and its application to LQ control**|Luis A. Mora, Kirsten Morris et.al.|[2503.16388](http://arxiv.org/abs/2503.16388)|null||In this manuscript, we present a mixed finite element discretization for a class of boundary-damped anisotropic port-Hamiltonian systems. Using a multiplier method, we demonstrate that the resulting approximation model uniformly preserves the exponential stability of the uncontrolled system, establishing a lower bound for the exponential decay rate that is independent of the mesh size. This property is illustrated through the spatial discretization of a piezoelectric beam. Furthermore, we show how the uniform preservation of exponential stability by the proposed model aids in the convergence of controllers derived from an infinite-time linear quadratic control design, in comparison to models obtained from the standard finite-element method.|\n", "2503.16312": "|**2025-03-20**|**Near-Linear Runtime for a Classical Matrix Preconditioning Algorithm**|Xufeng Cai, Jason M. Altschuler, Jelena Diakonikolas et.al.|[2503.16312](http://arxiv.org/abs/2503.16312)|null||In 1960, Osborne proposed a simple iterative algorithm for matrix balancing with outstanding numerical performance. Today, it is the default preconditioning procedure before eigenvalue computation and other linear algebra subroutines in mainstream software packages such as Python, Julia, MATLAB, EISPACK, LAPACK, and more. Despite its widespread usage, Osborne's algorithm has long resisted theoretical guarantees for its runtime: the first polynomial-time guarantees were obtained only in the past decade, and recent near-linear runtimes remain confined to variants of Osborne's algorithm with important differences that make them simpler to analyze but empirically slower. In this paper, we address this longstanding gap between theory and practice by proving that Osborne's original algorithm -- the de facto preconditioner in practice -- in fact has a near-linear runtime. This runtime guarantee (1) is optimal in the input size up to at most a single logarithm, (2) is the first runtime for Osborne's algorithm that does not dominate the runtime of downstream tasks like eigenvalue computation, and (3) improves upon the theoretical runtimes for all other variants of Osborne's algorithm.|\n", "2503.16240": "|**2025-03-20**|**Machine learning identifies nullclines in oscillatory dynamical systems**|Bartosz Prokop, Jimmy Billen, Nikita Frolov et.al.|[2503.16240](http://arxiv.org/abs/2503.16240)|null|7 pages, 4 figures|We introduce CLINE (Computational Learning and Identification of Nullclines), a neural network-based method that uncovers the hidden structure of nullclines from oscillatory time series data. Unlike traditional approaches aiming at direct prediction of system dynamics, CLINE identifies static geometric features of the phase space that encode the (non)linear relationships between state variables. It overcomes challenges such as multiple time scales and strong nonlinearities while producing interpretable results convertible into symbolic differential equations. We validate CLINE on various oscillatory systems, showcasing its effectiveness.|\n", "2503.16225": "|**2025-03-20**|**Energy-Adaptive Riemannian Conjugate Gradient Method for Density Functional Theory**|Daniel Peterseim, Jonas P\u00fcschel, Tatjana Stykel et.al.|[2503.16225](http://arxiv.org/abs/2503.16225)|null||This paper presents a novel Riemannian conjugate gradient method for the Kohn-Sham energy minimization problem in density functional theory (DFT), with a focus on non-metallic crystal systems. We introduce an energy-adaptive metric that preconditions the Kohn-Sham model, significantly enhancing optimization efficiency. Additionally, a carefully designed shift strategy and several algorithmic improvements make the implementation comparable in performance to highly optimized self-consistent field iterations. The energy-adaptive Riemannian conjugate gradient method has a sound mathematical foundation, including stability and convergence, offering a reliable and efficient alternative for DFT-based electronic structure calculations in computational chemistry.|\n", "2503.16222": "|**2025-03-20**|**Efficient Bayesian Computation Using Plug-and-Play Priors for Poisson Inverse Problems**|Teresa Klatzer, Savvas Melidonis, Marcelo Pereyra et.al.|[2503.16222](http://arxiv.org/abs/2503.16222)|null|31 pages, 17 figures|This paper introduces a novel plug-and-play (PnP) Langevin sampling methodology for Bayesian inference in low-photon Poisson imaging problems, a challenging class of problems with significant applications in astronomy, medicine, and biology. PnP Langevin sampling algorithms offer a powerful framework for Bayesian image restoration, enabling accurate point estimation as well as advanced inference tasks, including uncertainty quantification and visualization analyses, and empirical Bayesian inference for automatic model parameter tuning. However, existing PnP Langevin algorithms are not well-suited for low-photon Poisson imaging due to high solution uncertainty and poor regularity properties, such as exploding gradients and non-negativity constraints. To address these challenges, we propose two strategies for extending Langevin PnP sampling to Poisson imaging models: (i) an accelerated PnP Langevin method that incorporates boundary reflections and a Poisson likelihood approximation and (ii) a mirror sampling algorithm that leverages a Riemannian geometry to handle the constraints and the poor regularity of the likelihood without approximations. The effectiveness of these approaches is demonstrated through extensive numerical experiments and comparisons with state-of-the-art methods.|\n", "2503.16210": "|**2025-03-20**|**On the convergence of split exponential integrators for semilinear parabolic problems**|Marco Caliari, Fabio Cassini, Lukas Einkemmer et.al.|[2503.16210](http://arxiv.org/abs/2503.16210)|null||Splitting the exponential-like $\\varphi$ functions, which typically appear in exponential integrators, is attractive in many situations since it can dramatically reduce the computational cost of the procedure. However, depending on the employed splitting, this can result in order reduction. The aim of this paper is to analyze different such split approximations. We perform the analysis for semilinear problems in the abstract framework of commuting semigroups and derive error bounds that depend, in particular, on whether the vector (to which the $\\varphi$ functions are applied) satisfies appropriate boundary conditions. We then present the convergence analysis for two split versions of a second-order exponential Runge--Kutta integrator in the context of analytic semigroups, and show that one suffers from order reduction while the other does not. Numerical results for semidiscretized parabolic PDEs confirm the theoretical findings.|\n", "2503.16209": "|**2025-03-20**|**Instance optimal function recovery -- samples, decoders and asymptotic performance**|Moritz Moeller, Kateryna Pozharska, Tino Ullrich et.al.|[2503.16209](http://arxiv.org/abs/2503.16209)|null|2 Figures|In this paper we study non-linear sampling recovery of multivariate functions using techniques from compressed sensing. In the first part of the paper we prove that square root Lasso $({\\tt rLasso})$ with a particular choice of the regularization parameter $\\lambda>0$ as well as orthogonal matching pursuit $({\\tt OMP})$ after sufficiently many iterations provide noise blind decoders which efficiently recover multivariate functions from random samples. In contrast to basis pursuit the decoders $({\\tt rLasso})$ and $({\\tt OMP})$ do not require any additional information on the width of the function class in $L_\\infty$ and lead to instance optimal recovery guarantees. In the second part of the paper we relate the findings to linear recovery methods such as least squares $({\\tt Lsqr})$ or Smolyak's algorithm $({\\tt Smolyak})$ and compare the performance in a model situation, namely periodic multivariate functions with $L_p$-bounded mixed derivative will be approximated in $L_q$. The main observation is the fact, that $({\\tt rLasso})$ and $({\\tt OMP})$ outperform Smolyak's algorithm (sparse grids) in various situations, where $1<p<2\\leq q<\\infty$. For $q=2$ they even outperform any linear method including $({\\tt Lsqr})$ in combination with recently proposed subsampled random points.|\n", "2503.16202": "|**2025-03-20**|**3D Stochastic Geometry Model for Aerial Vehicle-Relayed Ground-Air-Satellite Connectivity**|Yulei Wang, Yalin Liu, Yaru Fu et.al.|[2503.16202](http://arxiv.org/abs/2503.16202)|null||Due to their flexibility, aerial vehicles (AVs), such as unmanned aerial vehicles and airships, are widely employed as relays to assist communications between massive ground users (GUs) and satellites, forming an AV-relayed ground-air-satellite solution (GASS). In GASS, the deployment of AVs is crucial to ensure overall performance from GUs to satellites. This paper develops a stochastic geometry-based analytical model for GASS under Matern hard-core point process (MHCPP) distributed AVs. The 3D distributions of AVs and GUs are modeled by considering their locations on spherical surfaces in the presence of high-altitude satellites. Accordingly, we derive an overall connectivity analytical model for GASS, which includes the average performance of AV-relayed two-hop transmissions. Extensive numerical results validate the accuracy of the connectivity model and provide essential insights for configuring AV deployments.|\n", "2503.16196": "|**2025-03-20**|**An interior penalty DG method with correct and minimal averages, jumps and penalties for the miscible displacement problem of nonnegative characteristic form, and SUPG-type error estimates under low regularity, dominating Darcy velocity**|Zhijie Du, Huoyuan Duan, Roger C E Tan et.al.|[2503.16196](http://arxiv.org/abs/2503.16196)|null||An interior penalty DG method is proposed for the steady-state linear partial differential equations of nonnegative characteristic form, suitable for mixed second-order elliptic-parabolic and first-order hyperbolic equations. Due to the different natures of the elliptic, parabolic, and hyperbolic equations. In the new DG method, the averages, jumps and penalties are minimal, correctly and only imposed on the diffusion-diffusion element boundaries, in addition to the well-known upwind jumps associating with the advection velocity. For the advection-dominated problem, the penalties can be further reduced only being imposed on the diffusion-dominated subset of the diffusion-diffusion element boundaries.This is based on the novel, crucial technique about the multiple partitions of the set of the interelement boundaries into a number of subsets with respect to the diffusion and to the advection and on the consistency result we have proven. The new DG method is the first DG method and the first time that the continuity and discontinuity of the solution are correctly identified and justified of the general steady-state linear partial differential equations of nonnegative characteristic form. The new DG method and its analysis are applied to the miscible displacement problem of vanishing diffusion coefficient and of low regularity, dominating Darcy flow velocity which lives in $H(\\operatorname{div};\\Omega)\\cap \\prod_{j=1}^J (H^r(D_j))^d$ for $r<1$ other than the usual assumption $(W^{1,\\infty}(\\Omega))^d$. We prove the SUPG-type error estimates $\\mathcal{O}(h^{\\ell+\\frac{1}{2}})$ for any element polynomial of degree $\\ell\\ge 1$ on generally shaped and nonconforming meshes, where the convergence order is independent of the regularity of the advection velocity. The SUPG-type error estimates obtained are new and the first time known under the low regularity of the advection velocity.|\n", "2503.16176": "|**2025-03-20**|**Nonnegative Biquadratic Tensors**|Chunfeng Cui, Liqun Qi et.al.|[2503.16176](http://arxiv.org/abs/2503.16176)|null||An M-eigenvalue of a nonnegative biquadratic tensor is referred to as an M$^+$-eigenvalue if it has a pair of nonnegative M-eigenvectors. If furthermore that pair of M-eigenvectors is positive, then that M$^+$-eigenvalue is called an M$^{++}$-eigenvalue. A nonnegative biquadratic tensor always has at least one M$^+$ eigenvalue, and the largest M$^+$-eigenvalue is also the largest M-eigenvalue and the M-spectral radius. In the case of an irreducible nonnegative biquadratic tensor, all the M$^+$-eigenvalues are M$^{++}$-eigenvalues. Although the M$^+$-eigenvalues of irreducible nonnegative biquadratic tensors are not unique in general, we establish a sufficient condition to ensure their uniqueness. For an irreducible nonnegative biquadratic tensor, the largest M$^+$-eigenvalue has a max-min characterization, while the smallest M$^+$-eigenvalue has a min-max characterization. A Collatz algorithm for computing the largest M$^+$-eigenvalues is proposed. Numerical results are reported.|\n", "2503.16119": "|**2025-03-20**|**Transverse Nucleon Single-Spin Asymmetry for Single-Inclusive Hadron and Jet Production at NLO Accuracy**|Daniel Rein, Marc Schlegel, Patrick Tollk\u00fchn et.al.|[2503.16119](http://arxiv.org/abs/2503.16119)|null|79 pages, 27 figures|We investigate the single-spin asymmetry for the single-inclusive production of hadrons and jets in collisions of transversely polarized nucleons and unpolarized leptons, $\\ell N^\\uparrow \\to (h\\,\\mathrm{or\\,jet})X$. We compute the spin-dependent cross section within collinear twist-3 factorization in perturbative QCD at next-to-leading order (NLO) accuracy. In this approach, multiparton correlations generate a non-vanishing effect. For the present paper, we focus on correlations in the nucleon initial-state rather than in the fragmentation process. We explicitly verify that collinear twist-3 factorization is valid at the one-loop level. Our analytical results show that at NLO the relevant multiparton correlation functions in the nucleon are probed on their full support in momentum fractions. Our numerical analysis for collisions at the Electron-Ion Collider indicates that the NLO corrections can be large and are sensitive to the functional form of the twist-3 correlation functions.|\n", "2503.16110": "|**2025-03-20**|**Compact implicit high resolution numerical method for solving transport problems with sorption isotherms**|Dagmar Zakova, Peter Frolkovic et.al.|[2503.16110](http://arxiv.org/abs/2503.16110)|null||This study investigates numerical methods to solve nonlinear transport problems characterized by various sorption isotherms with a focus on the Freundlich type of isotherms. We describe and compare second order accurate numerical schemes, focusing on implicit methods, to effectively model transport phenomena without stability restriction on the choice of time steps. Furthermore, a high resolution form of the method is proposed that limits a priori the second order accurate scheme towards first order accuracy to keep the values of numerical solutions in a physically acceptable range.   Through numerical experiments, we demonstrate the effectiveness of high resolution methods in minimizing oscillations near discontinuities, thereby enhancing solution plausibility. The observed convergence rates confirm that the second order accurate schemes achieve expected accuracy for smooth solutions and that they yield significant improvements when compared with the results of the first order scheme. As the computational cost of the compact implicit method seems to be comparable to similar explicit ones with a clear profit of unconditional stability, this research provides a practical tool toward numerical simulations of nonlinear transport phenomena applicable in various fields such as contaminant transport in porous media or column liquid chromatography.|\n", "2503.16078": "|**2025-03-20**|**Magnetic skyrmions embedded in a vortex**|Haobing Zhang, Xintao Fan, Weiwei Wang et.al.|[2503.16078](http://arxiv.org/abs/2503.16078)|null||Magnetic vortices and skyrmions represent two fundamental classes of topological spin textures in ferromagnetic systems, distinguished by their unique stabilization mechanisms and degrees of freedom. Vortices, characterized by circular in-plane magnetization (chirality) and out-of-plane core polarization, naturally arise in confined geometries due to the interplay between exchange and dipolar interactions. In contrast, skyrmions typically require the Dzyaloshinskii-Moriya interaction for stabilization and exhibit fixed chirality-polarity relationships. Through micromagnetic simulations, we reveal that these seemingly distinct topological states can coexist, forming a novel composite state termed the \\textit{n}-skyrmion vortex, which represents a skyrmion-embedded vortex state. These composite states possess quantized topological charges $Q$ that follow the relation $Q_{\\text{total}} = Q_{\\text{vortex}} + nQ_{\\text{skyrmion}}$, where $n$ denotes the number of embedded skyrmions. Similar to vortices, these states exhibit independent chirality and polarity and are energetically degenerate.|\n", "2503.16062": "|**2025-03-20**|**Constraint Phase Space Formulations for Finite-State Quantum Systems: The Relation between Commutator Variables and Complex Stiefel Manifolds**|Youhao Shang, Xiangsong Cheng, Jian Liu et.al.|[2503.16062](http://arxiv.org/abs/2503.16062)|null||We have recently developed the \\textit{constraint} coordinate-momentum \\textit{phase space} (CPS) formulation for finite-state quantum systems. It has been implemented for the electronic subsystem in nonadiabatic transition dynamics to develop practical trajectory-based approaches. In the generalized CPS formulation for the mapping Hamiltonian of the classical mapping model with commutator variables (CMMcv) method [\\textit{J. Phys. Chem. A} \\textbf{2021}, 125, 6845-6863], each {connected} component of the generalized CPS is the \\textit{complex Stiefel manifold} labeled by the eigenvalue set of the mapping kernel. Such a phase space structure allows for exact trajectory-based dynamics for pure discrete (electronic) degrees of freedom (DOFs), where the equations of motion of each trajectory are isomorphic to the time-dependent Schr\\\"odinger equation. We employ covariant kernels {within the generalized CPS framework} to develop two approaches that naturally yield exact evaluation of time correlation functions (TCFs) for pure discrete (electronic) DOFs. In addition, we briefly discuss the phase space mapping formalisms where the contribution of each trajectory to the integral expression of the {TCF} of population dynamics is strictly positive semi-definite. The generalized CPS formulation also indicates that the equations of motion in phase space mapping model I of our previous work [\\textit{J. Chem. Phys.} \\textbf{2016}, 145, 204105; \\textbf{2017}, 146, 024110; \\textbf{2019}, 151, 024105] lead to a complex Stiefel manifold $\\mathrm{U}(F)/\\mathrm{U}(F-2)$. It is expected that the generalized CPS formulation has implications for simulations of both nonadiabatic transition dynamics and many-body quantum dynamics for spins/bosons/fermions.|\n", "2503.16037": "|**2025-03-20**|**Scattering graph method for 3D radiative transfer**|Antti Mikkonen, Anssi Koskinen, Johanna Tamminen et.al.|[2503.16037](http://arxiv.org/abs/2503.16037)|null||A novel method for monochromatic scalar 3D radiative transfer, designed primarily for modelling remote sensing imaging, is presented. For simulating an observation of an imaging satellite instrument, the method uses a heuristic scattering coupling function to model the inter-pixel scattering of radiation, which is represented with a graph. The GPU-capable code implementation of the method, TURSCA, was validated against two established 3D RT models, Siro and SHDOM with relative agreement at 3% and 6%, respectively. The method opens up new avenues of research, especially in satellite-based remote sensing of atmospheres.|\n", "2503.16028": "|**2025-03-20**|**Sequential Monte Carlo with Gaussian Mixture Distributions for Infinite-Dimensional Statistical Inverse Problems**|Haoyu Lu, Junxiong Jia, Deyu Meng et.al.|[2503.16028](http://arxiv.org/abs/2503.16028)|null|35 pages|By formulating the inverse problem of partial differential equations (PDEs) as a statistical inference problem, the Bayesian approach provides a general framework for quantifying uncertainties. In the inverse problem of PDEs, parameters are defined on an infinite-dimensional function space, and the PDEs induce a computationally intensive likelihood function. Additionally, sparse data tends to lead to a multi-modal posterior. These features make it difficult to apply existing sequential Monte Carlo (SMC) algorithms. To overcome these difficulties, we propose new conditions for the likelihood functions, construct a Gaussian mixture based preconditioned Crank-Nicolson transition kernel, and demonstrate the universal approximation property of the infinite-dimensional Gaussian mixture probability measure. By combining these three novel tools, we propose a new SMC algorithm, named SMC-GM. For this new algorithm, we obtain a convergence theorem that allows Gaussian priors, illustrating that the sequential particle filter actually reproduces the true posterior distribution. Furthermore, the proposed new algorithm is rigorously defined on the infinite-dimensional function space, naturally exhibiting the discretization-invariant property. Numerical experiments demonstrate that the new approach has a strong ability to probe the multi-modality of the posterior, significantly reduces the computational burden, and numerically exhibits the discretization-invariant property (important for large-scale problems).|\n", "2503.16010": "|**2025-03-20**|**Patch-based learning of adaptive Total Variation parameter maps for blind image denoising**|Claudio Fantasia, Luca Calatroni, Xavier Descombes et.al.|[2503.16010](http://arxiv.org/abs/2503.16010)|null||We consider a patch-based learning approach defined in terms of neural networks to estimate spatially adaptive regularisation parameter maps for image denoising with weighted Total Variation and test it to situations when the noise distribution is unknown. As an example, we consider situations where noise could be either Gaussian or Poisson and perform preliminary model selection by a standard binary classification network. Then, we define a patch-based approach where at each image pixel an optimal weighting between TV regularisation and the corresponding data fidelity is learned in a supervised way using reference natural image patches upon optimisation of SSIM and in a sliding window fashion. Extensive numerical results are reported for both noise models, showing significant improvement w.r.t. results obtained by means of optimal scalar regularisation.|\n", "2503.15994": "|**2025-03-20**|**A framework for efficient reduced order modelling in the Julia programming language**|Nicholas Mueller, Santiago Badia et.al.|[2503.15994](http://arxiv.org/abs/2503.15994)|null|14 pages, 6 figures|In this paper we propose ROManifolds, a Julia-based package geared towards the numerical approximation of parameterized partial differential equations (PDEs) with a rich set of linear reduced order models (ROMs). The library favors extendibility and productivity, thanks to an expressive high level API, and the efficiency attained by the Julia just-in-time compiler. The implementation of the package is PDE agnostic, meaning that the same code can be used to solve a wide range of equations, including linear, nonlinear, single-field, multi-field, steady and unsteady problems. We highlight the main innovations of ROManifolds, we detail its implementation principles, we introduce its building blocks by providing usage examples, and we solve a fluid dynamics problem described by the Navier-Stokes equations in a 3d geometry.|\n", "2503.15959": "|**2025-03-20**|**Orbital-Free Density Functional Theory for Periodic Solids: Construction of the Pauli Potential**|Sangita Majumdar, Zekun Shi, Giovanni Vignale et.al.|[2503.15959](http://arxiv.org/abs/2503.15959)|null||The practical success of density functional theory (DFT) is largely credited to the Kohn-Sham approach, which enables the exact calculation of the non-interacting electron kinetic energy via an auxiliary noninteracting system. Yet, the realization of DFT's full potential awaits the discovery of a direct link between the electron density, $n$, and the non-interacting kinetic energy, $T_{S}[n]$. In this work, we address two key challenges towards this objective. First, we introduce a new algorithm for directly solving the constrained minimization problem yielding $T_{S}[n]$ for periodic densities -- a class of densities that, in spite of its central importance for materials science, has received limited attention in the literature. Second, we present a numerical procedure that allows us to calculate the functional derivative of $T_{S}[n]$ with respect to the density at constant electron number, also known as the Kohn-Sham potential $V_{S}[n](\\rv)$. Lastly, the algorithm is augmented with a subroutine that computes the ``derivative discontinuity\", i.e., the spatially uniform jump in $V_{S}[n](\\rv)$ which occurs upon increasing or decreasing the total number of electrons. This feature allows us to distinguish between ``insulating\" and ``conducting\" densities for non interacting electrons. The code integrates key methodological innovations, such as the use of an adaptive basis set (``equidensity orbitals\") for wave function expansion and the QR decomposition to accelerate the implementation of the orthogonality constraint. Notably, we derive a closed-form expression for the Pauli potential in one dimension, expressed solely in terms of the input density, without relying on Kohn-Sham eigenvalues and eigenfunctions. We validate this method on one-dimensional periodic densities, achieving results within ``chemical accuracy\".|\n", "2503.15912": "|**2025-03-20**|**Normal and inverse magnetocaloric effects in structurally disordered Laves phase Y$_{1-x}$Gd$_{x}$Co$_{2}$ (0 $\\leq$ x $\\leq$ 1) compounds**|Natalia Pierunek, Zbigniew \u015aniadecki, Miros\u0142aw Werwi\u0144ski et.al.|[2503.15912](http://arxiv.org/abs/2503.15912)|null||Magnetic and magnetocaloric properties of Y$_{1-x}$Gd$_{x}$Co$_{2}$ compounds, where x = 0.2, 0.4, 0.6, 0.8 and 1.0, were investigated experimentally and theoretically. Crystal structures were characterized by X-ray diffraction (Rietveld analysis) and investigated samples possess the MgCu$_{2}$-type single phase with Fd-3m space group. Melt-spinning process introduced a chemical and topological disorder, which directly affected the magnetic properties. Refrigerant capacity (RC), strictly connected to the full width at half maximum $\\delta$TFWHM of the $\\Delta$S$_M$(T) curve and the maximum of magnetic entropy changes $\\Delta$S$_{Mpk}$(T)(T,$\\Delta$H), increases from 29 to 148 J/kg with replacement of Y by Gd atoms from x = 0.2 to x = 0.8. RC and $\\delta$TFWHM indicate the presence of disorder. Temperature dependences of magnetic entropy change $\\Delta$S$_M$(T,$\\Delta$H) and RC were measured in as-quenched and annealed state for Y$_{0.4}$Gd$_{0.6}$Co$_{2}$. This particular composition was chosen for detailed investigation mainly due to its Curie point (T$_C$ = 282 K), which is close to the room temperature. After isothermal annealing ($\\tau_a$ = 60 min, Ta = 700$^o$C) RC decreased from 122 to 104 J/kg, which clearly indicates the homogenization of the heat treated sample. Furthermore, observed inverse magnetocaloric effect is associated with the presence of antiferromagnetically coupled Gd and Co magnetic moments. The phase transition temperature increases with increasing Gd content from 74 to 407 K for Y$_{0.8}$Gd$_{0.2}$Co$_{2}$ and GdCo2, respectively. Within the FPLO-LDA DFT method, the non-magnetic ground state for YCo$_{2}$ and the magnetic ground state for GdCo$_{2}$ are predicted in agreement with experiment. The dependence of calculated total and species-resolved magnetic moments on Gd concentration reasonably agrees with available experimental data.|\n", "2503.17357": "|**2025-03-21**|**Filtered Rayleigh-Ritz is all you need**|Ryan Abbott, Daniel C. Hackett, George T. Fleming et.al.|[2503.17357](http://arxiv.org/abs/2503.17357)|null|22+7 pages, 0 figures, 1 table|Recent work has shown that the (block) Lanczos algorithm can be used to extract approximate energy spectra and matrix elements from (matrices of) correlation functions in quantum field theory, and identified exact coincidences between Lanczos analysis methods and others. In this work, we note another coincidence: the Lanczos algorithm is equivalent to the well-known Rayleigh-Ritz method applied to Krylov subspaces. Rayleigh-Ritz provides optimal eigenvalue approximations within subspaces; we find that spurious-state filtering allows these optimality guarantees to be retained in the presence of statistical noise. We explore the relation between Lanczos and Prony's method, their block generalizations, generalized pencil of functions (GPOF), and methods based on the generalized eigenvalue problem (GEVP), and find they all fall into a larger \"Prony-Ritz equivalence class\", identified as all methods which solve a finite-dimensional spectrum exactly given sufficient correlation function (matrix) data. This equivalence allows simpler and more numerically stable implementations of (block) Lanczos analyses.|\n", "2503.17314": "|**2025-03-21**|**Numerical Investigation of Preferential Flow Paths in Enzymatically Induced Calcite Precipitation supported by Bayesian Model Analysis**|Rebecca Kohlhaas, Johannes Hommel, Felix Weinhardt et.al.|[2503.17314](http://arxiv.org/abs/2503.17314)|null||The usability of enzymatically induced calcium carbonate precipitation (EICP) as a method for altering porous-media properties, soil stabilization, or biocementation depends on our ability to predict the spatial distribution of the precipitated calcium carbonate in porous media. While current REV-scale models are able to reproduce the main features of laboratory experiments, they neglect effects like the formation of preferential flow paths and the appearance of multiple polymorphs of calcium carbonate with differing properties. We show that extending an existing EICP model by the conceptual assumption of a mobile precipitate, amorphous calcium carbonate (ACC), allows for the formation of preferential flow paths when the initial porosity is heterogeneous. We apply sensitivity analysis and Bayesian inference to gain an understanding of the influence of characteristic parameters of ACC that are uncertain or unknown and compare two variations of the model based on different formulations of the ACC detachment term to analyse the plausibility of our hypothesis. An arbitrary Polynomial Chaos (aPC) surrogate model is trained based on the full model and used to reduce the computational cost of this study.|\n", "2503.17283": "|**2025-03-21**|**Energy Efficiency trends in HPC: what high-energy and astrophysicists need to know**|Estela Suarez, Jorge Amaya, Martin Frank et.al.|[2503.17283](http://arxiv.org/abs/2503.17283)|null||The growing energy demands of HPC systems have made energy efficiency a critical concern for system developers and operators. However, HPC users are generally less aware of how these energy concerns influence the design, deployment, and operation of supercomputers even though they experience the consequences. This paper examines the implications of HPC's energy consumption, providing an overview of current trends aimed at improving energy efficiency. We describe how hardware innovations such as energy-efficient processors, novel system architectures, power management techniques, and advanced scheduling policies do have a direct impact on how applications need to be programmed and executed on HPC systems. For application developers, understanding how these new systems work and how to analyse and report the performances of their own software is critical in the dialog with HPC system designers and administrators. The paper aims to raise awareness about energy efficiency among users, particularly in the high energy physics and astrophysics domains, offering practical advice on how to analyse and optimise applications to reduce their energy consumption without compromising on performance.|\n", "2503.17265": "|**2025-03-21**|**Learning to Solve Related Linear Systems**|Disha Hegde, Jon Cockayne et.al.|[2503.17265](http://arxiv.org/abs/2503.17265)|null||Solving multiple parametrised related systems is an essential component of many numerical tasks. Borrowing strength from the solved systems and learning will make this process faster. In this work, we propose a novel probabilistic linear solver over the parameter space. This leverages information from the solved linear systems in a regression setting to provide an efficient posterior mean and covariance. We advocate using this as companion regression model for the preconditioned conjugate gradient method, and discuss the favourable properties of the posterior mean and covariance as the initial guess and preconditioner. We also provide several design choices for this companion solver. Numerical experiments showcase the benefits of using our novel solver in a hyperparameter optimisation problem.|\n", "2503.17234": "|**2025-03-21**|**High Accuracy Techniques Based Adaptive Finite Element Methods for Elliptic PDEs**|Jingjing Xiao, Ying Liu, Nianyu Yi et.al.|[2503.17234](http://arxiv.org/abs/2503.17234)|null||This paper aims to develop an efficient adaptive finite element method for the second-order elliptic problem. Although the theory for adaptive finite element methods based on residual-type a posteriori error estimator and bisection refinement has been well established, in practical computations, the use of non-asymptotic exact of error estimator and the excessive number of adaptive iteration steps often lead to inefficiency of the adaptive algorithm. We propose an efficient adaptive finite element method based on high-accuracy techniques including the superconvergence recovery technique and high-quality mesh optimization. The centroidal Voronoi Delaunay triangulation mesh optimization is embedded in the mesh adaption to provide high-quality mesh, and then assure that the superconvergence property of the recovered gradient and the asymptotical exactness of the error estimator. A tailored adaptive strategy, which could generate high-quality meshes with a target number of vertices, is developed to ensure the adaptive computation process terminated within $7$ steps. The effectiveness and robustness of the adaptive algorithm is numerically demonstrated.|\n", "2503.17190": "|**2025-03-21**|**Babu\u0161ka's paradox in a nonlinear bending model**|S\u00f6ren Bartels, Andrea Bonito, Peter Hornung et.al.|[2503.17190](http://arxiv.org/abs/2503.17190)|null||The Babu\\v{s}ka or plate paradox concerns the failure of convergence when a domain with curved boundary is approximated by polygonal domains in linear bending problems with simple support boundary conditions. It can be explained via a boundary integral representation of the total Gaussian curvature that is part of the Kirchhoff--Love bending energy. It is shown that the paradox also occurs for a nonlinear bending-folding model which enforces vanishing Gaussian curvature. A simple remedy that is compatible with simplicial finite element methods to avoid wrong convergence is devised.|\n", "2503.17145": "|**2025-03-21**|**Numerical Simulations of Fully Eulerian Fluid-Structure Contact Interaction using a Ghost-Penalty Cut Finite Element Approach**|Stefan Frei, Tobias Knoke, Marc C. Steinbach et.al.|[2503.17145](http://arxiv.org/abs/2503.17145)|null|29 pages, 8 figures, 3 tables|In this work, we develop a cut-based unfitted finite element formulation for solving nonlinear, nonstationary fluid-structure interaction with contact in Eulerian coordinates. In the Eulerian description fluid flow modeled by the incompressible Navier-Stokes equations remains in Eulerian coordinates, while elastic solids are transformed from Lagrangian coordinates into the Eulerian system. A monolithic description is adopted. For the spatial discretization, we employ an unfitted finite element method with ghost penalties based on inf-sup stable finite elements. To handle contact, we use a relaxation of the contact condition in combination with a unified Nitsche approach that takes care implicitly of the switch between fluid-structure interaction and contact conditions. The temporal discretization is based on a backward Euler scheme with implicit extensions of solutions at the previous time step. The nonlinear system is solved with a semi-smooth Newton's method with line search. Our formulation, discretization and implementation are substantiated with an elastic falling ball that comes into contact with the bottom boundary, constituting a challenging state-of-the-art benchmark.|\n", "2503.17012": "|**2025-03-21**|**Learning Non-Ideal Single Vortex Flows Using the Differentiable Vortex Particle Method**|Ziqi Ji, Gang Du, Penghao Duan et.al.|[2503.17012](http://arxiv.org/abs/2503.17012)|null||This study extends the differentiable vortex particle method (DVPM) beyond idealized flow scenarios to encompass more realistic, non-ideal conditions, including viscous flow and flow subjected to non-conservative body forces. We establish the Lamb-Oseen vortex as a benchmark case, representing a fundamental viscous single vortex flow in fluid mechanics. This selection offers significant analytical advantages, as the Lamb-Oseen vortex possesses an exact analytical solution derived from the Navier-Stokes (NS) equations, thereby providing definitive ground truth data for training and validation purposes. Through rigorous evaluation across a spectrum of Reynolds numbers, we demonstrate that DVPM achieves superior accuracy in modeling the Lamb-Oseen vortex compared to conventional convolutional neural networks (CNNs) and physics-informed neural networks (PINNs). Our results substantiate DVPM's robust capabilities in modeling non-ideal single vortex flows, establishing its distinct advantages over contemporary deep learning methodologies in fluid dynamics applications. The dataset and source code are publicly available on GitHub at the following link: https://github.com/jh36714753/Learning_Non-Ideal_Single_Vortex_Flows.|\n", "2503.16968": "|**2025-03-21**|**Modelling Material Injection Into Porous Structures With the Theory of Porous Media Under Non-isothermal Conditions**|Jan-S\u00f6ren L. V\u00f6lter, Zubin Trivedi, Tim Ricken et.al.|[2503.16968](http://arxiv.org/abs/2503.16968)|null|21 pages, 13 figures|In this work, the Theory of Porous Media (TPM) is employed to model percutaneous vertebroplasty, a medical procedure in which acrylic cement is injected into cancellous vertebral bone. Previously, isothermal macroscale models have been derived to describe this material injection and the mechanical interactions which arise. However, the temperature of the injected cement is typically below the human body temperature, necessitating the extension of these models to the non-isothermal case. Following the modelling principles of the TPM and considering local thermal non-equilibrium conditions, our model introduces three energy balances as well as additional constitutive relations. If restricted to local thermal equilibrium conditions, our model equations are in agreement with other examples of TPM-based models. We observe that our model elicits physically reasonable behaviour in numerical simulations which employ parameter values and initial and boundary conditions relevant for our application. Noting that we neglect capillary effects, we claim our model to be thermodynamically consistent despite the employment of simplifying assumptions during its derivation, such as the Coleman and Noll procedure.|\n", "2503.16877": "|**2025-03-21**|**A fourth-order cut-cell method for solving the two-dimensional advection-diffusion equation with moving boundaries**|Kaiyi Liang, Yuke Zhu, Jiyu Liu et.al.|[2503.16877](http://arxiv.org/abs/2503.16877)|null||We propose a fourth-order cut-cell method for solving the two-dimensional advection-diffusion equation with moving boundaries on a Cartesian grid. We employ the ARMS technique to give an explicit and accurate representation of moving boundaries, and introduce a cell-merging technique to overcome discontinuities caused by topological changes in cut cells and the small cell problem. We use a polynomial interpolation technique base on poised lattice generation to achieve fourth-order spatial discretization, and use a fourth-order implicit-explicit Runge-Kutta scheme for time integration. Numerical tests are performed on various moving regions, with advection velocity both matching and differing from boundary velocity, which demonstrate the fourth-order accuracy of the proposed method.|\n", "2503.16827": "|**2025-03-21**|**Discontinuous Galerkin Representation of the Maxwell-J\u00fcttner Distribution**|Grant Johnson, Ammar Hakim, James Juno et.al.|[2503.16827](http://arxiv.org/abs/2503.16827)|null||Kinetic simulations of relativistic gases and plasmas are critical for understanding diverse astrophysical and terrestrial systems, but the accurate construction of the relativistic Maxwellian, the Maxwell-J\\\"uttner (MJ) distribution, on a discrete simulation grid is challenging. Difficulties arise from the finite velocity bounds of the domain, which may not capture the entire distribution function, as well as errors introduced by projecting the function onto a discrete grid. Here we present a novel scheme for iteratively correcting the moments of the projected distribution applicable to all grid-based discretizations of the relativistic kinetic equation. In addition, we describe how to compute the needed nonlinear quantities, such as Lorentz boost factors, in a discontinuous Galerkin (DG) scheme through a combination of numerical quadrature and weak operations. The resulting method accurately captures the distribution function and ensures that the moments match the desired values to machine precision.|\n", "2503.16792": "|**2025-03-21**|**Numerical simulation of wormhole propagation with the mixed hybridized discontinuous Galerkin finite element method**|Jiansong Zhang, Jiang Zhu, Yiming Wang et.al.|[2503.16792](http://arxiv.org/abs/2503.16792)|null|18pages, 2 figures|The acid treatment of carbonate reservoirs is a widely employed technique for enhancing the productivity of oil and gas reservoirs. In this paper, we present a novel combined hybridized mixed discontinuous Galerkin (HMDG) finite element method to simulate the dissolution process near the wellbore, commonly referred to as the wormhole phenomenon. The primary contribution of this work lies in the application of hybridization techniques to both the pressure and concentration equations. Additionally, an upwind scheme is utilized to address convection-dominant scenarios, and a ``cut-off\" operator is introduced to maintain the boundedness of porosity. Compared to traditional discontinuous Galerkin methods, the proposed approach results in a global system with fewer unknowns and sparser stencils, thereby significantly reducing computational costs. We analyze the existence and uniqueness of the new combined method and derive optimal error estimates using the developed technique. Numerical examples are provided to validate the theoretical analysis.|\n", "2503.16784": "|**2025-03-21**|**Multi-property directed generative design of inorganic materials through Wyckoff-augmented transfer learning**|Shuya Yamazaki, Wei Nong, Ruiming Zhu et.al.|[2503.16784](http://arxiv.org/abs/2503.16784)|null||Accelerated materials discovery is an urgent demand to drive advancements in fields such as energy conversion, storage, and catalysis. Property-directed generative design has emerged as a transformative approach for rapidly discovering new functional inorganic materials with multiple desired properties within vast and complex search spaces. However, this approach faces two primary challenges: data scarcity for functional properties and the multi-objective optimization required to balance competing tasks. Here, we present a multi-property-directed generative framework designed to overcome these limitations and enhance site symmetry-compliant crystal generation beyond P1 (translational) symmetry. By incorporating Wyckoff-position-based data augmentation and transfer learning, our framework effectively handles sparse and small functional datasets, enabling the generation of new stable materials simultaneously conditioned on targeted space group, band gap, and formation energy. Using this approach, we identified previously unknown thermodynamically and lattice-dynamically stable semiconductors in tetragonal, trigonal, and cubic systems, with bandgaps ranging from 0.13 to 2.20 eV, as validated by density functional theory (DFT) calculations. Additionally, we assessed their thermoelectric descriptors using DFT, indicating their potential suitability for thermoelectric applications. We believe our integrated framework represents a significant step forward in generative design of inorganic materials.|\n", "2503.16765": "|**2025-03-21**|**A thermodynamically consistent phase-field model for mass transport with interfacial reaction and deformation**|Zhaoyang Wang, Huaxiong Huang, Ping Lin et.al.|[2503.16765](http://arxiv.org/abs/2503.16765)|null||In this paper, a thermodynamically consistent phase-field model is proposed to describe the mass transport and reaction processes of multiple species in a fluid. A key feature of this model is that reactions between different species occur only at the interface, and may induce deformation of the interface. For the governing equations derived based on the energy variational method, we propose a structure-preserving numerical scheme that satisfies the mass conservation and energy dissipation laws at the discrete level. Furthermore, we carry out a rigorous error analysis of the time-discrete scheme for a simplified case. A series of numerical experiments are conducted to validate the effectiveness of the model as well as the accuracy and stability of the scheme. In particular, we simulate microvessels with straight and bifurcated structures to illustrate the risk of microaneurysm formation.|\n", "2503.16717": "|**2025-03-20**|**Random-sketching Techniques to Enhance the Numerically Stability of Block Orthogonalization Algorithms for s-step GMRES**|Ichitaro Yamazaki, Andrew J. Higgins, Erik G. Boman et.al.|[2503.16717](http://arxiv.org/abs/2503.16717)|null|14 pages|We integrate random sketching techniques into block orthogonalization schemes needed for s-step GMRES. The resulting block orthogonalization schemes generate the basis vectors whose overall orthogonality error is bounded by machine precision as long as each of the corresponding block vectors are numerically full rank. We implement these randomized block orthogonalization schemes using standard distributed-memory linear algebra kernels for s-step GMRES available in the Trilinos software packages. Our performance results on the Perlmutter supercomputer (with four NVIDIA A100 GPUs per node) demonstrate that these randomized techniques can enhance the numerical stability of the orthogonalization and overall solver, without a significant increase in the execution time.|\n", "2503.16665": "|**2025-03-20**|**Evolution of Shock Structures and QPOs After Halting BHL Accretion onto Kerr Black Hole**|Orhan Donmez et.al.|[2503.16665](http://arxiv.org/abs/2503.16665)|null|29 pages, 7 figures, 3 Tables. Suggestions and comments are welcome|One of the mechanisms responsible for disk formation around the black holes is Bondi-Hoyle-Lyttleton (BHL) accretion.The fact that BHL accretion can be interrupted by various astrophysical phenomena, such as stellar winds or astrophysical jets, makes it crucial to study the behavior of shock cones formed by BHL accretion around black holes once the accretion process is halted. Investigating the new plasma structures that emerge in these scenarios can provide insights into observational results. In this context, a new plasma structure forming around the Kerr black hole has been numerically modeled as a function of the black hole spin parameter and the asymptotic velocity of BHL accretion. The numerical analysis revealed that high spin (a/M=0.9) and supersonic flow ( M > 1) are necessary conditions for low-frequency quasi-periodic oscillations (LFQPOs) formation. On the other hand, the fundamental mode of the high-frequency quasi-periodic oscillations (HFQPOs) are found to be independent of both the black hole spin and asymptotic velocity and are instead governed by general relativistic effects. Additionally, the study demonstrated that for 3:2 and 2:1 resonance states to form, nonlinear couplings needs to be occurred when the black hole rotates rapidly. These couplings produce harmonic frequencies, providing an explanation for the observed quasi-periodic oscillation (QPO) resonances in black hole binaries. These findings align with precession models and nonlinear resonance models, both of which play a crucial role in QPO generation. Finally, the LFQPOs and HFQPOs obtained from numerical simulations are consistent with the observed QPO frequencies in the microquasars GRS 1915+105 and XTE J1550-564, as well as in the AGN REJ1034+396, which harbors a supermassive black hole at its center.|\n", "2503.19847": "|**2025-03-25**|**Ab-initio simulation of excited-state potential energy surfaces with transferable deep quantum Monte Carlo**|Zeno Sch\u00e4tzle, P. Bern\u00e1t Szab\u00f3, Alice Cuzzocrea et.al.|[2503.19847](http://arxiv.org/abs/2503.19847)|null|21 pages, 4 figures|The accurate quantum chemical calculation of excited states is a challenging task, often requiring computationally demanding methods. When entire ground and excited potential energy surfaces (PESs) are desired, e.g., to predict the interaction of light excitation and structural changes, one is often forced to use cheaper computational methods at the cost of reduced accuracy. Here we introduce a novel method for the geometrically transferable optimization of neural network wave functions that leverages weight sharing and dynamical ordering of electronic states. Our method enables the efficient prediction of ground and excited-state PESs and their intersections at the highest accuracy, demonstrating up to two orders of magnitude cost reduction compared to single-point calculations. We validate our approach on three challenging excited-state PESs, including ethylene, the carbon dimer, and the methylenimmonium cation, indicating that transferable deep-learning QMC can pave the way towards highly accurate simulation of excited-state dynamics.|\n", "2503.19814": "|**2025-03-25**|**Machine Learning and Data-Driven Methods in Computational Surface and Interface Science**|Lukas H\u00f6rmann, Wojciech G. Stark, Reinhard J. Maurer et.al.|[2503.19814](http://arxiv.org/abs/2503.19814)|null|27 pages, 5 figures|Nanoscale design of surfaces and interfaces is essential for modern technologies like organic LEDs, batteries, fuel cells, superlubricating surfaces, and heterogeneous catalysis. However, these systems often exhibit complex surface reconstructions and polymorphism, with properties influenced by kinetic processes and dynamic behavior. A lack of accurate and scalable simulation tools has limited computational modeling of surfaces and interfaces. Recently, machine learning and data-driven methods have expanded the capabilities of theoretical modeling, enabling, for example, the routine use of machine-learned interatomic potentials to predict energies and forces across numerous structures. Despite these advances, significant challenges remain, including the scarcity of large, consistent datasets and the need for computational and data-efficient machine learning methods. Additionally, a major challenge lies in the lack of accurate reference data and electronic structure methods for interfaces. Density Functional Theory, while effective for bulk materials, is less reliable for surfaces, and too few accurate experimental studies on interface structure and stability exist. Here, we will sketch the current state of data-driven methods and machine learning in computational surface science and provide a perspective on how these methods will shape the field in the future.|\n", "2503.19807": "|**2025-03-25**|**Probabilistic combination of loads in topology optimization designs via cumulative damage criteria**|Luis Irastorza-Valera, Luis Saucedo-Mora et.al.|[2503.19807](http://arxiv.org/abs/2503.19807)|null||Topology optimization (TO) is a well-established methodology for structural design under user-defined constraints, e.g. minimum volume and maximum stiffness. However, such methods have traditionally been applied to static, deterministic loading, in which modulus, position and direction are known and invariant. This is against the probabilistic load combination used in the structural engineering designs, and entails two important shortcomings.   The first one is related to maintenance and reliability: static loading fails to consider naturally occurring uncertainties in the loading process, measurements or regular service; also ignoring (quantitatively) unforeseen phenomena such as vibrations, and the material's behavior is assumed linear isotropic, ignoring fatigue, plasticity and anisotropy in functionally-graded materials. The second one concerns optimality itself: often times, the structure presented as \"optimal\" in fact over-estimates loading and thus wastes material by exceeding its real needs and/or distributing it poorly throughout the design dominion.   In this article, a probabilistic framework is presented: uncertain and pseudo-dynamic loading is introduced to create robust topologies via a reinforced SIMP scheme with embedded penalization addressing fatigue damage, layer direction, mechanical response (traction/compression) and yield limit (von Mises equivalent stress). This computationally efficient framework is applied to various loading scenarios, generating diverse designs for the same volume fraction constraint with improved and more realistic performances. Under the proposed method, if loads are permanent and damage isotropic, the methodology converges to the traditional (deterministic) topological optimization results. Future ramifications of this work are pondered, especially regarding metamaterial design.|\n", "2503.19806": "|**2025-03-25**|**New analytic formulae for memory and prediction functions in reservoir computers with time delays**|Peyton Mullarkey, Sarah Marzen et.al.|[2503.19806](http://arxiv.org/abs/2503.19806)|null|9 pages, 4 figures|Time delays increase the effective dimensionality of reservoirs, thus suggesting that time delays in reservoirs can enhance their performance, particularly their memory and prediction abilities. We find new closed-form expressions for memory and prediction functions of linear time-delayed reservoirs in terms of the power spectrum of the input and the reservoir transfer function. We confirm this relationship numerically for some time-delayed reservoirs using simulations, including when the reservoir can be linearized but is actually nonlinear. Finally, we use these closed-form formulae to address the utility of multiple time delays in linear reservoirs in order to perform memory and prediction, finding similar results to previous work on nonlinear reservoirs. We hope these closed-form formulae can be used to understand memory and predictive capabilities in time-delayed reservoirs.|\n", "2503.19784": "|**2025-03-25**|**Adaptive refinement in defeaturing problems via an equilibrated flux a posteriori error estimator**|Annalisa Buffa, Denise Grappein, Rafael V\u00e1zquez et.al.|[2503.19784](http://arxiv.org/abs/2503.19784)|null||An adaptive refinement strategy, based on an equilibrated flux a posteriori error estimator, is proposed in the context of defeaturing problems. Defeaturing consists in removing features from complex domains in order to ease the meshing process, and to reduce the computational burden of simulations. It is a common procedure, for example, in computer aided design for simulation based manufacturing. However, depending on the problem at hand, the effect of geometrical simplification on the accuracy of the solution may be detrimental. The proposed adaptive strategy is hence twofold: starting from a defeatured geometry it allows both for standard mesh refinement and geometrical refinement, which consists in choosing, at each step, which features need to be included into the geometry in order to significantly increase the accuracy of the solution. With respect to other estimators that were previously proposed in the context of defeaturing, the use of an equilibrated flux reconstruction allows us to avoid the evaluation of the numerical flux on the boundary of features. This makes the estimator and the adaptive strategy particularly well-suited for finite element discretizations, in which the numerical flux is typically discontinuous across element edges. The inclusion of the features during the adaptive process is tackled by a CutFEM strategy, in order to preserve the non conformity of the mesh to the feature boundary and never remesh the computational domain as the features are added. Hence, the estimator also accounts for the error introduced by weakly imposing the boundary conditions on the boundary of the added features.|\n", "2503.19732": "|**2025-03-25**|**Grid-Free Evaluation of Phonon-Limited Electronic Relaxation Times and Transport Properties**|Nenad Vukmirovi\u0107 et.al.|[2503.19732](http://arxiv.org/abs/2503.19732)|null||Present calculations of electrical transport properties of materials require evaluations of electron-phonon coupling constants on dense predefined grids of electron and phonon momenta and performing the sums over these momenta. In this work, we present the methodology for calculation of carrier relaxation times and electrical transport properties without the use of a predefined grid. The relaxation times are evaluated by integrating out the delta function that ensures energy conservation and performing an average over the angular components of phonon momentum. The charge carrier mobility is then evaluated as a sum over appropriately sampled electronic momenta. We illustrate our methodology by applying to the Fr{\\\"o}hlich model and to a real semiconducting material ZnTe. We find that rather accurate results can be obtained with a modest number of electron and phonon momenta, on the order of one hundred each, regardless of the carrier effective mass.|\n", "2503.19723": "|**2025-03-25**|**Role of spatial embedding and planarity in shaping the topology of the Street Networks**|Ritish Khetarpal, Aradhana Singh et.al.|[2503.19723](http://arxiv.org/abs/2503.19723)|null|17 pages, 8 main figures, 6 supplementary figures|The topology of city street networks (SNs) is constrained by spatial embedding, requiring non-crossing links and preventing random node placement or overlap. Here, we analyzed SNs of $33$ Indian cities to explore how the spatial embedding and the planarity jointly shape their topology. Overall, we found that all the studied SNs have small-world properties with higher clustering and efficiency. The efficiency of the empirical networks is even higher than that of the corresponding degree of preserved random networks. This increased efficiency can be explained by Dijkstra's path-length distribution, which closely fits a right-skewed normal or log-normal distribution. Moreover, we observed that the connectivity of the streets is length-dependent: the smaller streets connect preferably to the smaller streets, while longer streets tend to connect with the longer counterparts. This length-dependent connectivity is more profound in the empirical SNs than in the corresponding degree preserved random and random planar networks. However, planar networks maintaining the empirical spatial coordinates replicate the connectivity behavior of empirical SNs, highlighting the influence of spatial embedding. Moreover, the robustness of the cities in terms of resilience to random errors and targeted attacks is independent of the SN's size, indicating other factors, such as geographical constraints, substantially influence network stability.|\n", "2503.19720": "|**2025-03-25**|**Defects and Impurity Properties of VN precipitates in ARAFM Steels: Modelling using a Universal Machine Learning Potential and Experimental Validation**|R. S. Stroud, C. Reynolds, T. Melichar et.al.|[2503.19720](http://arxiv.org/abs/2503.19720)|null|14 pages, 4 figures|VN precipitates used to strengthen ARAFM steels for fusion applications, have been shown to undergo dissolution under high Fe ion irradiation doses of 100 dpa at dose rates of 10^-3 dpa/s at 600 C. Here, point defects and solute substitutions have been studied using atom probe tomography (APT), universal machine learning interatomic potentials (uMLIPs), and density functional theory. Through a combination of transmission electron microscopy (TEM), APT, and atomic scale calculations, N-vacancies and substitutional Cr are found to be present in VN precipitates prior to irradiation. Ternary convex hulls were calculated for ten VNX (X=Cr, Fe, C, Si, Mn, W, Ta, B, S, P) systems with an uMLIP. These calculations predict Fe, P, Mn, and Si to be unstable solutes in the VN precipitate. Therefore, Fe, P, Mn and Si implantation via collision cascades is found to be a possible mechanism driving dissolution of VN.|\n", "2503.19701": "|**2025-03-25**|**Enhanced gradient recovery-based a posteriori error estimator and adaptive finite element method for elliptic equations**|Ying Liu, Jingjing Xiao, Nianyu Yi et.al.|[2503.19701](http://arxiv.org/abs/2503.19701)|null||Recovery type a posteriori error estimators are popular, particularly in the engineering community, for their computationally inexpensive, easy to implement, and generally asymptotically exactness. Unlike the residual type error estimators, one can not establish upper and lower a posteriori error bounds for the classical recovery type error estimators without the saturation assumption. In this paper, we first present three examples to show the unsatisfactory performance in the practice of standard residual or recovery-type error estimators, then, an improved gradient recovery-based a posteriori error estimator is constructed. The proposed error estimator contains two parts, one is the difference between the direct and post-processed gradient approximations, and the other is the residual of the recovered gradient. The reliability and efficiency of the enhanced estimator are derived. Based on the improved recovery-based error estimator and the newest-vertex bisection refinement method with a tailored mark strategy, an adaptive finite element algorithm is designed. We then prove the convergence of the adaptive method by establishing the contraction of gradient error plus oscillation. Numerical experiments are provided to illustrate the asymptotic exactness of the new recovery-based a posteriori error estimator and the high efficiency of the corresponding adaptive algorithm.|\n", "2503.19687": "|**2025-03-25**|**Excitability and travelling waves in renewable active matter**|Abhishek M, Ankit Dhanuka, Deb Sankar Banerjee et.al.|[2503.19687](http://arxiv.org/abs/2503.19687)|null|28 pages, 12 figures|Activity and renewability are distinctive features of living matter, and constitute a new class of materials that we term renewable active matter. A striking example is the cell cytoskeleton, where myosin filaments bind to the actin meshwork, apply contractile stresses and undergo continual stress/strain dependent turnover, thus acting as both force generators and sensors. As a consequence of nonreciprocity, arising from the independence of action and response, such living matter exhibits unusual mechanical properties like, segregation without attraction, fragility and force chains. Here we show that the interplay between activity and turnover gives rise to mechanical excitability in the form of travelling waves and pulses, and spatiotemporal chaos. We provide a systematic study of the nucleation, movement and shape of the travelling pulse, and present a boundary layer analysis to establish the existence of homoclinic orbits. Our analytical results are supported by detailed numerical analysis of the governing partial differential equations. This study has implications for the observed mechanical excitability in a variety of cellular contexts such as in isolated adherent cells and confluent cells within tissues.|\n", "2503.19684": "|**2025-03-25**|**Characteristic boundary conditions for Hybridizable Discontinuous Galerkin methods**|Jan Ellmenreich, Matteo Giacomini, Antonio Huerta et.al.|[2503.19684](http://arxiv.org/abs/2503.19684)|null||In this work we introduce the concept of characteristic boundary conditions (CBCs) within the framework of Hybridizable Discontinuous Galerkin (HDG) methods, including both the Navier-Stokes characteristic boundary conditions (NSCBCs) and a novel approach to generalized characteristic relaxation boundary conditions (GRCBCs). CBCs are based on the characteristic decomposition of the compressible Euler equations and are designed to prevent the reflection of waves at the domain boundaries. We show the effectiveness of the proposed method for weakly compressible flows through a series of numerical experiments by comparing the results with common boundary conditions in the HDG setting and reference solutions available in the literature. In particular, HDG with CBCs show superior performance minimizing the reflection of vortices at artificial boundaries, for both inviscid and viscous flows.|\n", "2503.19675": "|**2025-03-25**|**Tailoring nuclear spins order with defects: a Quantum-TCAD study**|Gaetano Calogero, Ioannis Deretzis, Giuseppe Fisicaro et.al.|[2503.19675](http://arxiv.org/abs/2503.19675)|null||The full design of relevant systems for quantum applications, ranging from quantum simulation to sensing, is presented using a combination of atomistic methods. A prototypical system features a two-dimensional ordered distribution of spins interacting with out-of-plane spin drivers/probes. It could be realized in wide-bandgap semiconductors through open-volume point defects and functionalized surfaces with low Miller indexes. We study the case of defect electron spins (driver / probe) interacting via hyperfine coupling with $S=1/2$ nuclear spins of H atoms chemisorbed onto \\hkl(001) and \\hkl(111) 3C-SiC surfaces. We simulate the system fabrication processes with super lattice kinetic Monte Carlo, demonstrating that epitaxial growth under time-dependent conditions is a viable method for achieving controlled abundance or depletion of near-surface point defects. Quantum features are evaluated by means of extensive numerical analysis at a full quantum mechanical level based on calibrated models of interacting spin systems. This analysis includes both stationary (relative stability of ordered states) and time-dependent (protocols) conditions, achieved varying the model parameters (in our case the atomic structure and the external field). We identify a rich scenario of metastable spin-waves in the quantum simulation setting. The interaction between protocols and variable system configurations could hinder the effectiveness of the preparation/measurement phases.|\n", "2503.19624": "|**2025-03-25**|**Derivative polynomials and infinite series for squigonometric functions**|Bart S. van Lith et.al.|[2503.19624](http://arxiv.org/abs/2503.19624)|null|23 pages, 3 figures|All squigonometric functions admit derivatives that can be expressed as polynomials of the squine and cosquine. We introduce a general framework that allows us to determine these polynomials recursively. We also provide an explicit formula for all coefficients of these polynomials. This also allows us to provide an explicit expression for the MacLaurin series coefficients of all squigonometric functions. We further discuss some methods that can compute the squigonometric functions up to any given tolerance over all of the real line.|\n", "2503.19620": "|**2025-03-25**|**Optimization through In-Context Learning and Iterative LLM Prompting for Nuclear Engineering Design Problems**|M. Rizki Oktavian, Anirudh Tunga, Amandeep Bakshi et.al.|[2503.19620](http://arxiv.org/abs/2503.19620)|null|Codes and data are available upon request|The optimization of nuclear engineering designs, such as nuclear fuel assembly configurations, involves managing competing objectives like reactivity control and power distribution. This study explores the use of Optimization by Prompting, an iterative approach utilizing large language models (LLMs), to address these challenges. The method is straightforward to implement, requiring no hyperparameter tuning or complex mathematical formulations. Optimization problems can be described in plain English, with only an evaluator and a parsing script needed for execution. The in-context learning capabilities of LLMs enable them to understand problem nuances, therefore, they have the potential to surpass traditional metaheuristic optimization methods. This study demonstrates the application of LLMs as optimizers to Boiling Water Reactor (BWR) fuel lattice design, showing the capability of commercial LLMs to achieve superior optimization results compared to traditional methods.|\n", "2503.19487": "|**2025-03-25**|**Asymptotic-preserving and positivity-preserving discontinuous Galerkin method for the semiconductor Boltzmann equation in the diffusive scaling**|Huan Ding, Liu Liu, Xinghui Zhong et.al.|[2503.19487](http://arxiv.org/abs/2503.19487)|null||In this paper, we develop an asymptotic-preserving and positivity-preserving discontinuous Galerkin (DG) method for solving the semiconductor Boltzmann equation in the diffusive scaling. We first formulate the diffusive relaxation system based on the even-odd decomposition method, which allows us to split into one relaxation step and one transport step. We adopt a robust implicit scheme that can be explicitly implemented for the relaxation step that involves the stiffness of the collision term, while the third-order strong-stability-preserving Runge-Kutta method is employed for the transport step. We couple this temporal scheme with the DG method for spatial discretization, which provides additional advantages including high-order accuracy, $h$-$p$ adaptivity, and the ability to handle arbitrary unstructured meshes. A positivity-preserving limiter is further applied to preserve physical properties of numerical solutions. The stability analysis using the even-odd decomposition is conducted for the first time. We demonstrate the accuracy and performance of our proposed scheme through several numerical examples.|\n", "2503.19483": "|**2025-03-25**|**Empirical Hyper Element Integration Method (EHEIM) with Unified Integration Criteria for Efficient Hyper Reduced FE$^2$ Simulations**|Nils Lange, Geralf H\u00fctter, Bjoern Kiefer et.al.|[2503.19483](http://arxiv.org/abs/2503.19483)|null||Numerical homogenization for mechanical multiscale modeling by means of the finite element method (FEM) is an elegant way of obtaining structure-property relations, if the behavior of the constituents of the lower scale is well understood. However, the computational costs of this so-called FE$^2$ method are so high that reduction methods are essential. While the construction of a reduced basis for the microscopic nodal displacements using proper orthogonal decomposition (POD) has become a standard technique, the reduction of the computational effort for the projected nodal forces, the so-called hyper reduction, is an additional challenge, for which different strategies have been proposed in the literature. The empirical cubature method (ECM), which has been proven to be very robust, implemented the conservation of the total volume is used as a constraint in the resulting optimization problem, while energy-based criteria have been proposed in other contributions.   The present contribution presents a unified integration criteria concept, involving the aforementioned criteria, among others. These criteria are used both with a Gauss point-based as well as with an element-based hyper reduction scheme, the latter retaining full compatibility with the common modular finite element framework. The methods are combined with a previously proposed clustered training strategy and a monolithic solver. Numerical examples empirically demonstrate that the additional criteria improve the accuracy for a given number of modes. Vice verse, less modes and thus lower computational costs are required to reach a given level of accuracy.|\n", "2503.19424": "|**2025-03-25**|**A linear, unconditionally stable, second order decoupled method for the nematic liquid crystal flows with SAV approach**|Ruonan Cao, Nianyu Yi et.al.|[2503.19424](http://arxiv.org/abs/2503.19424)|null||In this paper, we present a second order, linear, fully decoupled, and unconditionally energy stable scheme for solving the Erickson-Leslie model. This approach integrates the pressure correction method with a scalar auxiliary variable technique. We rigorously demonstrate the unconditional energy stability of the proposed scheme. Furthermore, we present several numerical experiments to validate its convergence order, stability, and computational efficiency.|\n", "2503.19379": "|**2025-03-25**|**Kernel compensation method for Maxwell eigenproblem with mimetic finite difference discretization**|Chenhao Jin, Yinhua Xia, Yan Xu et.al.|[2503.19379](http://arxiv.org/abs/2503.19379)|null|9 figures. This work has been accepted for publication in Numerical   Methods for Partial Differential Equations|We present a kernel compensation method for Maxwell eigenproblem for photonic crystals to avoid the infinite-dimensional kernels that cause many difficulties in the calculation of energy gaps. The quasi-periodic problem is first transformed into a periodic one on the cube by the Floquet-Bloch theory. Then the compensation operator is introduced in Maxwell's equation with the shifted curl operator. The discrete problem depends on the compatible discretization of the de Rham complex, which is implemented by the mimetic finite difference method in this paper. We prove that the compensation term exactly fills up the kernel of the original problem and avoids spurious eigenvalues. Also, we propose an efficient preconditioner and its FFT and multigrid solvers, which allow parallel computing. Numerical experiments for different three-dimensional lattices are performed to validate the accuracy and effectiveness of the method.|\n", "2503.19346": "|**2025-03-25**|**A Wong--Zakai resonance-based integrator for nonlinear Schr\u00f6dinger equation with white noise dispersion**|Jianbo Cui, Georg Maierhofer et.al.|[2503.19346](http://arxiv.org/abs/2503.19346)|null|39 pages|We introduce a novel approach to numerical approximation of nonlinear Schr\\\"odinger equation with white noise dispersion in the regime of low-regularity solutions. Approximating such solutions in the stochastic setting is particularly challenging due to randomized frequency interactions and presents a compelling challenge for the construction of tailored schemes. In particular, we design the first resonance-based schemes for this equation, which achieve provable convergence for solutions of much lower regularity than previously required. A crucial ingredient in this construction is the Wong--Zakai approximation of stochastic dispersive system, which introduces piecewise linear phases that capture nonlinear frequency interactions and can subsequently be approximated to construct resonance-based schemes. We prove the well-posedness of the Wong--Zakai approximated equation and establish its proximity to the original full stochastic dispersive system. Based on this approximation, we demonstrate an improved strong convergence rate for our new scheme, which exploits the stochastic nature of the dispersive terms. Finally, we provide numerical experiments underlining the favourable performance of our novel method in practice.|\n", "2503.19333": "|**2025-03-25**|**E-PINNs: Epistemic Physics-Informed Neural Networks**|Ashish S. Nair, Bruno Jacob, Amanda A. Howard et.al.|[2503.19333](http://arxiv.org/abs/2503.19333)|null|27 pages, 13 figures|Physics-informed neural networks (PINNs) have demonstrated promise as a framework for solving forward and inverse problems involving partial differential equations. Despite recent progress in the field, it remains challenging to quantify uncertainty in these networks. While approaches such as Bayesian PINNs (B-PINNs) provide a principled approach to capturing uncertainty through Bayesian inference, they can be computationally expensive for large-scale applications. In this work, we propose Epistemic Physics-Informed Neural Networks (E-PINNs), a framework that leverages a small network, the \\emph{epinet}, to efficiently quantify uncertainty in PINNs. The proposed approach works as an add-on to existing, pre-trained PINNs with a small computational overhead. We demonstrate the applicability of the proposed framework in various test cases and compare the results with B-PINNs using Hamiltonian Monte Carlo (HMC) posterior estimation and dropout-equipped PINNs (Dropout-PINNs). Our experiments show that E-PINNs provide similar coverage to B-PINNs, with often comparable sharpness, while being computationally more efficient. This observation, combined with E-PINNs' more consistent uncertainty estimates and better calibration compared to Dropout-PINNs for the examples presented, indicates that E-PINNs offer a promising approach in terms of accuracy-efficiency trade-off.|\n", "2503.21663": "|**2025-03-27**|**DiPolMol-Py: A Python package for calculations for $^2\u03a3$ ground-state molecules**|Bethan Humphreys, Alex J. Matthies, Hannah J. Williams et.al.|[2503.21663](http://arxiv.org/abs/2503.21663)|null|25 pages, 5 figures|We present the python package DiPolMol-Py, which can be used to calculate the rotational and hyperfine structure of $^2\\Sigma$ molecules. The calculations can be performed in the presence of dc magnetic fields, dc electric fields and far off-resonant optical fields. We additionally include functions to calculate the polarisability of the molecule and the transition dipole moment between different energy eigenstates. The package is applicable to many of the molecules which can be laser cooled, specifically the alkaline earth fluorides. We provide a constants file which includes many of the required literature values for CaF, SrF and BaF. Additional species can easily be added by updating this file.|\n", "2503.21658": "|**2025-03-27**|**Numerical Analysis of the Stability of Iron Dust Bunsen Flames**|Thijs Hazenberg, Daniel Braig, Johannes Mich et.al.|[2503.21658](http://arxiv.org/abs/2503.21658)|null|Submitted to ECM-PROCI track|This article presents numerical simulations of the response of an iron dust Bunsen flame to particle seeding changes. A validated numerical model is used to study the impact of particle seeding fluctuations on flame stability. Simulations are conducted for the Bunsen setup in the right-side up and up-side down configuration. No significant differences in flame response are identified in flame stability between the right-side up and up-side down configurations. We find that the Bunsen flame is surprisingly robust to abrupt changes in particle loading. The sudden change in particle loading does not excite any intrinsic instabilities in the flame. Based on our results, the iron dust flames are robust to imposed fluctuations. We hypothesize that this is due to the lack of a feedback mechanism between the burned temperature and the heat release rate. This mechanism is present in conventional, chemistry-driven, gaseous flames. However, such a mechanism is absent in iron dust flames because the combustion of individual iron particles is limited by oxygen diffusion, which is insensitive to temperature.|\n", "2503.21653": "|**2025-03-27**|**Strong convergence and stability of stochastic theta method for time-changed stochastic differential equations with local Lipschitz coefficients**|Jingwei Chen, Jun Ye, Jinwen Chen et.al.|[2503.21653](http://arxiv.org/abs/2503.21653)|null||In this paper, the stochastic theta (ST) method is investigated for a class of stochastic differential equations driven by a time-changed Brownian motion, whose coefficients are time-space-dependent and satisfy the local Lipschitz condition. It is proved that under the local Lipschitz and some additional assumptions, the ST method with $\\theta\\in[1/2,1]$ is strongly convergent. It is also obtained that, for all positive stepsizes, the ST method with $\\theta\\in[1/2,1]$ is asymptotically mean square stable under a coercivity condition. With some restrictions on the stepsize, the ST method with $\\theta\\in[0,1/2)$ is asymptotically mean square stable under a stronger assumption. Some numerical simulations are presented to illustrate the theoretical results.|\n", "2503.21626": "|**2025-03-27**|**Inverse Lax-Wendroff boundary treatment for solving conservation laws with finite difference HWENO methods**|Guangyao Zhu, Yan Jiang, Zhuang Zhao et.al.|[2503.21626](http://arxiv.org/abs/2503.21626)|null||This paper presents a novel inverse Lax-Wendroff (ILW) boundary treatment for finite difference Hermite weighted essentially non-oscillatory (HWENO) schemes to solve hyperbolic conservation laws on arbitrary geometries. The complex geometric domain is divided by a uniform Cartesian grid, resulting in challenge in boundary treatment. The proposed ILW boundary treatment could provide high order approximations of both solution values and spatial derivatives at ghost points outside the computational domain. Distinct from existing ILW approaches, our boundary treatment constructs the extrapolation via optimized through a least squares formulation, coupled with the spatial derivatives at the boundary obtained via the ILW procedure. Theoretical analysis indicates that compared with other ILW methods, our proposed one would require fewer terms by using the relatively complicated ILW procedure and thus improve computational efficiency while preserving accuracy and stability. The effectiveness and robustness of the method are validated through numerical experiments.|\n", "2503.21618": "|**2025-03-27**|**A shifted Laplace rational filter for large-scale eigenvalue problems**|Biyi Wang, Karl Meerbergen, Raf Vandebril et.al.|[2503.21618](http://arxiv.org/abs/2503.21618)|null||We present a rational filter for computing all eigenvalues of a symmetric definite eigenvalue problem lying in an interval on the real axis. The linear systems arising from the filter embedded in the subspace iteration framework, are solved via a preconditioned Krylov method.   The choice of the poles of the filter is based on two criteria. On the one hand, the filter should enhance the eigenvalues in the interval of interest, which suggests that the poles should be chosen close to or in the interval. On the other hand, the choice of poles has an important impact on the convergence speed of the iterative method. For the solution of problems arising from vibrations, the two criteria contradict each other, since fast convergence of the eigensolver requires poles to be in or close to the interval, whereas the iterative linear system solver becomes cheaper when the poles lie further away from the eigenvalues. In the paper, we propose a selection of poles inspired by the shifted Laplace preconditioner for the Helmholtz equation.   We show numerical experiments from finite element models of vibrations. We compare the shifted Laplace rational filter with rational filters based on quadrature rules for contour integration.|\n", "2503.21532": "|**2025-03-27**|**Multiscale geometrical Lagrangian statistics of heavy impurities in drift-wave turbulence**|Zetao Lin, Benjamin Kadoch, Saddrudin Benkadda et.al.|[2503.21532](http://arxiv.org/abs/2503.21532)|null|19 pages, 2 tables, 7 figures|We investigate the behavior of heavy impurities in edge plasma turbulence by analyzing their trajectories using the Hasegawa-Wakatani model. Through direct numerical simulations, we track ensembles of charged impurity particles over hundreds of eddy turnover times within statistically steady turbulent flows. Assuming that heavy impurities lag behind the flow, a novel derivation of relaxation time of heavy impurities is proposed. Our results reveal that heavy impurities can cluster within turbulence. We provide multiscale geometrical Lagrangian statistics of heavy impurities trajectories. To quantify directional changes, we analyze the scale-dependent curvature angle, along with the influence of the Stokes number on the mean curvature angles and the probability distribution function of curvature angles.|\n", "2503.21456": "|**2025-03-27**|**Mechanostat-type effective density correction for Carter-Hayes growth applied to topology optimization and its efficient interpolation for a target strain energy and volume fraction**|Luis Irastorza-Valera, Ricardo Larra\u00ednzar-Garijo, Javier Montoya-Ad\u00e1rraga et.al.|[2503.21456](http://arxiv.org/abs/2503.21456)|null||The need for optimized structures with good mechanical performance for the minimum weight is common in industry. Solid Isotropic Material with Penalization (SIMP) is a Topology Optimization (TO) method offering a trade-off between minimum compliance (i.e., maximum stiffness) and a fixed material amount for a given set of boundary conditions. Since TO is a non-convex problem, its gradient can be tuned by filtering the topology's contour, creating sharper material profiles without necessarily compromising optimality. However, despite simplifying the layout, some filters fail to address manufacturability concerns such as capillarity (thin tweaks as struts) generated by uncertain loading, vibration or fatigue.   A tailored density-based filtering strategy is offered to tackle this issue. Additionally, volume fraction is left unconstrained so material can be strategically replenished through a logarithmic rule acting on the updated compliance. In doing so, an interpolation space with three degrees of freedom (volume, compliance, minimum thickness) is created, yielding diverse topologies for the same boundary conditions and design values along different stages of evolving topological families with distinct features.   The optimization process is further accelerated by introducing the volume-compliance iterative scheme as a physical loss function in a Double Distance Neural Network (D$^2$NN), obtaining similar results to 2,000 steps worth of vanilla iteration within 500 training epochs. This proposal offers a novel topology optimization design space based on minimum strut thickness - via filtering - and topological families defined by minimum volume fraction and compliance. The methodology is tested on several examples with diverse loading and boundary conditions, obtaining similarly satisfactory results, and then boosted via Machine Learning, acting as a fast and cheap surrogate.|\n", "2503.21452": "|**2025-03-27**|**Numerical solution of locally loaded Volterra integral equations**|Vladislav Byankin, Aleksandr Tynda, Denis Sidorov et.al.|[2503.21452](http://arxiv.org/abs/2503.21452)|null|7 pages, 2 figures|Volterra's integral equations with local and nonlocal loads represent the novel class of integral equations that have attracted considerable attention in recent years. These equations are a generalisation of the classic Volterra integral equations, which were first introduced by Vito Volterra in the late 19th century. The loaded Volterra integral equations are characterised by the presence of a load which complicates the process of their theoretical and numerical study. Sometimes these equation are called the equations with ``frozen'' argument. The present work is devoted to the study of Volterra equations with locally loaded integral operators. The existence and uniquness theorems are proved. Among the main contributions is the collocation method for approximate solution of such equations based on the piecewise linear approximation. To confirm the convergence of the method, a number of numerical results for solving model problems are provided.|\n", "2503.21382": "|**2025-03-27**|**Limited Diffusion of Silicon in GaN: A DFT Study Supported by Experimental Evidence**|Karol Kawka, Pawel Kempisty, Akira Kusaba et.al.|[2503.21382](http://arxiv.org/abs/2503.21382)|null||Silicon (Si) is the primary donor dopant in gallium nitride (GaN), introduced through epitaxial growth or ion implantation. However, precise control over Si diffusion remains a critical challenge for high-performance device applications. This study investigates Si diffusion mechanisms in bulk GaN using first-principles density functional theory (DFT) calculations, supported by ultra-high-pressure annealing (UHPA) experiments. Vacancy-mediated diffusion pathways were analyzed using the SIESTA code, with minimum energy paths (MEPs) and activation barriers determined via the nudged elastic band (NEB) method. The results indicate that Si diffusion barriers vary with crystallographic direction, with the lowest barrier of 3.2 eV along [11-20] and the highest barrier of ~9.9 eV along [1-100], rendering diffusion in this direction highly improbable. Alternative diffusion mechanisms, including direct exchange and ring-like migration, exhibit prohibitively high barriers ($>$12 eV). Phonon calculations confirm that temperature-induced reductions in effective diffusion barriers are minimal. Experimental validation using SIMS analysis on Si-implanted GaN samples subjected to UHPA (1450{\\deg}C, 1 GPa) confirms negligible Si diffusion under these extreme conditions. These findings resolve inconsistencies in prior reports and establish that Si-doped GaN remains highly stable, ensuring reliable doping profiles for advanced electronic and optoelectronic applications.|\n", "2503.21361": "|**2025-03-27**|**Computing adjoint mismatch of linear maps**|Jonas Bresch, Dirk A. Lorenz, Felix Schneppe et.al.|[2503.21361](http://arxiv.org/abs/2503.21361)|null||This paper considers the problem of detecting adjoint mismatch for two linear maps. To clarify, this means that we aim to calculate the operator norm for the difference of two linear maps, where for one we only have a black-box implementation for the evaluation of the map, and for the other we only have a black-box for the evaluation of the adjoint map. We give two stochastic algorithms for which we prove the almost sure convergence to the operator norm. The algorithm is a random search method for a generalization of the Rayleigh quotient and uses optimal step sizes. Additionally, a convergence analysis is done for the corresponding singular vector and the respective eigenvalue equation.|\n", "2503.21318": "|**2025-03-27**|**Explicit error bounds and guaranteed convergence of the Koopman-Hill projection stability method for linear time-periodic dynamics**|Fabia Bayer, Remco I. Leine et.al.|[2503.21318](http://arxiv.org/abs/2503.21318)|null|preprint, 34 pages, 10 figures|The Koopman-Hill projection method is used to approximate the fundamental solution matrix of linear time-periodic ordinary differential equations, possibly stemming from linearization around a periodic solution of a nonlinear dynamical system. By expressing both the true fundamental solution and its approximation as series, we derive an upper bound for the approximation error that decays exponentially with the size of the Hill matrix. Exponential decay of the Fourier coefficients of the system dynamics is key to guarantee convergence. The paper also analyzes a subharmonic formulation that improves the convergence rate. Two numerical examples, including a Duffing oscillator, illustrate the theoretical findings.|\n", "2503.21252": "|**2025-03-27**|**Multi-fidelity Learning of Reduced Order Models for Parabolic PDE Constrained Optimization**|Benedikt Klein, Mario Ohlberger et.al.|[2503.21252](http://arxiv.org/abs/2503.21252)|null|36 pages, 5 figures|This article builds on the recently proposed RB-ML-ROM approach for parameterized parabolic PDEs and proposes a novel hierarchical Trust Region algorithm for solving parabolic PDE constrained optimization problems. Instead of using a traditional offline/online splitting approach for model order reduction, we adopt an active learning or enrichment strategy to construct a multi-fidelity hierarchy of reduced order models on-the-fly during the outer optimization loop. The multi-fidelity surrogate model consists of a full order model, a reduced order model and a machine learning model. The proposed hierarchical framework adaptively updates its hierarchy when querying parameters, utilizing a rigorous a posteriori error estimator in an error aware trust region framework. Numerical experiments are given to demonstrate the efficiency of the proposed approach.|\n", "2503.21234": "|**2025-03-27**|**Continuous Data Assimilation for the Navier-Stokes Equations with Nonlinear Slip Boundary Conditions**|W. C. Wu, H. Y. Dong, K. Wang et.al.|[2503.21234](http://arxiv.org/abs/2503.21234)|null||This paper focuses on continuous data assimilation (CDA) for the Navier-Stokes equations with nonlinear slip boundary conditions. CDA methods are typically employed to recover the original system when initial data or viscosity coefficients are unknown, by incorporating a feedback control term generated by observational data over a time period. In this study, based on a regularized form derived from the variational inequalities of the Navier-Stokes equations with nonlinear slip boundary conditions, we first investigate the classical CDA problem when initial data is absent. After establishing the existence, uniqueness and regularity of the solution, we prove its exponential convergence with respect to the time. Additionally, we extend the CDA to address the problem of missing viscosity coefficients and analyze its convergence order, too. Furthermore, utilizing the predictive capabilities of partial evolutionary tensor neural networks (pETNNs) for time-dependent problems, we propose a novel CDA by replacing observational data with predictions got by pETNNs. Compared with the classical CDA, the new one can achieve similar approximation accuracy but need much less computational cost. Some numerical experiments are presented, which not only validate the theoretical results, but also demonstrate the efficiency of the CDA.|\n", "2503.21201": "|**2025-03-27**|**Efficient Crystal Structure Prediction Using Genetic Algorithm and Universal Neural Network Potential**|Takuya Shibayama, Hideaki Imamura, Katsuhiko Nishimra et.al.|[2503.21201](http://arxiv.org/abs/2503.21201)|null||Crystal structure prediction (CSP) is crucial for identifying stable crystal structures in given systems and is a prerequisite for computational atomistic simulations. Recent advances in neural network potentials (NNPs) have reduced the computational cost of CSP. However, searching for stable crystal structures across the entire composition space in multicomponent systems remains a significant challenge. Here, we propose a novel genetic algorithm (GA) -based CSP method using a universal NNP. Our GA-based methods are designed to efficiently expand convex hull volumes while preserving the diversity of crystal structures. This approach draws inspiration from the similarity between convex hull updates and Pareto front evolution in multi-objective optimization. Our evaluation shows that the present method outperforms the symmetry-aware random structure generation, achieving a larger convex hull with fewer trials. We demonstrated that our approach, combined with the developed universal NNP (PFP), can accurately reproduce and explore phase diagrams obtained through DFT calculations; this indicates the validity of PFP across a wide range of crystal structures and element combinations. This study, which integrates a universal NNP with a GA-based CSP method, highlights the promise of these methods in materials discovery.|\n", "2503.21196": "|**2025-03-27**|**Reaction Dynamics of the H + HeH$^+$ $\\rightarrow$ He + H$_2^+$ System**|Meenu Upadhyay, Silvan K\u00e4ser, Jayakrushna Sahoo et.al.|[2503.21196](http://arxiv.org/abs/2503.21196)|null||The reaction dynamics for the H + HeH$^+$ $\\rightarrow$ He + H$_2^+$ reaction in its electronic ground state is investigated using two different representations of the potential energy surface (PES). The first uses a combined kernel and neural network representation of UCCSD(T) reference data whereas the second is a corrected PES (cR-PES) that eliminates an artificial barrier in the entrance channel appearing in its initial expansion based on full configuration interaction reference data. Despite the differences between the two PESs, both yield $k_{v=0,j=0} \\approx 2 \\times 10^{-9}$ cm$^3$/molecule/s at $T = 10$ K which is consistent with a $T-$independent Langevin rate $k_{\\rm L} = 2.1 \\times 10^{-9}$ cm$^3$/molecule/s but considerably larger than the only experimentally reported value $k_{\\rm ICR} = (9.1 \\pm 2.5) \\times 10^{-10}$ cm$^3$/molecule/s from ion cyclotron resonance experiments. Similarly, branching ratios for the reaction outcomes are comparable for the two PESs. However, when analysing less averaged properties such as initial state-selected $T-$dependent rate coefficients and final vibrational states of the H$_2^+$ product for low temperatures, the differences in the two PESs manifest themselves in the observables. Thus, depending on the property analyzed, accurate and globally valid representations of the PES are required, whereas more approximate and empirical construction schemes can be followed for state-averaged observables.|\n", "2503.21182": "|**2025-03-27**|**Optimal Transportation for the Far-field Reflector Problem**|Gang Bao, Yixuan Zhang et.al.|[2503.21182](http://arxiv.org/abs/2503.21182)|null||The inverse reflector problem aims to design a freeform reflecting surface that can direct the light from a specified source to produce the desired illumination in the target area, which is significant in the field of geometrical non-imaging optics. Mathematically, it can be formulated as an optimization problem, which is exactly the optimal transportation problem (OT) when the target is in the far field. The gradient of OT is governed by the generalized Monge-Amp`ere equation that models the far-field reflector system. Based on the gradient, this work presents a Sobolev gradient descent method implemented within a finite element framework to solve the corresponding OT. Convergence of the method is established and numerical examples are provided to demonstrate the effectiveness of the method.|\n", "2503.21176": "|**2025-03-27**|**GPU-Accelerated Charge-Equilibration for Shadow Molecular Dynamics in Python**|Mehmet Cagri Kaymak, Nicholas Lubbers, Christian F. A. Negre et.al.|[2503.21176](http://arxiv.org/abs/2503.21176)|null||With recent advancements in machine learning for interatomic potentials, Python has become the go-to programming language for exploring new ideas. While machine-learning potentials are often developed in Python-based frameworks, existing molecular dynamics software is predominantly written in lower-level languages. This disparity complicates the integration of machine learning potentials into these molecular dynamics libraries. Additionally, machine learning potentials typically focus on local features, often neglecting long-range electrostatics due to computational complexities. This is a key limitation as applications can require long-range electrostatics and even flexible charges to achieve the desired accuracy. Recent charge equilibration models can address these issues, but they require iterative solvers to assign relaxed flexible charges to the atoms. Conventional implementations also demand very tight convergence to achieve long-term stability, further increasing computational cost. In this work, we present a scalable Python implementation of a recently proposed shadow molecular dynamics scheme based on a charge equilibration model, which avoids the convergence problem while maintaining long-term energy stability and accuracy of observable properties. To deliver a functional and user-friendly Python-based library, we implemented an efficient neighbor list algorithm, Particle Mesh Ewald, and traditional Ewald summation techniques, leveraging the GPU-accelerated power of Triton and PyTorch. We integrated these approaches with the Python-based shadow molecular dynamics scheme, enabling fast charge equilibration for scalable machine learning potentials involving systems with hundreds of thousands of atoms.|\n", "2503.21103": "|**2025-03-27**|**Low Stein Discrepancy via Message-Passing Monte Carlo**|Nathan Kirk, T. Konstantin Rusch, Jakob Zech et.al.|[2503.21103](http://arxiv.org/abs/2503.21103)|null|8 pages, 2 figures, Accepted at the ICLR 2025 Workshop on Frontiers   in Probabilistic Inference|Message-Passing Monte Carlo (MPMC) was recently introduced as a novel low-discrepancy sampling approach leveraging tools from geometric deep learning. While originally designed for generating uniform point sets, we extend this framework to sample from general multivariate probability distributions with known probability density function. Our proposed method, Stein-Message-Passing Monte Carlo (Stein-MPMC), minimizes a kernelized Stein discrepancy, ensuring improved sample quality. Finally, we show that Stein-MPMC outperforms competing methods, such as Stein Variational Gradient Descent and (greedy) Stein Points, by achieving a lower Stein discrepancy.|\n", "2503.21078": "|**2025-03-27**|**Sub-ODEs Simplify Taylor Series Algorithms for Ordinary Differential Equations**|Nedialko S. Nedialkov, John D. Pryce et.al.|[2503.21078](http://arxiv.org/abs/2503.21078)|null|25 pages|A Taylor method for solving an ordinary differential equation initial-value problem $\\dot x = f(t,x)$, $x(t_0) = x_0$, computes the Taylor series (TS) of the solution at the current point, truncated to some order, and then advances to the next point by summing the TS with a suitable step size.   A standard ODE method (e.g. Runge-Kutta) treats function $f$ as a black box, but a Taylor solver requires $f$ to be preprocessed into a code-list of elementary operations that it interprets as operations on (truncated) TS.   The trade-off for this extra work includes arbitrary order, typically enabling much larger step sizes.   For a standard function, such as $\\exp$, this means evaluating $v(t)=\\exp(u(t))$, where $u(t),v(t)$ are TS.   The sub-ODE method applies the ODE $d v/d u=v$, obeyed by $v=\\exp(u)$, to in-line this operation as $\\dot v=v\\dot u$.   This gives economy of implementation: each function that satisfies a simple ODE goes into the \"Taylor library\" with a few lines of code--not needing a separate recurrence relation, which is the typical approach.   Mathematically, however, the use of sub-ODEs generally transforms the original ODE into a differential-algebraic system, making it nontrivial to ensure a sound system of recurrences for Taylor coefficients.   We prove that, regardless of how many sub-ODEs are incorporated into $f$, this approach guarantees a sound system.   We introduce our sub-ODE-based Matlab ODE solver and show that its performance compares favorably with solvers from the Matlab ODE suite.|\n", "2503.21037": "|**2025-03-26**|**Optimal Rejection-Free Path Sampling**|Gianmarco Lazzeri, Peter G. Bolhuis, Roberto Covino et.al.|[2503.21037](http://arxiv.org/abs/2503.21037)|null||We propose an efficient novel path sampling-based framework designed to accelerate the investigation of rare events in complex molecular systems. A key innovation is the shift from sampling restricted path ensemble distributions, as in transition path sampling, to directly sampling the distribution of shooting points. This allows for a rejection-free algorithm that samples the entire path ensemble efficiently. Optimal sampling is achieved by applying a selection bias that is the inverse of the free energy along a reaction coordinate. The optimal reaction coordinate, the committor, is iteratively constructed as a neural network using AI for Molecular Mechanism Discovery (AIMMD), concurrently with the free energy profile, which is obtained through reweighting the sampled path ensembles. We showcase our algorithm on theoretical and molecular bechnmarks, and demonstrate how it provides at the same time molecular mechanism, free energy, and rates at a moderate computational cost.|\n", "2503.22652": "|**2025-03-28**|**Residual-based Chebyshev filtered subspace iteration for sparse Hermitian eigenvalue problems tolerant to inexact matrix-vector products**|Nikhil Kodali, Kartick Ramakrishnan, Phani Motamarri et.al.|[2503.22652](http://arxiv.org/abs/2503.22652)|null|32 Pages, 12 Figures, 1 Table|Chebyshev Filtered Subspace Iteration (ChFSI) has been widely adopted for computing a small subset of extreme eigenvalues in large sparse matrices. This work introduces a residual-based reformulation of ChFSI, referred to as R-ChFSI, designed to accommodate inexact matrix-vector products while maintaining robust convergence properties. By reformulating the traditional Chebyshev recurrence to operate on residuals rather than eigenvector estimates, the R-ChFSI approach effectively suppresses the errors made in matrix-vector products, improving the convergence behaviour for both standard and generalized eigenproblems. This ability of R-ChFSI to be tolerant to inexact matrix-vector products allows one to incorporate approximate inverses for large-scale generalized eigenproblems, making the method particularly attractive where exact matrix factorizations or iterative methods become computationally expensive for evaluating inverses. It also allows us to compute the matrix-vector products in lower-precision arithmetic allowing us to leverage modern hardware accelerators. Through extensive benchmarking, we demonstrate that R-ChFSI achieves desired residual tolerances while leveraging low-precision arithmetic. For problems with millions of degrees of freedom and thousands of eigenvalues, R-ChFSI attains final residual norms in the range of 10$^{-12}$ to 10$^{-14}$, even with FP32 and TF32 arithmetic, significantly outperforming standard ChFSI in similar settings. In generalized eigenproblems, where approximate inverses are used, R-ChFSI achieves residual tolerances up to ten orders of magnitude lower, demonstrating its robustness to approximation errors. Finally, R-ChFSI provides a scalable and computationally efficient alternative for solving large-scale eigenproblems in high-performance computing environments.|\n", "2503.22649": "|**2025-03-28**|**Stochastic reduced-order Koopman model for turbulent flows**|Tianyi Chu, Oliver T. Schmidt et.al.|[2503.22649](http://arxiv.org/abs/2503.22649)|null||A stochastic data-driven reduced-order model applicable to a wide range of turbulent natural and engineering flows is presented. Combining ideas from Koopman theory and spectral model order reduction, the stochastic low-dimensional inflated convolutional Koopman model (SLICK) accurately forecasts short-time transient dynamics while preserving long-term statistical properties. A discrete Koopman operator is used to evolve convolutional coordinates that govern the temporal dynamics of spectral orthogonal modes, which in turn represent the energetically most salient large-scale coherent flow structures. Turbulence closure is achieved in two steps: first, by inflating the convolutional coordinates to incorporate nonlinear interactions between different scales, and second, by modeling the residual error as a stochastic source. An empirical dewhitening filter informed by the data is used to maintain the second-order flow statistics within the long-time limit. The model uncertainty is quantified through either Monte Carlo simulation or by directly propagating the model covariance matrix. The model is demonstrated on the Ginzburg-Landau equations, large-eddy simulation (LES) data of a turbulent jet, and particle image velocimetry (PIV) data of the flow over an open cavity. In all cases, the model is predictive over time horizons indicated by a detailed error analysis and integrates stably over arbitrary time horizons, generating realistic surrogate data.|\n", "2503.22646": "|**2025-03-28**|**Finding Unknown Unknowns using Cyber-Physical System Simulators (Extended Report)**|Semaan Douglas Wehbe, Stanley Bak et.al.|[2503.22646](http://arxiv.org/abs/2503.22646)|null||Simulation-based approaches are among the most practical means to search for safety violations, bugs, and other unexpected events in cyber-physical systems (CPS). Where existing approaches search for simulations violating a formal specification or maximizing a notion of coverage, in this work we propose a new goal for testing: to discover unknown rare behaviors by examining discrete mode sequences. We assume a CPS simulator outputs mode information, and strive to explore the sequences of modes produced by varying the initial state or time-varying uncertainties. We hypothesize that rare mode sequences are often the most interesting to a designer, and we develop two accelerated sampling algorithms that speed up the process of finding such sequences. We evaluate our approach on several benchmarks, ranging from synthetic examples to Simulink diagrams of a CPS, demonstrating in some cases a speedup of over 100x compared with a random sampling strategy.|\n", "2503.22631": "|**2025-03-28**|**Accelerating a restarted Krylov method for matrix functions with randomization**|Nicolas L. Guidotti, Per-Gunnar Martinsson, Juan A. Acebr\u00f3n et.al.|[2503.22631](http://arxiv.org/abs/2503.22631)|null||Many scientific applications require the evaluation of the action of the matrix function over a vector and the most common methods for this task are those based on the Krylov subspace. Since the orthogonalization cost and memory requirement can quickly become overwhelming as the basis grows, the Krylov method is often restarted after a few iterations. This paper proposes a new acceleration technique for restarted Krylov methods based on randomization. The numerical experiments show that the randomized method greatly outperforms the classical approach with the same level of accuracy. In fact, randomization can actually improve the convergence rate of restarted methods in some cases. The paper also compares the performance and stability of the randomized methods proposed so far for solving very large finite element problems, complementing the numerical analyses from previous studies.|\n", "2503.22621": "|**2025-03-28**|**Improved error estimates for low-regularity integrators using space-time bounds**|Maximilian Ruff et.al.|[2503.22621](http://arxiv.org/abs/2503.22621)|null|14 pages|We prove optimal convergence rates for certain low-regularity integrators applied to the one-dimensional periodic nonlinear Schr\\\"odinger and wave equations under the assumption of $H^1$ solutions. For the Schr\\\"odinger equation we analyze the exponential-type scheme proposed by Ostermann and Schratz in 2018, whereas in the wave case we treat the corrected Lie splitting proposed by Li, Schratz, and Zivcovich in 2023. We show that the integrators converge with their full order of one and two, respectively. In this situation only fractional convergence rates were previously known. The crucial ingredients in the proofs are known space-time bounds for the solutions to the corresponding linear problems. More precisely, in the Schr\\\"odinger case we use the $L^4$ Strichartz inequality, and for the wave equation a null form estimate. To our knowledge, this is the first time that a null form estimate is exploited in numerical analysis. We apply the estimates for continuous time, thus avoiding potential losses resulting from discrete-time estimates.|\n", "2503.22604": "|**2025-03-28**|**Enhanced Variational Quantum Kolmogorov-Arnold Network**|Hikaru Wakaura, Rahmat Mulyawan, Andriyan B. Suksmono et.al.|[2503.22604](http://arxiv.org/abs/2503.22604)|null|arXiv admin note: substantial text overlap with arXiv:2503.21336|The Kolmogorov-Arnold Network (KAN) is a novel multi-layer network model recognized for its efficiency in neuromorphic computing, where synapses between neurons are trained linearly. Computations in KAN are performed by generating a polynomial vector from the state vector and layer-wise trained synapses, enabling efficient processing. While KAN can be implemented on quantum computers using block encoding and Quantum Signal Processing, these methods require fault-tolerant quantum devices, making them impractical for current Noisy Intermediate-Scale Quantum (NISQ) hardware. We propose the Enhanced Variational Quantum Kolmogorov-Arnold Network (EVQKAN) to overcome this limitation, which emulates KAN through variational quantum algorithms. The EVQKAN ansatz employs a tiling technique to emulate layer matrices, leading to significantly higher accuracy compared to conventional Variational Quantum Kolmogorov-Arnold Network (VQKAN) and Quantum Neural Networks (QNN), even with a smaller number of layers. EVQKAN achieves superior performance with a single-layer architecture, whereas QNN and VQKAN typically struggle. Additionally, EVQKAN eliminates the need for Quantum Signal Processing, enhancing its robustness to noise and making it well-suited for practical deployment on NISQ-era quantum devices.|\n", "2503.22596": "|**2025-03-28**|**Pressure-temperature phase diagram calculations using polynomial machine learning potentials: A comprehensive study based on global structure prediction and self-consistent phonon calculations**|Hayato Wakai, Atsuto Seko, Isao Tanaka et.al.|[2503.22596](http://arxiv.org/abs/2503.22596)|null|REVTeX 4-2, 18 pages, 16 figures|Polynomial machine learning potentials (MLPs) based on polynomial rotational invariants have been systematically developed for various systems and applied to efficiently predict crystal structures. In this study, we propose a robust methodology founded on polynomial MLPs to comprehensively enumerate crystal structures under high-pressure conditions and to evaluate their phase stability at finite temperatures. The proposed approach involves constructing polynomial MLPs with high predictive accuracy across a broad range of pressures, conducting reliable global structure searches, and performing exhaustive self-consistent phonon calculations. We demonstrate the effectiveness of this approach by examining elemental silicon at pressures up to 100 GPa and temperatures up to 1000 K, revealing stable phases across these conditions. The framework established in this study offers a powerful strategy for predicting crystal structures and phase stability under high-pressure and finite-temperature conditions.|\n", "2503.22528": "|**2025-03-28**|**MixFunn: A Neural Network for Differential Equations with Improved Generalization and Interpretability**|Tiago de Souza Farias, Gubio Gomes de Lima, Jonas Maziero et.al.|[2503.22528](http://arxiv.org/abs/2503.22528)|**[link](https://github.com/tiago939/MixFunn)**|21 pages|We introduce MixFunn, a novel neural network architecture designed to solve differential equations with enhanced precision, interpretability, and generalization capability. The architecture comprises two key components: the mixed-function neuron, which integrates multiple parameterized nonlinear functions to improve representational flexibility, and the second-order neuron, which combines a linear transformation of its inputs with a quadratic term to capture cross-combinations of input variables. These features significantly enhance the expressive power of the network, enabling it to achieve comparable or superior results with drastically fewer parameters and a reduction of up to four orders of magnitude compared to conventional approaches. We applied MixFunn in a physics-informed setting to solve differential equations in classical mechanics, quantum mechanics, and fluid dynamics, demonstrating its effectiveness in achieving higher accuracy and improved generalization to regions outside the training domain relative to standard machine learning models. Furthermore, the architecture facilitates the extraction of interpretable analytical expressions, offering valuable insights into the underlying solutions.|\n", "2503.22481": "|**2025-03-28**|**Charge creation via quantum tunneling in one-dimensional Mott insulators: A numerical study of the extended Hubbard model**|Thomas Hansen, Lars Bojer Madsen, Yuta Murakami et.al.|[2503.22481](http://arxiv.org/abs/2503.22481)|null|14 pages including bibliography and the appendix (11 pages without   them), 8 figures in the main text and 1 in the appendix for a total of 9   figures, and 2 tables in the main text|Charge creation via quantum tunneling, i.e. dielectric breakdown, is one of the most fundamental and significant phenomena arising from strong light(field)-matter coupling. In this work, we conduct a systematic numerical analysis of quantum tunneling in one-dimensional Mott insulators described by the extended ($U$-$V$) Hubbard model. We discuss the applicability of the analytical formula for doublon-holon (DH) pair production, previously derived for the one-dimensional Hubbard model, which highlights the relationship between the tunneling threshold, the charge gap, and the correlation length. We test the formulas ability to predict both DH pair production and energy increase rate. Using tensor-network-based approaches, we demonstrate that the formula provides accurate predictions in the absence of excitonic states facilitated by the nearest-neighbor interaction $V$. However, when excitonic states emerge, the formula more accurately describes the rate of energy increase than the DH pair creation rate and in both cases gets improved by incorporating the exciton energy as the effective gap.|\n", "2503.22476": "|**2025-03-28**|**The 2D Materials Roadmap**|Wencai Ren, Peter B\u00f8ggild, Joan Redwing et.al.|[2503.22476](http://arxiv.org/abs/2503.22476)|null|104 pages, to be published in 2D Materials|Over the past two decades, 2D materials have rapidly evolved into a diverse and expanding family of material platforms. Many members of this materials class have demonstrated their potential to deliver transformative impact on fundamental research and technological applications across different fields. In this roadmap, we provide an overview of the key aspects of 2D material research and development, spanning synthesis, properties and commercial applications. We specifically present roadmaps for high impact 2D materials, including graphene and its derivatives, transition metal dichalcogenides, MXenes as well as their heterostructures and moir\\'e systems. The discussions are organized into thematic sections covering emerging research areas (e.g., twisted electronics, moir\\'e nano-optoelectronics, polaritronics, quantum photonics, and neuromorphic computing), breakthrough applications in key technologies (e.g., 2D transistors, energy storage, electrocatalysis, filtration and separation, thermal management, flexible electronics, sensing, electromagnetic interference shielding, and composites) and other important topics (computational discovery of novel materials, commercialization and standardization). This roadmap focuses on the current research landscape, future challenges and scientific and technological advances required to address, with the intent to provide useful references for promoting the development of 2D materials.|\n", "2503.22455": "|**2025-03-28**|**A high order multigrid-preconditioned immersed interface solver for the Poisson equation with boundary and interface conditions**|James Gabbard, Andrea Paris, Wim M. van Rees et.al.|[2503.22455](http://arxiv.org/abs/2503.22455)|null||This work presents a multigrid preconditioned high order immersed finite difference solver to accurately and efficiently solve the Poisson equation on complex 2D and 3D domains. The solver employs a low order Shortley-Weller multigrid method to precondition a high order matrix-free Krylov subspace solver. The matrix-free approach enables full compatibility with high order IIM discretizations of boundary and interface conditions, as well as high order wavelet-adapted multiresolution grids. Through verification and analysis on 2D domains, we demonstrate the ability of the algorithm to provide high order accurate results to Laplace and Poisson problems with Dirichlet, Neumann, and/or interface jump boundary conditions, all effectively preconditioned using the multigrid method. We further show that the proposed method is able to efficiently solve high order discretizations of Laplace and Poisson problems on complex 3D domains using thousands of compute cores and on multiresolution grids. To our knowledge, this work presents the largest problem sizes tackled with high order immersed methods applied to elliptic partial differential equations, and the first high order results on 3D multiresolution adaptive grids. Together, this work paves the way for employing high order immersed methods to a variety of 3D partial differential equations with boundary or inter-face conditions, including linear and non-linear elasticity problems, the incompressible Navier-Stokes equations, and fluid-structure interactions.|\n", "2503.22386": "|**2025-03-28**|**Spectral coefficient learning physics informed neural network for time-dependent fractional parametric differential problems**|S M Sivalingam, V Govindaraj, A. S. Hendy et.al.|[2503.22386](http://arxiv.org/abs/2503.22386)|null||The study of parametric differential equations plays a crucial role in weather forecasting and epidemiological modeling. These phenomena are better represented using fractional derivatives due to their inherent memory or hereditary effects. This paper introduces a novel scientific machine learning approach for solving parametric time-fractional differential equations by combining traditional spectral methods with neural networks. Instead of relying on automatic differentiation techniques, commonly used in traditional Physics-Informed Neural Networks (PINNs), we propose a more efficient global discretization method based on Legendre polynomials. This approach eliminates the need to simulate the parametric fractional differential equations across multiple parameter values. By applying the Legendre-Galerkin weak formulation to the differential equation, we construct a loss function for training the neural network. The trial solutions are represented as linear combinations of Legendre polynomials, with the coefficients learned by the neural network. The convergence of this method is theoretically established, and the theoretical results are validated through numerical experiments on several well-known differential equations.|\n", "2503.22372": "|**2025-03-28**|**A Morphotropic Phase Boundary in MA$_{1-x}$FA$_x$PbI$_3$: Linking Structure, Dynamics, and Electronic Properties**|Tobias Hainer, Erik Fransson, Sangita Dutta et.al.|[2503.22372](http://arxiv.org/abs/2503.22372)|null|11 pages, 6 figures|Understanding the phase behavior of mixed-cation halide perovskites is critical for optimizing their structural stability and optoelectronic performance. Here, we map the phase diagram of MA$_{1-x}$FA$_x$PbI$_3$ using a machine-learned interatomic potential in molecular dynamics simulations. We identify a morphotropic phase boundary (MPB) at approximately 27% FA content, delineating the transition between out-of-phase and in-phase octahedral tilt patterns. Phonon mode projections reveal that this transition coincides with a mode crossover composition, where the free energy landscapes of the M and R phonon modes become nearly degenerate. This results in nanoscale layered structures with alternating tilt patterns, suggesting minimal interface energy between competing phases. Our results provide a systematic and consistent description of this important system, complementing earlier partial and sometimes conflicting experimental assessments. Furthermore, density functional theory calculations show that band edge fluctuations peak near the MPB, indicating an enhancement of electron-phonon coupling and dynamic disorder effects. These findings establish a direct link between phonon dynamics, phase behavior, and electronic structure, providing a further composition-driven pathway for tailoring the optoelectronic properties of perovskite materials. By demonstrating that phonon overdamping serves as a hallmark of the MPB, our study offers new insights into the design principles for stable, high-performance perovskite solar cells.|\n", "2503.22360": "|**2025-03-28**|**Improvement of conformal maps combined with the Sinc approximation for derivatives over infinite intervals**|Tomoaki Okayama, Yuito Kuwashita, Ao Kondo et.al.|[2503.22360](http://arxiv.org/abs/2503.22360)|null|Keywords: Sinc approximation, single-exponential transformation,   numerical differentiation|F. Stenger proposed efficient approximation formulas for derivatives over infinite intervals. Those formulas were derived by the combination of the Sinc approximation and appropriate conformal maps. It has been shown that those formulas can attain root-exponential convergence. In this study, we enhance the convergence rate by improving the conformal maps employed in those formulas. We provide theoretical error analysis and numerical experiments that confirm the effectiveness of our new formulas.|\n", "2503.22335": "|**2025-03-28**|**A numerical Bernstein splines approach for nonlinear initial value problems with Hilfer fractional derivative**|Niels Goedegebure, Kateryna Marynets et.al.|[2503.22335](http://arxiv.org/abs/2503.22335)|null||The Hilfer fractional derivative interpolates the commonly used Riemann-Liouville and Caputo fractional derivative. In general, solutions to Hilfer fractional differential equations are singular for $t \\downarrow 0$ and are difficult to approximate with high numerical accuracy. We propose a numerical Bernstein splines technique to approximate solutions to generalized nonlinear initial values problems with Hilfer fractional derivatives. Convergent approximations are obtained using an efficient vectorized solution setup with few convergence requirements for a wide range of nonlinear fractional differential equations. We demonstrate efficiency of the developed method by applying it to the fractional Van der Pol oscillator, a system with applications in control systems and electronic circuits.|\n", "2503.22301": "|**2025-03-28**|**Approximation results on neural network operators of convolution type**|Asiye Arif, Tu\u011fba Yurdakadim et.al.|[2503.22301](http://arxiv.org/abs/2503.22301)|null|30 pages, no figures|In the present paper, we introduce three neural network operators of convolution type activated by symmetrized, deformed and parametrized B-generalized logistic function. We deal with the approximation properties of these operators to the identity by using modulus of continuity. Furthermore, we show that our operators preserve global smoothness and consider the iterated versions of them. Here, we find it is worthy to mention that these operators play important roles in neural network approximation since most of the basic network models are activated by logistic functions.|\n", "2503.22297": "|**2025-03-28**|**A posteriori error estimates for the finite element discretization of second-order PDEs set in unbounded domains**|T. Chaumont-Frelet et.al.|[2503.22297](http://arxiv.org/abs/2503.22297)|null||We consider second-order PDE problems set in unbounded domains and discretized by Lagrange finite elements on a finite mesh, thus introducing an artificial boundary in the discretization. Specifically, we consider the reaction diffusion equation as well as Helmholtz problems in waveguides with perfectly matched layers. The usual procedure to deal with such problems is to first consider a modeling error due to the introduction of the artificial boundary, and estimate the remaining discretization error with a standard a posteriori technique. A shortcoming of this method, however, is that it is typically hard to obtain sharp bounds on the modeling error. In this work, we propose a new technique that allows to control the whole error by an a posteriori error estimator. Specifically, we propose a flux-equilibrated estimator that is slightly modified to handle the truncation boundary. For the reaction diffusion equation, we obtain fully-computable guaranteed error bounds, and the estimator is locally efficient and polynomial-degree-robust provided that the elements touching the truncation boundary are not too refined. This last condition may be seen as an extension of the notion of shape-regularity of the mesh, and does not prevent the design of efficient adaptive algorithms. For the Helmholtz problem, as usual, these statements remain valid if the mesh is sufficiently refined. Our theoretical findings are completed with numerical examples which indicate that the estimator is suited to drive optimal adaptive mesh refinements.|\n", "2503.22286": "|**2025-03-28**|**Connecting Kaporin's condition number and the Bregman log determinant divergence**|Andreas A. Bock, Martin S. Andersen et.al.|[2503.22286](http://arxiv.org/abs/2503.22286)|null|14 pages|This paper presents some theoretical results relating the Bregman log determinant matrix divergence to Kaporin's condition number. These can be viewed as nearness measures between a preconditioner and a given matrix, and we show under which conditions these two functions coincide. We also give examples of constraint sets over which it is equivalent to minimise these two objectives. We focus on preconditioners that are the sum of a positive definite and low-rank matrix, which were developed in a previous work. These were constructed as minimisers of the aforementioned divergence, and we show that they are only a constant scaling from also minimising Kaporin's condition number. We highlight connections to information geometry and comment on future directions.|\n", "2503.22273": "|**2025-03-28**|**General form of the Gauss-Seidel equation to linearly approximate the Moore-Penrose pseudoinverse in random non-square systems and high order tensors**|Luis Saucedo-Mora, Luis Irastorza-Valera et.al.|[2503.22273](http://arxiv.org/abs/2503.22273)|null||The Gauss-Seidel method has been used for more than 100 years as the standard method for the solution of linear systems of equations under certain restrictions. This method, as well as Cramer and Jacobi, is widely used in education and engineering, but there is a theoretical gap when we want to solve less restricted systems, or even non-square or non-exact systems of equation. Here, the solution goes through the use of numerical systems, such as the minimization theories or the Moore-Penrose pseudoinverse. In this paper we fill this gap with a global analytical iterative formulation that is capable to reach the solutions obtained with the Moore-Penrose pseudoinverse and the minimization methodologies, but that analytically lies to the solutions of Gauss-Seidel, Jacobi, or Cramer when the system is simplified.|\n", "2503.22246": "|**2025-03-28**|**Lee-Yang zeros in heavy-quark QCD**|Masakiyo Kitazawa, Tatsuya Wada, Kazuyuki Kanaya et.al.|[2503.22246](http://arxiv.org/abs/2503.22246)|null|8 pages, 6 figures; contribution to the Proceedings of QCHSC24|We explore the distribution of Lee-Yang zeros around the critical point that appears in the heavy-quark region of QCD at nonzero temperature in lattice numerical simulations. With the aid of the hopping-parameter expansion that is well justified around the critical point in our setting, our numerical analysis is capable of analyzing the partition function for complex parameters with high accuracy. This enables precise analyses of the Lee-Yang zeros around the critical point. We study their finite-size scaling around the critical point. We also propose new methods to utilize the scaling behavior of the Lee-Yang zeros for fixing the location of the critical point.|\n", "2503.24286": "|**2025-03-31**|**Pyrometheus: Symbolic abstractions for XPU and automatically differentiated computation of combustion kinetics and thermodynamics**|Esteban Cisneros-Garibay, Henry Le Berre, Spencer H. Bryngelson et.al.|[2503.24286](http://arxiv.org/abs/2503.24286)|**[link](https://github.com/pyrometheus/pyrometheus)**|41 pages, 12 figures, 29 listings, 60 references|The cost of combustion simulations is often dominated by the evaluation of net production rates of chemical species and mixture thermodynamics (thermochemistry). Execution on computing accelerators (XPUs) like graphic processing units (GPUs) can greatly reduce this cost. However, established thermochemistry software is not readily portable to such devices or sacrifices valuable analytical forms that enable differentiation for sensitivity analysis and implicit time integration. Symbolic abstractions are developed with corresponding transformations that enable computation on accelerators and automatic differentiation by avoiding premature specification of detail. The software package Pyrometheus is introduced as an implementation of these abstractions and their transformations for combustion thermochemistry. The formulation facilitates code generation from the symbolic representation of a specific thermochemical mechanism in multiple target languages, including Python, C++, and Fortran. Computational concerns are separated: the generated code processes array-valued expressions but does not specify their semantics. These semantics are provided by compatible array libraries, such as NumPy, Pytato, and Google JAX. Thus, the generated code retains a symbolic representation of the thermochemistry, which translates to computation on accelerators and CPUs and automatic differentiation. The design and operation of these symbolic abstractions and their companion tool, Pyrometheus, are discussed throughout. Roofline demonstrations show that the computation of chemical source terms within MFC, a Fortran-based flow solver we link to Pyrometheus, is performant.|\n", "2503.24234": "|**2025-03-31**|**Beyond Gaussian Assumptions: A Nonlinear Generalization of Linear Inverse Modeling**|Justin Lien, Hiroyasu Ando et.al.|[2503.24234](http://arxiv.org/abs/2503.24234)|null||The Linear Inverse Model (LIM) is a class of data-driven methods that construct approximate linear stochastic models to represent complex observational data. The stochastic forcing can be modeled using either Gaussian white noise or Ornstein-Uhlenbeck colored noise; the corresponding models are called White-LIM and Colored-LIM, respectively. Although LIMs are widely applied in climate sciences, they inherently approximate observed distributions as Gaussian, limiting their ability to capture asymmetries.   In this study, we extend LIMs to incorporate nonlinear dynamics, introducing White-nLIM and Colored-nLIM which allow for a more flexible and accurate representation of complex dynamics from observations. The proposed methods not only account for the nonlinear nature of the underlying system but also effectively capture the skewness of the observed distribution. Moreover, we apply these methods to a lower-dimensional representation of ENSO and demonstrate that both White-nLIM and Colored-nLIM successfully capture its nonlinear characteristic.|\n", "2503.24232": "|**2025-03-31**|**Polynomial Inequalities and Optimal Stability of Numerical Integrators**|Luke Shaw et.al.|[2503.24232](http://arxiv.org/abs/2503.24232)|null||A numerical integrator for $\\dot{x}=f(x)$ is called \\emph{stable} if, when applied to the 1D Dahlquist test equation $\\dot{x}=\\lambda x,\\lambda\\in\\mathbb{C}$ with fixed timestep $h>0$, the numerical solution remains bounded as the number of steps tends to infinity. It is well known that no explicit integrator may remain stable beyond certain limits in $\\lambda$. Furthermore, these stability limits are only tight for certain specific integrators (different in each case), which may then be called `optimally stable'. Such optimal stability results are typically proven using sophisticated techniques from complex analysis, leading to rather abstruse proofs. In this article, we pursue an alternative approach, exploiting connections with the Bernstein and Markov brothers inequalities for polynomials. This simplifies the proofs greatly and offers a framework which unifies the diverse results that have been obtained.|\n", "2503.24208": "|**2025-03-31**|**Data-driven construction of a generalized kinetic collision operator from molecular dynamics**|Yue Zhao, William Burby, Andrew Christlieb et.al.|[2503.24208](http://arxiv.org/abs/2503.24208)|null||We introduce a data-driven approach to learn a generalized kinetic collision operator directly from molecular dynamics. Unlike the conventional (e.g., Landau) models, the present operator takes an anisotropic form that accounts for a second energy transfer arising from the collective interactions between the pair of collision particles and the environment. Numerical results show that preserving the broadly overlooked anisotropic nature of the collision energy transfer is crucial for predicting the plasma kinetics with non-negligible correlations, where the Landau model shows limitations.|\n", "2503.24195": "|**2025-03-31**|**Computational Orthodontic Force Simulation: A Review**|Waheed Ahmad, Jing Xiong, Zeyang Xia et.al.|[2503.24195](http://arxiv.org/abs/2503.24195)|null|19 pages, 4 figure, 1 table|In orthodontic treatment, the biological response of the tooth, periodontal ligament, and bone complex to orthodontic force is crucial in influencing treatment outcomes. The challenge lies in accurately measuring, estimating, and predicting these forces during clinical procedures. This review aims to fill the gap in the literature by systematically summarizing existing research on orthodontic force simulation, examining common loading techniques and technologies, and discussing the potential for refining the orthodontic force simulation process. The literature was comprehensively reviewed, with an emphasis on the exploration of the biological mechanism of tooth movement. Studies were categorized based on force-loading techniques for both fixed and invisible orthodontic appliances. Finite element (FE) analysis stands out as the predominant technique for orthodontic force simulation, with a significant focus on fixed orthodontics but limited emphasis on invisible orthodontics. Current orthodontic force simulations tend to be fragmented, often considering only the instantaneous response to applied forces. There exists an urgent demand for a sophisticated analytical simulation model. Such a model, possibly leveraging advanced technologies like deep learning, holds the promise of forecasting orthodontic treatment outcomes with heightened precision and efficiency.|\n", "2503.24131": "|**2025-03-31**|**A simple and general framework for the construction of exactly div-curl-grad compatible discontinuous Galerkin finite element schemes on unstructured simplex meshes**|R. Abgrall, M. Dumbser, P. H. Maire et.al.|[2503.24131](http://arxiv.org/abs/2503.24131)|null||We introduce a new family of discontinuous Galerkin (DG) finite element schemes for the discretization of first order systems of hyperbolic partial differential equations (PDE) on unstructured simplex meshes in two and three space dimensions that respect the two basic vector calculus identities exactly also at the discrete level, namely that the curl of the gradient is zero and that the divergence of the curl is zero. The key ingredient here is the construction of two compatible discrete nabla operators, a primary one and a dual one, both defined on general unstructured simplex meshes in multiple space dimensions. Our new schemes extend existing cell-centered finite volume methods based on corner fluxes to arbitrary high order of accuracy in space. An important feature of our new method is the fact that only two different discrete function spaces are needed to represent the numerical solution, and the choice of the appropriate function space for each variable is related to the origin and nature of the underlying PDE. The first class of variables is discretized at the aid of a discontinuous Galerkin approach, where the numerical solution is represented via piecewise polynomials of degree N and which are allowed to jump across element interfaces. This set of variables is related to those PDE which are mere consequences of the definitions, derived from some abstract scalar and vector potentials, and for which involutions like the divergence-free or the curl-free property must hold if satisfied by the initial data. The second class of variables is discretized via classical continuous Lagrange finite elements of approximation degree M=N+1 and is related to those PDE which can be derived as the Euler-Lagrange equations of an underlying variational principle.|\n", "2503.24074": "|**2025-03-31**|**Physics-informed neural networks for hidden boundary detection and flow field reconstruction**|Yongzheng Zhu, Weizheng Chen, Jian Deng et.al.|[2503.24074](http://arxiv.org/abs/2503.24074)|null|21 pages, 17 figures|Simultaneously detecting hidden solid boundaries and reconstructing flow fields from sparse observations poses a significant inverse challenge in fluid mechanics. This study presents a physics-informed neural network (PINN) framework designed to infer the presence, shape, and motion of static or moving solid boundaries within a flow field. By integrating a body fraction parameter into the governing equations, the model enforces no-slip/no-penetration boundary conditions in solid regions while preserving conservation laws of fluid dynamics. Using partial flow field data, the method simultaneously reconstructs the unknown flow field and infers the body fraction distribution, thereby revealing solid boundaries. The framework is validated across diverse scenarios, including incompressible Navier-Stokes and compressible Euler flows, such as steady flow past a fixed cylinder, an inline oscillating cylinder, and subsonic flow over an airfoil. The results demonstrate accurate detection of hidden boundaries, reconstruction of missing flow data, and estimation of trajectories and velocities of a moving body. Further analysis examines the effects of data sparsity, velocity-only measurements, and noise on inference accuracy. The proposed method exhibits robustness and versatility, highlighting its potential for applications when only limited experimental or numerical data are available.|\n", "2503.24069": "|**2025-03-31**|**Impact of Amplitude and Phase Damping Noise on Quantum Reinforcement Learning: Challenges and Opportunities**|Mar\u00eda Laura Olivera-Atencio, Lucas Lamata, Jes\u00fas Casado-Pascual et.al.|[2503.24069](http://arxiv.org/abs/2503.24069)|null|11 pages, 3 figures|Quantum machine learning (QML) is an emerging field with significant potential, yet it remains highly susceptible to noise, which poses a major challenge to its practical implementation. While various noise mitigation strategies have been proposed to enhance algorithmic performance, the impact of noise is not fully understood. In this work, we investigate the effects of amplitude and phase damping noise on a quantum reinforcement learning algorithm. Through analytical and numerical analysis, we assess how these noise sources influence the learning process and overall performance. Our findings contribute to a deeper understanding of the role of noise in quantum learning algorithms and suggest that, rather than being purely detrimental, unavoidable noise may present opportunities to enhance QML processes.|\n", "2503.24050": "|**2025-04-01**|**A Deep Learning Framework for the Electronic Structure of Water: Towards a Universal Model**|Xinyuan Liang, Renxi Liu, Mohan Chen et.al.|[2503.24050](http://arxiv.org/abs/2503.24050)|null||Accurately modeling the electronic structure of water across scales, from individual molecules to bulk liquid, remains a grand challenge. Traditional computational methods face a critical trade-off between computational cost and efficiency. We present an enhanced machine-learning Deep Kohn-Sham (DeePKS) method for improved electronic structure, DeePKS-ES, that overcomes this dilemma. By incorporating the Hamiltonian matrix and their eigenvalues and eigenvectors into the loss function, we establish a universal model for water systems, which can reproduce high-level hybrid functional (HSE06) electronic properties from inexpensive generalized gradient approximation (PBE) calculations. Validated across molecular clusters and liquid-phase simulations, our approach reliably predicts key electronic structure properties such as band gaps and density of states, as well as total energy and atomic forces. This work bridges quantum-mechanical precision with scalable computation, offering transformative opportunities for modeling aqueous systems in catalysis, climate science, and energy storage.|\n", "2503.24005": "|**2025-03-31**|**Attraction of a jerk**|Sini Peltonen, Laura Vasko, Inka Tenhunen et.al.|[2503.24005](http://arxiv.org/abs/2503.24005)|null|Prepared for April Fools' Day. 5 pages|Jerk plays a pivotal role in the thrilling experience of many amusemement park rides. In addition to exploring the physical aspect of jerks, we tackle the empirical observation of an attractive force between passengers in the popular attraction, the spinning teacups. By modeling the complex system of rotating platforms, we show that pseudotorques induced by changing acceleration lead to jerky movements and an attractive interaction among riders. Our numerical analysis confirms the empirical observations, highlighting the connection between attraction and jerks.|\n", "2503.24001": "|**2025-03-31**|**Convergence of a finite volume scheme for a model for ants**|Maria Bruna, Markus Schmidtchen, Oscar de Wit et.al.|[2503.24001](http://arxiv.org/abs/2503.24001)|null||We develop and analyse a finite volume scheme for a nonlocal active matter system known to exhibit a rich array of complex behaviours. The model under investigation was derived from a stochastic system of interacting particles describing a foraging ant colony coupled to pheromone dynamics. In this work, we prove that the unique numerical solution converges to the unique weak solution as the mesh size and the time step go to zero. We also show discrete long-time estimates, which prove that certain norms are preserved for all times, uniformly in the mesh size and time step. In particular, we prove higher regularity estimates which provide an analogue of continuum parabolic higher regularity estimates. Finally, we numerically study the rate of convergence of the scheme, and we provide examples of the existence of multiple metastable steady states.|\n", "2503.23969": "|**2025-03-31**|**Electronic structure of UGe$_2$ at ambient pressure: comparison with X-ray photoemission spectra**|M. Samsel-Czeka\u0142a, M. Werwi\u0144ski, A. Szajek et.al.|[2503.23969](http://arxiv.org/abs/2503.23969)|null||Based on experimental crystallographic data, electronic structure of UGe$_2$ have been calculated and compared with our results of X-ray photoelectron spectroscopy (XPS) measurements. We employed two different advanced full potential (FP) methods: FP-local-orbital (FPLO) and FP-linear augmented plane waves (Wien2k) codes for non-magnetic and ferromagnetic states. Starting from the local spin-density approximation (LSDA) or generalised gradient approximation (GGA), we verified either the orbital polarisation (OP) correction or the GGA+U approach for the U 5f-electrons, changing Coulomb-repulsion energies U in the range 0-4 eV. Satisfying agreement was achieved between experimental and our calculated magnetic moments using ab-initio LSDA+OP and non-ab-initio GGA+U approaches, the latter for realistic U values of 2-3 eV. We proved by the LSDA+OP approach an existence of the Fermi surface nesting vector along the a axis, possibly responsible for the triplet superconducting pairing. The calculated data reveal predominantly an itinerant U 5f-electron character of bands near the Fermi level, EF, with only small contributions from the U 6d and Ge 4p states. The experimental XPS spectrum of valence bands (VB) also contains the sharp main 5f-electron peak at EF, a wide hump (around -2 eV), and broad small peaks at higher energies. In the calculated XPS spectrum, the width of the main 5f-electron peak varies between 0.8 and 1.4 eV, depending on a method used in computations, but the hump remains unresolved. A newly observed asymmetric 1-eV satellite in the experimental 4f-core XPS spectrum together with known 3-eV and 7-eV satellites suggest dual behaviour of U-5f-electrons in UGe$_2$, the feature is inferred also from the VB studies.|\n", "2503.23900": "|**2025-03-31**|**Convergence of Calder\u00f3n residuals**|Ralf Hiptmair, Carolina Urz\u00faa-Torres, Anouk Wisse et.al.|[2503.23900](http://arxiv.org/abs/2503.23900)|null||In this paper, we describe a framework to compute expected convergence rates for residuals based on the Calder\\'on identities for general second order differential operators for which fundamental solutions are known. The idea is that these rates could be used to validate implementations of boundary integral operators and allow to test operators separately by choosing solutions where parts of the Calder\\'on identities vanish. Our estimates rely on simple vector norms, and thus avoid the use of hard-to-compute norms and the residual computation can be easily implemented in existing boundary element codes. We test the proposed Calder\\'on residuals as debugging tool by introducing artificial errors into the Galerkin matrices of some of the boundary integral operators for the Laplacian and time-harmonic Maxwell's equations. From this, we learn that our estimates are not sharp enough to always detect errors, but still provide a simple and useful debugging tool in many situations.|\n", "2503.23874": "|**2025-03-31**|**He-Mg compounds and helium-driven nonmetal transition in metallic magnesium**|Y. S. Huang, H. X. Song, Q. D. Hao et.al.|[2503.23874](http://arxiv.org/abs/2503.23874)|null|22 pages, 5 figures, with supporting materials|The polymorphism and mechanism of helium compounds is crucial for understanding the physical and chemical nature of He-bearing materials under pressures. Here, we predict two new types of He-bearing compounds, MgHe and MgnHe (n = 6, 8, 10, 15, 18), being formed above 750 GPa by unbiased ab initio structure search. An unexpected bandgap is opened up in MgHe at as low as around 200 GPa. This is the first case of noble gas driven metal-nonmetal transition in all elements. The same mechanism is demonstrated also being applicable to other metallic elements, and making beryllium transform into a non-metallic state, a triumph that is impossible otherwise. Furthermore, the stability of the simple cubic phase of Mg (Mg-sc) is greatly enhanced by mixing with He, which lowers the critical pressure of pure Mg-sc from about 1.1 TPa down to 750 GPa to form ordered substitutional alloying phase of MgnHe on a simple cubic lattice of Mg. This is the first report on Mg-based noble gas substitutional alloy, in sharp contrast to the conventional wisdom that He preferring interstitial sites. The observed striking influences of He demonstrate the rich physics and chemistry of He-bearing compounds under ultra-high pressures.|\n", "2503.23794": "|**2025-03-31**|**Force-Free Molecular Dynamics Through Autoregressive Equivariant Networks**|Fabian L. Thiemann, Thiago Resch\u00fctzegger, Massimiliano Esposito et.al.|[2503.23794](http://arxiv.org/abs/2503.23794)|**[link](https://github.com/ibm/trajcast)**|25 pages total (19 manuscript, 6 SI). 5 figures in manuscript, 3   figures and 2 tables in SI|Molecular dynamics (MD) simulations play a crucial role in scientific research. Yet their computational cost often limits the timescales and system sizes that can be explored. Most data-driven efforts have been focused on reducing the computational cost of accurate interatomic forces required for solving the equations of motion. Despite their success, however, these machine learning interatomic potentials (MLIPs) are still bound to small time-steps. In this work, we introduce TrajCast, a transferable and data-efficient framework based on autoregressive equivariant message passing networks that directly updates atomic positions and velocities lifting the constraints imposed by traditional numerical integration. We benchmark our framework across various systems, including a small molecule, crystalline material, and bulk liquid, demonstrating excellent agreement with reference MD simulations for structural, dynamical, and energetic properties. Depending on the system, TrajCast allows for forecast intervals up to $30\\times$ larger than traditional MD time-steps, generating over 15 ns of trajectory data per day for a solid with more than 4,000 atoms. By enabling efficient large-scale simulations over extended timescales, TrajCast can accelerate materials discovery and explore physical phenomena beyond the reach of traditional simulations and experiments. An open-source implementation of TrajCast is accessible under https://github.com/IBM/trajcast.|\n", "2503.23782": "|**2025-03-31**|**Distributional regression with reject option**|Ahmed Zaoui, Cl\u00e9ment Dombry et.al.|[2503.23782](http://arxiv.org/abs/2503.23782)|null||Selective prediction, where a model has the option to abstain from making a decision, is crucial for machine learning applications in which mistakes are costly. In this work, we focus on distributional regression and introduce a framework that enables the model to abstain from estimation in situations of high uncertainty. We refer to this approach as distributional regression with reject option, inspired by similar concepts in classification and regression with reject option. We study the scenario where the rejection rate is fixed. We derive a closed-form expression for the optimal rule, which relies on thresholding the entropy function of the Continuous Ranked Probability Score (CRPS). We propose a semi-supervised estimation procedure for the optimal rule, using two datasets: the first, labeled, is used to estimate both the conditional distribution function and the entropy function of the CRPS, while the second, unlabeled, is employed to calibrate the desired rejection rate. Notably, the control of the rejection rate is distribution-free. Under mild conditions, we show that our procedure is asymptotically as effective as the optimal rule, both in terms of error rate and rejection rate. Additionally, we establish rates of convergence for our approach based on distributional k-nearest neighbor. A numerical analysis on real-world datasets demonstrates the strong performance of our procedure|\n", "2503.23729": "|**2025-03-31**|**Integral regularization PINNs for evolution equations**|Xiaodong Feng, Haojiong Shangguan, Tao Tang et.al.|[2503.23729](http://arxiv.org/abs/2503.23729)|null||Evolution equations, including both ordinary differential equations (ODEs) and partial differential equations (PDEs), play a pivotal role in modeling dynamic systems. However, achieving accurate long-time integration for these equations remains a significant challenge. While physics-informed neural networks (PINNs) provide a mesh-free framework for solving PDEs, they often suffer from temporal error accumulation, which limits their effectiveness in capturing long-time behaviors. To alleviate this issue, we propose integral regularization PINNs (IR-PINNs), a novel approach that enhances temporal accuracy by incorporating an integral-based residual term into the loss function. This method divides the entire time interval into smaller sub-intervals and enforces constraints over these sub-intervals, thereby improving the resolution and correlation of temporal dynamics. Furthermore, IR-PINNs leverage adaptive sampling to dynamically refine the distribution of collocation points based on the evolving solution, ensuring higher accuracy in regions with sharp gradients or rapid variations. Numerical experiments on benchmark problems demonstrate that IR-PINNs outperform original PINNs and other state-of-the-art methods in capturing long-time behaviors, offering a robust and accurate solution for evolution equations.|\n", "2503.23728": "|**2025-03-31**|**Performing Path Integral Molecular Dynamics Using Artificial Intelligence Enhanced Molecular Simulation Framework**|Cheng Fan, Maodong Li, Sihao Yuan et.al.|[2503.23728](http://arxiv.org/abs/2503.23728)|null||This study employed an artificial intelligence-enhanced molecular simulation framework to enable efficient Path Integral Molecular Dynamics (PIMD) simulations. Owing to its modular architecture and high-throughput capabilities, the framework effectively mitigates the computational complexity and resource-intensive limitations associated with conventional PIMD approaches. By integrating machine learning force fields (MLFFs) into the framework, we rigorously tested its performance through two representative cases: a small-molecule reaction system (double proton transfer in formic acid dimer) and a bulk-phase transition system (water-ice phase transformation). Computational results demonstrate that the proposed framework achieves accelerated PIMD simulations while preserving quantum mechanical accuracy. These findings show that nuclear quantum effects can be captured for complex molecular systems, using relatively low computational cost.|\n", "2503.23716": "|**2025-03-31**|**On blowup solution in NLS equation under dispersion or nonlinearity management**|Jing Li, Cui Ning, Xiaofei Zhao et.al.|[2503.23716](http://arxiv.org/abs/2503.23716)|null||In this paper, we study the dispersion-managed nonlinear Schr\\\"odinger (DM-NLS) equation $$ i\\partial_t u(t,x)+\\gamma(t)\\Delta u(t,x)=|u(t,x)|^{\\frac4d}u(t,x),\\quad x\\in\\R^d, $$ and the nonlinearity-managed NLS (NM-NLS) equation: $$ i\\partial_t u(t,x)+\\Delta u(t,x)=\\gamma(t)|u(t,x)|^{\\frac4d}u(t,x), \\quad x\\in\\R^d, $$ where $\\gamma(t)$ is a periodic function which is equal to $-1$ when $t\\in (0,1]$ and is equal to $1$ when $t\\in (1,2]$. The two models share the feature that the focusing and defocusing effects convert periodically. For the classical focusing NLS, it is known that the initial data $$ u_0(x)=T^{-\\frac{d}{2}}\\fe^{i\\frac{|x|^2}{4T} -i\\frac{\\omega^2}{T}}Q_\\omega\\left(\\frac{x}{T}\\right) $$ leads to a blowup solution $$(T-t)^{-\\frac{d}{2}}\\fe^{i\\frac{|x|^2}{4(T-t)} -i\\frac{\\omega^2}{T-t}}Q_\\omega\\left(\\frac{x}{T-t}\\right), $$ so when $T\\leq1$, this is also a blowup solution for DM-NLS and NM-NLS which blows up in the first focusing layer.   For DM-NLS, we prove that when $T>1$, the initial data $u_0$ above does not lead to a finite-time blowup and the corresponding solution is globally well-posed. For NM-NLS, we prove the global well-posedness for $T\\in(1,2)$ and we construct solution that can blow up at any focusing layer. The theoretical studies are complemented by extensive numerical explorations towards understanding the stabilization effects in the two models and addressing their difference.|\n", "2503.23690": "|**2025-03-31**|**Effects of the delocalized charge distribution in trapped ion-atom collisions**|Ruiren Shi, Michael Drewsen, Jes\u00fas P\u00e9rez-R\u00edos et.al.|[2503.23690](http://arxiv.org/abs/2503.23690)|null||In the study of ion-atom interactions, the ion often remain trapped during the experiments. However, the effects of the trapping potential of the ion on ion-neutral interactions remain largely unexplored. Although trap-assisted ion-neutral complex formation has been experimentally studied and described by applying semiclassical theories where the ion is treated as a point charge particle, the potential effect of a delocalized charge distribution of a confined ion due to its quantum mechanical wavefunction has not been considered. To remedy this, in the present theoretical work we substitute the point charge of the ion with a delocalized charged distribution according to its motional ground state in the trap. Our results show that the trapping frequency and hence the spatial extension of the ion's ground-state wavefunction drastically affects the elastic and transport cross sections in interactions with neutral atoms. Stimulated by these results, we propose experimental procedures to verify the effects of the delocalize charge distribution in ion-atom interactions via measuring the heating rate of the ion due to the energy transfer in atomic collisions. Our novel approach brings new possibilities for investigating ion-neutral systems and, through them, new perspectives on ionic polarons and potentially a better understanding of trap-induced losses in ion-neutral experiments.|\n", "2504.02814": "|**2025-04-03**|**Convergence of the Markovian iteration for coupled FBSDEs via a differentiation approach**|Zhipeng Huang, Cornelis W. Oosterlee et.al.|[2504.02814](http://arxiv.org/abs/2504.02814)|null|28 pages, 2 figures|In this paper, we investigate the Markovian iteration method for solving coupled forward-backward stochastic differential equations (FBSDEs) featuring a fully coupled forward drift, meaning the drift term explicitly depends on both the forward and backward processes. An FBSDE system typically involves three stochastic processes: the forward process $X$, the backward process $Y$ representing the solution, and the $Z$ process corresponding to the scaled derivative of $Y$. Prior research by Bender and Zhang (2008) has established convergence results for iterative schemes dealing with $Y$-coupled FBSDEs. However, extending these results to equations with $Z$ coupling poses significant challenges, especially in uniformly controlling the Lipschitz constant of the decoupling fields across iterations and time steps within a fixed-point framework.   To overcome this issue, we propose a novel differentiation-based method for handling the $Z$ process. This approach enables improved management of the Lipschitz continuity of decoupling fields, facilitating the well-posedness of the discretized FBSDE system with fully coupled drift. We rigorously prove the convergence of our Markovian iteration method in this more complex setting. Finally, numerical experiments confirm our theoretical insights, showcasing the effectiveness and accuracy of the proposed methodology.|\n", "2504.02721": "|**2025-04-03**|**Phase transitions for interacting particle systems on random graphs**|Benedetta Bertoli, Grigorios A. Pavliotis, Niccol\u00f2 Zagli et.al.|[2504.02721](http://arxiv.org/abs/2504.02721)|null|28 pages, 4 figures|In this paper, we study weakly interacting diffusion processes on random graphs. Our main focus is on the properties of the mean-field limit and, in particular, on the nonuniqueness of stationary states. By extending classical bifurcation analysis to include multichromatic interaction potentials and random graph structures, we explicitly identify bifurcation points and relate them to the eigenvalues of the graphon integral operator. Furthermore, we characterize the resulting McKean-Vlasov PDE as a gradient flow with respect to a suitable metric. We combine these theoretical results with the spectral analysis of the linearized McKean-Vlasov operator and extensive numerical simulations to gain insight into the stability and long-term behaviour of stationary solutions. In addition, we provide strong evidence that (minus) the interaction energy of the interacting particle system serves as a natural order parameter. In particular, beyond the transition point and for multichromatic interactions, we observe an energy cascade that is strongly linked to the dynamical metastability of the system.|\n", "2504.02700": "|**2025-04-03**|**Centroidal Voronoi Tessellations as Electrostatic Equilibria: A Generalized Thomson Problem in Convex Domains**|Zachary Mullaghy et.al.|[2504.02700](http://arxiv.org/abs/2504.02700)|null||We present a variational framework in which Centroidal Voronoi Tessellations (CVTs) arise as local minimizers of a generalized electrostatic energy functional. By modeling interior point distributions in a convex domain as repelling charges balanced against a continuous boundary charge, we show that the resulting equilibrium configurations converge to CVT structures. We prove this by showing that CVTs minimize both the classical centroidal energy and the electrostatic potential, establishing a connection between geometric quantization and potential theory. Finally, we introduce a thermodynamic annealing scheme for global CVT optimization, rooted in Boltzmann statistics and random walk dynamics. By introducing a scheme for varying time steps (faster or slower cooling) we show that the set of minima of the centroid energy functional (and therefore the electrostatic potential) can be recovered. By recovering a set of generator locations corresponding to each minimum we can create a lattice continuation that allows for a customizable framework for individual minimum seeking.|\n", "2504.02672": "|**2025-04-03**|**Certified Model Order Reduction for parametric Hermitian eigenproblems**|Mattia Manucci, Benjamin Stamm, Zhuoyao Zeng et.al.|[2504.02672](http://arxiv.org/abs/2504.02672)|null||This article deals with the efficient and certified numerical approximation of the smallest eigenvalue and the associated eigenspace of a large-scale parametric Hermitian matrix. For this aim, we rely on projection-based model order reduction (MOR), i.e., we approximate the large-scale problem by projecting it onto a suitable subspace and reducing it to one of a much smaller dimension. Such a subspace is constructed by means of weak greedy-type strategies. After detailing the connections with the reduced basis method for source problems, we introduce a novel error estimate for the approximation error related to the eigenspace associated with the smallest eigenvalue. Since the difference between the second smallest and the smallest eigenvalue, the so-called spectral gap, is crucial for the reliability of the error estimate, we propose efficiently computable upper and lower bounds for higher eigenvalues and for the spectral gap, which enable the assembly of a subspace for the MOR approximation of the spectral gap. Based on that, a second subspace is then generated for the MOR approximation of the eigenspace associated with the smallest eigenvalue. We also provide efficiently computable conditions to ensure that the multiplicity of the smallest eigenvalue is fully captured in the reduced space. This work is motivated by a specific application: the repeated identifications of the states with minimal energy, the so-called ground states, of parametric quantum spin system models.|\n", "2504.02629": "|**2025-04-03**|**An efficient and energy-stable IMEX splitting scheme for dispersed multiphase flows**|Douglas Pacheco, Richard Schussnig et.al.|[2504.02629](http://arxiv.org/abs/2504.02629)|null||Volume-averaged Navier--Stokes equations are used in various applications to model systems with two or more interpenetrating phases. Each fluid obeys its own momentum and mass equations, and the phases are typically coupled via drag forces and a shared pressure. Monolithic solvers can therefore be very expensive and difficult to implement. On the other hand, designing robust splitting schemes requires making both pressure and drag forces explicit without sacrificing temporal stability. In this context, we derive a new first-order pressure-correction method based on the incompressibility of the mean velocity field, combined with an explicit treatment of the drag forces. Furthermore, the convective terms are linearised using extrapolated velocities, while the viscous terms are treated semi-implicitly. This gives us an implicit-explicit (IMEX) method that is very robust not only due to its unconditional energy stability, but also because it does not require any type of fixed-point iterations. Each time step involves only linear, scalar transport equations and a single Poisson problem as building blocks, thereby offering both efficiency and simplicity. We rigorously prove temporal stability without any time-step size restrictions, and the theory is confirmed through two-phase numerical examples.|\n", "2504.02513": "|**2025-04-03**|**Adaptive Bivariate Quarklet Tree Approximation via Anisotropic Tensor Quarklets**|Marc Hovemann et.al.|[2504.02513](http://arxiv.org/abs/2504.02513)|null||This paper deals with near-best approximation of a given bivariate function using elements of quarkonial tensor frames. For that purpose we apply anisotropic tensor products of the univariate B-spline quarklets introduced around 2017 by Dahlke, Keding and Raasch. We introduce the concept of bivariate quarklet trees and develop an adaptive algorithm which allows for generalized hp-approximation of a given bivariate function by selected frame elements. It is proved that this algorithm is near-best, which means that as long as some standard conditions concerning local errors are fulfilled it provides an approximation with an error close to that one of the best possible quarklet tree approximation. For this algorithm the complexity is investigated. Moreover, we use our techniques to approximate a bivariate test function with inverse-exponential rates of convergence. It can be expected that the results presented in this paper serve as important building block for the design of adaptive wavelet-hp-methods for solving PDEs in the bivariate setting with very good convergence properties.|\n", "2504.02488": "|**2025-04-03**|**A Behaviour and Disease Model of Testing and Isolation**|Matthew Ryan, Roslyn I. Hickson, Edward M. Hill et.al.|[2504.02488](http://arxiv.org/abs/2504.02488)|**[link](https://github.com/Matthew-Ryan1995/BaD_testing_and_isolation)**|22 pages, 10 figures|There has been interest in the interactions between infectious disease dynamics and behaviour for most of the history of mathematical epidemiology. This has included consideration of which mathematical models best capture each phenomenon, as well as their interaction, but typically in a manner that is agnostic to the exact behaviour in question. Here, we investigate interacting behaviour and disease dynamics specifically related to behaviours around testing and isolation. This epidemiological-behavioural interaction is of particular interest as, prospectively, it is well-placed to be informed by real-world data temporally monitoring test results and compliance with testing policy. To carry out our investigation we extend an existing \"behaviour and disease\" (BaD) model by incorporating the dynamics of symptomatic testing and isolation. We provide a dynamical systems analysis of the ordinary differential equations that define this model, providing theoretical results on its behaviour early in a new outbreak (particularly its basic reproduction number) and endemicity of the system (its steady states and associated stability criteria). We then supplement these findings with a numerical analysis to inform how temporal and cumulative outbreak metrics depend on the model parameter values for epidemic and endemic regimes. As the presented interdisciplinary modelling approach can accommodate further extensions (including, but not limited to, adding testing capacity, decay in behavioural effects and multiple pathogen variants), we hope that our work will encourage further modelling studies integrating specific measured behaviours and disease dynamics that may reduce the health and economic impacts of future epidemics.|\n", "2504.02475": "|**2025-04-03**|**Heat Conduction with Phase Change in Permafrost Modules of Vegetation Models**|David H\u00f6tten, Jenny Niebsch, Ronny Ramlau et.al.|[2504.02475](http://arxiv.org/abs/2504.02475)|null||We consider the problem of heat conduction with phase change, that is essential for permafrost modeling in Land Surface Models and Dynamic Global Vegetation Models. These models require minimal computational effort and an extremely robust solver for large-scale, long-term simulations. The weak enthalpy formulation of the Stefan problem is used as the mathematical model and a finite element method is employed for the discretization. Leveraging the piecewise affine structure of the nonlinear time-stepping equation system, we demonstrate that this system has a unique solution and provide a solver that is guaranteed to find this solution in a finite number of steps from any initial guess. Comparisons with the Neumann analytical solution and tests in the Lund-Potsdam-Jena managed Land vegetation model reveal that the new method does not introduce significantly higher computational costs than the widely used DECP method while providing greater accuracy. In particular, it avoids a known nonphysical artifact in the solution.|\n", "2504.02432": "|**2025-04-03**|**Robust Randomized Low-Rank Approximation with Row-Wise Outlier Detection**|Aidan Tiruvan et.al.|[2504.02432](http://arxiv.org/abs/2504.02432)|null|27 pages, 9 figures, preprint|Robust low-rank approximation under row-wise adversarial corruption can be achieved with a single pass, randomized procedure that detects and removes outlier rows by thresholding their projected norms. We propose a scalable, non-iterative algorithm that efficiently recovers the underlying low-rank structure in the presence of row-wise adversarial corruption. By first compressing the data with a Johnson Lindenstrauss projection, our approach preserves the geometry of clean rows while dramatically reducing dimensionality. Robust statistical techniques based on the median and median absolute deviation then enable precise identification and removal of outlier rows with abnormally high norms. The subsequent rank-k approximation achieves near-optimal error bounds with a one pass procedure that scales linearly with the number of observations. Empirical results confirm that combining random sketches with robust statistics yields efficient, accurate decompositions even in the presence of large fractions of corrupted rows.|\n", "2504.02422": "|**2025-04-04**|**Applying Space-Group Symmetry to Speed up Hybrid-Functional Calculations within the Framework of Numerical Atomic Orbitals**|Yu Cao, Min-Ye Zhang, Peize Lin et.al.|[2504.02422](http://arxiv.org/abs/2504.02422)|null||Building upon the efficient implementation of hybrid density functionals (HDFs) for large-scale periodic systems within the framework of numerical atomic orbital bases using the localized resolution of identity (RI) technique, we have developed an algorithm that exploits the space group symmetry in key operation steps of HDF calculations, leading to further improvements in two ways. First, the reduction of $\\mathbf{k}$-points in the Brillouin zone can reduce the number of Kohn-Sham equations to be solved. This necessitates the correct implementation of the rotation relation between the density matrices of equivalent $\\mathbf{k}$-points within the representation of atomic orbitals. Second, the reduction of the real-space sector can accelerate the construction of the exact-exchange part of the Hamiltonian in real space. We have implemented this algorithm in the ABACUS software interfaced with LibRI, and tested its performance for several types of crystal systems with different symmetries. The expected speed-up is achieved in both aspects: the time of solving the Kohn-Sham equations decreases in proportion with the reduction of $\\mathbf{k}$-points, while the construction of the Hamiltonian in real space is sped up by several times, with the degree of acceleration depending on the size and symmetry of the system.|\n", "2504.02413": "|**2025-04-03**|**Dislocation-density based crystal plasticity: stability and attractors in slip rate driven processes**|Jalal Smiri, O\u011fuz Umut Salman, Ioan R. Ionescu et.al.|[2504.02413](http://arxiv.org/abs/2504.02413)|null||Dislocation-density based crystal plasticity (CP) models are introduced to account for the microstructral changes throughout the deformation process, enabling more quantitative predictions of the deformation process compared to slip-system resistance-based plasticity models. In this work, we present a stability analysis of slip rate driven processes for some established dislocation density-based models, including the Kocks and Mecking (KM) model and its variants. Our analysis can be generalized to any type of dislocation density model, providing a broader framework for understanding the stability of such systems. Interestingly, we demonstrate that even size-independent models can exhibit size-dependent effects through variations in initial dislocation density. Notably, the initial dislocation density significantly influences material hardening or softening responses. To further explore these phenomena, we conduct numerical simulations of micro-pillar compression using an Eulerian crystal plasticity framework. Our results show that dislocation-density-based CP models effectively capture microstructural evolution in small-scale materials, offering critical insights for the design of miniaturized mechanical devices and advanced materials in nanotechnology.|\n", "2504.02367": "|**2025-04-03**|**CrystalFormer-RL: Reinforcement Fine-Tuning for Materials Design**|Zhendong Cao, Lei Wang et.al.|[2504.02367](http://arxiv.org/abs/2504.02367)|**[link](https://github.com/deepmodeling/crystalformer)**|8 pages, 6 figures|Reinforcement fine-tuning has instrumental enhanced the instruction-following and reasoning abilities of large language models. In this work, we explore the applications of reinforcement fine-tuning to the autoregressive transformer-based materials generative model CrystalFormer (arXiv:2403.15734) using discriminative machine learning models such as interatomic potentials and property prediction models. By optimizing reward signals-such as energy above the convex hull and material property figures of merit-reinforcement fine-tuning infuses knowledge from discriminative models into generative models. The resulting model, CrystalFormer-RL, shows enhanced stability in generated crystals and successfully discovers crystals with desirable yet conflicting material properties, such as substantial dielectric constant and band gap simultaneously. Notably, we observe that reinforcement fine-tuning enables not only the property-guided novel material design ability of generative pre-trained model but also unlocks property-driven material retrieval from the unsupervised pre-training dataset. Leveraging rewards from discriminative models to fine-tune materials generative models opens an exciting gateway to the synergies of the machine learning ecosystem for materials.|\n", "2504.02267": "|**2025-04-03**|**Third-Order Spontaneous Parametric Down Conversion in Dielectric Nonlinear Resonant Metasurfaces**|Miguel Y. Bacaoco, Kirill Koshelev, Alexander S. Solntsev et.al.|[2504.02267](http://arxiv.org/abs/2504.02267)|null||We propose a general scheme to investigate photon triplet generation (PTG) via third-order spontaneous parametric downconversion (TOSPDC) in $\\chi^{(3)}$ nonlinear structures. Our approach leverages the quantum-classical correspondence between TOSPDC and its reverse classical process, three-wave sum-frequency generation (TSFG), to efficiently estimate the PTG rate. We apply this framework to nonlinear metasurfaces supporting quasi-bound states in the continuum (qBICs) in the optical range. From numerical analysis of non-collinear TSFG with degenerate input waves at qBIC wavelengths, we predict wavelength-tunable three-photon emission with spatio-angular correlations. These findings establish a novel method for modelling TOSPDC and also highlight the potential of nonlinear resonant metasurfaces as compact free-space photon triplet sources with quantum state control.|\n", "2504.02245": "|**2025-04-03**|**Traffic Flow Data Completion and Anomaly Diagnosis via Sparse and Low-Rank Tensor Optimization**|Junxi Man, Yumin Lin, Xiaoyu Li et.al.|[2504.02245](http://arxiv.org/abs/2504.02245)|null||Spatiotemporal traffic time series, such as traffic speed data, collected from sensing systems are often incomplete, with considerable corruption and large amounts of missing values. A vast amount of data conceals implicit data structures, which poses significant challenges for data recovery issues, such as mining the potential spatio-temporal correlations of data and identifying abnormal data. In this paper, we propose a Tucker decomposition-based sparse low-rank high-order tensor optimization model (TSLTO) for data imputation and anomaly diagnosis. We decompose the traffic tensor data into low-rank and sparse tensors, and establish a sparse low-rank high-order tensor optimization model based on Tucker decomposition. By utilizing tools of non-smooth analysis for tensor functions, we explore the optimality conditions of the proposed tensor optimization model and design an ADMM optimization algorithm for solving the model. Finally, numerical experiments are conducted on both synthetic data and a real-world dataset: the urban traffic speed dataset of Guangzhou. Numerical comparisons with several representative existing algorithms demonstrate that our proposed approach achieves higher accuracy and efficiency in traffic flow data recovery and anomaly diagnosis tasks.|\n", "2504.02228": "|**2025-04-03**|**Stochastic positivity-preserving symplectic splitting methods for stochastic Lotka--Volterra predator-prey model**|Liying Zhang, Xinyue Kang, Lihai Ji et.al.|[2504.02228](http://arxiv.org/abs/2504.02228)|null||In this paper, we present two stochastic positive-preserving symplectic methods for the stochastic Lotka-Volterra predator-prey model driven by a multiplicative noise. To inherit the intrinsic characteristic of the original system, the stochastic Lie--Trotter splitting method and the stochastic Strang splitting method are introduced, which are proved to preserve the positivity of the numerical solution and possess the discrete stochastic symplectic conservation law as well. By deriving the uniform boundedness of the $p$-th moment of the numerical solution, we prove that the strong convergence orders of these two methods are both one in the $L^2(\\Omega)$-norm. Finally, we validate the theoretical results through two and four dimensional numerical examples.|\n", "2504.02226": "|**2025-04-03**|**Error analysis of the diffuse domain finite element method for second order parabolic equations**|Wenrui Hao, Lili Ju, Yuejin Xu et.al.|[2504.02226](http://arxiv.org/abs/2504.02226)|null||In this paper, we analyze the diffuse domain finite element method (DDFE) to solve a class of second-order parabolic partial differential equations defined in general irregular domains. The proposed method first applies the diffuse domain method (DDM) with a phase-field function to extend the target parabolic equation to a similar problem defined over a larger rectangular domain that contains the original physical domain. The transformed equation is then discretized by using the finite element method with continuous piecewise multilinear basis functions in space and the BDF2 scheme in time to produce a fully discrete numerical scheme. Based on the weighted Sobolev spaces, we prove the convergence of the DDM solution to the original solution as the interface thickness parameter goes to zero, with the corresponding approximation errors under the $L^2$ and $H^1$ norms. Furthermore, the optimal error estimate for the fully discrete DDFE scheme is also obtained under the $H^1$ norm. Various numerical experiments are finally carried out to validate the theoretical results and demonstrate the performance of the proposed method.|\n", "2504.02198": "|**2025-04-03**|**Error Analysis of Sampling Algorithms for Approximating Stochastic Optimal Control**|Anant A. Joshi, Amirhossein Taghvaei, Prashant G. Mehta et.al.|[2504.02198](http://arxiv.org/abs/2504.02198)|null||This paper is concerned with the error analysis of two types of sampling algorithms, namely model predictive path integral (MPPI) and an interacting particle system (\\IPS) algorithm, that have been proposed in the literature for numerical approximation of the stochastic optimal control. The analysis is presented through the lens of Gibbs variational principle. For an illustrative example of a single-stage stochastic optimal control problem, analytical expressions for approximation error and scaling laws, with respect to the state dimension and sample size, are derived. The analytical results are illustrated with numerical simulations.|\n", "2504.02183": "|**2025-04-03**|**Numerical Framework for Multimode Jaynes- and Tavis-Cummings Models Incorporating the Modified Langevin Noise Formalism: Non-Markovian Analysis of Atom-Field Interactions in Dissipative Electromagnetic Environments**|Hyunwoo Choi, Weng Cho Chew, Dong-Yeop Na et.al.|[2504.02183](http://arxiv.org/abs/2504.02183)|null||We present a novel numerical framework that integrates the modified Langevin noise formalism into the multimode Jaynes- and Tavis-Cummings models, enabling a first-principles, non-Markovian analysis of atom-field interactions in dissipative electromagnetic (EM) environments that account for both radiative losses and absorptive dissipation in lossy dielectric media (or satisfying general inhomogeneous causal media exhibiting both dispersion and absorption effects). In the modified Langevin noise formalism, the boundary- and medium-assisted (BA and MA) fields, which constitute a continuum set of EM modes in dissipative EM environments, are numerically obtained using the finite-element method (FEM). Specifically, BA field modes are extracted by solving plane-wave scattering problems, while MA field modes are determined through point-source radiation problems. These numerically obtained BA and MA field modes are then incorporated into the multimode Jaynes- and Tavis-Cummings models such that the coupling strength between atoms and BA-MA field modes can be calculated for the study of atom-field interactions in dissipative EM environments. The proposed methodology captures non-Markovian atomic dynamics that cannot be described by traditional quantum master equations under the Markovian approximation. To validate the accuracy of the proposed numerical framework, we present four numerical examples: (i) a two-level system (TLS) in a perfect electric conductor (PEC) half-space; (ii) dissipative cavity electrodynamics with two limiting cases approaching spontaneous emission in free space and ideal Rabi oscillations; (iii) super-radiance in TLS arrays; and (iv) entanglement sudden death of two TLSs inside dissipative cavities. The proposed methodology can serve as a ground-truth numerical simulator for studying atom-field interactions in general dissipative EM environments.|\n", "2504.02117": "|**2025-04-02**|**Vectorised Parallel in Time methods for low-order discretizations with application to Porous Media problems**|Christian Engwer, Alexander Schell, Nils-Arne Dreier et.al.|[2504.02117](http://arxiv.org/abs/2504.02117)|null||High order methods have shown great potential to overcome performance issues of simulations of partial differential equations (PDEs) on modern hardware, still many users stick to low-order, matrixbased simulations, in particular in porous media applications. Heterogeneous coefficients and low regularity of the solution are reasons not to employ high order discretizations. We present a new approach for the simulation of instationary PDEs that allows to partially mitigate the performance problems. By reformulating the original problem we derive a parallel in time time integrator that increases the arithmetic intensity and introduces additional structure into the problem. By this it helps accelerate matrix-based simulations on modern hardware architectures. Based on a system for multiple time steps we will formulate a matrix equation that can be solved using vectorised solvers like Block Krylov methods. The structure of this approach makes it applicable for a wide range of linear and nonlinear problems. In our numerical experiments we present some first results for three different PDEs, a linear convection-diffusion equation, a nonlinear diffusion-reaction equation and a realistic example based on the Richards' equation.|\n", "2504.02089": "|**2025-04-02**|**Perturbations and Phase Transitions in Swarm Optimization Algorithms**|Tom\u00e1\u0161 Vantuch, Ivan Zelinka, Andrew Adamatzky et.al.|[2504.02089](http://arxiv.org/abs/2504.02089)|null||Natural systems often exhibit chaotic behavior in their space-time evolution. Systems transiting between chaos and order manifest a potential to compute, as shown with cellular automata and artificial neural networks. We demonstrate that swarm optimization algorithms also exhibit transitions from chaos, analogous to a motion of gas molecules, when particles explore solution space disorderly, to order, when particles follow a leader, similar to molecules propagating along diffusion gradients in liquid solutions of reagents. We analyze these `phase-like' transitions in swarm optimization algorithms using recurrence quantification analysis and Lempel-Ziv complexity estimation. We demonstrate that converging iterations of the optimization algorithms are statistically different from non-converging ones in a view of applied chaos, complexity and predictability estimating indicators.   An identification of a key factor responsible for the intensity of their phase transition is the main contribution of this paper. We examined an optimization as a process with three variable factors -- an algorithm, number generator and optimization function. More than 9.000 executions of the optimization algorithm revealed that the nature of an applied algorithm itself is the main source of the phase transitions. Some of the algorithms exhibit larger transition-shifting behavior while others perform rather transition-steady computing. These findings might be important for future extensions of these algorithms.|\n", "2504.05300": "|**2025-04-07**|**Dimension-Free Convergence of Diffusion Models for Approximate Gaussian Mixtures**|Gen Li, Changxiao Cai, Yuting Wei et.al.|[2504.05300](http://arxiv.org/abs/2504.05300)|null||Diffusion models are distinguished by their exceptional generative performance, particularly in producing high-quality samples through iterative denoising. While current theory suggests that the number of denoising steps required for accurate sample generation should scale linearly with data dimension, this does not reflect the practical efficiency of widely used algorithms like Denoising Diffusion Probabilistic Models (DDPMs). This paper investigates the effectiveness of diffusion models in sampling from complex high-dimensional distributions that can be well-approximated by Gaussian Mixture Models (GMMs). For these distributions, our main result shows that DDPM takes at most $\\widetilde{O}(1/\\varepsilon)$ iterations to attain an $\\varepsilon$-accurate distribution in total variation (TV) distance, independent of both the ambient dimension $d$ and the number of components $K$, up to logarithmic factors. Furthermore, this result remains robust to score estimation errors. These findings highlight the remarkable effectiveness of diffusion models in high-dimensional settings given the universal approximation capability of GMMs, and provide theoretical insights into their practical success.|\n", "2504.05266": "|**2025-04-07**|**Differential forms: Lagrange interpolation, sampling and approximation on polynomial admissible integral k-meshes**|Ludovico Bruni Bruno, Federico Piazzon et.al.|[2504.05266](http://arxiv.org/abs/2504.05266)|null||In this work we address the problem of interpolating and approximating differential forms starting from data defined by integration. We show that many aspects of nodal interpolation can naturally be carried to this more general framework; in contrast, some of them require the introduction of geometric and measure theoretic hypotheses. After characterizing the norms of the operators involved, we introduce the concept of admissible integral k-mesh, which allows for the construction of robust approximation schemes, and is used to extract interpolation sets with high stability properties. To this end, the concepts of Fekete currents and Leja sequences of currents are formalized, and a numerical scheme for their approximation is proposed.|\n", "2504.05248": "|**2025-04-07**|**PINNverse: Accurate parameter estimation in differential equations from noisy data with constrained physics-informed neural networks**|Marius Almanst\u00f6tter, Roman Vetter, Dagmar Iber et.al.|[2504.05248](http://arxiv.org/abs/2504.05248)|null|17 pages, 5 figures|Parameter estimation for differential equations from measured data is an inverse problem prevalent across quantitative sciences. Physics-Informed Neural Networks (PINNs) have emerged as effective tools for solving such problems, especially with sparse measurements and incomplete system information. However, PINNs face convergence issues, stability problems, overfitting, and complex loss function design. Here we introduce PINNverse, a training paradigm that addresses these limitations by reformulating the learning process as a constrained differential optimization problem. This approach achieves a dynamic balance between data loss and differential equation residual loss during training while preventing overfitting. PINNverse combines the advantages of PINNs with the Modified Differential Method of Multipliers to enable convergence on any point on the Pareto front. We demonstrate robust and accurate parameter estimation from noisy data in four classical ODE and PDE models from physics and biology. Our method enables accurate parameter inference also when the forward problem is expensive to solve.|\n", "2504.05169": "|**2025-04-07**|**Machine learning interatomic potential can infer electrical response**|Peichen Zhong, Dongjin Kim, Daniel S. King et.al.|[2504.05169](http://arxiv.org/abs/2504.05169)|null||Modeling the response of material and chemical systems to electric fields remains a longstanding challenge. Machine learning interatomic potentials (MLIPs) offer an efficient and scalable alternative to quantum mechanical methods but do not by themselves incorporate electrical response. Here, we show that polarization and Born effective charge (BEC) tensors can be directly extracted from long-range MLIPs within the Latent Ewald Summation (LES) framework, solely by learning from energy and force data. Using this approach, we predict the infrared spectra of bulk water under zero or finite external electric fields, ionic conductivities of high-pressure superionic ice, and the phase transition and hysteresis in ferroelectric PbTiO$_3$ perovskite. This work thus extends the capability of MLIPs to predict electrical response--without training on charges or polarization or BECs--and enables accurate modeling of electric-field-driven processes in diverse systems at scale.|\n", "2504.05151": "|**2025-04-07**|**Error formulas for block rational Krylov approximations of matrix functions**|Stefano Massei, Leonardo Robol et.al.|[2504.05151](http://arxiv.org/abs/2504.05151)|null||This paper investigates explicit expressions for the error associated with the block rational Krylov approximation of matrix functions. Two formulas are proposed, both derived from characterizations of the block FOM residual. The first formula employs a block generalization of the residual polynomial, while the second leverages the block collinearity of the residuals. A posteriori error bounds based on the knowledge of spectral information of the argument are derived and tested on a set of examples. Notably, both error formulas and their corresponding upper bounds do not require the use of quadratures for their practical evaluation.|\n", "2504.05149": "|**2025-04-07**|**Fast Convolutions on $\\mathbb{Z}^2\\backslash SE(2)$ via Radial Translational Dependence and Classical FFT**|Arash Ghaani Farashahi, Gregory S. Chirikjian et.al.|[2504.05149](http://arxiv.org/abs/2504.05149)|null||Let $\\mathbb{Z}^2\\backslash SE(2)$ denote the right coset space of the subgroup consisting of translational isometries of the orthogonal lattice $\\mathbb{Z}^2$ in the non-Abelian group of planar motions $SE(2)$. This paper develops a fast and accurate numerical scheme for approximation of functions on $\\mathbb{Z}^2\\backslash SE(2)$. We address finite Fourier series of functions on the right coset space $\\mathbb{Z}^2\\backslash SE(2)$ using finite Fourier coefficients. The convergence/error analysis of finite Fourier coefficients are investigated. Conditions are established for the finite Fourier coefficients to converge to the Fourier coefficients. The matrix forms of the finite transforms are discussed. The implementation of the discrete method to compute numerical approximation of $SE(2)$-convolutions with functions which are radial in translations are considered. The paper is concluded by discussing capability of the numerical scheme to develop fast algorithms for approximating multiple convolutions with functions with are radial in translations.|\n", "2504.05124": "|**2025-04-07**|**Generators of $H^1(\u0393, \\partial \u0393^c)$ with $\\partial \u0393^c \\subset \\partial \u0393$ for Triangulated Surfaces $\u0393$: Construction and Classification of Global Loops**|Silvano Pitassi et.al.|[2504.05124](http://arxiv.org/abs/2504.05124)|null||Given a compact surface $\\Gamma$ embedded in $\\mathbb R^3$ with boundary $\\partial \\Gamma$, our goal is to construct a set of representatives for a basis of the relative cohomology group $H^1(\\Gamma, \\partial \\Gamma^c)$, where $\\Gamma^c$ is a specified subset of $\\partial \\Gamma$. To achieve this, we propose a novel graph-based algorithm with two key features: it is applicable to non-orientable surfaces, thereby generalizing previous approaches, and it has a worst-case time complexity that is linear in the number of edges of the mesh $\\mathcal{K}$ triangulating $\\Gamma$. Importantly, this algorithm serves as a critical pre-processing step to address the low-frequency breakdown encountered in boundary element discretizations of integral equation formulations.|\n", "2504.05066": "|**2025-04-07**|**Turing instability for nonlocal heterogeneous reaction-diffusion systems: A computer-assisted proof approach**|Maxime Breden, Maxime Payan, Cordula Reisch et.al.|[2504.05066](http://arxiv.org/abs/2504.05066)|null||This paper provides a computer-assisted proof for the Turing instability induced by heterogeneous nonlocality in reaction-diffusion systems. Due to the heterogeneity and nonlocality, the linear Fourier analysis gives rise to \\textit{strongly coupled} infinite differential systems. By introducing suitable changes of basis as well as the Gershgorin disks theorem for infinite matrices, we first show that all $N$-th Gershgorin disks lie completely on the left half-plane for sufficiently large $N$. For the remaining finitely many disks, a computer-assisted proof shows that if the intensity $\\delta$ of the nonlocal term is large enough, there is precisely one eigenvalue with positive real part, which proves the Turing instability. Moreover, by detailed study of this eigenvalue as a function of $\\delta$, we obtain a sharp threshold $\\delta^*$ which is the bifurcation point for Turing instability.|\n", "2504.05036": "|**2025-04-07**|**Hybrid Nitsche for distributed computing**|Tom Gustafsson, Antti Hannukainen, Vili Kohonen et.al.|[2504.05036](http://arxiv.org/abs/2504.05036)|null||We extend the distributed finite element method of [1], built upon model order reduction, to arbitrary polynomial degree using a hybrid Nitsche scheme. This new method considerably simplifies the transformation of the finite element system to the reduced basis for large problems. We prove that the error of the reduced Nitsche solution converges optimally with respect to the approximation order of the finite element spaces and linearly with respect to the dimension reduction parameter $\\epsilon$. Numerical tests with nontrivial tetrahedral meshes using second-degree polynomial bases support the theoretical results.|\n", "2504.05026": "|**2025-04-07**|**Multi-level Neural Networks for high-dimensional parametric obstacle problems**|Martin Eigel, Cosmas Hei\u00df, Janina E. Sch\u00fctte et.al.|[2504.05026](http://arxiv.org/abs/2504.05026)|null||A new method to solve computationally challenging (random) parametric obstacle problems is developed and analyzed, where the parameters can influence the related partial differential equation (PDE) and determine the position and surface structure of the obstacle. As governing equation, a stationary elliptic diffusion problem is assumed. The high-dimensional solution of the obstacle problem is approximated by a specifically constructed convolutional neural network (CNN). This novel algorithm is inspired by a finite element constrained multigrid algorithm to represent the parameter to solution map. This has two benefits: First, it allows for efficient practical computations since multi-level data is used as an explicit output of the NN thanks to an appropriate data preprocessing. This improves the efficacy of the training process and subsequently leads to small errors in the natural energy norm. Second, the comparison of the CNN to a multigrid algorithm provides means to carry out a complete a priori convergence and complexity analysis of the proposed NN architecture. Numerical experiments illustrate a state-of-the-art performance for this challenging problem.|\n", "2504.05022": "|**2025-04-07**|**Solving the fully nonlinear Monge-Amp\u00e8re equation using the Legendre-Kolmogorov-Arnold Network method**|Bingcheng Hu, Lixiang Jin, Zhaoxiang Li et.al.|[2504.05022](http://arxiv.org/abs/2504.05022)|null|20 pages, 12 figures|In this paper, we propose a novel neural network framework, the Legendre-Kolmogorov-Arnold Network (Legendre-KAN) method, designed to solve fully nonlinear Monge-Amp\\`ere equations with Dirichlet boundary conditions. The architecture leverages the orthogonality of Legendre polynomials as basis functions, significantly enhancing both convergence speed and solution accuracy compared to traditional methods. Furthermore, the Kolmogorov-Arnold representation theorem provides a strong theoretical foundation for the interpretability and optimization of the network. We demonstrate the effectiveness of the proposed method through numerical examples, involving both smooth and singular solutions in various dimensions. This work not only addresses the challenges of solving high-dimensional and singular Monge-Amp\\`ere equations but also highlights the potential of neural network-based approaches for complex partial differential equations. Additionally, the method is applied to the optimal transport problem in image mapping, showcasing its practical utility in geometric image transformation. This approach is expected to pave the way for further enhancement of KAN-based applications and numerical solutions of PDEs across a wide range of scientific and engineering fields.|\n", "2504.04989": "|**2025-04-07**|**Randomized block Krylov method for approximation of truncated tensor SVD**|Malihe Nobakht Kooshkghazi, Salman Ahmadi-Asl, Andre L. F. de Almeida et.al.|[2504.04989](http://arxiv.org/abs/2504.04989)|null||This paper is devoted to studying the application of the block Krylov subspace method for approximation of the truncated tensor SVD (T-SVD). The theoretical results of the proposed randomized approach are presented. Several experimental experiments using synthetics and real-world data are conducted to verify the efficiency and feasibility of the proposed randomized approach, and the numerical results show that the proposed method provides promising results. Applications of the proposed approach to data completion and data compression are presented.|\n", "2504.04951": "|**2025-04-07**|**Anisotropic space-time goal-oriented error control and mesh adaptivity for convection-diffusion-reaction equations**|M. Bause, M. Bruchh\u00e4user, B. Endtmayer et.al.|[2504.04951](http://arxiv.org/abs/2504.04951)|null||We present an anisotropic goal-oriented error estimator based on the Dual Weighted Residual (DWR) method for time-dependent convection-diffusion-reaction (CDR) equations. Using anisotropic interpolation operators the estimator is elementwise separated with respect to the single directions in space and time leading to adaptive, anisotropic mesh refinement in a natural way. To prevent spurious oscillations the streamline upwind Petrov-Galerkin (SUPG) method is applied to stabilize the underlying system in the case of high P\\'{e}clet numbers. Efficiency and robustness of the underlying algorithm are demonstrated for different goal functionals. The directional error indicators quantify anisotropy of the solution with respect to the goal, and produce meshes that efficiently capture sharp layers. Numerical examples show the superiority of the proposed approach over isotropic adaptive and global mesh refinement using established benchmarks for convection-dominated transport.|\n", "2504.04947": "|**2025-04-07**|**Phase transitions in swarm optimization algorithms**|Tom\u00e1\u0161 Vantuch, Ivan Zelinka, Andrew Adamatzky et.al.|[2504.04947](http://arxiv.org/abs/2504.04947)|null||Natural systems often exhibit chaotic behavior in their space-time evolution. Systems transiting between chaos and order manifest a potential to compute, as shown with cellular automata and artificial neural networks. We demonstrate that swarms optimisation algorithms also exhibit transitions from chaos, analogous to motion of gas molecules, when particles explore solution space disorderly, to order, when particles follow a leader, similar to molecules propagating along diffusion gradients in liquid solutions of reagents. We analyse these `phase-like' transitions in swarm optimization algorithms using recurrence quantification analysis and Lempel-Ziv complexity estimation. We demonstrate that converging and non-converging iterations of the optimization algorithms are statistically different in a view of applied chaos, complexity and predictability estimating indicators.|\n", "2504.04941": "|**2025-04-07**|**The Kratos Framework for Heterogeneous Astrophysical Simulations: Ray Tracing, Reacting Flow and Thermochemistry**|Lile Wang et.al.|[2504.04941](http://arxiv.org/abs/2504.04941)|null|15 pages, 7 figures, submitted to ApJS|Thermochemistry, ray-tracing radiation, and radiation-matter interactions are important processes which are computationally difficult to model in astrophysical simulations, addressed by introducing novel algorithms optimized for heterogeneous architectures in the Kratos framework. Key innovations include a stoichiometry-compatible reconstruction scheme for consistent chemical species advection, which ensures element conservation while avoiding matrix inversions, and a LU decomposition method specifically designed for multi-thread parallelization in order to solve stiff thermochemical ordinary differential equations with high efficiency. The framework also implements efficient ray-tracing techniques for radiation transport for radiation-matter interactions. Various verification tests, spanning from chemical advection, combustion, Str\\\"omgren spheres, and detonation dynamics, are conducted to demonstrate the accuracy and robustness of Kratos, with results closely matching semi-analytic solutions and benchmarks such as Cantera and the Shock and Detonation Toolbox. The modular design and performance optimizations position it as a versatile tool for studying coupled microphysical processes in the diverse environments of contemporary astrophysical studies.|\n", "2504.04929": "|**2025-04-07**|**The Linearized Vlasov-Maxwell System as a Hamiltonian System**|Dominik Bell, Martin Campos Pinto, Stefan Possanner et.al.|[2504.04929](http://arxiv.org/abs/2504.04929)|null||We present a Hamiltonian formulation for the linearized Vlasov-Maxwell system with a Maxwellian background distribution function. We discuss the geometric properties of the model at the continuous level, and how to discretize the model in the GEMPIC framework [1]. This method allows us to keep the structure of the system at the semi-discrete level. To integrate the model in time, we employ a Poisson splitting and discuss how to integrate each subsystem separately. We test the model against the full Vlasov-Maxwell model with a control variate method for noise reduction; the two chosen test-cases are the weak Landau damping and the Bernstein waves. Both test-cases exhibit the same physical properties for short simulations but our model enjoys better long-time stability and energy conservation due to its geometric construction. The model is implemented in the open-source Python library STRUPHY [2, 3].|\n", "2504.04912": "|**2025-04-07**|**An iterative process for the feasibility-seeking problem with sets that are unions of convex sets**|Yair Censor, Alexander J. Zaslavski et.al.|[2504.04912](http://arxiv.org/abs/2504.04912)|null|Accepted for publication in: Communications in Optimization Theory|In this paper we deal with the feasibility-seeking problem for unions of convex sets (UCS) sets and propose an iterative process for its solution. Renewed interest in this problem stems from the fact that it was recently discovered to serve as a modeling approach in fields of applications and from the ongoing recent research efforts to handle non-convexity in feasibility-seeking.|\n", "2504.04887": "|**2025-04-07**|**A mechanism for growth of topological entropy and global changes of the shape of chaotic attractors**|Daniel Wilczak, Sergio Serrano, Roberto Barrio et.al.|[2504.04887](http://arxiv.org/abs/2504.04887)|null||The theoretical and numerical understanding of the key concept of topological entropy is an important problem in dynamical systems. Most studies have been carried out on maps (discrete-time systems). We analyse a scenario of global changes of the structure of an attractor in continuous-time systems leading to an unbounded growth of the topological entropy of the underlying dynamical system. As an example, we consider the classical Roessler system. We show that for an explicit range of parameters a chaotic attractor exists. We also prove the existence of a sequence of bifurcations leading to the growth of the topological entropy. The proofs are computer-aided.|\n", "2504.04859": "|**2025-04-07**|**Block BDDC/FETI-DP Preconditioners for Three-Field mixed finite element Discretizations of Biot's consolidation model**|Hanyu Chu, Luca Franco Pavarino, Stefano Zampini et.al.|[2504.04859](http://arxiv.org/abs/2504.04859)|null||In this paper, we construct and analyze a block dual-primal preconditioner for Biot's consolidation model approximated by three-field mixed finite elements based on a displacement, pressure, and total pressure formulation. The domain is decomposed into nonoverlapping subdomains, and the continuity of the displacement component across the subdomain interface is enforced by introducing a Lagrange multiplier. After eliminating all displacement variables and the independent subdomain interior components of pressure and total pressure, the problem is reduced to a symmetric positive definite linear system for the subdomain interface pressure, total pressure, and the Lagrange multiplier. This reduced system is solved by a preconditioned conjugate gradient method, with a block dual-primal preconditioner using a Balancing Domain Decomposition by Constraints (BDDC) preconditioner for both the interface total pressure block and the interface pressure blocks, as well as a Finite Element Tearing and Interconnecting-Dual Primal (FETI-DP) preconditioner for the Lagrange multiplier block. By analyzing the conditioning of the preconditioned subsystem associated with the interface pressure and total pressure components, we obtain a condition number bound of the preconditioned system, which is scalable in the number of subdomains, poly-logarithmic in the ratio of subdomain and mesh sizes, and robust with respect to the parameters of the model. Extensive numerical experiments confirm the theoretical result of the proposed algorithm.|\n", "2504.04852": "|**2025-04-07**|**An invariant-region-preserving scheme for a convection-reaction-Cahn-Hilliard multiphase model of biofilm growth in slow sand filters**|Julio Careaga, Stefan Diehl, Jaime Manr\u00edquez et.al.|[2504.04852](http://arxiv.org/abs/2504.04852)|null|32 pages, 5 images|A multidimensional model of biofilm growth present in the supernatant water of a Slow Sand Filter is derived. The multiphase model, consisting of solid and liquid phases, is written as a convection-reaction system with a Cahn-Hilliard-type equation with degenerate mobility coupled to a Stokes-flow equation for the mixture velocity. An upwind discontinuous Galerkin approach is used to approximate the convection-reaction equations, whereas an $H^1$-conforming primal formulation is proposed for the Stokes system. By means of a splitting procedure due to the reaction terms, an invariant-region principle is shown for the concentration unknowns, namely non-negativity for all phases and an upper bound for the total concentration of the solid phases. Numerical examples with reduced biofilm reactions are presented to illustrate the performance of the model and numerical scheme.|\n", "2504.07058": "|**2025-04-09**|**Physics informed neural network for forward and inverse modeling of low grade brain tumors**|K. Murari, P. Roul, S. Sundar et.al.|[2504.07058](http://arxiv.org/abs/2504.07058)|null||A low grade tumor is a slow growing tumor with a lower likelihood of spreading compared to high grade tumors. Mathematical modeling using partial differential equations (PDEs) plays a crucial role in describing tumor behavior, growth and progression. This study employs the Burgess and extended Fisher Kolmogorov equations to model low-grade brain tumors. We utilize Physics Informed Neural Networks (PINNs) based algorithm to develop an automated numerical solver for these models and explore their application in solving forward and inverse problems in brain tumor modeling. The study aims to demonstrate that the PINN based algorithms serve as advanced methodologies for modeling brain tumor dynamics by integrating deep learning with physics-informed principles. Additionally, we establish generalized error bounds in terms of training and quadrature errors. The convergence and stability of the neural network are derived for both models. Numerical tests confirm the accuracy and efficiency of the algorithms in both linear and nonlinear cases. Additionally, a statistical analysis of the numerical results is presented.|\n", "2504.06998": "|**2025-04-09**|**A Krylov projection algorithm for large symmetric matrices with dense spectra**|Vladimir Druskin J\u00f6rn Zimmerling et.al.|[2504.06998](http://arxiv.org/abs/2504.06998)|null|Block Lanczos, Quadrature, Transfer function, Kre\\u{i}n-Nudelman,   Hermite-Pad\\'e|We consider the approximation of $B^T (A+sI)^{-1} B$ for large s.p.d. $A\\in\\mathbb{R}^{n\\times n}$ with dense spectrum and $B\\in\\mathbb{R}^{n\\times p}$, $p\\ll n$. We target the computations of Multiple-Input Multiple-Output (MIMO) transfer functions for large-scale discretizations of problems with continuous spectral measures, such as linear time-invariant (LTI) PDEs on unbounded domains. Traditional Krylov methods, such as the Lanczos or CG algorithm, are known to be optimal for the computation of $(A+sI)^{-1}B$ with real positive $s$, resulting in an adaptation to the distinctively discrete and nonuniform spectra. However, the adaptation is damped for matrices with dense spectra. It was demonstrated in [Zimmerling, Druskin, Simoncini, Journal of Scientific Computing 103(1), 5 (2025)] that averaging Gau{\\ss} and Gau\\ss -Radau quadratures computed using the block-Lanczos method significantly reduces approximation errors for such problems. Here, we introduce an adaptive Kre\\u{i}n-Nudelman extension to the (block) Lanczos recursions, allowing further acceleration at negligible $o(n)$ cost. Similar to the Gau\\ss -Radau quadrature, a low-rank modification is applied to the (block) Lanczos matrix. However, unlike the Gau\\ss -Radau quadrature, this modification depends on $\\sqrt{s}$ and can be considered in the framework of the Hermite-Pad\\'e approximants, which are known to be efficient for problems with branch-cuts, that can be good approximations to dense spectral intervals. Numerical results for large-scale discretizations of heat-diffusion and quasi-magnetostatic Maxwell's operators in unbounded domains confirm the efficiency of the proposed approach.|\n", "2504.06993": "|**2025-04-10**|**Screening of material defects using universal machine-learning interatomic potentials**|Ethan Berger, Mohammad Bagheri, Hannu-Pekka Komsa et.al.|[2504.06993](http://arxiv.org/abs/2504.06993)|null||Finding new materials with previously unknown atomic structure or materials with optimal set of properties for a specific application greatly benefits from computational modeling. Recently, such screening has been dramatically accelerated by the invent of universal machine-learning interatomic potentials that offer first principles accuracy at orders of magnitude lower computational cost. Their application to the screening of defects with desired properties or to finding new stable compounds with high density of defects, however, has not been explored. Here, we show that the universal machine-learning interatomic potentials have reached sufficient accuracy to enable large-scale screening of defective materials. We carried out vacancy calculations for 86 259 materials in the Materials Project database and analyzed the formation energies in terms of oxidation numbers. We further demonstrate the application of these models for finding new materials at or below the convex hull of known materials and for simulated etching of low-dimensional materials.|\n", "2504.06951": "|**2025-04-09**|**GLT hidden structures in mean-field quantum spin systems**|Christiaan J. F. van de Ven, Muhammad Faisal Khan, S. Serra-Capizzano et.al.|[2504.06951](http://arxiv.org/abs/2504.06951)|null|20 pages, 6 figures|This work explores structured matrix sequences arising in mean-field quantum spin systems. We express these sequences within the framework of generalized locally Toeplitz (GLT) $*$-algebras, leveraging the fact that each GLT matrix sequence has a unique GLT symbol. This symbol characterizes both the asymptotic singular value distribution and, for Hermitian or quasi-Hermitian sequences, the asymptotic spectral distribution. Specifically, we analyze two cases of real symmetric matrix sequences stemming from mean-field quantum spin systems and determine their associated distributions using GLT theory. Our study concludes with visualizations and numerical tests that validate the theoretical findings, followed by a discussion of open problems and future directions.|\n", "2504.06938": "|**2025-04-09**|**On the Compressibility of Integral Operators in Anisotropic Wavelet Coordinates**|Helmut Harbrecht, Remo von Rickenbach et.al.|[2504.06938](http://arxiv.org/abs/2504.06938)|null||The present article is concerned with the s*-compressibility of classical boundary integral operators in anisotropic wavelet coordinates. Having the s*-compressibility at hand, one can design adaptive wavelet algorithms which are asymptotically optimal, meaning that any target accuracy can be achieved at a computational expense that stays proportional to the number of degrees of freedom (within the setting determined by an underlying wavelet basis) that would ideally be necessary for realising that target accuracy if full knowledge about the unknown solution were given. As we consider here anisotropic wavelet coordinates, we can achieve higher convergence rates compared to the standard, isotropic setting. Especially, edge singularities of anisotropic nature can be resolved.|\n", "2504.06912": "|**2025-04-09**|**A modified-residue prescription to calculate dynamical correlation functions**|Igor Benek-Lins, Jonathan Discenza, Saurabh Maiti et.al.|[2504.06912](http://arxiv.org/abs/2504.06912)|null|12 pages, 8 figures|One of the challenges in using numerical methods to address many-body problems is the multi-dimensional integration over poles. More often that not, one needs such integrations to be evaluated as a function of an external variable. An example would be calculating dynamical correlations functions that are used to model response functions, where the external variable is the frequency. The standard numerical techniques rely on building an adaptive mesh, using special points in the Brillouin zone or using advanced smearing techniques. Most of these techniques, however, suffer when the grid is coarse. Here we propose that, if one knows the nature of the singularity in the integrand, one can define a residue and use it to faithfully estimate the integral and reproduce all the resulting singular features even with a coarse grid. We demonstrate the effectiveness of the method for different scenarios of calculating correlation functions with different resulting singular features, for calculating collective modes and densities of states. We also present a quantitative analysis of the error and show that this method can be widely applicable.|\n", "2504.06889": "|**2025-04-09**|**Mixed-Precision in High-Order Methods: the Impact of Floating-Point Precision on the ADER-DG Algorithm**|Marc Marot-Lassauzaie, Michael Bader et.al.|[2504.06889](http://arxiv.org/abs/2504.06889)|null||We present a mixed-precision implementation of the high-order discontinuous Galerkin method with ADER time stepping (ADER-DG) for solving hyperbolic systems of partial differential equations (PDEs) in the hyperbolic PDE engine ExaHyPE. The implementation provides a simple API extension for specifying the numerical precision for individual kernels, and thus allows for testing the effect of low and mixed precision on the accuracy of the solution. To showcase this, we study the impact of precision on the overall convergence order and actual accuracy of the method as achieved for four common hyperbolic PDE systems and five relevant scenarios that feature an analytic solution. For all scenarios, we also assess how sensitive each kernel of the ADER-DG algorithm is to using double, single or even half precision. This addresses the question where thoughtful adoption of mixed precision can mitigate hurtful effects of low precision on the overall simulation.|\n", "2504.06837": "|**2025-04-09**|**Discrete-to-continuum limit for nonlinear reaction-diffusion systems via EDP convergence for gradient systems**|Georg Heinze, Alexander Mielke, Artur Stephan et.al.|[2504.06837](http://arxiv.org/abs/2504.06837)|null||We investigate the convergence of spatial discretizations for reaction-diffusion systems with mass-action law satisfying a detailed balance condition. Considering systems on the d-dimensional torus, we construct appropriate space-discrete processes and show convergence not only on the level of solutions, but also on the level of the gradient systems governing the evolutions. As an important step, we prove chain rule inequalities for the reaction-diffusion systems as well as their discretizations, featuring a non-convex dissipation functional. The convergence is then obtained with variational methods by building on the recently introduced notion of gradient systems in continuity equation format.|\n", "2504.06809": "|**2025-04-09**|**Modeling and analysis methods for early detection of leakage points in gas transmission systems**|Ilgar Aliyev et.al.|[2504.06809](http://arxiv.org/abs/2504.06809)|null|16 pages, 1 figures, Graph 5, 14th International Conference on   Control, Modelling, Computing and Applications (CMCA 2025)|Early detection of leaks in gas transmission systems is crucial for ensuring uninterrupted gas supply, enhancing operational efficiency, and minimizing environmental and economic risks. This study aims to develop an analytical method for accurately identifying leak locations in gas pipelines based on unsteady gas flow dynamics. A novel approach is proposed that utilizes pressure variations at the inlet and outlet points to determine the minimum fixation time (t = t1) required for real-time leak detection. Through mathematical modeling and numerical analysis, the study demonstrates that the ratio of pressure drops at different points along the pipeline can be effectively used to pinpoint leakage locations. The results indicate that the proposed method significantly improves detection accuracy and response time, making it a viable solution for integration into gas pipeline monitoring and control systems.|\n", "2504.06782": "|**2025-04-09**|**Probabilistic Grading and Classification System for End-of-Life Building Components Toward Circular Economy Loop**|Yiping Meng, Sergio Cavalaro, Mohamed Osmani et.al.|[2504.06782](http://arxiv.org/abs/2504.06782)|null|23 pages, 12 figures, 8tables|The longevity and viability of construction components in a circular economy demand a robust, data-informed framework for reuse decision-making. This paper introduces a multi-level grading and classification system that combines Bayesian probabilistic modeling with scenario-based performance thresholds to assess the reusability of end-of-life modular components. By grading components across a five-tier scale, the system supports strategic decisions for reuse, up-use, or down-use, ensuring alignment with engineering standards and sustainability objectives. The model's development is grounded in empirical data from precast concrete wall panels, and its explainability is enhanced through decision tree logic and Sankey visualizations that trace the influence of contextual scenarios on classification outcomes. MGCS addresses the environmental, economic, and operational challenges of EoL management--reducing material waste, optimizing value recovery, and improving workflow efficiency. Through dynamic feature weighting and transparent reasoning, the system offers a practical yet rigorous pathway to embed circular thinking into construction industry practices.|\n", "2504.06774": "|**2025-04-09**|**Hybrid machine learning models based on physical patterns to accelerate CFD simulations: a short guide on autoregressive models**|Arindam Sengupta, Rodrigo Abad\u00eda-Heredia, Ashton Hetherington et.al.|[2504.06774](http://arxiv.org/abs/2504.06774)|null||Accurate modeling of the complex dynamics of fluid flows is a fundamental challenge in computational physics and engineering. This study presents an innovative integration of High-Order Singular Value Decomposition (HOSVD) with Long Short-Term Memory (LSTM) architectures to address the complexities of reduced-order modeling (ROM) in fluid dynamics. HOSVD improves the dimensionality reduction process by preserving multidimensional structures, surpassing the limitations of Singular Value Decomposition (SVD). The methodology is tested across numerical and experimental data sets, including two- and three-dimensional (2D and 3D) cylinder wake flows, spanning both laminar and turbulent regimes. The emphasis is also on exploring how the depth and complexity of LSTM architectures contribute to improving predictive performance. Simpler architectures with a single dense layer effectively capture the periodic dynamics, demonstrating the network's ability to model non-linearities and chaotic dynamics. The addition of extra layers provides higher accuracy at minimal computational cost. These additional layers enable the network to expand its representational capacity, improving the prediction accuracy and reliability. The results demonstrate that HOSVD outperforms SVD in all tested scenarios, as evidenced by using different error metrics. Efficient mode truncation by HOSVD-based models enables the capture of complex temporal patterns, offering reliable predictions even in challenging, noise-influenced data sets. The findings underscore the adaptability and robustness of HOSVD-LSTM architectures, offering a scalable framework for modeling fluid dynamics.|\n", "2504.06763": "|**2025-04-09**|**Convergence of a continuous Galerkin method for the Biot-Allard poroelasticity system**|Jakob S. Stokke, Markus Bause, Florin A. Radu et.al.|[2504.06763](http://arxiv.org/abs/2504.06763)|null||We study a space-time finite element method for a system of poromechanics with memory effects that are modeled by a convolution integral. In the literature, the system is referred to as the Biot-Allard model. We recast the model as a first-order system in time, where the memory effects are transformed into an auxiliary differential equation. This allows for a computationally efficient numerical scheme. The system is discretized by continuous Galerkin methods in time and equal-order finite element methods in space. An optimal order error estimate is proved for the norm of the first-order energy of the unknowns of the system. The estimate is confirmed by numerical experiments.|\n", "2504.06621": "|**2025-04-09**|**Computation of shape Taylor expansions**|Gang Bao, Jun Lai, Haoran Ma et.al.|[2504.06621](http://arxiv.org/abs/2504.06621)|null||Shape derivative is an important analytical tool for studying scattering problems involving perturbations in scatterers. Many applications, including inverse scattering, optimal design, and uncertainty quantification, are based on shape derivatives. However, computing high order shape derivatives is challenging due to the complexity of shape calculus. This work introduces a comprehensive method for computing shape Taylor expansions in two dimensions using recurrence formulas. The approach is developed under sound-soft, sound-hard, impedance, and transmission boundary conditions. Additionally, we apply the shape Taylor expansion to uncertainty quantification in wave scattering, enabling high order moment estimation for the scattered field under random boundary perturbations. Numerical examples are provided to illustrate the effectiveness of the shape Taylor expansion in achieving high order approximations.|\n", "2504.06603": "|**2025-04-09**|**Asymptotic Variance in the Central Limit Theorem for Multilevel Markovian Stochastic Approximation**|Ajay Jasra, Abylay Zhumekenov et.al.|[2504.06603](http://arxiv.org/abs/2504.06603)|null||In this note we consider the finite-dimensional parameter estimation problem associated to inverse problems. In such scenarios, one seeks to maximize the marginal likelihood associated to a Bayesian model. This latter model is connected to the solution of partial or ordinary differential equation. As such, there are two primary difficulties in maximizing the marginal likelihood (i) that the solution of differential equation is not always analytically tractable and (ii) neither is the marginal likelihood. Typically (i) is dealt with using a numerical solution of the differential equation, leading to a numerical bias and (ii) has been well studied in the literature using, for instance, Markovian stochastic approximation. It is well-known that to reduce the computational effort to obtain the maximal value of the parameter, one can use a hierarchy of solutions of the differential equation and combine with stochastic gradient methods. Several approaches do exactly this. In this paper we consider the asymptotic variance in the central limit theorem, associated to known estimates and find bounds on the asymptotic variance in terms of the precision of the solution of the differential equation. The significance of these bounds are the that they provide missing theoretical guidelines on how to set simulation parameters; that is, these appear to be the first mathematical results which help to run the methods efficiently in practice.|\n", "2504.06583": "|**2025-04-09**|**Aplicando diferencias finitas para resolver ecuaciones y sistemas de ecuaciones diferenciales parciales sobre dominios planos irregulares simplemente conexos y no conexos**|Miriam Sosa-D\u00edaz, Faustino Sanchez-Garduno et.al.|[2504.06583](http://arxiv.org/abs/2504.06583)|null|In Spanish language, in preparation for journal submission|Using exhaustion method and finite differences a new method to solve system of partial differential equations and is presented. This method allows design algorithm to solve linear and nonlinear systems in irregular domains. Applying this method to solve linear and nonlinear problems with prescribed conditions Dirichlet over two-dimensional irregular domains are analyzed.|\n", "2504.06475": "|**2025-04-08**|**Successive randomized compression: A randomized algorithm for the compressed MPO-MPS product**|Chris Cama\u00f1o, Ethan N. Epperly, Joel A. Tropp et.al.|[2504.06475](http://arxiv.org/abs/2504.06475)|null|29 pages, 5 figures|Tensor networks like matrix product states (MPSs) and matrix product operators (MPOs) are powerful tools for representing exponentially large states and operators, with applications in quantum many-body physics, machine learning, numerical analysis, and other areas. In these applications, computing a compressed representation of the MPO--MPS product is a fundamental computational primitive. For this operation, this paper introduces a new single-pass, randomized algorithm, called successive randomized compression (SRC), that improves on existing approaches in speed or in accuracy. The performance of the new algorithm is evaluated on synthetic problems and unitary time evolution problems for quantum spin systems.|\n", "2504.06458": "|**2025-04-08**|**Solving Power System Problems using Adiabatic Quantum Computing**|Zeynab Kaseb, Matthias Moller, Peter Palensky et.al.|[2504.06458](http://arxiv.org/abs/2504.06458)|null|3 pages, 3 figures|This letter proposes a novel combinatorial optimization framework that reformulates existing power system problems into a format executable on quantum annealers. The proposed framework accommodates both normal and complex numbers and enables efficient handling of large-scale problems, thus ensuring broad applicability across power system problems. As a proof of concept, we demonstrate its applicability in two classical problems: (i) power system parameter identification, where we estimate the admittance matrix given voltage and current measurements, and (ii) power flow analysis, where we reformulate the nonlinear equations governing active and reactive power balance. The results show that the proposed framework effectively and efficiently solves both linear and nonlinear power system problems, and thus offers significant advantages in scenarios where traditional solvers face challenges, such as ill-conditioned systems and fault conditions.|\n", "2504.06425": "|**2025-04-08**|**Neural Network Enhanced Polyconvexification of Isotropic Energy Densities in Computational Mechanics**|Lo\u00efc Balazi, Timo Neumeier, Malte A. Peter et.al.|[2504.06425](http://arxiv.org/abs/2504.06425)|null|24 pages, 14 figures|We present a neural network approach for fast evaluation of parameter-dependent polyconvex envelopes, which are crucial in computational mechanics. Our method uses a neural network architecture that inherently encodes polyconvexity in the main variable by combining a feature extraction layer that computes the minors function on the signed singular value characterisation of isotropic energy densities with a partially input convex neural network (PICNN). Polyconvex underestimation is weakly enforced by penalisation during training, as are the symmetries of the function. As a guiding example, we focus on a well-known isotropic damage problem, reformulated in terms of signed singular values, and apply a splitting approach to reduce the dimensionality of the parameter space, thereby making training more tractable. Numerical experiments show that the networks achieve sufficient accuracy for engineering applications while providing high compression and significant speed-up over traditional polyconvexification schemes. Most importantly, the network adapts to varying physical or material parameters, enabling real-time polyconvexification in large-scale computational mechanics scenarios.|\n", "2504.06388": "|**2025-04-08**|**Elimination of spurious oscillations on photoemission spectra**|Mart\u00edn Barlari, Diego G. Arb\u00f3, Mar\u00eda Silvia Gravielle et.al.|[2504.06388](http://arxiv.org/abs/2504.06388)|null|23 pages, 11 figures|We present a method for accurately computing transition probabilities in one-dimensional photoionization problems. Our approach involves solving the time-dependent Schr\\\"odinger equation and projecting its solution onto scattering states that satisfy the correct incoming or outgoing boundary conditions. Conventionally, the photoelectron emission spectrum is obtained by projecting the time-evolved wavefunction onto the stationary continuum eigenstates of the unperturbed, time-independent Hamiltonian. However, when the spatial potential is symmetric, both the initial bound state and the final continuum states exhibit well-defined parity. The propagated wavefunction retains structural features of the initial bound state, including its parity. As a result, changes in the parity of the continuum states can introduce substantial variations in the projections, leading to spurious oscillations in the computed electron emission spectrum. Our method circumvents this issue by employing scattering states without defined parity. Furthermore, it enables the calculation of directional emission, making it possible to study emission asymmetries. To illustrate the capabilities of our scattering projection method, we analyze the partial differential photoionization probabilities of Al(111) metallic surfaces under short laser pulses at grazing incidence.|\n", "2504.06378": "|**2025-04-08**|**On Mixed-Precision Iterative Methods and Analysis for Nearly Completely Decomposable Markov Processes**|Vasileios Kalantzis, Mark S. Squillante, Chai Wah Wu et.al.|[2504.06378](http://arxiv.org/abs/2504.06378)|null|30 pages, 3 figures|In this paper we consider the problem of computing the stationary distribution of nearly completely decomposable Markov processes, a well-established area in the classical theory of Markov processes with broad applications in the design, modeling, analysis and optimization of computer systems. We design general classes of algorithmic solution approaches that exploit forms of mixed-precision computation to significantly reduce computation times and that exploit forms of iterative approximate methods to mitigate the impact of inaccurate computations, further reduce computation times, and ensure convergence. Then we derive a mathematical analysis of our general algorithmic approaches that establishes theoretical results on approximation errors, convergence behaviors, and other algorithmic properties. Numerical experiments demonstrate that our general algorithmic approaches provide significant improvements in computation times over the most efficient existing numerical methods.|\n", "2504.07893": "|**2025-04-10**|**Molecular excited state in the interaction quench dynamics of two different atoms in a two-dimensional anisotropic trap**|I. S. Ishmukhamedov, A. S. Ishmukhamedov, Zh. E. Jalankuzov et.al.|[2504.07893](http://arxiv.org/abs/2504.07893)|null|This preprint has not undergone peer review (when applicable) or any   post-submission improvements or corrections. The Version of Record of this   article is published in The European Physical Journal Plus, and is available   online at https://doi.org/10.1140/epjp/s13360-024-04864-2|We explore the interaction quench dynamics of two atoms with different masses and subject to different trapping potentials. Notably, under such anisotropic conditions, the nonequilibrium dynamics can lead to the occupation of molecular excited states. We consider cases of quenching from attractive to repulsive interaction and vice versa, analyzing the impact of the pre- and postquench states. The analysis of overlap integrals for the both states reveals a significant contribution from the molecular excited state. Moreover, the overlap with the prequench states might serve as an indicator of when this excited state may emerge. Additionally, we calculate the energy spectrum for the lowest levels in the both isotropic and anisotropic harmonic traps. Throughout our study, we use a Gaussian-shaped finite-range interaction potential.|\n", "2504.07850": "|**2025-04-10**|**Probabilistic Multi-Criteria Decision-Making for Circularity Performance of Modern Methods of Construction Products**|Yiping Meng, Sergio Cavalaro, Frozan Dizaye Mohamed Osmani et.al.|[2504.07850](http://arxiv.org/abs/2504.07850)|null|37 pages,30 figures,4 tables|The construction industry faces increasingly more significant pressure to reduce resource consumption, minimise waste, and enhance environmental performance. Towards the transition to a circular economy in the construction industry, one of the challenges is the lack of a standardised assessment framework and methods to measure circularity at the product level. To support a more sustainable and circular construction industry through robust and enhanced scenario analysis, this paper integrates probabilistic analysis into the coupled assessment framework; this research addresses uncertainties associated with multiple criteria and diverse stakeholders in the construction industry to enable more robust decision-making support on both circularity and sustainability performance. By demonstrating the application in three real-world MMC products, the proposed framework offers a novel approach to simultaneously assess the circularity and sustainability of MMC products with robustness and objectiveness.|\n", "2504.07835": "|**2025-04-10**|**Pychop: Emulating Low-Precision Arithmetic in Numerical Methods and Neural Networks**|Erin Carson, Xinye Chen et.al.|[2504.07835](http://arxiv.org/abs/2504.07835)|null||Motivated by the growing demand for low-precision arithmetic in computational science, we exploit lower-precision emulation in Python -- widely regarded as the dominant programming language for numerical analysis and machine learning. Low-precision training has revolutionized deep learning by enabling more efficient computation and reduced memory and energy consumption while maintaining model fidelity. To better enable numerical experimentation with and exploration of low precision computation, we developed the Pychop library, which supports customizable floating-point formats and a comprehensive set of rounding modes in Python, allowing users to benefit from fast, low-precision emulation in numerous applications. Pychop also introduces interfaces for both PyTorch and JAX, enabling efficient low-precision emulation on GPUs for neural network training and inference with unparalleled flexibility.   In this paper, we offer a comprehensive exposition of the design, implementation, validation, and practical application of Pychop, establishing it as a foundational tool for advancing efficient mixed-precision algorithms. Furthermore, we present empirical results on low-precision emulation for image classification and object detection using published datasets, illustrating the sensitivity of the use of low precision and offering valuable insights into its impact. Pychop enables in-depth investigations into the effects of numerical precision, facilitates the development of novel hardware accelerators, and integrates seamlessly into existing deep learning workflows. Software and experimental code are publicly available at https://github.com/inEXASCALE/pychop.|\n", "2504.07809": "|**2025-04-10**|**A Riemannian Gradient Descent Method for the Least Squares Inverse Eigenvalue Problem**|Alban Bloor Riley, Marcus Webb, Michael L. Baker et.al.|[2504.07809](http://arxiv.org/abs/2504.07809)|null||We address an algorithm for the least squares fitting of a subset of the eigenvalues of an unknown Hermitian matrix lying an an affine subspace, called the Lift and Projection (LP) method, due to Chen and Chu (SIAM Journal on Numerical Analysis, 33 (1996), pp.2417-2430). The LP method iteratively `lifts' the current iterate onto the spectral constraint manifold then 'projects' onto the solution's affine subspace. We prove that this is equivalent to a Riemannian Gradient Descent with respect to a natural Riemannian metric. This insight allows us to derive a more efficient implementation, analyse more precisely its global convergence properties, and naturally append additional constraints to the problem. We provide several numerical experiments to demonstrate the improvement in computation time, which can be more than an order of magnitude if the eigenvalue constraints are on the smallest eigenvalues, the largest eigenvalues, or the eigenvalues closest to a given number. These experiments include an inverse eigenvalue problem arising in Inelastic Neutron Scattering of Manganese-6, which requires the least squares fitting of 16 experimentally observed eigenvalues of a $32400\\times32400$ sparse matrix from a 5-dimensional subspace of spin Hamiltonian matrices.|\n", "2504.07796": "|**2025-04-10**|**Numerical solution by shape optimization method to an inverse shape problem in multi-dimensional advection-diffusion problem with space dependent coefficients**|Elmehdi Cherrat, Lekbir Afraites, Julius Fergy Tiongson Rabago et.al.|[2504.07796](http://arxiv.org/abs/2504.07796)|null||This work focuses on numerically solving a shape identification problem related to advection-diffusion processes with space-dependent coefficients using shape optimization techniques. Two boundary-type cost functionals are considered, and their corresponding variations with respect to shapes are derived using the adjoint method, employing the chain rule approach. This involves firstly utilizing the material derivative of the state system and secondly using its shape derivative. Subsequently, an alternating direction method of multipliers (ADMM) combined with the Sobolev-gradient-descent algorithm is applied to stably solve the shape reconstruction problem. Numerical experiments in two and three dimensions are conducted to demonstrate the feasibility of the methods.|\n", "2504.07712": "|**2025-04-10**|**On the instabilities of naive FEM discretizations for PDEs with sign-changing coefficients**|Martin Halla, Florian Oberender et.al.|[2504.07712](http://arxiv.org/abs/2504.07712)|null||We consider a scalar diffusion equation with a sign-changing coefficient in its principle part. The well-posedness of such problems has already been studied extensively provided that the contrast of the coefficient is non-critical. Furthermore, many different approaches have been proposed to construct stable discretizations thereof, because naive finite element discretizations are expected to be non-reliable in general. However, no explicit example proving the actual instability is known and numerical experiments often do not manifest instabilities in a conclusive manner. To this end we construct an explicit example with a broad family of meshes for which we prove that the corresponding naive finite element discretizations are unstable. On the other hand, we also provide a broad family of (non-symmetric) meshes for which we prove that the discretizations are stable. Together, these two findings explain the results observed in numerical experiments.|\n", "2504.07637": "|**2025-04-10**|**Global approximation to the Boys functions for vectorized computation**|Dimitri N. Laikov et.al.|[2504.07637](http://arxiv.org/abs/2504.07637)|null|Boys, Boys, Boys. I'm looking for a good time|A fast approximation to the Boys functions (related to the lower incomplete gamma function of half-integer parameter) by a single closed-form analytical expression for all argument values have been developed and tested. Besides the exponential function needed anyway for downward recursion, it uses a small number of addition, multiplication, division, and square root operations, and thus is straightforward to vectorize.|\n", "2504.07580": "|**2025-04-10**|**A computational study of low precision incomplete Cholesky factorization preconditioners for sparse linear least-squares problems**|Jennifer Scott, Miroslav T\u016fma et.al.|[2504.07580](http://arxiv.org/abs/2504.07580)|null|25 pages, 5 figures, 11 tables|Our interest lies in the robust and efficient solution of large sparse linear least-squares problems. In recent years, hardware developments have led to a surge in interest in exploiting mixed precision arithmetic within numerical linear algebra algorithms to take advantage of potential savings in memory requirements, runtime and energy use, whilst still achieving the requested accuracy. We explore employing mixed precision when solving least-squares problems, focusing on the practicalities of developing robust approaches using low precision incomplete Cholesky factorization preconditioners. Key penalties associated with lower precision include a loss of reliability and less accuracy in the computed solution. Through experiments involving problems from practical applications, we study computing incomplete Cholesky factorizations of the normal matrix using low precision and using the factors to precondition LSQR using mixed precision. We investigate level-based and memory-limited incomplete factorization preconditioners. We find that the former are not effective for least-squares problems while the latter can provide high-quality preconditioners. In particular, half precision arithmetic can be considered if high accuracy is not required in the solution or the memory for the incomplete factors is very restricted; otherwise, single precision can be used, and double precision accuracy recovered while reducing memory consumption, even for ill-conditioned problems.|\n", "2504.07568": "|**2025-04-10**|**Ground State Energy of Helium Using a Four-Qubit Photonic Processor with the Variational Quantum Eigensolver (VQE)**|Badie Ghavami, Forouzan Mirmasoudi et.al.|[2504.07568](http://arxiv.org/abs/2504.07568)|null|7 pages, 3 figures, 1 table|To understand the properties and interactions of materials, and determining the ground state energies is one of the important challenges in quantum chemistry, materials science, and quantum mechanics, where quantum computing can play an important role for studying the properties of materials. In this study, we have explored the quantum processor application to compute the Helium (He) molecule ground state energy which utilizes the Variational Quantum Eigensolver (VQE) algorithm. In here, we have implemented VQE on a state-of-the-art quantum processor, optimizing a parameterized quantum circuit to minimize the energy expectation value of the He molecule's Hamiltonian on the four qubits processor. The obtained results of this work show a significant improvement in accuracy compared to classical computational methods, such as Hartree-Fock and density functional theory, which demonstrate the compute potential of quantum algorithms in quantum many-body problems. Thus, these results demonstrate the advantages of quantum computing in achieving high accuracy in simulations of molecular and material properties, and pave the way for future applications in more complex systems. This work highlights the potential of quantum processors in the fields of quantum chemistry, computational physics, and data science.|\n", "2504.07558": "|**2025-04-10**|**Atomic structure analysis of PL5 in silicon carbide with single-spin spectroscopy**|Yu Chen, Qi Zhang, Mingzhe Liu et.al.|[2504.07558](http://arxiv.org/abs/2504.07558)|null|6 pages, 5 figures|Divacancy (VV) spin defects in 4H polytype of silicon carbide (4H-SiC) are emerging candidates for quantum information processing and quantum sensing. Among these defects, PL5 and PL6 stand out due to their superior charge stability and optically detected magnetic resonance (ODMR) properties at room temperature. However, their atomic structures remain unresolved, with ongoing controversy regarding their potential association with stacking faults. Previous measurements relying on spin ensemble detection are insufficient to draw definitive conclusions. In this study, we conduct correlative imaging of stacking faults and PL5-6 at single-defect level, conclusively demonstrating that PL5-6 are not associated with stacking faults. Further investigation of PL5 through single-spin ODMR spectroscopy allows us to determine its six spatial orientations, as well as to measure the orientation of its transverse anisotropy spin splitting (E) and the statistical distribution of hyperfine splitting. These results and ab initio calculations suggest that PL5 should be VsiVc(hk) divacancy coupled with a nearby antisite atom (VVA). The structure resolution of PL5 starts the first step toward its controllable fabrication, paving the way for various applications.|\n", "2504.07520": "|**2025-04-10**|**Stability and Convergence of Strang Splitting Method for the Allen-Cahn Equation with Homogeneous Neumann Boundary Condition**|Chaoyu Quan, Zhijun Tan, Yanyao Wu et.al.|[2504.07520](http://arxiv.org/abs/2504.07520)|null||The Strang splitting method has been widely used to solve nonlinear reaction-diffusion equations, with most theoretical convergence analysis assuming periodic boundary conditions. However, such analysis presents additional challenges for the case of homogeneous Neumann boundary condition. In this work the Strang splitting method with variable time steps is investigated for solving the Allen--Cahn equation with homogeneous Neumann boundary conditions. Uniform $H^k$-norm stability is established under the assumption that the initial condition $u^0$ belongs to the Sobolev space $H^k(\\Omega)$ with integer $k\\ge 0$, using the Gagliardo--Nirenberg interpolation inequality and the Sobolev embedding inequality. Furthermore, rigorous convergence analysis is provided in the $H^k$-norm for initial conditions $u^0 \\in H^{k+6}(\\Omega)$, based on the uniform stability. Several numerical experiments are conducted to verify the theoretical results, demonstrating the effectiveness of the proposed method.|\n", "2504.07508": "|**2025-04-10**|**Parton Distribution Functions in the Schwinger model from Tensor Network States**|Mari Carmen Ban\u0169ls, Krzysztof Cichy, C. -J. David Lin et.al.|[2504.07508](http://arxiv.org/abs/2504.07508)|null|14 pages, 9 figures|Parton distribution functions (PDFs) describe the inner, non-perturbative structure of hadrons. Their computation involves matrix elements with a Wilson line along a direction on the light cone, posing significant challenges in Euclidean lattice calculations, where the time direction is not directly accessible. We propose implementing the light-front Wilson line within the Hamiltonian formalism using tensor network techniques. The approach is demonstrated in the massive Schwinger model (quantum electrodynamics in 1+1 dimensions), a toy model that shares key features with quantum chromodynamics. We present accurate continuum results for the fermion PDF of the vector meson at varying fermion masses, obtained from first principle calculations directly in Minkowski space. Our strategy also provides a useful path for quantum simulations and quantum computing.|\n", "2504.07391": "|**2025-04-10**|**High-order discretization errors for the Caputo derivative in H\u00f6lder spaces**|Xiangyi Peng, Lisen Ding, Dongling Wang et.al.|[2504.07391](http://arxiv.org/abs/2504.07391)|null||Building upon the recent work of Teso and Plociniczak (2025) regarding L1 discretization errors for the Caputo derivative in H\\\"{o}lder spaces, this study extends the analysis to higher-order discretization errors within the same functional framework. We first investigate truncation errors for the L2 and L1-2 methods, which approximate the Caputo derivative via piecewise quadratic interpolation. Then we generalize the results to arbitrary high-order discretization. Theoretical analyses reveal a unified error structure across all schemes: the convergence order equals the difference between the smoothness degree of the function space and the fractional derivative order, i.e., order of error = degree of smoothness - order of the derivative. Numerical experiments validate these theoretical findings.|\n", "2504.07388": "|**2025-04-10**|**Min-Max Optimisation for Nonconvex-Nonconcave Functions Using a Random Zeroth-Order Extragradient Algorithm**|Amir Ali Farzin, Yuen Man Pun, Philipp Braun et.al.|[2504.07388](http://arxiv.org/abs/2504.07388)|null||This study explores the performance of the random Gaussian smoothing Zeroth-Order ExtraGradient (ZO-EG) scheme considering min-max optimisation problems with possibly NonConvex-NonConcave (NC-NC) objective functions. We consider both unconstrained and constrained, differentiable and non-differentiable settings. We discuss the min-max problem from the point of view of variational inequalities. For the unconstrained problem, we establish the convergence of the ZO-EG algorithm to the neighbourhood of an $\\epsilon$-stationary point of the NC-NC objective function, whose radius can be controlled under a variance reduction scheme, along with its complexity. For the constrained problem, we introduce the new notion of proximal variational inequalities and give examples of functions satisfying this property. Moreover, we prove analogous results to the unconstrained case for the constrained problem. For the non-differentiable case, we prove the convergence of the ZO-EG algorithm to a neighbourhood of an $\\epsilon$-stationary point of the smoothed version of the objective function, where the radius of the neighbourhood can be controlled, which can be related to the ($\\delta,\\epsilon$)-Goldstein stationary point of the original objective function.|\n", "2504.07377": "|**2025-04-10**|**Euler-Lagrange study of Microbubble-Laden Turbulent Flow over Superhydrophobic surfaces**|Byeong-Cheon Kim, Kyoungsik Chang, Sang-Wook Lee et.al.|[2504.07377](http://arxiv.org/abs/2504.07377)|null|28 pages, 9 figures|For slow-speed ships, underwater vehicles, and pipe transportation systems, viscous resistance accounts for a large proportion of the total energy losses. As such, various technologies have been developed to reduce viscous resistance and enhance energy efficiency in these applications. Air injection and surface treatment are two representative drag reduction techniques. Additionally, efforts to combine multiple drag-reduction techniques have been the subject of extensive research. In this study, the synergistic effects of integrating microbubble injection and superhydrophobic Surface(SHS) drag reduction approaches were analyzed. A 2-way coupling Euler-Lagrange approach was used alongside direct numerical simulation, based on the spectral element method, to investigate the synergistic effects of applying two separate drag reduction methods. Three types of SHS were investigated in our simulations; post type, transverse ridge type, and ridge type. The drag reduction performances and flow characteristics of the various configurations, with and without microbubble injection, were compared in a turbulent horizontal channel flow with $Re_{\\tau}=180$. The results of these tests showed that, combining post-type SHS with microbubbles was the most effective, producing a synergistic drag reduction effect. However, combining microbubble injection with ridge-type SHS increased drag relative to ridge-type SHS alone, showing the importance of carefully selecting wall type for the best possible performance.|\n", "2504.07295": "|**2025-04-09**|**Advanced measurement techniques in quantum Monte Carlo: The permutation matrix representation approach**|Nic Ezzell, Itay Hen et.al.|[2504.07295](http://arxiv.org/abs/2504.07295)|**[link](https://github.com/naezzell/PMRQMC_fidsus)**|33 pages, 3 figures, 2 tables|In a typical finite temperature quantum Monte Carlo (QMC) simulation, estimators for simple static observables such as specific heat and magnetization are known. With a great deal of system-specific manual labor, one can sometimes also derive more complicated non-local or even dynamic observable estimators. Within the permutation matrix representation (PMR) flavor of QMC, however, we show that one can derive formal estimators for arbitrary static observables. We also derive exact, explicit estimators for general imaginary-time correlation functions and non-trivial integrated susceptibilities thereof. We demonstrate the practical versatility of our method by estimating various non-local, random observables for the transverse-field Ising model on a square lattice.|\n", "2504.07269": "|**2025-04-09**|**A Space-Time Continuous Galerkin Finite Element Method for Linear Schr\u00f6dinger Equations**|Marco Zank et.al.|[2504.07269](http://arxiv.org/abs/2504.07269)|null|8 pages|We introduce a space-time finite element method for the linear time-dependent Schr\\\"odinger equation with Dirichlet conditions in a bounded Lipschitz domain. The proposed discretization scheme is based on a space-time variational formulation of the time-dependent Schr\\\"odinger equation. In particular, the space-time method is conforming and is of Galerkin-type, i.e., trial and test spaces are equal. We consider a tensor-product approach with respect to time and space, using piecewise polynomial, continuous trial and test functions. In this case, we state the global linear system and efficient direct space-time solvers based on exploiting the Kronecker structure of the global system matrix. This leads to the Bartels-Stewart method and the fast diagonalization method. Both methods result in solving a sequence of spatial subproblems. In particular, the fast diagonalization method allows for solving the spatial subproblems in parallel, i.e., a time parallelization is possible. Numerical examples for a two-dimensional spatial domain illustrate convergence in space-time norms and show the potential of the proposed solvers.|\n", "2504.07218": "|**2025-04-09**|**Numerical analysis of three-dimensional magnetohydrodynamic effects in an inductively coupled plasma wind tunnel**|Sanjeev Kumar, Alessandro Munafo, Daniel J Bodony et.al.|[2504.07218](http://arxiv.org/abs/2504.07218)|null|36 pages, 22 figures|This paper introduces a three-dimensional model for the 350kW Plasmatron X inductively coupled plasma facility at the University of Illinois Urbana-Champaign, designed for testing high-temperature materials. Simulations of the facility have been performed using a three-dimensional, multiphysics computational framework, which reveals pronounced three-dimensional characteristics within the facility. The analysis of the plasma and electromagnetic field in the torch region reveals the influence of the helical coils, which cause a non-axisymmetric distribution of the plasma discharge. Additionally, simulations of the torch-chamber configuration at two operating pressures have been conducted to examine the impact of plasma asymmetry in the torch on jet characteristics in the chamber. The results indicate an unsteady, three-dimensional behavior of the plasma jet at high pressure. Spectral Proper Orthogonal Decomposition (SPOD) has been performed on the unsteady flow field to identify the dominant modes and their associated frequencies. At low pressure, a steady, supersonic, nearly axisymmetric plasma jet forms with consistent flow properties, such as temperature and velocity. However, strong non-equilibrium effects at low pressures lead to substantial deviations in species concentrations from axial symmetry despite having an almost axisymmetric distribution for quantities such as velocity and temperatures.|\n", "2504.08730": "|**2025-04-11**|**Dimension reduction for derivative-informed operator learning: An analysis of approximation errors**|Dingcheng Luo, Thomas O'Leary-Roseberry, Peng Chen et.al.|[2504.08730](http://arxiv.org/abs/2504.08730)|null||We study the derivative-informed learning of nonlinear operators between infinite-dimensional separable Hilbert spaces by neural networks. Such operators can arise from the solution of partial differential equations (PDEs), and are used in many simulation-based outer-loop tasks in science and engineering, such as PDE-constrained optimization, Bayesian inverse problems, and optimal experimental design. In these settings, the neural network approximations can be used as surrogate models to accelerate the solution of the outer-loop tasks. However, since outer-loop tasks in infinite dimensions often require knowledge of the underlying geometry, the approximation accuracy of the operator's derivatives can also significantly impact the performance of the surrogate model. Motivated by this, we analyze the approximation errors of neural operators in Sobolev norms over infinite-dimensional Gaussian input measures. We focus on the reduced basis neural operator (RBNO), which uses linear encoders and decoders defined on dominant input/output subspaces spanned by reduced sets of orthonormal bases. To this end, we study two methods for generating the bases; principal component analysis (PCA) and derivative-informed subspaces (DIS), which use the dominant eigenvectors of the covariance of the data or the derivatives as the reduced bases, respectively. We then derive bounds for errors arising from both the dimension reduction and the latent neural network approximation, including the sampling errors associated with the empirical estimation of the PCA/DIS. Our analysis is validated on numerical experiments with elliptic PDEs, where our results show that bases informed by the map (i.e., DIS or output PCA) yield accurate reconstructions and generalization errors for both the operator and its derivatives, while input PCA may underperform unless ranks and training sample sizes are sufficiently large.|\n", "2504.08608": "|**2025-04-11**|**Discretization Error Analysis of a High Order Unfitted Space-Time Method for moving domain problems**|Fabian Heimann, Christoph Lehrenfeld, Janosch Preu\u00df et.al.|[2504.08608](http://arxiv.org/abs/2504.08608)|null||We present a numerical analysis of a higher order unfitted space-time Finite Element method applied to a convection-diffusion model problem posed on a moving bulk domain. The method uses isoparametric space-time mappings for the geometry approximation of level set domains and has been presented and investigated computationally in [Heimann, Lehrenfeld, Preu{\\ss}, SIAM J. Sci. Comp. 45(2), 2023, B139 - B165]. Recently, in [Heimann, Lehrenfeld, IMA J. Numer. Anal., 2025] error bounds for the geometry approximation have been proven. In this paper we prove stability and accuracy including the influence of the geometry approximation.|\n", "2504.08544": "|**2025-04-11**|**Slicing the Gaussian Mixture Wasserstein Distance**|Moritz Piening, Robert Beinert et.al.|[2504.08544](http://arxiv.org/abs/2504.08544)|null||Gaussian mixture models (GMMs) are widely used in machine learning for tasks such as clustering, classification, image reconstruction, and generative modeling. A key challenge in working with GMMs is defining a computationally efficient and geometrically meaningful metric. The mixture Wasserstein (MW) distance adapts the Wasserstein metric to GMMs and has been applied in various domains, including domain adaptation, dataset comparison, and reinforcement learning. However, its high computational cost -- arising from repeated Wasserstein distance computations involving matrix square root estimations and an expensive linear program -- limits its scalability to high-dimensional and large-scale problems. To address this, we propose multiple novel slicing-based approximations to the MW distance that significantly reduce computational complexity while preserving key optimal transport properties. From a theoretical viewpoint, we establish several weak and strong equivalences between the introduced metrics, and show the relations to the original MW distance and the well-established sliced Wasserstein distance. Furthermore, we validate the effectiveness of our approach through numerical experiments, demonstrating computational efficiency and applications in clustering, perceptual image comparison, and GMM minimization|\n", "2504.08461": "|**2025-04-11**|**Astrophysical constraints on the simulation hypothesis for this Universe: why it is (nearly) impossible that we live in a simulation**|F. Vazza et.al.|[2504.08461](http://arxiv.org/abs/2504.08461)|null|17 pages, 4 figures. Frontiers in Physics, in press|We assess how physically realistic the ''simulation hypothesis'' for this Universe is, based on physical constraints arising from the link between information and energy, and on known astrophysical constraints. We investigate three cases: the simulation of the entire visible Universe, the simulation of Earth only, or a low resolution simulation of Earth, compatible with high-energy neutrino observations. In all cases, the amounts of energy or power required by any version of the simulation hypothesis are entirely incompatible with physics, or (literally) astronomically large, even in the lowest resolution case. Only universes with very different physical properties can produce some version of this Universe as a simulation. On the other hand, our results show that it is just impossible that this Universe is simulated by a universe sharing the same properties, regardless of technological advancements of the far future.|\n", "2504.08450": "|**2025-04-11**|**Well-Posedness of Discretizations for Fractional Elasto-Plasticity**|Michael Feischl, David Niederkofler, Barbara Wohlmuth et.al.|[2504.08450](http://arxiv.org/abs/2504.08450)|null||We consider a fractional plasticity model based on linear isotropic and kinematic hardening as well as a standard von-Mises yield function, where the flow rule is replaced by a Riesz--Caputo fractional derivative. The resulting mathematical model is typically non-local and non-smooth. Our numerical algorithm is based on the well-known radial return mapping and exploits that the kernel is finitely supported. We propose explicit and implicit discretizations of the model and show the well-posedness of the explicit in time discretization in combination with a standard finite element approach in space. Our numerical results in 2D and 3D illustrate the performance of the algorithm and the influence of the fractional parameter.|\n", "2504.08434": "|**2025-04-11**|**Elasticity of bidisperse attractive particle systems**|Yaqi Zhao, Antoine Sanner, Luca Michel et.al.|[2504.08434](http://arxiv.org/abs/2504.08434)|null|20 pages, 5 figures|Bidisperse particle systems are common in both natural and engineered materials, and it is known to influence packing, flow, and stability. However, their direct effect on elastic properties, particularly in systems with attractive interactions, remains poorly understood. Gaining insight into this relationship is important for designing soft particle-based materials with desired mechanical response. In this work, we study how particle size ratio and composition affect the shear modulus of attractive particle systems. Using coarse-grained molecular simulations, we analyze systems composed of two particle sizes at fixed total packing fraction and find that the shear modulus increases systematically with bidispersity. To explain this behavior, we develop two asymptotic models following limiting cases: one where a percolated network of large particles is stiffened by small particles, and another where a small-particle network is modified by embedded large particles. Both models yield closed-form expressions that capture the qualitative trends observed in simulations, including the dependence of shear modulus on size ratio and relative volume fraction. Our results demonstrate that bidispersity can enhance elastic stiffness through microstructural effects, independently of overall density, offering a simple strategy to design particle-based materials with tunable mechanical properties.|\n", "2504.08382": "|**2025-04-11**|**An posteriori error estimator for discontinuous Galerkin discretisations of convection-diffusion problems with application to Earth's mantle convection simulations**|Tiffany Barry, Andrea Cangiani, Samuel P. Cox et.al.|[2504.08382](http://arxiv.org/abs/2504.08382)|null||We present new aposteriori error estimates for the interior penalty discontinuous Galerkin method applied to non-stationary convection-diffusion equations. The focus is on strongly convection-dominated problems without zeroth-order reaction terms, which leads to the absence of positive L^2-like components. An important specific example is the energy/temperature equation of the Boussinesq system arising from the modelling of mantle convection of the Earth. The key mathematical challenge of mitigating the effects of exponential factors with respect to the final time, arising from the use of Gronwall-type arguments, is addressed by an exponential fitting technique. The latter results to a new class of aposteriori error estimates for the stationary problem, which are valid in cases of convection and reaction coefficient combinations not covered by the existing literature. This new class of estimators is combined with an elliptic reconstruction technique to derive new respective estimates for the non-stationary problem, exhibiting reduced dependence on Gronwall-type exponents and, thus, offer more accurate estimation for longer time intervals. We showcase the superior performance of the new class of aposteriori error estimators in driving mesh adaptivity in Earth's mantle convection simulations, in a setting where the energy/temperature equation is discretised by the discontinuous Galerkin method, coupled with the Taylor-Hood finite element for the momentum and mass conservation equations. We exploit the community code ASPECT, to present numerical examples showing the effectivity of the proposed approach.|\n", "2504.08346": "|**2025-04-11**|**Stochastic surfing turbulent vorticity**|Ziqi Wang, Xander M. de Wit, Roberto Benzi et.al.|[2504.08346](http://arxiv.org/abs/2504.08346)|null|8 pages, 5 figures|The chaotic dynamics of small-scale vorticity plays a key role in understanding and controlling turbulence, with direct implications for energy transfer, mixing, and coherent structure evolution. However, measuring or controlling its dynamics remains a major conceptual and experimental challenge due to its transient and chaotic nature. Here we use a combination of experiments, theory and simulations to show that small magnetic particles of different densities, exploring flow regions of distinct vorticity statistics, can act as effective probes for measuring and forcing turbulence at its smallest scale. The interplay between the magnetic torque, from an externally controllable magnetic field, and hydrodynamic stresses, from small-scale turbulent vorticity, reveals an extremely rich phenomenology. Notably, we present the first observation of stochastic resonance for particles in turbulence: turbulent fluctuations, effectively acting as noise, counterintuitively enhance the particle rotational response to external forcing. We identify a pronounced resonant peak in particle rotational phase-lag when the applied magnetic field matches the characteristic intensity of small-scale vortices. Furthermore, we uncover a novel symmetry-breaking mechanism: an oscillating magnetic field with zero-mean angular velocity remarkably induces net particle rotation in turbulence with zero-mean vorticity, as turbulent fluctuations aid the particle in \"surfing\" the magnetic field. Our findings offer insights into flexible particle manipulation in complex flows and open up a novel magnetic resonance-based approach for measuring vorticity: magnetic particles as probes emit detectable magnetic fields, enabling turbulence quantification even under optically-inaccessible conditions.|\n", "2504.08342": "|**2025-04-11**|**An Efficient Integrator Scheme for Sampling the (Quantum) Isobaric-Isothermal Ensemble in (Path Integral) Molecular Dynamics Simulations**|Weihao Liang, Sihan Wang, Cong Wang et.al.|[2504.08342](http://arxiv.org/abs/2504.08342)|null||Because most chemical or biological experiments are performed under conditions of controlled pressure and temperature, it is important to simulate the isobaric-isothermal ensemble at the atomic level to reveal the microscopic mechanism. By extending our configuration sampling protocol for the canonical ensemble, we propose a unified middle scheme to sample the coordinate (configuration) and volume distribution and thereby are able to accurately simulate either classical or quantum isobaric-isothermal processes. Various barostats and thermostats can be employed in the unified middle scheme for simulating real molecular systems with or without holonomic constraints. In particular, we demonstrate the recommended middle scheme by employing the Martyna-Tuckerman-Tobias-Klein barostat and stochastic cell-rescaling barostat, with the Langevin thermostat, in molecular simulation packages (DL_POLY, Amber, Gromacs, etc.). Benchmark numerical tests show that, without additional numerical effort, the middle scheme is competent in increasing the time interval by a factor of 5~10 to achieve the same accuracy of converged results for most thermodynamic properties in (path integral) molecular dynamics simulations.|\n", "2504.08341": "|**2025-04-11**|**Deep learning-based moment closure for multi-phase computation of semiclassical limit of the Schr\u00f6dinger equation**|Jin Woo Jang, Jae Yong Lee, Liu Liu et.al.|[2504.08341](http://arxiv.org/abs/2504.08341)|null|27 pages, 11 figures|We present a deep learning approach for computing multi-phase solutions to the semiclassical limit of the Schr\\\"odinger equation. Traditional methods require deriving a multi-phase ansatz to close the moment system of the Liouville equation, a process that is often computationally intensive and impractical. Our method offers an efficient alternative by introducing a novel two-stage neural network framework to close the $2N\\times 2N$ moment system, where $N$ represents the number of phases in the solution ansatz. In the first stage, we train neural networks to learn the mapping between higher-order moments and lower-order moments (along with their derivatives). The second stage incorporates physics-informed neural networks (PINNs), where we substitute the learned higher-order moments to systematically close the system. We provide theoretical guarantees for the convergence of both the loss functions and the neural network approximations. Numerical experiments demonstrate the effectiveness of our method for one- and two-dimensional problems with various phase numbers $N$ in the multi-phase solutions. The results confirm the accuracy and computational efficiency of the proposed approach compared to conventional techniques.|\n", "2504.08262": "|**2025-04-11**|**A General DoF and Pattern Analyzing Scheme for Electromagnetic Information Theory**|Zhongzhichao Wan, Jieao Zhu, Yongli Yan et.al.|[2504.08262](http://arxiv.org/abs/2504.08262)|null|In this paper, we provide a general DoF and pattern analyzing scheme   for EIT|Electromagnetic information theory (EIT) is one of the emerging topics for 6G communication due to its potential to reveal the performance limit of wireless communication systems. For EIT, one of the most important research directions is degree of freedom (DoF) analysis. Existing research works on DoF analysis for EIT focus on asymptotic conclusions of DoF, which do not well fit the practical wireless communication systems with finite spatial regions and finite frequency bandwidth. In this paper, we use the theoretical analyzing tools from Slepian concentration problem and extend them to three-dimensional space domain and four-dimensional space-time domain under electromagnetic constraints. Then we provide asymptotic DoF conclusions and non-asymptotic DoF analyzing scheme, which suits practical scenarios better, under different scenarios like three-dimensional antenna array. Moreover, we theoretically prove that the channel DoF is upper bounded by the proposed DoF of electromagnetic fields. Finally, we use numerical analysis to provide some insights about the optimal spatial sampling interval of the antenna array, the DoF of three-dimensional antenna array, the impact of unequal antenna spacing, the orthogonal space-time patterns, etc.|\n", "2504.08250": "|**2025-04-11**|**Nonadiabatic Field: A Conceptually Novel Approach for Nonadiabatic Quantum Molecular Dynamics**|Baihua Wu, Bingqi Li, Xin He et.al.|[2504.08250](http://arxiv.org/abs/2504.08250)|null||Reliable trajectory-based nonadiabatic quantum dynamics methods at the atomic level are critical for understanding many important processes in real systems. The paper reports latest progress of nonadiabatic field (NaF), a conceptually novel approach for nonadiabatic quantum dynamics with independent trajectories. Substantially different from the mainstreams of Ehrenfest-like dynamics and surface hopping methods, the nuclear force in NaF involves the nonadiabatic force arising from the nonadiabatic coupling between different electronic states, in addition to the adiabatic force contributed by a single adiabatic electronic state. NaF is capable of faithfully describing the interplay between electronic and nuclear motion in a broad regime, which covers where the relevant electronic states keep coupled in a wide range or all the time and where the bifurcation characteristic of nuclear motion is essential. NaF is derived from the exact generalized phase space formulation with coordinate-momentum variables, where constraint phase space (CPS) is employed for discrete electronic-state degrees of freedom. We propose efficient integrators for the equations of motion of NaF in both adiabatic and diabatic representations. Since the formalism in the CPS formulation is not unique, NaF can in principle be implemented with various phase space representations of the time correlation function (TCF) for the time-dependent property. They are applied to a suite of representative gas-phase and condensed-phase benchmark models where numerically exact results are available for comparison. It is shown that NaF is relatively insensitive to the phase space representation of the electronic TCF and will be a potential tool for practical and reliable simulations of the quantum mechanical behavior of both electronic and nuclear dynamics of nonadiabatic transition processes in real systems.|\n", "2504.08223": "|**2025-04-11**|**Stochastic Momentum ADMM for nonconvex and nonsmooth optimization with application to PnP algorithm**|Kangkang Deng, Shuchang Zhang, Boyu Wang et.al.|[2504.08223](http://arxiv.org/abs/2504.08223)|null|28 Pages|This paper introduces a single-loop Stochastic Momentum Alternating Direction Method of Multipliers (SMADMM) for tackling a class of nonconvex and nonsmooth optimization problems. We establish that SMADMM achieves an optimal oracle complexity of $\\mathcal{O}(\\epsilon^{-\\frac{3}{2}})$ in the online setting, where only stochastic first-order oracle, is available. In particular, SMADMM requires only $\\mathcal{O}(1)$ stochastic gradient evaluations per iteration and avoids the need for restarting with large batch gradient estimates. This is the first stochastic ADMM method achieving optimal oracle complexity for nonconvex and nonsmooth problems, requiring $\\mathcal{O}(1)$ batch size. Furthermore, we extend our method by integrating it with plug-and-play (PnP) priors, resulting in the PnP-SMADMM algorithm. Numerical experiments on classification, CT image reconstruction and phase retrieve demonstrate the practical effectiveness of our approach and validate the theoretical findings.|\n", "2504.08170": "|**2025-04-10**|**Efficient measurement of neutral-atom qubits with matched filters**|Robert M. Kent, Linipun Phuttitarn, Chaithanya Naik Mude et.al.|[2504.08170](http://arxiv.org/abs/2504.08170)|null||Quantum computers require high-fidelity measurement of many qubits to achieve a quantum advantage. Traditional approaches suffer from readout crosstalk for a neutral-atom quantum processor with a tightly spaced array. Although classical machine learning algorithms based on convolutional neural networks can improve fidelity, they are computationally expensive, making it difficult to scale them to large qubit counts. We present two simpler and scalable machine learning algorithms that realize matched filters for the readout problem. One is a local model that focuses on a single qubit, and the other uses information from neighboring qubits in the array to prevent crosstalk among the qubits. We demonstrate error reductions of up to 32% and 43% for the site and array models, respectively, compared to a conventional Gaussian threshold approach. Additionally, our array model uses two orders of magnitude fewer trainable parameters and four orders of magnitude fewer multiplications and nonlinear function evaluations than a recent convolutional neural network approach, with only a minor (3.5%) increase in error across different readout times. Another strength of our approach is its physical interpretability: the learned filter can be visualized to provide insights into experimental imperfections. We also show that a convolutional neural network model for improved can be pruned to have 70x and 4000x fewer parameters, respectively, while maintaining similar errors. Our work shows that simple machine learning approaches can achieve high-fidelity qubit measurements while remaining scalable to systems with larger qubit counts.|\n", "2504.08157": "|**2025-04-10**|**A GPU-accelerated simulation of rapid intensification of a tropical cyclone with observed heating**|Soonpil Kang, Francis X. Giraldo, Seth Camp et.al.|[2504.08157](http://arxiv.org/abs/2504.08157)|null|10 pages, 12 figures|This paper presents a limited-area atmospheric simulation of a tropical cyclone accelerated using GPUs. The OpenACC directive-based programming model is used to port the atmospheric model to the GPU. The GPU implementation of the main functions and kernels is discussed. The GPU-accelerated code produces high-fidelity simulations of a realistic tropical cyclone forced by observational latent heating. Performance tests show that the GPU-accelerated code yields energy-efficient simulations and scales well in both the strong and weak limit.|\n", "2504.08155": "|**2025-04-10**|**Benchmarking and contrasting exchange-correlation functional differences in response to static correlation in unrestricted Kohn-Sham and a hybrid 1-electron reduced density matrix functional theory**|Daniel Gibney, Jan-Niklas Boyn et.al.|[2504.08155](http://arxiv.org/abs/2504.08155)|null||A hybrid Kohn-Sham Density Functional Theory (KS-DFT) and 1-electron Reduced Density Matrix Functional Theory (1-RDMFT) has recently been developed to describe strongly correlated systems at mean-field computational cost. This approach relies on combining a Reduced Density Matrix Functional to capture strong correlation effects with existing exchange correlation (XC) functionals to capture the remaining dynamical correlation effects. In this work, we systematically benchmark the performance of nearly 200 different XC functionals available within LibXC in this DFA 1-RDMFT framework, contrasting it with their performance in unrestricted KS-DFT. We identify optimal XC functionals for use within DFA 1-RDMFT and elucidate fundamental trends in the response of different XC functionals to strong correlation in both DFA 1-RDMFT and UKS-DFT.|\n", "2504.08136": "|**2025-04-10**|**A physics informed neural network approach to simulating ice dynamics governed by the shallow ice approximation**|Kapil Chawla, William Holmes et.al.|[2504.08136](http://arxiv.org/abs/2504.08136)|null||In this article we develop a Physics Informed Neural Network (PINN) approach to simulate ice sheet dynamics governed by the Shallow Ice Approximation. This problem takes the form of a time-dependent parabolic obstacle problem. Prior work has used this approach to address the stationary obstacle problem and here we extend it to the time dependent problem. Through comprehensive 1D and 2D simulations, we validate the model's effectiveness in capturing complex free-boundary conditions. By merging traditional mathematical modeling with cutting-edge deep learning methods, this approach provides a scalable and robust solution for predicting temporal variations in ice thickness. To illustrate this approach in a real world setting, we simulate the dynamics of the Devon Ice Cap, incorporating aerogeophysical data from 2000 and 2018.|\n", "2504.08096": "|**2025-04-10**|**Cellular Development Follows the Path of Minimum Action**|Rohola Zandie, Farhan Khodaee, Yufan Xia et.al.|[2504.08096](http://arxiv.org/abs/2504.08096)|null||Cellular development follows a stochastic yet rule-governed trajectory, though the underlying principles remain elusive. Here, we propose that cellular development follows paths of least action, aligning with foundational physical laws that govern dynamic systems across nature. We introduce a computational framework that takes advantage of the deep connection between the principle of least action and maximum entropy to model developmental processes using Transformers architecture. This approach enables precise quantification of entropy production, information flow curvature, and local irreversibility for developmental asymmetry in single-cell RNA sequence data. Within this unified framework, we provide interpretable metrics: entropy to capture exploration-exploitation trade-offs, curvature to assess plasticity-elasticity dynamics, and entropy production to characterize dedifferentiation and transdifferentiation. We validate our method across both single-cell and embryonic development datasets, demonstrating its ability to reveal hidden thermodynamic and informational constraints shaping cellular fate decisions.|\n", "2504.11435": "|**2025-04-15**|**Robust Containment Queries over Collections of Trimmed NURBS Surfaces via Generalized Winding Numbers**|Jacob Spainhour, Kenneth Weiss et.al.|[2504.11435](http://arxiv.org/abs/2504.11435)|null|20 Pages, 18 Figures, 2 Tables|Efficient and accurate evaluation of containment queries for regions bound by trimmed NURBS surfaces is important in many graphics and engineering applications. However, the algebraic complexity of surface-surface intersections makes gaps and overlaps between surfaces difficult to avoid for in-the-wild surface models. By considering this problem through the lens of the generalized winding number (GWN), a mathematical construction that is indifferent to the arrangement of surfaces in the shape, we can define a containment query that is robust to model watertightness. Applying contemporary techniques for the 3D GWN on arbitrary curved surfaces would require some form of geometric discretization, potentially inducing containment misclassifications near boundary components. In contrast, our proposed method computes an accurate GWN directly on the curved geometry of the input model. We accomplish this using a novel reformulation of the relevant surface integral using Stokes' theorem, which in turn permits an efficient adaptive quadrature calculation on the boundary and trimming curves of the model. While this is sufficient for \"far-field\" query points that are distant from the surface, we augment this approach for \"near-field\" query points (i.e., within a bounding box) and even those coincident to the surface patches via a strategy that directly identifies and accounts for the jump discontinuity in the scalar field. We demonstrate that our method of evaluating the GWN field is robust to complex trimming geometry in a CAD model, and is accurate up to arbitrary precision at arbitrary distances from the surface. Furthermore, the derived containment query is robust to non-watertightness while respecting all curved features of the input shape.|\n", "2504.11433": "|**2025-04-15**|**Predicting Wave Dynamics using Deep Learning with Multistep Integration Inspired Attention and Physics-Based Loss Decomposition**|Indu Kant Deo, Rajeev K. Jaiman et.al.|[2504.11433](http://arxiv.org/abs/2504.11433)|null|30 pages, 14 figures|In this paper, we present a physics-based deep learning framework for data-driven prediction of wave propagation in fluid media. The proposed approach, termed Multistep Integration-Inspired Attention (MI2A), combines a denoising-based convolutional autoencoder for reduced latent representation with an attention-based recurrent neural network with long-short-term memory cells for time evolution of reduced coordinates. This proposed architecture draws inspiration from classical linear multistep methods to enhance stability and long-horizon accuracy in latent-time integration. Despite the efficiency of hybrid neural architectures in modeling wave dynamics, autoregressive predictions are often prone to accumulating phase and amplitude errors over time. To mitigate this issue within the MI2A framework, we introduce a novel loss decomposition strategy that explicitly separates the training loss function into distinct phase and amplitude components. We assess the performance of MI2A against two baseline reduced-order models trained with standard mean-squared error loss: a sequence-to-sequence recurrent neural network and a variant using Luong-style attention. To demonstrate the effectiveness of the MI2A model, we consider three benchmark wave propagation problems of increasing complexity, namely one-dimensional linear convection, the nonlinear viscous Burgers equation, and the two-dimensional Saint-Venant shallow water system. Our results demonstrate that the MI2A framework significantly improves the accuracy and stability of long-term predictions, accurately preserving wave amplitude and phase characteristics. Compared to the standard long-short term memory and attention-based models, MI2A-based deep learning exhibits superior generalization and temporal accuracy, making it a promising tool for real-time wave modeling.|\n", "2504.11425": "|**2025-04-15**|**MINDS: The very low-mass star and brown dwarf sample -- Hidden water in carbon-dominated protoplanetary disks**|Aditya M. Arabhavi, Inga Kamp, Ewine F. van Dishoeck et.al.|[2504.11425](http://arxiv.org/abs/2504.11425)|null|Accepted for publication in Astrophysical Journal Letters, 21 pages,   24 figures|Infrared observations of the inner disks around very low-mass stars (VLMS, $<$0.3$\\,M_{\\odot}$) have revealed a carbon-rich gas composition in the terrestrial planet-forming regions. Contrary to the typically water-rich T Tauri disk spectra, only two disks around VLMS have been observed to be water-rich among more than ten VLMS disks observed so far with JWST/MIRI. In this letter, we systematically search for the presence of water and other oxygen-bearing molecules in the JWST/MIRI spectra of ten VLMS disks from the MIRI mid-INfrared Disk Survey (MINDS). In addition to the two previously reported detections of water emission in this VLMS sample, we detect water emission in the spectra of three other sources and tentatively in one source, and we provide strong evidence for water emission in the remaining disks in the MINDS sample, most of which have bright emission from carbon-bearing molecules. We show that the $\\rm C_2H_2$ emission is much stronger than that of water for sources with low luminosities, and the hydrocarbons outshine the water emission in such conditions. We propose that the appearance of water-rich vs. hydrocarbon-rich spectra is related to the location of the water reservoir in the disk relative to the main hydrocarbon reservoir. Our findings indicate that the terrestrial planet forming regions in VLMS disks have high carbon-to-oxygen ratios (C/O$>$1), but can still harbor ample water similar to those in the T Tauri disks.|\n", "2504.11397": "|**2025-04-15**|**MLPs and KANs for data-driven learning in physical problems: A performance comparison**|Raghav Pant, Sikan Li, Xingjian Li et.al.|[2504.11397](http://arxiv.org/abs/2504.11397)|null|30 pages, 18 figures, 8 tables|There is increasing interest in solving partial differential equations (PDEs) by casting them as machine learning problems. Recently, there has been a spike in exploring Kolmogorov-Arnold Networks (KANs) as an alternative to traditional neural networks represented by Multi-Layer Perceptrons (MLPs). While showing promise, their performance advantages in physics-based problems remain largely unexplored. Several critical questions persist: Can KANs capture complex physical dynamics and under what conditions might they outperform traditional architectures? In this work, we present a comparative study of KANs and MLPs for learning physical systems governed by PDEs. We assess their performance when applied in deep operator networks (DeepONet) and graph network-based simulators (GNS), and test them on physical problems that vary significantly in scale and complexity. Drawing inspiration from the Kolmogorov Representation Theorem, we examine the behavior of KANs and MLPs across shallow and deep network architectures. Our results reveal that although KANs do not consistently outperform MLPs when configured as deep neural networks, they demonstrate superior expressiveness in shallow network settings, significantly outpacing MLPs in accuracy over our test cases. This suggests that KANs are a promising choice, offering a balance of efficiency and accuracy in applications involving physical systems.|\n", "2504.11350": "|**2025-04-15**|**Adaptive Compressible Smoothed Particle Hydrodynamics**|Navaneet Villodi, Prabhu Ramachandran et.al.|[2504.11350](http://arxiv.org/abs/2504.11350)|null|60 pages, 21 figures|Modulating the number of particles in a region is key to accurately capturing the nuances in compressible flows with Smoothed Particle Hydrodynamics (SPH). This paper details the implementation of a volume-based adaptive refinement and derefinement procedure, incorporating state-of-the-art features such as automatic local adaptivity and solution adaptivity. A shock-aware particle shifting procedure is introduced to regularize the particle distribution while preserving the integrity of shocks. To our knowledge, this is the first demonstration of shock-based solution adaptivity and shock-aware particle shifting in the literature. A wide variety of test problems, which involve flow in and around boundaries, are employed to highlight the utility of these adaptivity features in improving the results and in making simulations faster. For instance, the adaptive resolution procedure is shown to deliver an order of magnitude speedup. We also demonstrate the effectiveness of the adaptivity procedure in resolving existing issues like errors due to interaction with differently spaced ghost particles at boundaries, formation of spot-like structures due to particle clumping, and poorly resolved low-density regions. In essence, the adaptivity technique presented in this paper is positioned as a powerful tool for simulating compressible flows with enhanced accuracy and efficiency.|\n", "2504.11339": "|**2025-04-15**|**Optimal and Scalable Augmented Lagrangian preconditioners for Fictitious Domain problems**|Michele Benzi, Marco Feder, Luca Heltai et.al.|[2504.11339](http://arxiv.org/abs/2504.11339)|null||We present optimal and scalable preconditioning techniques to solve linear systems of equations with a block two-by-two and three-by-three structure arising from fictitious domain problems and from finite element discretizations of immersed boundary methods. In particular, we propose two augmented Lagrangian-based preconditioners to accelerate the convergence of iterative solvers for these two classes of linear. We consider two relevant examples to illustrate the performance of these preconditioners when used in conjunction with flexible GMRES: the Poisson and the Stokes fictitious domain problems. A spectral analysis is established for both exact and inexact versions of these preconditioners. We show the effectiveness of the proposed approach and the robustness of our preconditioning strategy through extensive numerical tests in both two and three dimensions.|\n", "2504.11333": "|**2025-04-15**|**Implicit dual time-stepping positivity-preserving entropy-stable schemes for the compressible Navier-Stokes equations**|Mohammed Sayyari, Nail K. Yamaleev et.al.|[2504.11333](http://arxiv.org/abs/2504.11333)|null||We generalize the explicit high-order positivity-preserving entropy-stable spectral collocation schemes developed in [30, 34] for the three-dimensional (3D) compressible Navier Stokes equations to a time implicit formulation. The time derivative terms are discretized by using the first- and second-order implicit backward difference formulas (BDF1 and BDF2) that are well suited for solving steady-state and time-dependent viscous flows at high Reynolds numbers, respectively. The nonlinear system of discrete equations at each physical timestep is solved by using a dual time-stepping technique. The proposed scheme is provably entropy-stable and positivity-preserving and provides unconditional stability properties in the physical time. Numerical results demonstrating accuracy and positivity-preserving properties of the new dual time-stepping scheme are presented for supersonic viscous flows with strong shock waves and contact discontinuities.|\n", "2504.11292": "|**2025-04-15**|**Optimal finite element approximations of monotone semilinear elliptic PDE with subcritical nonlinearities**|Florian Spicher, Thomas P. Wihler et.al.|[2504.11292](http://arxiv.org/abs/2504.11292)|null||We study iterative finite element approximations for the numerical approximation of semilinear elliptic boundary value problems with monotone nonlinear reactions of subcritical growth. The focus of our contribution is on an optimal a priori error estimate for a contractive Picard type iteration scheme on meshes that are locally refined towards possible corner singularities in polygonal domains. Our analysis involves, in particular, an elliptic regularity result in weighted Sobolev spaces and the use of the Trudinger inequality, which is instrumental in dealing with subcritically growing nonlinearities. A series of numerical experiments confirm the accuracy and efficiency of our method.|\n", "2504.11291": "|**2025-04-15**|**Policy heterogeneity improves collective olfactory search in 3-D turbulence**|Lorenzo Piro, Robin A. Heinonen, Maurizio Carbone et.al.|[2504.11291](http://arxiv.org/abs/2504.11291)|null||We investigate the role of policy heterogeneity in enhancing the olfactory search capabilities of cooperative agent swarms operating in complex, real-world turbulent environments. Using odor fields from direct numerical simulations of the Navier-Stokes equations, we demonstrate that heterogeneous groups, with exploratory and exploitative agents, consistently outperform homogeneous swarms where the exploration-exploitation tradeoff is managed at the individual level. Our results reveal that policy diversity enables the group to reach the odor source more efficiently by mitigating the detrimental effects of spatial correlations in the signal. These findings provide new insights into collective search behavior in biological systems and offer promising strategies for the design of robust, bioinspired search algorithms in engineered systems.|\n", "2504.11287": "|**2025-04-15**|**On kinetic energy localization in fluid flow**|Damian \u015anie\u017cek et.al.|[2504.11287](http://arxiv.org/abs/2504.11287)|null||This works focuses on participation number -- a parameter that allows to quantitatively asses the level of kinetic energy localization. The author presents a clear way of deriving participation number in a continuous case without making any assumptions about the system, fluid or flow regime. Moreover, a method of computing participation number in discretized cases is discussed and verified against well known analytical solutions using three methods, in which one was used previously in research on fluid flow through porous media. A robust formula, that works for both uniform and nonuniform discretization grids is presented.|\n", "2504.11236": "|**2025-04-15**|**Influence of a Xenon interlayer on dissociative electron attachment to deuterated methane on a platinum substrate**|Norhan Omar, Pierre Cloutier, Christophe Ramseyer et.al.|[2504.11236](http://arxiv.org/abs/2504.11236)|null||We investigate the impact of intercalating a xenon layer between a thin condensed CD4 film of two monolayers (ML) and a platinum surface on the dissociative electron attachment (DEA). The observed desorption results are compared with density functional theory (DFT) calculations, which reveal the binding energies of various anionic and neutral species as a function of the xenon film thickness on the Pt (111) substrate. The theoretical results suggest that 6 ML of xenon are sufficient to diminish the surface effect, enabling physisorbed anionic fragments to desorb from the CD4 film. In contrast, 20 ML (approximately 10 nm) are experimentally necessary to achieve saturation in the desorption of D-. In addition, the presence of xenon layers enables the coupling of resonance states with Xe excited states, thereby inhibiting the electrons from returning to the metal. Aside from reducing surface interactions, the xenon interlayer significantly enhances DEA to CD4.|\n", "2504.11213": "|**2025-04-15**|**Characterizing High Schmidt Number Witnesses in Arbitrary Dimensions System**|Liang Xiong, Nung-sing Sze et.al.|[2504.11213](http://arxiv.org/abs/2504.11213)|null||A profound comprehension of quantum entanglement is crucial for the progression of quantum technologies. The degree of entanglement can be assessed by enumerating the entangled degrees of freedom, leading to the determination of a parameter known as the Schmidt number. In this paper, we develop an efficient analytical tool for characterizing high Schmidt number witnesses for bipartite quantum states in arbitrary dimensions. Our methods not only offer viable mathematical methods for constructing high-dimensional Schmidt number witnesses in theory but also simplify the quantification of entanglement and dimensionality. Most notably, we develop high-dimensional Schmidt number witnesses within arbitrary-dimensional systems, with our Schmidt witness coefficients relying solely on the operator Schmidt coefficient. Subsequently, we demonstrate our theoretical advancements and computational superiority by constructing Schmidt number witnesses in arbitrary dimensional bipartite quantum systems with Schmidt numbers four and five.|\n", "2504.11212": "|**2025-04-15**|**SDFs from Unoriented Point Clouds using Neural Variational Heat Distances**|Samuel Weidemaier, Florine Hartwig, Josua Sassen et.al.|[2504.11212](http://arxiv.org/abs/2504.11212)|null|14 pages, 16 figures, 4 tables|We propose a novel variational approach for computing neural Signed Distance Fields (SDF) from unoriented point clouds. To this end, we replace the commonly used eikonal equation with the heat method, carrying over to the neural domain what has long been standard practice for computing distances on discrete surfaces. This yields two convex optimization problems for whose solution we employ neural networks: We first compute a neural approximation of the gradients of the unsigned distance field through a small time step of heat flow with weighted point cloud densities as initial data. Then we use it to compute a neural approximation of the SDF. We prove that the underlying variational problems are well-posed. Through numerical experiments, we demonstrate that our method provides state-of-the-art surface reconstruction and consistent SDF gradients. Furthermore, we show in a proof-of-concept that it is accurate enough for solving a PDE on the zero-level set.|\n", "2504.11181": "|**2025-04-15**|**A Quantum-Inspired Algorithm for Wave Simulation Using Tensor Networks**|Kevin Lively, Vittorio Pagni, Gonzalo Camacho et.al.|[2504.11181](http://arxiv.org/abs/2504.11181)|null|12 pages, 7 figures|We present an efficient classical algorithm based on the construction of a unitary quantum circuit for simulating the Isotropic Wave Equation (IWE) in one, two, or three dimensions. Using an analogy with the massless Dirac equation, second order time and space derivatives in the IWE are reduced to first order, resulting in a Schr\\\"odinger equation of motion. Exact diagonalization of the unitary circuit in combination with Tensor Networks allows simulation of the wave equation with a resolution of $10^{13}$ grid points on a laptop. A method for encoding arbitrary analytical functions into diagonal Matrix Product Operators is employed to prepare and evolve a Matrix Product State (MPS) encoding the solution. Since the method relies on the Quantum Fourier Transform, which has been shown to generate small entanglement when applied to arbitrary MPSs, simulating the evolution of initial conditions with sufficiently low bond dimensions to high accuracy becomes highly efficient, up to the cost of Trotterized propagation and sampling of the wavefunction. We conclude by discussing possible extensions of the approach for carrying out Tensor Network simulations of other partial differential equations such as Maxwell's equations.|\n", "2504.11167": "|**2025-04-15**|**Low-Rank SPIKE Framework for Solving Large Sparse Linear Systems with Applications**|Braegan S. Spring, Eric Polizzi, Ahmed H. Sameh et.al.|[2504.11167](http://arxiv.org/abs/2504.11167)|null|26 pages|The SPIKE family of linear system solvers provides parallelism using a block tridiagonal partitioning. Typically SPIKE-based solvers are applied to banded systems, resulting in structured off-diagonal blocks with non-zeros elements restricted to relatively small submatrices comprising the band of the original matrix. In this work, a low-rank SVD based approximation of the off-diagonal blocks is investigated. This produces a representation which more effectively handles matrices with large, sparse bands. A set of flexible distributed solvers, the LR-SPIKE variants, are implemented. There are applicable to a wide range of applications -- from use as a \"black-box\" preconditioner which straightforwardly improves upon the classic Block Jacobi preconditioner, to use as a specialized \"approximate direct solver.\" An investigation of the effectiveness of the new preconditioners for a selection of SuiteSparse matrices is performed, particularly focusing on matrices derived from 3D finite element simulations. In addition, the SPIKE approximate linear system solvers are also paired with the FEAST eigenvalue solver, where they are shown to be particularly effective due to the former's rapid convergence, and the latter's acceptance of loose linear system solver convergence, resulting in a combination which requires very few solver iterations.|\n", "2504.11140": "|**2025-04-15**|**An Unsupervised Network Architecture Search Method for Solving Partial Differential Equations**|Qing Li, Jingrun Chen et.al.|[2504.11140](http://arxiv.org/abs/2504.11140)|null||Solving partial differential equations (PDEs) has been indispensable in scientific and engineering applications. Recently, deep learning methods have been widely used to solve high-dimensional problems, one of which is the physics-informed neural network (PINN). Typically, a deep learning method has three main components: a neural network, a loss function, and an optimizer. While the construction of the loss function is rooted in the definition of solution space, how to choose a optimal neural network is somewhat ad hoc, leaving much room for improvement. In the framework of PINN, we propose an unsupervised network architecture search method for solving PDEs, termed PINN-DARTS, which applies the differentiable architecture search (DARTS) to find the optimal network architecture structure in a given set of neural networks. In this set, the number of layers and the number of neurons in each layer can change. In the searching phase, both network and architecture parameters are updated simultaneously, so the running time is close to that of PINN with a pre-determined network structure. Unlike available works, our approach is unsupervised and purely based on the PDE residual without any prior usage of solutions. PINN-DARTS outputs the optimal network structure as well as the associated numerical solution. The performance of PINN-DARTS is verified on several benchmark PDEs, including elliptic, parabolic, wave, and Burgers' equations. Compared to traditional architecture search methods, PINN-DARTS achieves significantly higher architectural accuracy. Another interesting observation is that both the solution complexity and the PDE type have a prominent impact on the optimal network architecture. Our study suggests that architectures with uneven widths from layer to layer may have superior performance across different solution complexities and different PDE types.|\n", "2504.11096": "|**2025-04-15**|**A fully variational numerical method for structural topology optimization based on a Cahn-Hilliard model**|Edmund Bell-Navas, David Portillo, Ignacio Romero et.al.|[2504.11096](http://arxiv.org/abs/2504.11096)|null||We formulate a novel numerical method suitable for the solution of topology optimization problems in solid mechanics. The most salient feature of the new approach is that the space and time discrete equations of the numerical method can be obtained as the optimality conditions of a single incremental potential. The governing equations define a gradient flow of the mass in the domain that maximizes the stiffness of the proposed solid, while exactly preserving the mass of the allocated material. Moreover, we propose a change of variables in the model equations that constrains the value of the density within admissible bounds and a continuation strategy that speeds up the evolution of the flow. The proposed strategy results in a robust and efficient topology optimization method that is exactly mass-preserving, does not employ Lagrange multipliers, and is fully variational.|\n", "2504.11074": "|**2025-04-16**|**Dynamical errors in machine learning forecasts**|Zhou Fang, Gianmarco Mengaldo et.al.|[2504.11074](http://arxiv.org/abs/2504.11074)|null||In machine learning forecasting, standard error metrics such as mean absolute error (MAE) and mean squared error (MSE) quantify discrepancies between predictions and target values. However, these metrics do not directly evaluate the physical and/or dynamical consistency of forecasts, an increasingly critical concern in scientific and engineering applications.   Indeed, a fundamental yet often overlooked question is whether machine learning forecasts preserve the dynamical behavior of the underlying system. Addressing this issue is essential for assessing the fidelity of machine learning models and identifying potential failure modes, particularly in applications where maintaining correct dynamical behavior is crucial.   In this work, we investigate the relationship between standard forecasting error metrics, such as MAE and MSE, and the dynamical properties of the underlying system. To achieve this goal, we use two recently developed dynamical indices: the instantaneous dimension ($d$), and the inverse persistence ($\\theta$). Our results indicate that larger forecast errors -- e.g., higher MSE -- tend to occur in states with higher $d$ (higher complexity) and higher $\\theta$ (lower persistence). To further assess dynamical consistency, we propose error metrics based on the dynamical indices that measure the discrepancy of the forecasted $d$ and $\\theta$ versus their correct values. Leveraging these dynamical indices-based metrics, we analyze direct and recursive forecasting strategies for three canonical datasets -- Lorenz, Kuramoto-Sivashinsky equation, and Kolmogorov flow -- as well as a real-world weather forecasting task. Our findings reveal substantial distortions in dynamical properties in ML forecasts, especially for long forecast lead times or long recursive simulations, providing complementary information on ML forecast fidelity that can be used to improve ML models.|\n", "2504.11056": "|**2025-04-15**|**A study of troubled-cell indicators applied to finite volume methods using a novel monotonicity parameter**|R. Shivananda Rao, M. Ramakrishna et.al.|[2504.11056](http://arxiv.org/abs/2504.11056)|null||We adapt a troubled-cell indicator developed for discontinuous Galerkin (DG) methods to the finite volume method (FVM) framework for solving hyperbolic conservation laws. This indicator depends solely on the cell-average data of the target cell and its immediate neighbours. Once the troubled-cells are identified, we apply the limiter only in these cells instead of applying in all computational cells. We introduce a novel technique to quantify the quality of the solution in the neighbourhood of the shock by defining a monotonicity parameter $\\mu$. Numerical results from various two-dimensional simulations on the hyperbolic systems of Euler equations using a finite volume solver employing MUSCL reconstruction validate the performance of the troubled-cell indicator and the approach of limiting only in the troubled-cells. These results show that limiting only in the troubled-cells is preferable to limiting everywhere as it improves convergence without compromising on the solution accuracy.|\n", "2504.11006": "|**2025-04-15**|**A Navier-Stokes-Peridynamics hybrid algorithm for the coupling of compressible flows and fracturing materials**|Mingshuo Han, Shiwei Hu, Tianbai Xiao et.al.|[2504.11006](http://arxiv.org/abs/2504.11006)|null|25 pages, 17 figures, 3 tables|Modeling and simulation of fluid-structure interactions are crucial to the success of aerospace engineering. This work addresses a novel hybrid algorithm that models the close coupling between compressible flows and deformable materials using a mesoscopic approach. Specifically, the high-speed flows are described by the gas-kinetic scheme, which is a robust Navier-Stokes alternative solver built on the molecular kinetic theory. The deformation, damage, and fracture of materials are depicted using the bond-based peridynamics, which serves as coarse-grained molecular dynamics to construct non-local extensions of classical continuum mechanics. The evolution of fluids and materials are closely coupled using the ghost-cell immersed boundary method. Within each time step, the solutions of flow and solid fields are updated simultaneously, and physics-driven boundary conditions are exchanged for each other via ghost cells. Extensive numerical experiments, including crack propagation in a pre-cracked plate, subsonic flow around the NACA0012 airfoil, supersonic flow around the circular cylinder, and shock wave impacting on the elastic panel, are performed to validate the algorithm. The simulation results demonstrate the unique advantages of current hybrid algorithm in solving fracture propagation induced by high-speed flows.|\n", "2504.15201": "|**2025-04-21**|**Phase-separated lipid vesicles: continuum modeling, simulation, and validation**|Maxim Olshanskii, Annalisa Quaini et.al.|[2504.15201](http://arxiv.org/abs/2504.15201)|null||The paper presents a complete research cycle comprising continuum-based modeling, computational framework development, and validation setup to predict phase separation and surface hydrodynamics in lipid bilayer membranes. We starting with an overview of the key physical characteristics of lipid bilayers, including their composition, mechanical properties, and thermodynamics, and then discuss continuum models of multi-component bilayers. The most complex model is a Navier--Stokes--Cahn--Hilliard (NSCH) type system, describing the coupling of incompressible surface fluid dynamics with phase-field dynamics on arbitrarily curved geometries. It is discretized using trace finite element methods, which offer geometric flexibility and stability in representing surface PDEs. Numerical studies are conducted to examine physical features such as coarsening rates and interfacial dynamics. The computational results obtained from the NSCH model are compared against experimental data for membrane compositions with distinct phase behaviors, demonstrating that including both phase-field models and surface hydrodynamics is essential to accurately reproduce domain evolution observed in epi-fluorescence microscopy. Lastly, we extend the model to incorporate external forces that enable the simulation of vesicles containing cationic lipids, used to enhance membrane fusion.|\n", "2504.15177": "|**2025-04-21**|**An $rp$-adaptive method for accurate resolution of shock-dominated viscous flow based on implicit shock tracking**|Huijing Dong, Masayuki Yano, Tianci Huang et.al.|[2504.15177](http://arxiv.org/abs/2504.15177)|null|43 pages, 35 figures,|This work introduces an optimization-based $rp$-adaptive numerical method to approximate solutions of viscous, shock-dominated flows using implicit shock tracking and a high-order discontinuous Galerkin discretization on traditionally coarse grids without nonlinear stabilization (e.g., artificial viscosity or limiting). The proposed method adapts implicit shock tracking methods, originally developed to align mesh faces with solution discontinuities, to compress elements into viscous shocks and boundary layers, functioning as a novel approach to aggressive $r$-adaptation. This form of $r$-adaptation is achieved naturally as the minimizer of the enriched residual with respect to the discrete flow variables and coordinates of the nodes of the grid. Several innovations to the shock tracking optimization solver are proposed to ensure sufficient mesh compression at viscous features to render stabilization unnecessary, including residual weighting, step constraints and modifications, and viscosity-based continuation. Finally, $p$-adaptivity is used to locally increase the polynomial degree with three clear benefits: (1) lessens the mesh compression requirements near shock waves and boundary layers, (2) reduces the error in regions where $r$-adaptivity is not sufficient with the given grid topology, and (3) reduces computational cost by performing a majority of the $r$-adaptivity iterations on the coarsest discretization. A series of numerical experiments show the proposed method effectively resolves viscous, shock-dominated flows, including accurate prediction of heat flux profiles produced by hypersonic flow over a cylinder, and compares favorably in terms of accuracy per degree of freedom to $h$-adaptation with a high-order discretization.|\n", "2504.15173": "|**2025-04-21**|**Poroelastic flow across a permeable interface: a Hamilton's principle approach and its finite element implementation**|Francesco Costanzo, Mohammad Jannesari, Beatrice Ghitti et.al.|[2504.15173](http://arxiv.org/abs/2504.15173)|null||We consider fluid flow across a permeable interface within a deformable porous medium. We use mixture theory. The mixture's constituents are assumed to be incompressible in their pure form. We use Hamilton's principle to obtain the governing equations, and we propose a corresponding finite element implementation. The filtration velocity and the pore pressure are allowed to be discontinuous across the interface while some control of these discontinuities is built into the interfacial constitutive behavior. To facilitate the practical implementation of the formulation in a finite element scheme, we introduce a Lagrange multiplier field over the interface for the explicit enforcement of the jump condition of the balance of mass. Our formulation appears to recover some basic results from the literature. The novelty of the work is the formulation of an approach that can accommodate specific constitutive assumptions pertaining to the behavior of the interface that do not necessarily imply the continuity of the filtration velocity and/or of the pore pressure across it.|\n", "2504.15151": "|**2025-04-21**|**Artificial compressibility method for the incompressible Navier-Stokes equations with variable density**|Cappanera Loic, Giordano Salvatore et.al.|[2504.15151](http://arxiv.org/abs/2504.15151)|null||We introduce a novel artificial compressibility technique to approximate the incompressible Navier-Stokes equations with variable fluid properties such as density and dynamical viscosity. The proposed scheme used the couple pressure and momentum, equal to the density times the velocity, as primary unknowns. It also involves an adequate treatment of the diffusive operator such that treating the nonlinear convective term explicitly leads to a scheme with time independent stiffness matrices that is suitable for pseudo-spectral methods. The stability and temporal convergence of the semi-implicit version of the scheme is established under the hypothesis that the density is approximated with a method that conserves the minimum-maximum principle. Numerical illustrations confirm that both the semi-implicit and explicit scheme are stable and converge with order one under classic CFL condition. Moreover, the proposed scheme is shown to perform better than a momentum based pressure projection method, previously introduced by one of the authors, on setups involving gravitational waves and immiscible multi-fluids in a cylinder.|\n", "2504.15110": "|**2025-04-21**|**Kolmogorov-Arnold Networks: Approximation and Learning Guarantees for Functions and their Derivatives**|Anastasis Kratsios, Takashi Furuya et.al.|[2504.15110](http://arxiv.org/abs/2504.15110)|null||Inspired by the Kolmogorov-Arnold superposition theorem, Kolmogorov-Arnold Networks (KANs) have recently emerged as an improved backbone for most deep learning frameworks, promising more adaptivity than their multilayer perception (MLP) predecessor by allowing for trainable spline-based activation functions. In this paper, we probe the theoretical foundations of the KAN architecture by showing that it can optimally approximate any Besov function in $B^{s}_{p,q}(\\mathcal{X})$ on a bounded open, or even fractal, domain $\\mathcal{X}$ in $\\mathbb{R}^d$ at the optimal approximation rate with respect to any weaker Besov norm $B^{\\alpha}_{p,q}(\\mathcal{X})$; where $\\alpha < s$. We complement our approximation guarantee with a dimension-free estimate on the sample complexity of a residual KAN model when learning a function of Besov regularity from $N$ i.i.d. noiseless samples. Our KAN architecture incorporates contemporary deep learning wisdom by leveraging residual/skip connections between layers.|\n", "2504.15100": "|**2025-04-21**|**Application of Sensitivity Analysis Methods for Studying Neural Network Models**|Jiaxuan Miao, Sergey Matveev et.al.|[2504.15100](http://arxiv.org/abs/2504.15100)|null|11 pages, 16 figures, 32 references|This study demonstrates the capabilities of several methods for analyzing the sensitivity of neural networks to perturbations of the input data and interpreting their underlying mechanisms. The investigated approaches include the Sobol global sensitivity analysis, the local sensitivity method for input pixel perturbations and the activation maximization technique. As examples, in this study we consider a small feedforward neural network for analyzing an open tabular dataset of clinical diabetes data, as well as two classical convolutional architectures, VGG-16 and ResNet-18, which are widely used in image processing and classification. Utilization of the global sensitivity analysis allows us to identify the leading input parameters of the chosen tiny neural network and reduce their number without significant loss of the accuracy. As far as global sensitivity analysis is not applicable to larger models we try the local sensitivity analysis and activation maximization method in application to the convolutional neural networks. These methods show interesting patterns for the convolutional models solving the image classification problem. All in all, we compare the results of the activation maximization method with popular Grad-CAM technique in the context of ultrasound data analysis.|\n", "2504.15073": "|**2025-04-21**|**Hermitian Quaternion Toeplitz Matrices by Quaternion-valued Generating Functions**|Xue-lei Lin, Michael K. Ng, Junjun Pan et.al.|[2504.15073](http://arxiv.org/abs/2504.15073)|null||In this paper, we study Hermitian quaternion Toeplitz matrices generated by quaternion-valued functions. We show that such generating function must be the sum of a real-valued function and an odd function with imaginary component. This setting is different from the case of Hermitian complex Toeplitz matrices generated by real-valued functions only. By using of 2-by-2 block complex representation of quaternion matrices, we give a quaternion version of Grenander-Szeg\\\"{o} theorem stating the distribution of eigenvalues of Hermitian quaternion Toeplitz matrices in terms of its generating function. As an application, we investigate Strang's circulant preconditioners for Hermitian quaternion Toeplitz linear systems arising from quaternion signal processing. We show that Strang's circulant preconditioners can be diagionalized by discrete quaternion Fourier transform matrices whereas general quaternion circulant matrices cannot be diagonalized by them. Also we verify the theoretical and numerical convergence results of Strang's circulant preconditioned conjugate gradient method for solving Hermitian quaternion Toeplitz systems.|\n", "2504.14978": "|**2025-04-21**|**clusttraj: a solvent-informed clustering tool for molecular modeling**|Rafael Bicudo Ribeiro, Henrique Musseli Cezar et.al.|[2504.14978](http://arxiv.org/abs/2504.14978)|null||Clustering techniques are consolidated as a powerful strategy for analyzing the extensive data generated from molecular modeling. In particular, some tools have been developed to cluster configurations from classical simulations with a standard focus on individual units, ranging from small molecules to complex proteins. Since the standard approach includes computing the Root Mean Square Deviation (RMSD) of atomic positions, accounting for the permutation between atoms is crucial for optimizing the clustering procedure in the presence of identical molecules. To address this issue, we present the clusttraj program, a solvent-informed clustering package that fixes inflated RMSD values by finding the optimal pairing between configurations. The program combines reordering schemes with the Kabsch algorithm to minimize the RMSD of molecular configurations before running a hierarchical clustering protocol. By considering evaluation metrics, one can determine the ideal threshold in an automated fashion and compare the different linkage schemes available. The program capabilities are exemplified by considering solute-solvent systems ranging from pure water clusters to a solvated protein or a small solute in different solvents. As a result, we investigate the dependence on different parameters, such as the system size and reordering method, and also the representativeness of the cluster medoids for the characterization of optical properties. clusttraj is implemented as a Python library and can be employed to cluster generic ensembles of molecular configurations that go beyond solute-solvent systems.|\n", "2504.14939": "|**2025-04-21**|**Full Discretization of Stochastic Semilinear Schr\u00f6dinger equation driven by multiplicative Wiener noise**|Suprio Bhar, Mrinmay Biswas, Mangala Prasad et.al.|[2504.14939](http://arxiv.org/abs/2504.14939)|null|27 pages, Comments are welcome|In this article, we have analyzed the full discretization of the Stochastic semilinear Schr\\\"{o}dinger equation in a bounded convex polygonal domain driven by multiplicative Wiener noise. We use the finite element method for spatial discretization and the stochastic trigonometric method for time discretization and derive a strong convergence rate with respect to both parameters (temporal and spatial). Numerical experiments have also been performed to support theoretical bounds.|\n", "2504.14930": "|**2025-04-21**|**Kernel-learning parameter prediction and evaluation in algebraic multigrid method for several PDEs**|Juan Zhang, Junyue Luo, Fangfang Zhang et.al.|[2504.14930](http://arxiv.org/abs/2504.14930)|null||This paper explores the application of kernel learning methods for parameter prediction and evaluation in the Algebraic Multigrid Method (AMG), focusing on several Partial Differential Equation (PDE) problems. AMG is an efficient iterative solver for large-scale sparse linear systems, particularly those derived from elliptic and parabolic PDE discretizations. However, its performance heavily relies on numerous parameters, which are often set empirically and are highly sensitive to AMG's effectiveness. Traditional parameter optimization methods are either computationally expensive or lack theoretical support. To address this, we propose a Gaussian Process Regression (GPR)-based strategy to optimize AMG parameters and introduce evaluation metrics to assess their effectiveness. Trained on small-scale datasets, GPR predicts nearly optimal parameters, bypassing the time-consuming parameter sweeping process. We also use kernel learning techniques to build a kernel function library and determine the optimal kernel function through linear combination, enhancing prediction accuracy. In numerical experiments, we tested typical PDEs such as the constant-coefficient Poisson equation, variable-coefficient Poisson equation, diffusion equation, and Helmholtz equation. Results show that GPR-predicted parameters match grid search results in iteration counts while significantly reducing computational time. A comprehensive analysis using metrics like mean squared error, prediction interval coverage, and Bayesian information criterion confirms GPR's efficiency and reliability. These findings validate GPR's effectiveness in AMG parameter optimization and provide theoretical support for AMG's practical application.|\n", "2504.14790": "|**2025-04-21**|**Enhanced Data-driven Topology Design Methodology with Multi-level Mesh and Correlation-based Mutation for Stress-related Multi-objective Optimization**|Jun Yang, Shintaro Yamasaki et.al.|[2504.14790](http://arxiv.org/abs/2504.14790)|null|23 pages, 22 figures|Topology optimization (TO) serves as a widely applied structural design approach to tackle various engineering problems. Nevertheless, sensitivity-based TO methods usually struggle with solving strongly nonlinear optimization problems. By leveraging high capacity of deep generative model, which is an influential machine learning technique, the sensitivity-free data-driven topology design (DDTD) methodology is regarded as an effective means of overcoming these issues. The DDTD methodology depends on initial dataset with a certain regularity, making its results highly sensitive to initial dataset quality. This limits its effectiveness and generalizability, especially for optimization problems without priori information. In this research, we proposed a multi-level mesh DDTD-based method with correlation-based mutation module to escape from the limitation of the quality of the initial dataset on the results and enhance computational efficiency. The core is to employ a correlation-based mutation module to assign new geometric features with physical meaning to the generated data, while utilizing a multi-level mesh strategy to progressively enhance the refinement of the structural representation, thus avoiding the maintenance of a high degree-of-freedom (DOF) representation throughout the iterative process. The proposed multi-level mesh DDTD-based method can be driven by a low quality initial dataset without the need for time-consuming construction of a specific dataset, thus significantly increasing generality and reducing application difficulty, while further lowering computational cost of DDTD methodology. Various comparison experiments with the traditional sensitivity-based TO methods on stress-related strongly nonlinear problems demonstrate the generality and effectiveness of the proposed method.|\n", "2504.14768": "|**2025-04-20**|**A note on unshifted lattice rules for high-dimensional integration in weighted unanchored Sobolev spaces**|Takashi Goda et.al.|[2504.14768](http://arxiv.org/abs/2504.14768)|null|6 pages|This short article studies a deterministic quasi-Monte Carlo lattice rule in weighted unanchored Sobolev spaces of smoothness $1$. Building on the error analysis by Kazashi and Sloan, we prove the existence of unshifted rank-1 lattice rules that achieve a worst-case error of $O(n^{-1/4}(\\log n)^{1/2})$, with the implied constant independent of the dimension, under certain summability conditions on the weights. Although this convergence rate is inferior to the one achievable for the shifted-averaged root mean squared worst-case error, the result does not rely on random shifting or transformation and holds unconditionally without any conjecture, as assumed by Kazashi and Sloan.|\n", "2504.14722": "|**2025-04-20**|**Path sampling challenges in large biomolecular systems: RETIS and REPPTIS for ABL-imatinib kinetics**|Wouter Vervust, Daniel T. Zhang, Enrico Riccardi et.al.|[2504.14722](http://arxiv.org/abs/2504.14722)|null|Preprint|Predicting the kinetics of drug-protein interactions is crucial for understanding drug efficacy, particularly in personalized medicine, where protein mutations can significantly alter drug residence times. This study applies Replica Exchange Transition Interface Sampling (RETIS) and its Partial Path variant (REPPTIS) to investigate the dissociation kinetics of imatinib from Abelson nonreceptor tyrosine kinase (ABL) and mutants relevant to chronic myeloid leukemia therapy. These path-sampling methods offer a bias-free alternative to conventional approaches requiring qualitative predefined reaction coordinates. Nevertheless, the complex free-energy landscape of ABL-imatinib dissociation presents significant challenges. Multiple metastable states and orthogonal barriers lead to parallel unbinding pathways, complicating convergence in TIS-based methods. Despite employing computational efficiency strategies such as asynchronous replica exchange, full convergence remained elusive. This work provides a critical assessment of path sampling in high-dimensional biological systems, discussing the need for enhanced initialization strategies, advanced Monte Carlo path generation moves, and machine learning-derived reaction coordinates to improve kinetic predictions of drug dissociation with minimal prior knowledge.|\n", "2504.14721": "|**2025-04-20**|**Data-driven model order reduction for T-Product-Based dynamical systems**|Shenghan Mei, Ziqin He, Yidan Mei et.al.|[2504.14721](http://arxiv.org/abs/2504.14721)|null|12 pages, 1 figure|Model order reduction plays a crucial role in simplifying complex systems while preserving their essential dynamic characteristics, making it an invaluable tool in a wide range of applications, including robotic systems, signal processing, and fluid dynamics. However, traditional model order reduction techniques like balanced truncation are not designed to handle tensor data directly and instead require unfolding the data, which may lead to the loss of important higher-order structural information. In this article, we introduce a novel framework for data-driven model order reduction of T-product-based dynamical systems (TPDSs), which are often used to capture the evolution of third-order tensor data such as images and videos through the T-product. Specifically, we develop advanced T-product-based techniques, including T-balanced truncation, T-balanced proper orthogonal decomposition, and the T-eigensystem realization algorithm for input-output TPDSs by leveraging the unique properties of T-singular value decomposition. We demonstrate that these techniques offer significant memory and computational savings while achieving reduction errors that are comparable to those of conventional methods. The effectiveness of the proposed framework is further validated through synthetic and real-world examples.|\n", "2504.14498": "|**2025-04-20**|**Assessing the Performance of Mixed-Precision ILU(0)-Preconditioned Multiple-Precision Real and Complex Krylov Subspace Methods**|Tomonori Kouya et.al.|[2504.14498](http://arxiv.org/abs/2504.14498)|null||Krylov subspace methods are linear solvers based on matrix-vector multiplications and vector operations. While easily parallelizable, they are sensitive to rounding errors and may experience convergence issues. ILU(0), an incomplete LU factorization with zero fill-in, is a well-known preconditioning technique that enhances convergence for sparse matrices. In this paper, we implement a double-precision and multiple-precision ILU(0) preconditioner, compatible with product-type Krylov subspace methods, and evaluate its performance.|\n", "2504.14479": "|**2025-04-20**|**Stochastic Norton Dynamics: An Alternative Approach for the Computation of Transport Coefficients in Dissipative Particle Dynamics**|Xinyi Wu, Xiaocheng Shang et.al.|[2504.14479](http://arxiv.org/abs/2504.14479)|null||We study a novel alternative approach for the computation of transport coefficients at mesoscales. While standard nonequilibrium molecular dynamics (NEMD) approaches fix the forcing and measure the average induced flux in the system driven out of equilibrium, the so-called ``stochastic Norton dynamics'' instead fixes the value of the flux and measures the average magnitude of the forcing needed to induce it. We extend recent results obtained in Langevin dynamics to consider the generalisation of the stochastic Norton dynamics in the popular dissipative particle dynamics (DPD) at mesoscales, important for a wide range of complex fluids and soft matter applications. We demonstrate that the responses profiles for both the NEMD and stochastic Norton dynamics approaches coincide in both linear and nonlinear regimes, indicating that the stochastic Norton dynamics can indeed act as an alternative approach for the computation of transport coefficients, including the mobility and the shear viscosity, as the NEMD dynamics. In addition, based on the linear response of the DPD system with small perturbations, we derive a closed-form expression for the shear viscosity, and numerically validate its effectiveness with various types of external forces. Moreover, our numerical experiments demonstrate that the stochastic Norton dynamics approach clearly outperforms the NEMD dynamics in controlling the asymptotic variance, a key metric to measure the associated computational costs, particularly in the high friction limit.|\n", "2504.14473": "|**2025-04-20**|**Invariance-embedded Machine Learning Sub-grid-scale Stress Models for Meso-scale Hurricane Boundary Layer Flow Simulation I: Model Development and $\\textit{a priori}$ Studies**|Md Badrul Hasan, Meilin Yu, Tim Oates et.al.|[2504.14473](http://arxiv.org/abs/2504.14473)|null||This study develops invariance-embedded machine learning sub-grid-scale (SGS) stress models admitting turbulence kinetic energy (TKE) backscatter towards more accurate large eddy simulation (LES) of meso-scale turbulent hurricane boundary layer flows. The new machine learning SGS model consists of two parts: a classification model used to distinguish regions with either strong energy cascade or energy backscatter from those with mild TKE transfer and a regression model used to calculate SGS stresses in regions with strong TKE transfer. To ease model implementation in computational fluid dynamics (CFD) solvers, the Smagorinsky model with a signed coefficient $C_s$, where a positive value indicates energy cascade while a negative one indicates energy backscatter, is employed as the carrier of the machine learning model. To improve its robustness and generality, both physical invariance and geometric invariance features of turbulent flows are embedded into the model input for classification and regression, and the signed Smagorinsky model coefficient is used as the output of the regression model. Different machine-learning methods and input setups have been used to test the classification model's performance. The F1-scores, which measure balanced precision and recall of a model, of the classification models with physical and geometric invariance embedded can be improved by about $17\\%$ over those without considering geometric invariance. Regression models based on ensemble neural networks have demonstrated superior performance in predicting the signed Smagorinsky model coefficient, exceeding that of the dynamic Smagorinsky model in $\\textit{a priori}$ tests.|\n", "2504.14422": "|**2025-04-19**|**Optimal Lattice Boltzmann Closures through Multi-Agent Reinforcement Learning**|Paul Fischer, Sebastian Kaltenbach, Sergey Litvinov et.al.|[2504.14422](http://arxiv.org/abs/2504.14422)|null||The Lattice Boltzmann method (LBM) offers a powerful and versatile approach to simulating diverse hydrodynamic phenomena, spanning microfluidics to aerodynamics. The vast range of spatiotemporal scales inherent in these systems currently renders full resolution impractical, necessitating the development of effective closure models for under-resolved simulations. Under-resolved LBMs are unstable, and while there is a number of important efforts to stabilize them, they often face limitations in generalizing across scales and physical systems. We present a novel, data-driven, multiagent reinforcement learning (MARL) approach that drastically improves stability and accuracy of coarse-grained LBM simulations. The proposed method uses a convolutional neural network to dynamically control the local relaxation parameter for the LB across the simulation grid. The LB-MARL framework is showcased in turbulent Kolmogorov flows. We find that the MARL closures stabilize the simulations and recover the energy spectra of significantly more expensive fully resolved simulations while maintaining computational efficiency. The learned closure model can be transferred to flow scenarios unseen during training and has improved robustness and spectral accuracy compared to traditional LBM models. We believe that MARL closures open new frontiers for efficient and accurate simulations of a multitude of complex problems not accessible to present-day LB methods alone.|\n", "2504.14405": "|**2025-04-19**|**Sensitivity-aware rock physics enhanced digital shadow for underground-energy storage monitoring**|Abhinav Prakash Gahlot, Huseyin Tuna Erdinc, Felix J. Herrmann et.al.|[2504.14405](http://arxiv.org/abs/2504.14405)|null||Underground energy storage, which includes storage of hydrogen, compressed air, and CO2, requires careful monitoring to track potential leakage pathways, a situation where time-lapse seismic imaging alone may be inadequate. A recently developed Digital Shadow (DS) enhances forecasting using machine learning and Bayesian inference, yet their accuracy depends on assumed rock physics models, the mismatch of which can lead to unreliable predictions for the reservoir's state (saturation/pressure). Augmenting DS training with multiple rock physics models mitigates errors but averages over uncertainties, obscuring their sources. To address this challenge, we introduce context-aware sensitivity analysis inspired by amortized Bayesian inference, allowing the DS to learn explicit dependencies between seismic data, the reservoir state, e.g., CO2 saturation, and rock physics models. At inference time, this approach allows for real-time ''what if'' scenario testing rather than relying on costly retraining, thereby enhancing interpretability and decision-making for safer, more reliable underground storage.|\n", "2504.14343": "|**2025-04-19**|**Numerical analysis of a particle system for the calibrated Heston-type local stochastic volatility model**|Christoph Reisinger, Maria Olympia Tsianni et.al.|[2504.14343](http://arxiv.org/abs/2504.14343)|null||We analyse a Monte Carlo particle method for the simulation of the calibrated Heston-type local stochastic volatility (H-LSV) model. The common application of a kernel estimator for a conditional expectation in the calibration condition results in a McKean-Vlasov (MV) stochastic differential equation (SDE) with non-standard coefficients. The primary challenges lie in certain mean-field terms in the drift and diffusion coefficients and the $1/2$-H\\\"{o}lder regularity of the diffusion coefficient. We establish the well-posedness of this equation for a fixed but arbitrarily small bandwidth of the kernel estimator. Moreover, we prove a strong propagation of chaos result, ensuring convergence of the particle system under a condition on the Feller ratio and up to a critical time. For the numerical simulation, we employ an Euler-Maruyama scheme for the log-spot process and a full truncation Euler scheme for the CIR volatility process. Under certain conditions on the inputs and the Feller ratio, we prove strong convergence of the Euler-Maruyama scheme with rate $1/2$ in time, up to a logarithmic factor. Numerical experiments illustrate the convergence of the discretisation scheme and validate the propagation of chaos in practice.|\n", "2504.16924": "|**2025-04-23**|**Ultradense Sphere Packings Derived From Disordered Stealthy Hyperuniform Ground States**|Jaeuk Kim, Salvatore Torquato et.al.|[2504.16924](http://arxiv.org/abs/2504.16924)|null|18 pages, 9 figures|Disordered stealthy hyperuniform (SHU) packings are an emerging class of exotic amorphous two-phase materials endowed with novel physical properties. Such packings of identical spheres have been created from SHU point patterns via a modified collective-coordinate optimization scheme that includes a soft-core repulsion, besides the standard `stealthy' pair potential. Using the distributions of minimum pair distances and nearest-neighbor distances, we find that when the stealthiness parameter $\\chi$ is lower than 0.5, the maximal values of $\\phi$, denoted by $\\phi_{\\max}$, decrease to zero on average as the particle number $N$ increases if there are no soft-core repulsions. By contrast, the inclusion of soft-core repulsions results in very large $\\phi_{\\max}$ independent of $N$, reaching up to $\\phi_{\\max}=1.0, 0.86, 0.63$ in the zero-$\\chi$ limit and decreasing to $\\phi_{\\max}=1.0, 0.67, 0.47$ at $\\chi=0.45$ for $d=1,2,3$, respectively. We obtain explicit formulas for $\\phi_{\\max}$ as functions of $\\chi$ and $N$ for a given $d$. For $d=2,3$, our soft-core SHU packings for small $\\chi$ become configurationally very close to the jammed hard-particle packings created by fast compression algorithms, as measured by the pair statistics. As $\\chi$ increases beyond $0.20$, the packings form fewer contacts and linear polymer-like chains. The resulting structure factors $S(k)$ and pair correlation functions $g_2(r)$ reveal that soft-core repulsions significantly alter the short- and intermediate-range correlations in the SHU ground states. We also compute the spectral density $\\tilde{\\chi}_{_V}(k)$, which can be used to estimate various physical properties (e.g., electromagnetic properties, fluid permeability, and mean survival time) of SHU two-phase dispersions. Our results offer a new route for discovering novel disordered hyperuniform two-phase materials with unprecedentedly high density.|\n", "2504.16899": "|**2025-04-23**|**Linear convergence of a one-cut conditional gradient method for total variation regularization**|Giacomo Cristinelli, Jos\u00e9 A. Iglesias, Daniel Walter et.al.|[2504.16899](http://arxiv.org/abs/2504.16899)|null|23 pages, 6 Figures|We introduce a fully-corrective generalized conditional gradient method for convex minimization problems involving total variation regularization on multidimensional domains. It relies on alternating between updating an active set of subsets of the spatial domain as well as of an iterate given by a conic combination of the associated characteristic functions. Different to previous approaches in the same spirit, the computation of a new candidate set only requires the solution of one prescribed mean curvature problem instead of the resolution of a fractional minimization task analogous to finding a generalized Cheeger set. After discretization, the former can be realized by a single run of a graph cut algorithm leading to significant speedup in practice. We prove the global sublinear convergence of the resulting method, under mild assumptions, and its asymptotic linear convergence in a more restrictive two-dimensional setting which uses results of stability of surfaces of prescribed curvature under perturbations of the curvature. Finally, we numerically demonstrate this convergence behavior in some model PDE-constrained minimization problems.|\n", "2504.16893": "|**2025-04-23**|**Practical approaches for crystal structure predictions with inpainting generation and universal interatomic potentials**|Peichen Zhong, Xinzhe Dai, Bowen Deng et.al.|[2504.16893](http://arxiv.org/abs/2504.16893)|null||We present Crystal Host-Guided Generation (CHGGen), a diffusion-based framework for crystal structure prediction. Unconditional generation with diffusion models demonstrates limited efficacy in identifying symmetric crystals as the unit cell size increases. CHGGen addresses this limitation through conditional generation with the inpainting method, which optimizes a fraction of atomic positions within a predefined and symmetrized host structure. We demonstrate the method on the ZnS-P$_2$S$_5$ and Li-Si chemical systems, where the inpainting method generates a higher fraction of symmetric structures than unconditional generation. The practical significance of CHGGen extends to enabling the structural modification of crystal structures, particularly for systems with partial occupancy, surface absorption and defects. The inpainting method also allows for seamless integration with other generative models, providing a versatile framework for accelerating materials discovery.|\n", "2504.16865": "|**2025-04-23**|**General method for solving nonlinear optical scattering problems using fix point iterations**|Per Kristen Jakobsen et.al.|[2504.16865](http://arxiv.org/abs/2504.16865)|null|24 pages, 19 figures|In this paper we introduce a new fix point iteration scheme for solving nonlinear electromagnetic scattering problems. The method is based on a spectral formulation of Maxwell's equations called the Bidirectional Pulse Propagation Equations. The scheme can be applied to a wide array of slab-like geometries, and for arbitrary material responses. We derive the scheme and investigated how it performs with respect to convergence and accuracy by applying it to the case of light scattering from a simple slab whose nonlinear material response is a sum a very fast electronic vibrational response, and a much slower molecular vibrational response.|\n", "2504.16862": "|**2025-04-23**|**Neural Network Element Method for Partial Differential Equations**|Yifan Wang, Zhongshuo Lin, Hehu Xie et.al.|[2504.16862](http://arxiv.org/abs/2504.16862)|null|19 pages,0 figure|In this paper, based on the combination of finite element mesh and neural network, a novel type of neural network element space and corresponding machine learning method are designed for solving partial differential equations. The application of finite element mesh makes the neural network element space satisfy the boundary value conditions directly on the complex geometric domains. The use of neural networks allows the accuracy of the approximate solution to reach the high level of neural network approximation even for the problems with singularities. We also provide the error analysis of the proposed method for the understanding. The proposed numerical method in this paper provides the way to enable neural network-based machine learning algorithms to solve a broader range of problems arising from engineering applications.|\n", "2504.16823": "|**2025-04-23**|**Energy Variational Modeling and Numerical Simulation of Open Membranes in Stokes Flow**|Han Zhou, Yuan-Nan Young, Yoichiro Mori et.al.|[2504.16823](http://arxiv.org/abs/2504.16823)|null||Lipid bilayer membranes are fundamental biological structures that serve as cellular boundaries, mediating transport, signaling, and maintaining structural integrity. This study introduces a novel mathematical model for open membranes immersed in Stokes flows, accounting for membrane elasticity, line tension at the open edge, and fluid-membrane interactions. The model is derived from an energy functional that incorporates Helfrich bending energy and a line energy associated with the open edge. By balancing dissipation in both the bulk fluid and the membrane surface, following the maximal dissipation principle, we derive the governing equations within an energy variational framework. Assuming axisymmetry and employing a boundary integral reduction, we transform the 3D problem into an effectively 1D problem, for which we develop a finite element-based numerical method to solve the resulting moving boundary problem. Several numerical examples are provided to validate the model and compare the results with existing studies.|\n", "2504.16816": "|**2025-04-23**|**Simple and accurate nonlinear pendulum motion for the full range of amplitudes**|Teepanis Chachiyo et.al.|[2504.16816](http://arxiv.org/abs/2504.16816)|null||A simple closed-form formula for the period of a pendulum with finite amplitude is proposed. It reproduces the exact analytical forms both in the small and large amplitude limits, while in the mid-amplitude range maintains average error of 0.06% and maximum error of 0.17%. The accuracy should be sufficient for typical engineering applications. Its unique simplicity should be useful in a theoretical development that requires trackable mathematical framework or in an introductory physics course that aims to discuss a finite amplitude pendulum. A simple and formally exact solution of angular displacement for the full range of amplitudes is illustrated.|\n", "2504.16797": "|**2025-04-23**|**The extended adjoint state and nonlinearity in correlation-based passive imaging**|Tram Thi Ngoc Nguyen et.al.|[2504.16797](http://arxiv.org/abs/2504.16797)|null||This articles investigates physics-based passive imaging problem, wherein one infers an unknown medium using ambient noise and correlation of the noise signal. We develop a general backpropagation framework via the so-called extended adjoint state, suitable for any linear PDE; crucially, this approach reduces by half the number of required PDE solves. Applications to several different PDE models demonstrate the universality of our method. In addition, we analyze the nonlinearity of the correlated model, revealing a surprising tangential cone condition-like structure, thereby advancing the state of the art towards a convergence guarantee for regularized reconstruction in passive imaging.|\n", "2504.16756": "|**2025-04-23**|**The root-exponential convergence of lightning plus polynomial approximation on corner domains (II)**|Shuhuang Xiang, Shunfeng Yang, Yanghao Wu et.al.|[2504.16756](http://arxiv.org/abs/2504.16756)|null|62 pages,17 figures|This paper builds rigorous analysis on the root-exponential convergence for the lightning schemes via rational functions in approximating corner singularity problems with uniform exponentially clustered poles proposed by Gopal and Trefethen. The start point is to set up the representations of $z^\\alpha$ and $z^\\alpha\\log z$ in the slit disk and develop results akin to Paley-Wiener theorem, from which, together with the Poisson summation formula, the root-exponential convergence of the lightning plus polynomial scheme with an exact order for each clustered parameter is established in approximation of prototype functions $g(z)z^\\alpha$ or $g(z)z^\\alpha\\log z$ on a sector-shaped domain, which includes $[0,1]$ as a special case. In addition, the fastest convergence rate is confirmed based upon the best choice of the clustered parameter. Furthermore, the optimal choice of the clustered parameter and the convergence rate for corner singularity problems in solving Laplace equations are attested based on Lehman and Wasow's study of corner singularities and along with the decomposition of Gopal and Trefethen. The thorough analysis provides a solid foundation for lightning schemes and rational approximation. Ample numerical evidences demonstrate the optimality and sharpness of the estimates.|\n", "2504.16713": "|**2025-04-23**|**Mixing Data-Driven and Physics-Based Constitutive Models using Uncertainty-Driven Phase Fields**|J. Storm, W. Sun, I. B. C. M. Rocha et.al.|[2504.16713](http://arxiv.org/abs/2504.16713)|null||There is a high interest in accelerating multiscale models using data-driven surrogate modeling techniques. Creating a large training dataset encompassing all relevant load scenarios is essential for a good surrogate, yet the computational cost of producing this data quickly becomes a limiting factor. Commonly, a pre-trained surrogate is used throughout the computational domain. Here, we introduce an alternative adaptive mixture approach that uses a fast probabilistic surrogate model as constitutive model when possible, but resorts back to the true high-fidelity model when necessary. The surrogate is thus not required to be accurate for every possible load condition, enabling a significant reduction in the data collection time. We achieve this by creating phases in the computational domain corresponding to the different models. These phases evolve using a phase-field model driven by the surrogate uncertainty. When the surrogate uncertainty becomes large, the phase-field model causes a local transition from the surrogate to the high-fidelity model, maintaining a highly accurate simulation. We discuss the requirements of this approach to achieve accurate and numerically stable results and compare the phase-field model to a purely local approach that does not enforce spatial smoothness for the phase mixing. Using a Gaussian Process surrogate for an elasto-plastic material, we demonstrate the potential of this mixture of models to accelerate multiscale simulations.|\n", "2504.16652": "|**2025-04-23**|**Non-linearity Effect Analysis of Gaussian Pulse Propagation In Optical Fiber**|Yogesh Deo, Meenu Shrestha, Om Nath Acharya et.al.|[2504.16652](http://arxiv.org/abs/2504.16652)|null||In this research, numerical analysis of nonlinear pulse propagation is carried out. This is done mainly by solving the nonlinear Schrodinger equation using the split step algorithm. In a nonlinear media, dispersive effects exist simultaneously with nonlinear effects. Refractive index dependence on intensity results in optical Kerr effect which causes narrowing of transmitted pulses by inducing self-phase modulation while second order group velocity dispersion causes the pulses to spread. In this project, group velocity dispersion is discussed followed by self-phase modulation. These individually detrimental effects are shown to combine beneficially for propagation of pulses here. Gaussian pulse is studied and propagated by using them as input in to the nonlinear Schrodinger equation. The split step algorithm is described in depth. Explanation of each step is included along with the relevant equations defining these steps.|\n", "2504.16608": "|**2025-04-23**|**A hybrid high-order method for the biharmonic problem**|Yizhou Liang, Ngoc Tien Tran et.al.|[2504.16608](http://arxiv.org/abs/2504.16608)|null||This paper proposes a new hybrid high-order discretization for the biharmonic problem and the corresponding eigenvalue problem. The discrete ansatz space includes degrees of freedom in $n-2$ dimensional submanifolds (e.g., nodal values in 2D and edge values in 3D), in addition to the typical degrees of freedom in the mesh and on the hyperfaces in the HHO literature. This approach enables the characteristic commuting property of the hybrid high-order methodology in any space dimension and allows for lower eigenvalue bounds of higher order for the eigenvalue problem. The main results are quasi-best approximation estimates as well as reliable and efficient error control. The latter motivates an adaptive mesh-refining algorithm that empirically recovers optimal convergence rates for singular solutions.|\n", "2504.16600": "|**2025-04-23**|**3D-1D modelling of cranial plate heating induced by low or medium frequency magnetic fields**|Alessandro Arduino, Oriano Bottauscio, Denise Grappein et.al.|[2504.16600](http://arxiv.org/abs/2504.16600)|null||Safety assessment of patients with one-dimensionally structured passive implants, like cranial plates or stents, exposed to low or medium frequency magnetic fields, like those generated in magnetic resonance imaging or magnetic hyperthermia, can be challenging, because of the different length scales of the implant and the human body. Most of the methods used to estimate the heating induced near such implants neglect the presence of the metallic materials within the body, modeling the metal as thermal seeds. To overcome this limitation, a novel numerical approach that solves three-dimensional and one-dimensional coupled problems is proposed. This method leads to improved results by modelling the thermal diffusion through the highly conductive metallic implants. A comparison of the proposed method predictions with measurements performed on a cranial plate exposed to the magnetic field generated by a gradient coil system for magnetic resonance imaging is presented, showing an improved accuracy up to 25 % with respect to the method based on thermal seeds. The proposed method is finally applied to a magnetic hyperthermia case study in which a patient with a cranial plate is exposed to the magnetic field generated by a collar-type magnetic hyperthermia applicator for neck tumour treatment, predicting a temperature increase in proximity of the implant that is 10 % lower than the one overestimated by relying on thermal seeds.|\n", "2504.16553": "|**2025-04-23**|**Least-Squares-Embedded Optimization for Accelerated Convergence of PINNs in Acoustic Wavefield Simulations**|Mohammad Mahdi Abedi, David Pardo, Tariq Alkhalifah et.al.|[2504.16553](http://arxiv.org/abs/2504.16553)|null||Physics-Informed Neural Networks (PINNs) have shown promise in solving partial differential equations (PDEs), including the frequency-domain Helmholtz equation. However, standard training of PINNs using gradient descent (GD) suffers from slow convergence and instability, particularly for high-frequency wavefields. For scattered acoustic wavefield simulation based on Helmholtz equation, we derive a hybrid optimization framework that accelerates training convergence by embedding a least-squares (LS) solver directly into the GD loss function. This formulation enables optimal updates for the linear output layer. Our method is applicable with or without perfectly matched layers (PML), and we provide practical tensor-based implementations for both scenarios. Numerical experiments on benchmark velocity models demonstrate that our approach achieves faster convergence, higher accuracy, and improved stability compared to conventional PINN training. In particular, our results show that the LS-enhanced method converges rapidly even in cases where standard GD-based training fails. The LS solver operates on a small normal matrix, ensuring minimal computational overhead and making the method scalable for large-scale wavefield simulations.|\n", "2504.16523": "|**2025-04-23**|**Alternately-optimized SNN method for acoustic scattering problem in unbounded domain**|Haoming Song, Zhiqiang Sheng, Dong Wang et.al.|[2504.16523](http://arxiv.org/abs/2504.16523)|null|30 pages, 8 figures|In this paper, we propose a novel machine learning-based method to solve the acoustic scattering problem in unbounded domain. We first employ the Dirichlet-to-Neumann (DtN) operator to truncate the physically unbounded domain into a computable bounded domain. This transformation reduces the original scattering problem in the unbounded domain to a boundary value problem within the bounded domain. To solve this boundary value problem, we design a neural network with a subspace layer, where each neuron in this layer represents a basis function. Consequently, the approximate solution can be expressed by a linear combination of these basis functions. Furthermore, we introduce an innovative alternating optimization technique which alternately updates the basis functions and their linear combination coefficients respectively by training and least squares methods. In our method, we set the coefficients of basis functions to 1 and use a new loss function each time train the subspace. These innovations ensure that the subspace formed by these basis functions is truly optimized. We refer to this method as the alternately-optimized subspace method based on neural networks (AO-SNN). Extensive numerical experiments demonstrate that our new method can significantly reduce the relative $l^2$ error to $10^{-7}$ or lower, outperforming existing machine learning-based methods to the best of our knowledge.|\n", "2504.16451": "|**2025-04-23**|**Efficient Design of Compliant Mechanisms Using Multi-Objective Optimization**|Alexander Humer, Sebastian Platzer et.al.|[2504.16451](http://arxiv.org/abs/2504.16451)|null|XI ECCOMAS Thematic Conference on Smart Structures and Materials   (SMART 2025)|Compliant mechanisms achieve motion through elastic deformation. In this work, we address the synthesis of a compliant cross-hinge mechanism capable of large angular strokes while approximating the behavior of an ideal revolute joint. To capture the competing demands of kinematic fidelity, rotational stiffness, and resistance to parasitic motion, we formulate a multi-objective optimization problem based on kinetostatic performance measures. A hybrid design strategy is employed: an efficient beam-based structural model enables extensive exploration of a high-dimensional design space using evolutionary algorithms, followed by fine-tuning with high-fidelity three-dimensional finite element analysis. The resulting Pareto-optimal designs reveal diverse geometric configurations and performance trade-offs.|\n", "2504.16418": "|**2025-04-23**|**Scalable Data-Driven Basis Selection for Linear Machine Learning Interatomic Potentials**|Tina Torabi, Matthias Militzer, Michael P. Friedlander et.al.|[2504.16418](http://arxiv.org/abs/2504.16418)|null||Machine learning interatomic potentials (MLIPs) provide an effective approach for accurately and efficiently modeling atomic interactions, expanding the capabilities of atomistic simulations to complex systems. However, a priori feature selection leads to high complexity, which can be detrimental to both computational cost and generalization, resulting in a need for hyperparameter tuning. We demonstrate the benefits of active set algorithms for automated data-driven feature selection. The proposed methods are implemented within the Atomic Cluster Expansion (ACE) framework. Computational tests conducted on a variety of benchmark datasets indicate that sparse ACE models consistently enhance computational efficiency, generalization accuracy and interpretability over dense ACE models. An added benefit of the proposed algorithms is that they produce entire paths of models with varying cost/accuracy ratio.|\n", "2504.16381": "|**2025-04-23**|**PINN-MEP: Continuous Neural Representations for Minimum-Energy Path Discovery in Molecular Systems**|Magnus Petersen, Roberto Covino et.al.|[2504.16381](http://arxiv.org/abs/2504.16381)|null||Characterizing conformational transitions in physical systems remains a fundamental challenge in the computational sciences. Traditional sampling methods like molecular dynamics (MD) or MCMC often struggle with the high-dimensional nature of molecular systems and the high energy barriers of transitions between stable states. While these transitions are rare events in simulation timescales, they often represent the most biologically significant processes - for example, the conformational change of an ion channel protein from its closed to open state, which controls cellular ion flow and is crucial for neural signaling. Such transitions in real systems may take milliseconds to seconds but could require months or years of continuous simulation to observe even once. We present a method that reformulates transition path generation as a continuous optimization problem solved through physics-informed neural networks (PINNs) inspired by string methods for minimum-energy path (MEP) generation. By representing transition paths as implicit neural functions and leveraging automatic differentiation with differentiable molecular dynamics force fields, our method enables the efficient discovery of physically realistic transition pathways without requiring expensive path sampling. We demonstrate our method's effectiveness on two proteins, including an explicitly hydrated bovine pancreatic trypsin inhibitor (BPTI) system with over 8,300 atoms.|\n", "2504.16344": "|**2025-04-23**|**Real-time Bayesian inference at extreme scale: A digital twin for tsunami early warning applied to the Cascadia subduction zone**|Stefan Henneking, Sreeram Venkat, Veselin Dobrev et.al.|[2504.16344](http://arxiv.org/abs/2504.16344)|null||We present a Bayesian inversion-based digital twin that employs acoustic pressure data from seafloor sensors, along with 3D coupled acoustic-gravity wave equations, to infer earthquake-induced spatiotemporal seafloor motion in real time and forecast tsunami propagation toward coastlines for early warning with quantified uncertainties. Our target is the Cascadia subduction zone, with one billion parameters. Computing the posterior mean alone would require 50 years on a 512 GPU machine. Instead, exploiting the shift invariance of the parameter-to-observable map and devising novel parallel algorithms, we induce a fast offline-online decomposition. The offline component requires just one adjoint wave propagation per sensor; using MFEM, we scale this part of the computation to the full El Capitan system (43,520 GPUs) with 92% weak parallel efficiency. Moreover, given real-time data, the online component exactly solves the Bayesian inverse and forecasting problems in 0.2 seconds on a modest GPU system, a ten-billion-fold speedup.|\n", "2504.16297": "|**2025-04-22**|**Augmenting Simulated Noisy Quantum Data Collection by Orders of Magnitude Using Pre-Trajectory Sampling with Batched Execution**|Taylor L. Patti, Thien Nguyen, Justin G. Lietz et.al.|[2504.16297](http://arxiv.org/abs/2504.16297)|null|8 pages, 5 figures|Classically simulating quantum systems is challenging, as even noiseless $n$-qubit quantum states scale as $2^n$. The complexity of noisy quantum systems is even greater, requiring $2^n \\times 2^n$-dimensional density matrices. Various approximations reduce density matrix overhead, including quantum trajectory-based methods, which instead use an ensemble of $m \\ll 2^n$ noisy states. While this method is dramatically more efficient, current implementations use unoptimized sampling, redundant state preparation, and single-shot data collection. In this manuscript, we present the Pre-Trajectory Sampling technique, increasing the efficiency and utility of trajectory simulations by tailoring error types, batching sampling without redundant computation, and collecting error information. We demonstrate the effectiveness of our method with both a mature statevector simulation of a 35-qubit quantum error-correction code and a preliminary tensor network simulation of 85 qubits, yielding speedups of up to $10^6$x and $16$x, as well as generating massive datasets of one trillion and one million shots, respectively.|\n", "2504.17729": "|**2025-04-24**|**Fully-Mixed Virtual Element Method for the Biot Problem**|Michele Botti, Daniele Prada, Anna Scotti et.al.|[2504.17729](http://arxiv.org/abs/2504.17729)|null||Poroelasticity describes the interaction of deformation and fluid flow in saturated porous media. A fully-mixed formulation of Biot's poroelasticity problem has the advantage of producing a better approximation of the Darcy velocity and stress field, as well as satisfying local mass and momentum conservation. In this work, we focus on a novel four-fields Virtual Element discretization of Biot's equations. The stress symmetry is strongly imposed in the definition of the discrete space, thus avoiding the use of an additional Lagrange multiplier. A complete a priori analysis is performed, showing the robustness of the proposed numerical method with respect to limiting material properties. The first order convergence of the lowest-order fully-discrete numerical method, which is obtained by coupling the spatial approximation with the backward Euler time-advancing scheme, is confirmed by a complete 3D numerical validation. A well known poroelasticity benchmark is also considered to assess the robustness properties and computational performance.|\n", "2504.17726": "|**2025-04-24**|**Optical to infrared mapping of vapor-to-liquid phase change dynamics using generative machine learning**|Siavash Khodakarami, Pouya Kabirzadeh, Chi Wang et.al.|[2504.17726](http://arxiv.org/abs/2504.17726)|null||Infrared thermography is a powerful tool for studying liquid-to-vapor phase change processes. However, its application has been limited in the study of vapor-to-liquid phase transitions due to the presence of complex liquid dynamics, multiple phases within the same field of view, and experimental difficulty. Here, we develop a calibration framework which is capable to studying one of the most complex two-phase heat transfer processes: dropwise condensation. The framework accounts for non-uniformities arising from dynamic two-phase interactions such as droplet nucleation, growth, coalescence, and departure, as well as substrate effects particularly observed on micro- and nanoengineered surfaces. This approach enables high-resolution temperature measurements with both spatial (12 $\\mu$m) and temporal (5 ms) precision, leading to the discovery of local temperature phenomena unobservable using conventional approaches. These observed temperature variations are linked to droplet statistics, showing how different regions contribute to local condensation heat transfer. We extend the developed method to quantify local thermal parameters by fusing it with a generative machine learning model to map visual images into temperature fields. The model is informed of the physical parameter by incorporating vapor pressure embedding as the conditional parameter. This work represents a significant step toward simplifying local temperature measurements for vapor-to-liquid phase change phenomena by developing a methodology as well as a machine learning approach to map local thermal phenomena using only optical images as the input.|\n", "2504.17649": "|**2025-04-24**|**On Josephy-Halley method for generalized equations**|Tom\u00e1\u0161 Roubal, Jan Valdman et.al.|[2504.17649](http://arxiv.org/abs/2504.17649)|null|17 pages, 3 figures|We extend the classical third-order Halley iteration to the setting of generalized equations of the form \\[ 0 \\in f(x) + F(x), \\] where \\(f\\colon X\\longrightarrow Y\\) is twice continuously Fr\\'echet-differentiable on Banach spaces and \\(F\\colon X\\tto Y\\) is a set-valued mapping with closed graph. Building on predictor-corrector framework, our scheme first solves a partially linearized inclusion to produce a predictor \\(u_{k+1}\\), then incorporates second-order information in a Halley-type corrector step to obtain \\(x_{k+1}\\). Under metric regularity of the linearization at a reference solution and H\\\"older continuity of \\(f''\\), we prove that the iterates converge locally with order \\(2+p\\) (cubically when \\(p=1\\)). Moreover, by constructing a suitable scalar majorant function we derive semilocal Kantorovich-type conditions guaranteeing well-definedness and R-cubic convergence from an explicit neighbourhood of the initial guess. Numerical experiments-including one- and two-dimensional test problems confirm the theoretical convergence rates and illustrate the efficiency of the Josephy-Halley method compared to its Josephy-Newton counterpart.|\n", "2504.17596": "|**2025-04-24**|**Rescaling and unconstrained minimisation of convex quadratic maps**|Alexandra Zverovich, Matthew Hutchings, Bertrand Gauthier et.al.|[2504.17596](http://arxiv.org/abs/2504.17596)|null|19 pages, 9 figures|We investigate the properties of a class of piecewise-fractional maps arising from the introduction of an invariance under rescaling into convex quadratic maps. The subsequent maps are quasiconvex, and pseudoconvex on specific convex cones; they can be optimised via exact line search along admissible directions, and the iterates then inherit a bidimensional optimality property. We study the minimisation of such relaxed maps via coordinate descents with gradient-based rules, placing a special emphasis on coordinate directions verifying a maximum-alignment property in the reproducing kernel Hilbert spaces related to the underlying positive-semidefinite matrices. In this setting, we illustrate that accounting for the optimal rescaling of the iterates can in certain situations substantially accelerate the unconstrained minimisation of convex quadratic maps.|\n", "2504.17476": "|**2025-04-24**|**Implicit Sub-stepping Scheme for Critical State Soil Models**|Hoang Giang Bui, Jelena Ninic, G\u00fcnther Meschke et.al.|[2504.17476](http://arxiv.org/abs/2504.17476)|null||The stress integration of critical soil model is usually based on implicit Euler algorithm, where the stress predictor is corrected by employing a return mapping algorithm. In the case of large load step, the solution of local nonlinear system to compute the plastic multiplier may not be attained. To overcome this problem, a sub-stepping scheme shall be used to improve the convergence of the local nonlin- ear system solution strategy. Nevertheless, the complexity of the tangent operator of the sub-stepping scheme is high. This complicates the use of Newton-Raphson algorithm to obtain global quadratic convergence. In this paper, a formulation for consistent tangent operator is developed for implicit sub-stepping integration for the modified Cam-Clay model and unified Clay and Sand model. This formulation is highly efficient and can be used with problem involving large load step, such as tun- nel simulation.|\n", "2504.17439": "|**2025-04-24**|**Self-consistent GW via conservation of spectral moments**|Oliver J. Backhouse, Marcus K. Allen, Charles C. J. Scott et.al.|[2504.17439](http://arxiv.org/abs/2504.17439)|null||We expand on a recently introduced alternate framework for $GW$ simulation of charged excitations [Scott et. al., J. Chem. Phys., 158, 124102 (2023)], based around the conservation of directly computed spectral moments of the GW self-energy. Featuring a number of desirable formal properties over other implementations, we also detail efficiency improvements and a parallelism strategy, resulting in an implementation with a demonstrable similar scaling to an established Hartree--Fock code, with only an order of magnitude increase in cost. We also detail the applicability of a range of self-consistent $GW$ variants within this framework, including a scheme for full self-consistency of all dynamical variables, whilst avoiding the Matsubara axis or analytic continuation, allowing formal convergence at zero temperature. By investigating a range of self-consistency protocols over the GW100 molecular test set, we find that a little-explored self-consistent variant based around a simpler coupled chemical potential and Fock matrix optimization to be the most accurate self-consistent $GW$ approach. Additionally, we validate recently observed evidence that Tamm--Dancoff based screening approximations within $GW$ lead to higher accuracy than traditional random phase approximation screening over these molecular test cases. Finally, we consider the Chlorophyll A molecule, finding agreement with experiment within the experimental uncertainty, and a description of the full-frequency spectrum of charged excitations.|\n", "2504.17368": "|**2025-04-24**|**Inverse-Designed Metasurfaces for Wavefront Restoration in Under-Display Camera Systems**|Jaegang Jo, Myunghoo Lee, Seunghyun Lee et.al.|[2504.17368](http://arxiv.org/abs/2504.17368)|null|25 pages, 8 figures|Under-display camera (UDC) systems enable full-screen displays in smartphones by embedding the camera beneath the display panel, eliminating the need for notches or punch holes. However, the periodic pixel structures of display panels introduce significant optical diffraction effects, leading to imaging artifacts and degraded visual quality. Conventional approaches to mitigate these distortions, such as deep learning-based image reconstruction, are often computationally expensive and unsuitable for real-time applications in consumer electronics. This work introduces an inverse-designed metasurface for wavefront restoration, addressing diffraction-induced distortions without relying on external software processing. The proposed metasurface effectively suppresses higher-order diffraction modes caused by the metallic pixel structures, restores the optical wavefront, and enhances imaging quality across multiple wavelengths. By eliminating the need for software-based post-processing, our approach establishes a scalable, real-time optical solution for diffraction management in UDC systems. This advancement paves the way to achieve software-free real-time image restoration frameworks for many industrial applications.|\n", "2504.17329": "|**2025-04-24**|**On Runge-Kutta methods of order 10**|Misha Stepanov et.al.|[2504.17329](http://arxiv.org/abs/2504.17329)|null|21 pages, 5 figures, 3 tables|A family of explicit 15-stage Runge-Kutta methods of order 10 is derived.|\n", "2504.17272": "|**2025-04-24**|**Development and Explainability of Models for Machine-Learning-Based Reconstruction of Signals in Particle Detectors**|Kalina Dimitrova, Venelin Kozhuharov, Peicho Petkov et.al.|[2504.17272](http://arxiv.org/abs/2504.17272)|null|This article was published in Particles, 2025, 8, 48, DOI:   10.3390/particles8020048|Machine learning methods are being introduced at all stages of data reconstruction and analysis in various high-energy physics experiments. We present the development and application of convolutional neural networks with modified autoencoder architecture for the reconstruction of the pulse arrival time and amplitude in individual scintillating crystals in electromagnetic calorimeters and other detectors. The network performance is discussed as well as the application of xAI methods for further investigation of the algorithm and improvement of the output accuracy.|\n", "2504.17233": "|**2025-04-24**|**An Adaptive Finite Element DtN Method for the Acoustic-Elastic Interaction Problem in Periodic Structures**|Lei Lin, Junliang Lv et.al.|[2504.17233](http://arxiv.org/abs/2504.17233)|null|28 pages, 9 figures|Consider a time-harmonic acoustic plane wave incident onto an elastic body with an unbounded periodic surface. The medium above the surface is supposed to be filled with a homogeneous compressible inviscid air/fluid of constant mass density, while the elastic body is assumed to be isotropic and linear. By introducing the Dirichlet-to-Neumann (DtN) operators for acoustic and elastic waves simultaneously, the model is formulated as an acoustic-elastic interaction problem in periodic structures. Based on a duality argument, an a posteriori error estimate is derived for the associated truncated finite element approximation. The a posteriori error estimate consists of the finite element approximation error and the truncation error of two different DtN operators, where the latter decays exponentially with respect to the truncation parameter. Based on the a posteriori error, an adaptive finite element algorithm is proposed for solving the acoustic-elastic interaction problem in periodic structures. Numerical experiments are presented to demonstrate the effectiveness of the proposed algorithm.|\n", "2504.17142": "|**2025-04-23**|**Reinforcement learning framework for the mechanical design of microelectronic components under multiphysics constraints**|Siddharth Nair, Timothy F. Walsh, Greg Pickrell et.al.|[2504.17142](http://arxiv.org/abs/2504.17142)|null|27 pages of main text, 15 figures|This study focuses on the development of reinforcement learning based techniques for the design of microelectronic components under multiphysics constraints. While traditional design approaches based on global optimization approaches are effective when dealing with a small number of design parameters, as the complexity of the solution space and of the constraints increases different techniques are needed. This is an important reason that makes the design and optimization of microelectronic components (characterized by large solution space and multiphysics constraints) very challenging for traditional methods. By taking as prototypical elements an application-specific integrated circuit (ASIC) and a heterogeneously integrated (HI) interposer, we develop and numerically test an optimization framework based on reinforcement learning (RL). More specifically, we consider the optimization of the bonded interconnect geometry for an ASIC chip as well as the placement of components on a HI interposer while satisfying thermoelastic and design constraints. This placement problem is particularly interesting because it features a high-dimensional solution space.|\n", "2504.17112": "|**2025-04-23**|**Physics-informed features in supervised machine learning**|Margherita Lampani, Sabrina Guastavino, Michele Piana et.al.|[2504.17112](http://arxiv.org/abs/2504.17112)|null||Supervised machine learning involves approximating an unknown functional relationship from a limited dataset of features and corresponding labels. The classical approach to feature-based machine learning typically relies on applying linear regression to standardized features, without considering their physical meaning. This may limit model explainability, particularly in scientific applications. This study proposes a physics-informed approach to feature-based machine learning that constructs non-linear feature maps informed by physical laws and dimensional analysis. These maps enhance model interpretability and, when physical laws are unknown, allow for the identification of relevant mechanisms through feature ranking. The method aims to improve both predictive performance in regression tasks and classification skill scores by integrating domain knowledge into the learning process, while also enabling the potential discovery of new physical equations within the context of explainable machine learning.|\n", "2504.17108": "|**2025-04-23**|**Computational Physics in the Advanced Lab: Experiment and Simulation of Thermal Diffusion in Metal Rods**|Yash Mohod, Matthew C. Sullivan et.al.|[2504.17108](http://arxiv.org/abs/2504.17108)|null|12 pages, 5 figures|Computational physics is integrated throughout the current undergraduate physics curriculum, though there are surprisingly few resources for computational physics in the advanced lab courses. This is despite the fact that a comparison of numerical simulations to experimental results is common practice in modern physics research. In this paper we present a simple experiment in thermal diffusion in metal rods. An analytical solution exists for the transient heat conduction in an infinite rod with a delta function heat input, but no analytical solution exists for short rods or for long duration heat inputs. Our apparatus is a copper rod with a heater and thermometers attached to the rod. The temperature difference on the metal rods due to transient heat conduction can be modeled using a simple numerical simulation using the finite centered difference method. Using a 22 cm long copper rod with the ends thermally sunk in aluminum blocks, we show poor agreement between the experimental results and the infinite-rod analytical model, but excellent agreement between the experimental results and our numerical simulation. Repeating the experiment with only one end of the rod sunk into an aluminum block (the other floating), we get good qualitative agreement between the experimental results and the numerical model. This experiment shows the power of a numerical simulation but also the limitations of the chosen model, which can be used as motivation for further exploration.|\n", "2504.17092": "|**2025-04-23**|**Lattice Dynamics of Energy Materials Investigated by Neutron Scattering**|Tyler C. Sterling et.al.|[2504.17092](http://arxiv.org/abs/2504.17092)|null|This is my doctoral dissertation; it contains original content in a   few places, so I am publishing on arxiv to make available|In this thesis, I discuss several basic science studies in the field of energy materials using neutron scattering as a probe for the lattice dynamics. To enable understanding of neutron scattering spectra, I also use computational and theoretical methods. These methods and neutron scattering in general are discussed in detail in Chapter 2. It is assumed that the reader is familiar with basic quantum mechanics as well as with solid state physics topics including the band theory of electrons, harmonic lattice dynamics, and molecular dynamics. For the unfamiliar reader, the details of electronic structure theory and lattice dynamics that are needed to understand the methods in Chapter 2 are provided in Chapters 3 and 4. In the remaining chapters, these methods are applied to the study of several energy materials: cuprate La2CuO4,(hybrid) solar perovskite CH3NH3PbI3, and thermoelectric clathrate Ba8Ga16Ge30.|\n", "2504.17077": "|**2025-04-23**|**Physics-guided and fabrication-aware inverse design of photonic devices using diffusion models**|Dongjin Seo, Soobin Um, Sangbin Lee et.al.|[2504.17077](http://arxiv.org/abs/2504.17077)|null|25 pages, 7 Figures|Designing free-form photonic devices is fundamentally challenging due to the vast number of possible geometries and the complex requirements of fabrication constraints. Traditional inverse-design approaches--whether driven by human intuition, global optimization, or adjoint-based gradient methods--often involve intricate binarization and filtering steps, while recent deep learning strategies demand prohibitively large numbers of simulations (10^5 to 10^6). To overcome these limitations, we present AdjointDiffusion, a physics-guided framework that integrates adjoint sensitivity gradients into the sampling process of diffusion models. AdjointDiffusion begins by training a diffusion network on a synthetic, fabrication-aware dataset of binary masks. During inference, we compute the adjoint gradient of a candidate structure and inject this physics-based guidance at each denoising step, steering the generative process toward high figure-of-merit (FoM) solutions without additional post-processing. We demonstrate our method on two canonical photonic design problems--a bent waveguide and a CMOS image sensor color router--and show that our method consistently outperforms state-of-the-art nonlinear optimizers (such as MMA and SLSQP) in both efficiency and manufacturability, while using orders of magnitude fewer simulations (approximately 2 x 10^2) than pure deep learning approaches (approximately 10^5 to 10^6). By eliminating complex binarization schedules and minimizing simulation overhead, AdjointDiffusion offers a streamlined, simulation-efficient, and fabrication-aware pipeline for next-generation photonic device design. Our open-source implementation is available at https://github.com/dongjin-seo2020/AdjointDiffusion.|\n", "2504.17012": "|**2025-04-23**|**Universal Methods for Nonlinear Spectral Problems**|Matthew J. Colbrook, Catherine Drysdale et.al.|[2504.17012](http://arxiv.org/abs/2504.17012)|null||Nonlinear spectral problems arise across a range of fields, including mechanical vibrations, fluid-solid interactions, and photonic crystals. Discretizing infinite-dimensional nonlinear spectral problems often introduces significant computational challenges, particularly spectral pollution and invisibility, which can distort or obscure the true underlying spectrum. We present the first general, convergent computational method for computing the spectra and pseudospectra of nonlinear spectral problems. Our approach uses new results on nonlinear injection moduli and requires only minimal continuity assumptions: specifically, continuity with respect to the gap metric on operator graphs, making it applicable to a broad class of problems. We use the Solvability Complexity Index (SCI) hierarchy, which has recently been used to resolve the classical linear problem, to systematically classify the computational complexity of nonlinear spectral problems. Our results establish the optimality of the method and reveal that Hermiticity does not necessarily simplify the computational complexity of these nonlinear problems. Comprehensive examples -- including nonlinear shifts, Klein--Gordon equations, wave equations with acoustic boundary conditions, time-fractional beam equations, and biologically inspired delay differential equations -- demonstrate the robustness, accuracy, and broad applicability of our methodology.|\n", "2504.18526": "|**2025-04-25**|**Robust semi-implicit multilevel SDC methods for conservation laws**|Erik Pfister, J\u00f6rg Stiller et.al.|[2504.18526](http://arxiv.org/abs/2504.18526)|null||Semi-implicit multilevel spectral deferred correction (SI-MLSDC) methods provide a promising approach for high-order time integration for nonlinear evolution equations including conservation laws. However, existing methods lack robustness and often do not achieve the expected advantage over single-level SDC. This work adopts the novel SI time integrators from [44] for enhanced stability and extends the single-level SI-SDC method with a multilevel approach to increase computational efficiency. The favourable properties of the resulting SI-MLSDC method are shown by linear temporal stability analysis for a convection-diffusion problem. The robustness and efficiency of the fully discrete method involving a high-order discontinuous Galerkin SEM discretization are demonstrated through numerical experiments for the convection-diffusion, Burgers, Euler and Navier-Stokes equations. The method is shown to yield substantial reductions in fine-grid iterations compared to single-level SI-SDC across a broad range of test cases. Finally, current limitations of the SI-MLSDC framework are identified and discussed, providing guidance for future improvements.|\n", "2504.18513": "|**2025-04-25**|**PODNO: Proper Orthogonal Decomposition Neural Operators**|Zilan Cheng, Zhongjian Wang, Li-Lian Wang et.al.|[2504.18513](http://arxiv.org/abs/2504.18513)|null||In this paper, we introduce Proper Orthogonal Decomposition Neural Operators (PODNO) for solving partial differential equations (PDEs) dominated by high-frequency components. Building on the structure of Fourier Neural Operators (FNO), PODNO replaces the Fourier transform with (inverse) orthonormal transforms derived from the Proper Orthogonal Decomposition (POD) method to construct the integral kernel. Due to the optimality of POD basis, the PODNO has potential to outperform FNO in both accuracy and computational efficiency for high-frequency problems. From analysis point of view, we established the universality of a generalization of PODNO, termed as Generalized Spectral Operator (GSO). In addition, we evaluate PODNO's performance numerically on dispersive equations such as the Nonlinear Schrodinger (NLS) equation and the Kadomtsev-Petviashvili (KP) equation.|\n", "2504.18486": "|**2025-04-25**|**Supporting Higher-Order Interactions in Practical Ising Machines**|Nafisa Sadaf Prova, H\u00fcsrev C\u0131lasun, Abhimanyu Kumar et.al.|[2504.18486](http://arxiv.org/abs/2504.18486)|null||Ising machines as hardware solvers of combinatorial optimization problems (COPs) can efficiently explore large solution spaces due to their inherent parallelism and physics-based dynamics. Many important COP classes such as satisfiability (SAT) assume arbitrary interactions between problem variables, while most Ising machines only support pairwise (second-order) interactions. This necessitates translation of higher-order interactions to pair-wise, which typically results in extra variables not corresponding to problem variables, and a larger problem for the Ising machine to solve than the original problem. This in turn can significantly increase time-to-solution and/or degrade solution accuracy. In this paper, considering a representative CMOS-compatible class of Ising machines, we propose a practical design to enable direct hardware support for higher order interactions. By minimizing the overhead of problem translation and mapping, our design leads to up to 4x lower time-to-solution without compromising solution accuracy.|\n", "2504.18473": "|**2025-04-25**|**Interface phonon modes governing the ideal limit of thermal transport across diamond/cubic boron nitride interfaces**|Xiaonan Wang, Xin Wu, Penghua Ying et.al.|[2504.18473](http://arxiv.org/abs/2504.18473)|null|11 pages, 7 figures|Understanding the ideal limit of interfacial thermal conductance (ITC) across semiconductor heterointerfaces is crucial for optimizing heat dissipation in practical applications. By employing a highly accurate and efficient machine-learned potential trained herein, we perform extensive non-equilibrium molecular dynamics simulations to investigate the ITC of diamond/cubic boron nitride ($c$BN) interfaces. The ideal diamond/$c$BN interface exhibits an unprecedented ITC of 11.0 $\\pm$ 0.1 GW m$^{-2}$ K$^{-1}$, setting a new upper bound for heterostructure interfaces. This exceptional conductance originates from extended phonon modes due to acoustic matching and localized C-atom modes that propagate through B-C bonds. However, atomic diffusion across the ideal interface creates mixing layers that disrupt these characteristic phonon modes, substantially suppressing the thermal transport from its ideal limit. Our findings reveal how interface phonon modes govern thermal transport across diamond/$c$BN interfaces, providing insights for thermal management in semiconductor devices.|\n", "2504.18465": "|**2025-04-25**|**Generalized Chebyshev Acceleration**|Nurg\u00fcl G\u00f6kg\u00f6z et.al.|[2504.18465](http://arxiv.org/abs/2504.18465)|null||We use generalized Chebyshev polynomials, associated with the root system $A_2$, to provide a new semi-iterative method for accelerating simple iterative methods for solving linear systems. We apply this semi-iterative method to the Jacobi method, and give an example. There are certain restrictions but the resulting acceleration is rather high.|\n", "2504.18367": "|**2025-04-25**|**Enhanced Sampling, Public Dataset and Generative Model for Drug-Protein Dissociation Dynamics**|Maodong Li, Jiying Zhang, Bin Feng et.al.|[2504.18367](http://arxiv.org/abs/2504.18367)|null|The code will be accessed from our GitHub repository   https://huggingface.co/SZBL-IDEA|Drug-protein binding and dissociation dynamics are fundamental to understanding molecular interactions in biological systems. While many tools for drug-protein interaction studies have emerged, especially artificial intelligence (AI)-based generative models, predictive tools on binding/dissociation kinetics and dynamics are still limited. We propose a novel research paradigm that combines molecular dynamics (MD) simulations, enhanced sampling, and AI generative models to address this issue. We propose an enhanced sampling strategy to efficiently implement the drug-protein dissociation process in MD simulations and estimate the free energy surface (FES). We constructed a program pipeline of MD simulations based on this sampling strategy, thus generating a dataset including 26,612 drug-protein dissociation trajectories containing about 13 million frames. We named this dissociation dynamics dataset DD-13M and used it to train a deep equivariant generative model UnbindingFlow, which can generate collision-free dissociation trajectories. The DD-13M database and UnbindingFlow model represent a significant advancement in computational structural biology, and we anticipate its broad applicability in machine learning studies of drug-protein interactions. Our ongoing efforts focus on expanding this methodology to encompass a broader spectrum of drug-protein complexes and exploring novel applications in pathway prediction.|\n", "2504.18358": "|**2025-04-25**|**Convergence analysis of Lie and Strang splitting for operator-valued differential Riccati equations**|Eskil Hansen, Tony Stillfjord, Teodor \u00c5berg et.al.|[2504.18358](http://arxiv.org/abs/2504.18358)|null||Differential Riccati equations (DREs) are semilinear matrix- or operator-valued differential equations with quadratic non-linearities. They arise in many different areas, and are particularly important in optimal control of linear quadratic regulators, where they provide the optimal feedback control laws. In the context of control of partial differential equations, these Riccati equations are operator-valued. To approximate their solutions, both spatial and temporal discretizations are needed. While the former have been well analyzed in the literature, there are very few rigorous convergence analyses of time stepping methods applied to DREs, particularly in the infinite-dimensional, operator-valued setting. In view of this, we analyze two numerical time-stepping schemes, the Lie and Strang splitting methods, in such a setting. The analysis relies on the assumption that the uncontrolled system evolves via an operator that generates an analytic semigroup, and that either the initial condition is sufficiently smooth, or the nonlinearity in the DRE is sufficiently smoothing. These assumptions are mild, in the sense that they are not enough to even guarantee continuity in operator-norm of the exact solution to the DRE. However, they imply certain regularity in a pointwise sense, which can be leveraged to prove convergence in operator-norm with the classical orders. The results are illustrated by four numerical experiments, where convergence with the expected order is correlated with the relevant assumptions being fulfilled. The experiments also demonstrate that matrix-valued DREs which arise as spatial discretizations of operator-valued DREs behave similarly, unless the discretization is coarse.|\n", "2504.18356": "|**2025-04-25**|**Numerical method for the inverse scattering by random periodic structures**|Yi Wang, Lei Lin, Junliang Lv et.al.|[2504.18356](http://arxiv.org/abs/2504.18356)|null|26 pages, 15 figures|Due to manufacturing defects or wear and tear, industrial components may have uncertainties. In order to evaluate the performance of machined components, it is crucial to quantify the uncertainty of the scattering surface. This brings up an important class of inverse scattering problems for random interface reconstruction. In this paper, we present an efficient numerical algorithm for the inverse scattering problem of acoustic-elastic interaction with random periodic interfaces. The proposed algorithm combines the Monte Carlo technique and the continuation method with respect to the wavenumber, which can accurately reconstruct the key statistics of random periodic interfaces from the measured data of the acoustic scattered field. In the implementation of our algorithm, a key two-step strategy is employed: Firstly, the elastic displacement field below the interface is determined by Tikhonov regularization based on the dynamic interface condition; Secondly, the profile function is iteratively updated and optimised using the Landweber method according to the kinematic interface condition. Such a algorithm does not require a priori information about the stochastic structures and performs well for both stationary Gaussian and non-Gaussian stochastic processes. Numerical experiments demonstrate the reliability and effectiveness of our proposed method.|\n", "2504.18323": "|**2025-04-25**|**Outlier-aware Tensor Robust Principal Component Analysis with Self-guided Data Augmentation**|Yangyang Xu, Kexin Li, Li Yang et.al.|[2504.18323](http://arxiv.org/abs/2504.18323)|null|12 pages, 6 figures, 3 tables|Tensor Robust Principal Component Analysis (TRPCA) is a fundamental technique for decomposing multi-dimensional data into a low-rank tensor and an outlier tensor, yet existing methods relying on sparse outlier assumptions often fail under structured corruptions. In this paper, we propose a self-guided data augmentation approach that employs adaptive weighting to suppress outlier influence, reformulating the original TRPCA problem into a standard Tensor Principal Component Analysis (TPCA) problem. The proposed model involves an optimization-driven weighting scheme that dynamically identifies and downweights outlier contributions during tensor augmentation. We develop an efficient proximal block coordinate descent algorithm with closed-form updates to solve the resulting optimization problem, ensuring computational efficiency. Theoretical convergence is guaranteed through a framework combining block coordinate descent with majorization-minimization principles. Numerical experiments on synthetic and real-world datasets, including face recovery, background subtraction, and hyperspectral denoising, demonstrate that our method effectively handles various corruption patterns. The results show the improvements in both accuracy and computational efficiency compared to state-of-the-art methods.|\n", "2504.18322": "|**2025-04-25**|**Stable localized orthogonal decomposition in Raviart-Thomas spaces**|Patrick Henning, Hao Li, Timo Sprekeler et.al.|[2504.18322](http://arxiv.org/abs/2504.18322)|null||This work proposes a computational multiscale method for the mixed formulation of a second-order linear elliptic equation subject to a homogeneous Neumann boundary condition, based on a stable localized orthogonal decomposition (LOD) in Raviart-Thomas finite element spaces. In the spirit of numerical homogenization, the construction provides low-dimensional coarse approximation spaces that incorporate fine-scale information from the heterogeneous coefficients by solving local patch problems on a fine mesh. The resulting numerical scheme is accompanied by a rigorous error analysis, and it is applicable beyond periodicity and scale-separation in spatial dimensions two and three. In particular, this novel realization circumvents the presence of pollution terms observed in a previous LOD construction for elliptic problems in mixed formulation. Finally, various numerical experiments are provided that demonstrate the performance of the method.|\n", "2504.18295": "|**2025-04-25**|**Sharp decay estimates and numerical analysis for weakly coupled systems of two subdiffusion equations**|Zhiyuan Li, Yikan Liu, Kazuma Wada et.al.|[2504.18295](http://arxiv.org/abs/2504.18295)|null|28 pages, 7 figures, 2 tables|This paper investigates the initial-boundary value problem for weakly coupled systems of time-fractional subdiffusion equations with spatially and temporally varying coupling coefficients. By combining the energy method with the coercivity of fractional derivatives, we convert the original partial differential equations into a coupled ordinary differential system. Through Laplace transform and maximum principle arguments, we reveal a dichotomy in decay behavior: When the highest fractional order is less than one, solutions exhibit sublinear decay, whereas systems with the highest order equal to one demonstrate a distinct superlinear decay pattern. This phenomenon fundamentally distinguishes coupled systems from single fractional diffusion equations, where such accelerated superlinear decay never occurs. Numerical experiments employing finite difference methods and implicit discretization schemes validate the theoretical findings.|\n", "2504.18248": "|**2025-04-25**|**A finite volume Simo-Reissner beam method for moored floating body dynamics**|Amirhossein Taran, Seevani Bali, Zeljko Tukovic et.al.|[2504.18248](http://arxiv.org/abs/2504.18248)|null|40 pages|This paper presents a novel finite volume mooring line model based on the geometrically exact Simo-Reissner beam model for analysing the interaction between a floating rigid body and its mooring lines. The coupled numerical model is implemented entirely within a finite volume-based discretisation framework using a popular computational fluid dynamics C++ toolbox, OpenFOAM. Unlike existing methods for modelling mooring lines, which rely on lumped mass models or finite element-based approaches, this work simulates the mooring cables using non-linear beam models implemented in a finite volume framework to account for bending, tensile, and torsional loading. This advancement makes the current work particularly valuable for simulating extreme sea conditions. The coupled model developed in this study has been validated and verified using experimental and numerical data for a floating box moored with four catenary mooring lines under regular wave conditions featuring different wave heights and periods. The results demonstrate strong agreement with both experimental and numerical data, highlighting the model's accuracy in capturing mooring dynamics and floating body motion.|\n", "2504.18209": "|**2025-04-25**|**A hybridizable discontinuous Galerkin method with transmission variables for time-harmonic acoustic problems in heterogeneous media**|Simone Pescuma, Gw\u00e9na\u00ebl Gabard, Th\u00e9ophile Chaumont-Frelet et.al.|[2504.18209](http://arxiv.org/abs/2504.18209)|null||We consider the finite element solution of time-harmonic wave propagation problems in heterogeneous media with hybridizable discontinuous Galerkin (HDG) methods. In the case of homogeneous media, it has been observed that the iterative solution of the linear system can be accelerated by hybridizing with transmission variables instead of numerical traces, as performed in standard approaches. In this work, we extend the HDG method with transmission variables, which is called the CHDG method, to the heterogeneous case with piecewise constant physical coefficients. In particular, we consider formulations with standard upwind and general symmetric fluxes. The CHDG hybridized system can be written as a fixed-point problem, which can be solved with stationary iterative schemes for a class of symmetric fluxes. The standard HDG and CHDG methods are systematically studied with the different numerical fluxes by considering a series of 2D numerical benchmarks. The convergence of standard iterative schemes is always faster with the extended CHDG method than with the standard HDG methods, with upwind and scalar symmetric fluxes.|\n", "2504.18177": "|**2025-04-25**|**On the approximation of the von Neumann equation in the semi-classical limit. Part II : numerical analysis**|Fran\u00e7ois Golse, Francis Filbet et.al.|[2504.18177](http://arxiv.org/abs/2504.18177)|null|arXiv admin note: text overlap with arXiv:2405.13436|This paper is devoted to the numerical analysis of the Hermite spectral method proposed in [14], which provides, in the semi-classical limit, an asymptotic preserving approximation of the von Neumann equation. More precisely, it relies on the use of so-called Weyl's variables to effectively address the stiffness associated to the equation. Then by employing a truncated Hermite expansion of the density operator, we successfully manage this stiffness and provide error estimates by leveraging the propagation of regularity in the exact solution.|\n", "2504.18139": "|**2025-04-25**|**Kalman-Langevin dynamics : exponential convergence, particle approximation and numerical approximation**|Axel Ringh, Akash Sharma et.al.|[2504.18139](http://arxiv.org/abs/2504.18139)|null|30 pages, 1 figure|Langevin dynamics has found a large number of applications in sampling, optimization and estimation. Preconditioning the gradient in the dynamics with the covariance - an idea that originated in literature related to solving estimation and inverse problems using Kalman techniques - results in a mean-field (McKean-Vlasov) SDE. We demonstrate exponential convergence of the time marginal law of the mean-field SDE to the Gibbs measure with non-Gaussian potentials. This extends previous results, obtained in the Gaussian setting, to a broader class of potential functions. We also establish uniform in time bounds on all moments and convergence in $p$-Wasserstein distance. Furthermore, we show convergence of a weak particle approximation, that avoids computing the square root of the empirical covariance matrix, to the mean-field limit. Finally, we prove that an explicit numerical scheme for approximating the particle dynamics converges, uniformly in number of particles, to its continuous-time limit, addressing non-global Lipschitzness in the measure.|\n", "2504.18054": "|**2025-04-25**|**A locking free multiscale method for linear elasticity in stress-displacement formulation with high contrast coefficients**|Eric T. Chung, Changqing Ye, Xiang Zhong et.al.|[2504.18054](http://arxiv.org/abs/2504.18054)|null||Achieving strongly symmetric stress approximations for linear elasticity problems in high-contrast media poses a significant computational challenge. Conventional methods often struggle with prohibitively high computational costs due to excessive degrees of freedom, limiting their practical applicability. To overcome this challenge, we introduce an efficient multiscale model reduction method and a computationally inexpensive coarse-grid simulation technique for linear elasticity equations in highly heterogeneous, high-contrast media. We first utilize a stable stress-displacement mixed finite element method to discretize the linear elasticity problem and then present the construction of multiscale basis functions for the displacement and the stress. The mixed formulation offers several advantages such as direct stress computation without post-processing, local momentum conservation (ensuring physical consistency), and robustness against locking effects, even for nearly incompressible materials. Theoretical analysis confirms that our method is inf-sup stable and locking-free, with first-order convergence relative to the coarse mesh size. Notably, the convergence remains independent of contrast ratios as enlarging oversampling regions. Numerical experiments validate the method's effectiveness, demonstrating its superior performance even under extreme contrast conditions.|\n", "2504.18036": "|**2025-04-25**|**Direct sampling method to retrieve small objects from two-dimensional limited-aperture scattered field data**|Won-Kwang Park et.al.|[2504.18036](http://arxiv.org/abs/2504.18036)|null|18 pages, 14 figures|In this study, we investigated the application of the direct sampling method (DSM) to identify small dielectric objects in a limited-aperture inverse scattering problem. Unlike previous studies, we consider the bistatic measurement configuration corresponding to the transmitter location and design indicator functions for both a single source and multiple sources, and we convert the unknown measurement data to a fixed nonzero constant. To explain the applicability and limitation of object detection, we demonstrate that the indicator functions can be expressed by an infinite series of Bessel functions, the material properties of the objects, the bistatic angle, and the converted constant. Based on the theoretical results, we explain how the imaging performance of the DSM is influenced by the bistatic angle and the converted constant. In addition, the results of our analyses demonstrate that a smaller bistatic angle enhances the imaging accuracy and that optimal selection of the converted constant is crucial to realize reliable object detection. The results of the numerical simulations obtained using a two-dimensional Fresnel dataset validated the theoretical findings and illustrate the effectiveness and limitations of the designed indicator functions for small objects.|\n", "2504.18034": "|**2025-04-25**|**Quantum effects in rotationally invariant spin glass models**|Yoshinori Hara, Yoshiyuki Kabashima et.al.|[2504.18034](http://arxiv.org/abs/2504.18034)|null|27 pages, 15 figures|This study investigates the quantum effects in transverse-field Ising spin glass models with rotationally invariant random interactions. The primary aim is to evaluate the validity of a quasi-static approximation that captures the imaginary-time dependence of the order parameters beyond the conventional static approximation. Using the replica method combined with the Suzuki--Trotter decomposition, we established a stability condition for the replica symmetric solution, which is analogous to the de Almeida--Thouless criterion. Numerical analysis of the Sherrington--Kirkpatrick model estimates a value of the critical transverse field, $\\Gamma_\\mathrm{c}$, which agrees with previous Monte Carlo-based estimations. For the Hopfield model, it provides an estimate of $\\Gamma_\\mathrm{c}$, which has not been previously evaluated. For the random orthogonal model, our analysis suggests that quantum effects alter the random first-order transition scenario in the low-temperature limit. This study supports a quasi-static treatment for analyzing quantum spin glasses and may offer useful insights into the analysis of quantum optimization algorithms.|\n", "2504.18033": "|**2025-04-25**|**Real-time inversion of two-dimensional Fresnel experimental database using orthogonality sampling method with single and multiple sources: the case of transverse electric polarized waves**|Junyong Eom, Sangwoo Kang, Minyeob Lee et.al.|[2504.18033](http://arxiv.org/abs/2504.18033)|null|26 pages, 21 figures|This paper concerns an application of the orthogonality sampling method (OSM) for a real-time identification of small objects from two-dimensional Fresnel experimental dataset in transverse electric polarization. First, we apply the OSM with a single source by designing an indicator function based on the asymptotic expansion formula for the scattered field in the presence of small objects. We demonstrate that the indicator function can be expressed by an infinite series of Bessel functions of integer order of the first kind, the range of the signal receiver, and the location of the emitter. Based on this, we then investigate the applicability and limitations of the designed OSM. Specifically, we find that the imaging performance is strongly dependent on the source and the applied frequency. We then apply the OSM with multiple sources to improve imaging performance. Based on the identified structure of the OSM with a single source, we design an indicator function with multiple sources and demonstrate that it can be expressed by an infinite series of the Bessel function of integer order of the first kind, and we explain that objects can be identified uniquely using the designed OSM. Numerical simulation results obtained with the Fresnel experimental dataset demonstrate the advantages and disadvantages of the OSM with a single source and confirm that the designed OSM with multiple sources improves imaging performance.|\n", "2504.17900": "|**2025-04-24**|**Model Error Covariance Estimation for Weak Constraint Data Assimilation**|Sandra R. Babyale, Jodi Mead, Donna Calhoun et.al.|[2504.17900](http://arxiv.org/abs/2504.17900)|null|32 pages, 7 figures, 5 tables. This paper is under review by the SIAM   Journal on Uncertainty Quantification|State estimates from weak constraint 4D-Var data assimilation can vary significantly depending on the data and model error covariances. As a result, the accuracy of these estimates heavily depends on the correct specification of both model and observational data error covariances. In this work, we assume that the data error is known and and focus on estimating the model error covariance by framing weak constraint 4D-Var as a regularized inverse problem, where the inverse model error covariance serves as the regularization matrix. We consider both isotropic and non-isotropic forms of the model error covariance. Using the representer method, we reduce the 4D-Var problem from state space to data space, enabling the efficient application of regularization parameter selection techniques. The Representer method also provides an analytic expression for the optimal state estimate, allowing us to derive matrix expressions for the three regularization parameter selection methods i.e. the L-curve, generalized cross-validation (GCV), and the Chi-square method. We validate our approach by assimilating simulated data into a 1D transport equation modeling wildfire smoke transport under various observational noise and forward model perturbations. In these experiments the goal is to identify the model error covariances that accurately capture the influence of observational data versus model predictions on assimilated state estimates. The regularization parameter selection methods successfully estimate hyperparameters for both isotropic and non-isotropic model error covariances, that reflect whether the first guess model predictions are more or less reliable than the observational data. The results further indicate that isotropic variances are sufficient when the first guess is more accurate than the data whereas non-isotropic covariances are preferred when the observational data is more reliable.|\n", "2504.20980": "|**2025-04-29**|**Jekyll-and-Hyde Tipping Point in an AI's Behavior**|Neil F. Johnson, Frank Yingjie Huo et.al.|[2504.20980](http://arxiv.org/abs/2504.20980)|null||Trust in AI is undermined by the fact that there is no science that predicts -- or that can explain to the public -- when an LLM's output (e.g. ChatGPT) is likely to tip mid-response to become wrong, misleading, irrelevant or dangerous. With deaths and trauma already being blamed on LLMs, this uncertainty is even pushing people to treat their 'pet' LLM more politely to 'dissuade' it (or its future Artificial General Intelligence offspring) from suddenly turning on them. Here we address this acute need by deriving from first principles an exact formula for when a Jekyll-and-Hyde tipping point occurs at LLMs' most basic level. Requiring only secondary school mathematics, it shows the cause to be the AI's attention spreading so thin it suddenly snaps. This exact formula provides quantitative predictions for how the tipping-point can be delayed or prevented by changing the prompt and the AI's training. Tailored generalizations will provide policymakers and the public with a firm platform for discussing any of AI's broader uses and risks, e.g. as a personal counselor, medical advisor, decision-maker for when to use force in a conflict situation. It also meets the need for clear and transparent answers to questions like ''should I be polite to my LLM?''|\n", "2504.20940": "|**2025-04-29**|**Energy-Based Coarse-Graining in Molecular Dynamics: A Flow-Based Framework Without Data**|Maximilian Stupp, P. S. Koutsourelakis et.al.|[2504.20940](http://arxiv.org/abs/2504.20940)|null||Coarse-grained (CG) models offer an effective route to reducing the complexity of molecular simulations, yet conventional approaches depend heavily on long all-atom molecular dynamics (MD) trajectories to adequately sample configurational space. This data-driven dependence limits their accuracy and generalizability, as unvisited configurations remain excluded from the resulting CG model. We introduce a data-free generative framework for coarse-graining that directly targets the all-atom Boltzmann distribution. Our model defines a structured latent space comprising slow collective variables, which are statistically associated with multimodal marginal densities capturing metastable states, and fast variables, which represent the remaining degrees of freedom with simple, unimodal conditional distributions. A potentially learnable, bijective map from the full latent space to the all-atom configuration space enables automatic and accurate reconstruction of molecular structures. The model is trained using an energy-based objective that minimizes the reverse Kullback-Leibler divergence, relying solely on the interatomic potential rather than sampled trajectories. A tempering scheme is used to stabilize training and promote exploration of diverse configurations. Once trained, the model can generate unbiased, one-shot equilibrium all-atom samples. We validate the method on two synthetic systems-a double-well potential and a Gaussian mixture-as well as on the benchmark alanine dipeptide. The model captures all relevant modes of the Boltzmann distribution, accurately reconstructs atomic configurations, and learns physically meaningful coarse-grained representations, all without any simulation data.|\n", "2504.20813": "|**2025-04-29**|**A high-order energy-conserving semi-Lagrangian discontinuous Galerkin method for the Vlasov-Ampere system**|Xiaofeng Cai, Qingtao Li, Hongtao Liu et.al.|[2504.20813](http://arxiv.org/abs/2504.20813)|null||In this paper, we propose a high-order energy-conserving semi-Lagrangian discontinuous Galerkin(ECSLDG) method for the Vlasov-Ampere system. The method employs a semi-Lagrangian discontinuous Galerkin scheme for spatial discretization of the Vlasov equation, achieving high-order accuracy while removing the Courant-Friedrichs-Lewy (CFL) constraint. To ensure energy conservation and eliminate the need to resolve the plasma period, we adopt an energy-conserving time discretization introduced by Liu et al. [J. Comput. Phys., 492 (2023), 112412]. Temporal accuracy is further enhanced through a high-order operator splitting strategy, yielding a method that is high-order accurate in both space and time. The resulting ECSLDG scheme is unconditionally stable and conserves both mass and energy at the fully discrete level, regardless of spatial or temporal resolution. Numerical experiments demonstrate the accuracy, stability, and conservation properties of the proposed method. In particular, the method achieves more accurate enforcement of Gauss's law and improved numerical fidelity over low-order schemes, especially when using a large CFL number.|\n", "2504.20742": "|**2025-04-29**|**Non-Equilibrium Phase Changes in Aircraft Exhaust: A Computational Study on Early Contrail Formation**|Katharina Tegethoff, Andrew P. S. Wheeler et.al.|[2504.20742](http://arxiv.org/abs/2504.20742)|null|29 pages, 7 figures. To be submitted to AIAA Journal of Propulsion   and Power. Selected aspects to be presented at EGU 2025|A numerical framework is developed to model contrail formation in the near-field exhaust of aircraft engines, resolving non-equilibrium phase transitions in compressible, multi-component, non-ideal fluid flows. The approach combines well-established methods from steam turbine modeling for liquid-phase transitions with cloud microphysics models for ice formation. It resolves homogeneous and heterogeneous nucleation, interphase momentum exchange, and polydispersed size distributions of droplets, ice crystals, and soot particles. These models are implemented in a parallelized finite-volume solver and applied to a high-bypass turbofan exhaust configuration with simplified geometry. Results indicate that non-equilibrium effects strongly influence condensation and freezing dynamics, while nozzle geometry and water vapor content modulate local supersaturation and phase transition pathways. The findings underscore the limitations of equilibrium-based models and highlight the value of physics-based, scalable tools for analyzing contrail formation across fuels and propulsion systems.|\n", "2504.20730": "|**2025-04-29**|**Avoided-crossings, degeneracies and Berry phases in the spectrum of quantum noise through analytic Bloch-Messiah decomposition**|Giuseppe Patera, Alessandro Pugliese et.al.|[2504.20730](http://arxiv.org/abs/2504.20730)|null|17 pages, 9 figures|The Bloch-Messiah decomposition (BMD) is a fundamental tool in quantum optics, enabling the analysis and tailoring of multimode Gaussian states by decomposing linear optical transformations into passive interferometers and single-mode squeezers. Its extension to frequency-dependent matrix-valued functions, recently introduced as the ``analytic Bloch-Messiah decomposition\" (ABMD), provides the most general approach for characterizing the driven-dissipative dynamics of quantum optical systems governed by quadratic Hamiltonians. In this work, we present a detailed study of the ABMD, focusing on the typical behavior of parameter-dependent singular values and of their corresponding singular vectors. In particular, we analyze the hitherto unexplored occurrence of avoided and genuine crossings in the spectrum of quantum noise, the latter being manifested by nontrivial topological Berry phases of the singular vectors. We demonstrate that avoided crossings arise naturally when a single parameter is varied, leading to hypersensitivity of the singular vectors and suggesting the presence of genuine crossings in nearby systems. We highlight the possibility of programming the spectral response of photonic systems through the deliberate design of avoided crossings. As a notable example, we show that such control can be exploited to generate broad, flat-band squeezing spectra -- a desirable feature for enhancing degaussification protocols. This study provides new insights into the structure of multimode quantum correlations and offers a theoretical framework for experimental exploitation of complex quantum optical systems.|\n", "2504.20728": "|**2025-04-29**|**On optimal error rates for strong approximation of SDEs with a H\u00f6lder continuous drift coefficient**|Simon Ellinger, Thomas M\u00fcller-Gronbach, Larisa Yaroslavtseva et.al.|[2504.20728](http://arxiv.org/abs/2504.20728)|null||In the present article we study strong approximation of solutions of scalar stochastic differential equations (SDEs) with bounded and $\\alpha$-H\\\"older continuous drift coefficient and constant diffusion coefficient at time point $1$. Recently, it was shown in [arXiv:1909.07961v4 (2021)] that for such SDEs the equidistant Euler scheme achieves an $L^p$-error rate of at least $(1+\\alpha)/2$, up to an arbitrary small $\\varepsilon$, for all $p\\geq 1$ and all $\\alpha\\in(0, 1]$ in terms of the number of evaluations of the driving Brownian motion $W$. In this article we prove a matching lower error bound for $\\alpha\\in(0, 1)$. More precisely, we show that for every $\\alpha\\in(0, 1)$, the $L^p$-error rate $(1+\\alpha)/2$ of the Euler scheme in [arXiv:1909.07961v4 (2021)] can not be improved in general by no numerical method based on finitely many evaluations of $W$ at fixed time points. Up to now, this result was known in the literature only for $\\alpha=1$.   Additionally, we extend a result from [arXiv:2402.13732v2 (2024)] on sharp lower errror bounds for strong approximation of SDEs with a bounded drift coefficient of fractional Sobolev regularity $\\alpha\\in (0,1)$ and constant diffusion coefficient at time point $1$. We prove that for every $\\alpha\\in (0,1)$, the $L^p$-error rate $ (1 + \\alpha)/2$ that was shown in [arXiv:2101.12185v2 (2022)] for the equidistant Euler scheme can, up to a logarithmic term, not be improved in general by no numerical method based on finitely many evaluations of W at fixed time points. This result was known from [arXiv:2402.13732v2 (2024)] only for $\\alpha\\in (1/2,1)$ and $p=2$.   For the proof of these lower bounds we use variants of the Weierstrass function as a drift coefficient and we employ the coupling of noise technique recently introduced in [arXiv:2010.00915v1 (2020)].|\n", "2504.20719": "|**2025-04-29**|**Particle-Hole Asymmetry and Pinball Liquid in a Triangular-Lattice Extended Hubbard Model within Mean-Field Approximation**|Aleksey Alekseev, Agnieszka Cichy, Konrad Jerzy Kapcia et.al.|[2504.20719](http://arxiv.org/abs/2504.20719)|null|11 pages, 6 figures, 74 references; RevTeX class, double-column   formatting, includes also Supplemental Materials (5 pages, 3 figures, 9   multimedia filies)|Recently, triangular lattice models have received a lot of attention since they can describe a number of strongly-correlated materials that exhibit superconductivity and various magnetic and charge orders. In this research we present an extensive analysis of the charge-ordering phenomenon of the triangular-lattice extended Hubbard model with repulsive onsite and nearest-neighbor interaction, arbitrary charge concentration, and $\\sqrt{3}\\times\\sqrt{3}$ supercell (3-sublattice assumption). The model is solved in the ground state with the mean-field approximation which allowed to identify $8$ charge-ordered phases and a large variety of phase transitions. An exotic pinball-liquid phase was found and described. Moreover, strong particle-hole asymmetry of the phase diagram is found to play an important role for triangular lattices. The analysis of band structures, unavailable for more advanced methods that take into account correlation effects, provided a great insight in the nature of triangular-lattice phases and phase transitions. The complexity of the mean-field phase diagram showed the importance and usefulness of the results for the further research with correlation effects included. Together with atomic-limit approximation it can serve them as both a starting point, and a tool to interpret results.|\n", "2504.20715": "|**2025-04-29**|**Neural semi-Lagrangian method for high-dimensional advection-diffusion problems**|Emmanuel Franck, Victor Michel-Dansac, Laurent Navoret et.al.|[2504.20715](http://arxiv.org/abs/2504.20715)|null||This work is devoted to the numerical approximation of high-dimensional advection-diffusion equations. It is well-known that classical methods, such as the finite volume method, suffer from the curse of dimensionality, and that their time step is constrained by a stability condition. The semi-Lagrangian method is known to overcome the stability issue, while recent time-discrete neural network-based approaches overcome the curse of dimensionality. In this work, we propose a novel neural semi-Lagrangian method that combines these last two approaches. It relies on projecting the initial condition onto a finite-dimensional neural space, and then solving an optimization problem, involving the backwards characteristic equation, at each time step. It is particularly well-suited for implementation on GPUs, as it is fully parallelizable and does not require a mesh. We provide rough error estimates, and present several high-dimensional numerical experiments to assess the performance of our approach, and compare it to other neural methods.|\n", "2504.20634": "|**2025-04-29**|**On Stochastic Rounding with Few Random Bits**|Andrew Fitzgibbon, Stephen Felix et.al.|[2504.20634](http://arxiv.org/abs/2504.20634)|null|Published at ARITH 2025|Large-scale numerical computations make increasing use of low-precision (LP) floating point formats and mixed precision arithmetic, which can be enhanced by the technique of stochastic rounding (SR), that is, rounding an intermediate high-precision value up or down randomly as a function of the value's distance to the two rounding candidates. Stochastic rounding requires, in addition to the high-precision input value, a source of random bits. As the provision of high-quality random bits is an additional computational cost, it is of interest to require as few bits as possible while maintaining the desirable properties of SR in a given computation, or computational domain. This paper examines a number of possible implementations of few-bit stochastic rounding (FBSR), and shows how several natural implementations can introduce sometimes significant bias into the rounding process, which are not present in the case of infinite-bit, infinite-precision examinations of these implementations. The paper explores the impact of these biases in machine learning examples, and hence opens another class of configuration parameters of which practitioners should be aware when developing or adopting low-precision floating point. Code is available at http://github.com/graphcore-research/arith25-stochastic-rounding.|\n", "2504.20586": "|**2025-04-29**|**Faster Random Walk-based Capacitance Extraction with Generalized Antithetic Sampling**|Periklis Liaskovitis, Marios Visvardis, Efthymios Efstathiou et.al.|[2504.20586](http://arxiv.org/abs/2504.20586)|null||Floating random walk-based capacitance extraction has emerged in recent years as a tried and true approach for extracting parasitic capacitance in very large scale integrated circuits. Being a Monte Carlo method, its performance is dependent on the variance of sampled quantities and variance reduction methods are crucial for the challenges posed by ever denser process technologies and layout-dependent effects. In this work, we present a novel, universal variance reduction method for floating random walk-based capacitance extraction, which is conceptually simple, highly efficient and provably reduces variance in all extractions, especially when layout-dependent effects are present. It is complementary to existing mathematical formulations for variance reduction and its performance gains are experienced on top of theirs. Numerical experiments demonstrate substantial such gains of up to 30% in number of walks necessary and even more in actual extraction times compared to the best previously proposed variance reduction approaches for the floating random-walk.|\n", "2504.20524": "|**2025-04-29**|**Finite element method with Gr\u00fcnwald-Letnikov type approximation in time for a constant time delay subdiffusion equation**|Weiping Bu, Xueqin Zhang, Weizhi Liao et.al.|[2504.20524](http://arxiv.org/abs/2504.20524)|null||In this work, a subdiffusion equation with constant time delay $\\tau$ is considered. First, the regularity of the solution to the considered problem is investigated, finding that its first-order time derivative exhibits singularity at $t=0^+$ and its second-order time derivative shows singularity at both $t=0^+$ and $\\tau^+$, while the solution can be decomposed into its singular and regular components. Then, we derive a fully discrete finite element scheme to solve the considered problem based on the standard Galerkin finite element method in space and the Gr\\\"unwald-Letnikov type approximation in time. The analysis shows that the developed numerical scheme is stable. In order to discuss the error estimate, a new discrete Gronwall inequality is established. Under the above decomposition of the solution, we obtain a local error estimate in time for the developed numerical scheme. Finally, some numerical tests are provided to support our theoretical analysis.|\n", "2504.20504": "|**2025-04-29**|**Quality-factor inspired deep neural network solver for solving inverse scattering problems**|Yutong Du, Zicheng Liu, Miao Cao et.al.|[2504.20504](http://arxiv.org/abs/2504.20504)|null||Deep neural networks have been applied to address electromagnetic inverse scattering problems (ISPs) and shown superior imaging performances, which can be affected by the training dataset, the network architecture and the applied loss function. Here, the quality of data samples is cared and valued by the defined quality factor. Based on the quality factor, the composition of the training dataset is optimized. The network architecture is integrated with the residual connections and channel attention mechanism to improve feature extraction. A loss function that incorporates data-fitting error, physical-information constraints and the desired feature of the solution is designed and analyzed to suppress the background artifacts and improve the reconstruction accuracy. Various numerical analysis are performed to demonstrate the superiority of the proposed quality-factor inspired deep neural network (QuaDNN) solver and the imaging performance is finally verified by experimental imaging test.|\n", "2504.20494": "|**2025-04-29**|**An energy-stable minimal deformation rate scheme for mean curvature flow and surface diffusion**|Guangwei Gao, Harald Garcke, Buyang Li et.al.|[2504.20494](http://arxiv.org/abs/2504.20494)|null||We propose a new parametric finite element method, referred to as the BGN-MDR method, for simulating both mean curvature flow and surface diffusion for closed hypersurfaces, as well as open hypersurfaces with moving contact lines in three dimensions. The method is also applicable to closed and open curves with moving contact points in two dimensions. The proposed scheme inherits the energy stability from the BGN scheme proposed by Barrett, Garcke, and N\\\"urnberg in 2008, and offers improved mesh quality similar to the minimal deformation rate (MDR) method proposed by Hu and Li in 2022, especially for small time step sizes where the BGN scheme may become unstable and result in deteriorated meshes.|\n", "2504.20408": "|**2025-04-29**|**FourierSpecNet: Neural Collision Operator Approximation Inspired by the Fourier Spectral Method for Solving the Boltzmann Equation**|Jae Yong Lee, Gwang Jae Jung, Byung Chan Lim et.al.|[2504.20408](http://arxiv.org/abs/2504.20408)|null|27 pages, 11 figures|The Boltzmann equation, a fundamental model in kinetic theory, describes the evolution of particle distribution functions through a nonlinear, high-dimensional collision operator. However, its numerical solution remains computationally demanding, particularly for inelastic collisions and high-dimensional velocity domains. In this work, we propose the Fourier Neural Spectral Network (FourierSpecNet), a hybrid framework that integrates the Fourier spectral method with deep learning to approximate the collision operator in Fourier space efficiently. FourierSpecNet achieves resolution-invariant learning and supports zero-shot super-resolution, enabling accurate predictions at unseen resolutions without retraining. Beyond empirical validation, we establish a consistency result showing that the trained operator converges to the spectral solution as the discretization is refined. We evaluate our method on several benchmark cases, including Maxwellian and hard-sphere molecular models, as well as inelastic collision scenarios. The results demonstrate that FourierSpecNet offers competitive accuracy while significantly reducing computational cost compared to traditional spectral solvers. Our approach provides a robust and scalable alternative for solving the Boltzmann equation across both elastic and inelastic regimes.|\n", "2504.20317": "|**2025-04-29**|**Ant Colony Optimization for Density Functionals in Strongly Correlated Systems**|G. M. Tonin, T. Pauletti, R. M. Dos Santos et.al.|[2504.20317](http://arxiv.org/abs/2504.20317)|null|7 pages,3 figures|The Ant Colony Optimization (ACO) algorithm is a nature-inspired metaheuristic method used for optimization problems. Although not a machine learning method per se, ACO is often employed alongside machine learning models to enhance performance through optimization. We adapt an ACO algorithm to optimize the so-called FVC density functional for the ground-state energy of strongly correlated systems. We find the parameter configurations that maximize optimization efficiency, while reducing the mean relative error ($MRE$) of the ACO functional. We then analyze the algorithm's performance across different dimensionalities ($1D-5D$), which are related to the number of parameters to be optimized within the FVC functional. Our results indicate that $15$ ants with a pheromone evaporation rate superior to $0.2$ are sufficient to minimize the $MRE$ for a vast regime of parameters of the strongly-correlated system -- interaction, particle density and spin magnetization. While the optimizations $1D$, $2D$, and $4D$ yield $1.5\\%< MRE< 2.7\\%$, the $3D$ and $5D$ optimizations lower the $MRE$ to $\\sim0.8\\%$, reflecting a $67\\%$ error reduction compared to the original FVC functional ($MRE = 2.4\\%$). As simulation time grows almost linearly with dimension, our results highlight the potential of ant colony algorithms for density-functional problems, combining effectiveness with low computational cost.|\n", "2504.20305": "|**2025-04-28**|**Fast LDL factorization for dense and sparse symmetric matrices over an arbitrary field**|Edgar Solomonik et.al.|[2504.20305](http://arxiv.org/abs/2504.20305)|null||While existing algorithms may be used to solve a linear system over a general field in matrix-multiplication time, the complexity of constructing a symmetric triangular factorization (LDL) has received relatively little formal study. The LDL factorization is a common tool for factorization of symmetric matrices, and, unlike orthogonal counterparts, generalizes to an arbitrary field. We provide algorithms for dense and sparse LDL factorization and for dense LU factorization that aim to minimize complexity for factorization over a general field. For LU of an $m\\times n$ rank $R$ matrix, we obtain an algorithm with complexity $O(mnR^{\\omega-2})$, where $\\omega$ is the matrix multiplication complexity exponent. For LDL of an $n\\times n$ matrix, we give an algorithm with complexity $O(n^\\omega)$ and for a sparse matrix corresponding to a graph with treewidth $\\tau$, we obtain $O(n\\tau^{\\omega-1})$. Our sparse LDL algorithm is based on an adaptation of the null-space method for solving saddle point systems of equations, which may be of independent interest.|\n", "2504.20269": "|**2025-04-28**|**Entropy based lower dimension bounds for finite-time prediction of Dynamic Mode Decomposition algorithms**|Till Hauser, Julian H\u00f6lz et.al.|[2504.20269](http://arxiv.org/abs/2504.20269)|null|16 pages|Motivated by Dynamic Mode Decomposition algorithms, we provide lower bounds on the dimension of a finite-dimensional subspace $F \\subseteq \\mathrm{L}^2(\\mathrm{X})$ required for predicting the behavior of dynamical systems over long time horizons. We distinguish between two cases: (i) If $F$ is determined by a finite partition of $X$ we derive a lower bound that depends on the dynamical measure-theoretic entropy of the partition. (ii) We consider general finite-dimensional subspaces $F$ and establish a lower bound for the dimension of $F$ that is contingent on the spectral structure of the Koopman operator of the system, via the approximation entropy of $F$ as studied by Voiculescu. Furthermore, we motivate the use of delay observables to improve the predictive qualities of Dynamic Mode Decomposition algorithms.|\n", "2504.20259": "|**2025-04-28**|**Global Optimality Characterizations and Algorithms for Minimizing Quartically-Regularized Third-Order Taylor Polynomials**|Wenqi Zhu, Coralia Cartis et.al.|[2504.20259](http://arxiv.org/abs/2504.20259)|null||High-order methods for convex and nonconvex optimization, particularly $p$th-order Adaptive Regularization Methods (AR$p$), have attracted significant research interest by naturally incorporating high-order Taylor models into adaptive regularization frameworks, resulting in algorithms with faster global and local convergence rates than first- and second-order methods. This paper establishes global optimality conditions for general, nonconvex cubic polynomials with quartic regularization. These criteria generalise existing results, recovering the optimality results for regularized quadratic polynomials, and can be further simplified in the low-rank and diagonal tensor cases. Under suitable assumptions on the Taylor polynomial, we derive a lower bound for the regularization parameter such that the necessary and sufficient criteria coincide, establishing a connection between this bound and the subproblem's convexification and sum-of-squares (SoS) convexification techniques. Leveraging the optimality characterization, we develop a Diagonal Tensor Method (DTM) for minimizing quartically-regularized cubic Taylor polynomials by iteratively minimizing a sequence of local models that incorporate both diagonal cubic terms and quartic regularization (DTM model). We show that the DTM algorithm is provably convergent, with a global evaluation complexity of $\\mathcal{O}(\\epsilon^{-3/2})$. Furthermore, when special structure is present (such as low rank or diagonal), DTM can exactly solve the given problem (in one iteration). In our numerical experiments, we propose practical DTM variants that exploit local problem information for model construction, which we then show to be competitive with cubic regularization and other subproblem solvers, with superior performance on problems with special structure.|\n", "2504.20256": "|**2025-04-28**|**Optimizing Hard Thresholding for Sparse Model Discovery**|Derek W. Jollie, Scott G. McCalla et.al.|[2504.20256](http://arxiv.org/abs/2504.20256)|null||Many model selection algorithms rely on sparse dictionary learning to provide interpretable and physics-based governing equations. The optimization algorithms typically use a hard thresholding process to enforce sparse activations in the model coefficients by removing library elements from consideration. By introducing an annealing scheme that reactivates a fraction of the removed terms with a cooling schedule, we are able to improve the performance of these sparse learning algorithms. We concentrate on two approaches to the optimization, SINDy, and an alternative using hard thresholding pursuit. We see in both cases that annealing can improve model accuracy. The effectiveness of annealing is demonstrated through comparisons on several nonlinear systems pulled from convective flows, excitable systems, and population dynamics. Finally we apply these algorithms to experimental data for projectile motion.|\n", "2504.20205": "|**2025-04-28**|**Parameter optimization for the unimon qubit**|Rostislav Duda, Eric Hyypp\u00e4, Olli Mukkula et.al.|[2504.20205](http://arxiv.org/abs/2504.20205)|null|14 pages, 5 figures|Inductively shunted superconducting qubits, such as the unimon qubit, combine high anharmonicity with protection from low-frequency charge noise, positioning them as promising candidates for the implementation of fault-tolerant superconducting quantum computers. In this work, we develop accurate closed-form approximations for the frequency and anharmonicity of the unimon qubit that are also applicable to any single-mode superconducting qubits with a single-well potential profile, such as the quarton qubit or the kinemon qubit. We use these results to theoretically explore the single-qubit gate fidelity and coherence times across the parameter space of qubits with a single-well potential. We find that the gate fidelity can be optimized by tuning the Hamiltonian to $(i)$ a high qubit mode impedance of $1-2$ $k\\Omega$, $(ii)$ a low qubit frequency of $1$ $GHz$, $(iii)$ and a perfect cancellation of the linear inductance and the Josephson inductance attained at a flux bias of half flux quantum. According to our theoretical analysis, the proposed qubit parameters have potential to enhance the single-qubit gate fidelity of the unimon beyond $99.99\\%$ even without significant improvements to the dielectric quality factor or the flux noise density measured for the first unimon qubits. Furthermore, we compare unimon, transmon and fluxonium qubits in terms of their energy spectra and qubit coherence subject to dielectric loss and $1/f$ flux noise in order to highlight the advantages and limitations of each qubit type.|\n", "2505.00667": "|**2025-05-01**|**A Practical Framework for Simulating Time-Resolved Spectroscopy Based on a Real-time Dyson Expansion**|Cian Reeves, Michael Kurniawan, Yuanran Zhu et.al.|[2505.00667](http://arxiv.org/abs/2505.00667)|null|54 pages, 5 figures|Time-resolved spectroscopy is a powerful tool for probing electron dynamics in molecules and solids, revealing transient phenomena on sub-femtosecond timescales. The interpretation of experimental results is often enhanced by parallel numerical studies, which can provide insight and validation for experimental hypotheses. However, developing a theoretical framework for simulating time-resolved spectra remains a significant challenge. The most suitable approach involves the many-body non-equilibrium Green's function formalism, which accounts for crucial dynamical many-body correlations during time evolution. While these dynamical correlations are essential for observing emergent behavior in time-resolved spectra, they also render the formalism prohibitively expensive for large-scale simulations. Substantial effort has been devoted to reducing this computational cost -- through approximations and numerical techniques -- while preserving the key dynamical correlations. The ultimate goal is to enable first-principles simulations of time-dependent systems ranging from small molecules to large, periodic, multidimensional solids. In this perspective, we outline key challenges in developing practical simulations for time-resolved spectroscopy, with a particular focus on Green's function methodologies. We highlight a recent advancement toward a scalable framework: the real-time Dyson expansion (RT-DE). We introduce the theoretical foundation of RT-DE and discuss strategies for improving scalability, which have already enabled simulations of system sizes beyond the reach of previous fully dynamical approaches. We conclude with an outlook on future directions for extending RT-DE to first-principles studies of dynamically correlated, non-equilibrium systems.|\n", "2505.00656": "|**2025-05-01**|**The local coupling of noise technique and its application to lower error bounds for strong approximation of SDEs with irregular coefficients**|Simon Ellinger et.al.|[2505.00656](http://arxiv.org/abs/2505.00656)|null||In recent years, interest in approximation methods for stochastic differential equations (SDEs) with non-Lipschitz continuous coefficients has increased. We show lower bounds for the $L^p$-error of such methods in the case of approximation at a single point in time or globally in time. On the one hand, we show that for a large class of piecewise Lipschitz continuous drifts and non-additive diffusions the best possible $L^p$-error rate for final time approximation that can be achieved by any method based on finitely many evaluations of the driving Brownian motion is at most $3/4$, which was previously known only for additive diffusions. Moreover, we show that the best $L^p$-error rate for global approximation that can be achieved by any method based on finitely many evaluations of the driving Brownian motion is at most $1/2$ when the drift is locally bounded and the diffusion is locally Lipschitz continuous.   For the derivation of the lower bounds we introduce a new method of proof: the local coupling of noise technique. Using this technique when approximating a solution $X$ of the SDE at the final time, a lower bound for the $L^p$-error of any approximation method based on evaluations of the driving Brownian motion at the points $t_1 < \\dots < t_n$ can be determined by the $L^p$-distances of solutions of the same SDE on $[t_{i-1}, t_i]$ with initial values $X_{t_{i-1}}$ and driving Brownian motions that are coupled at $t_{i-1}, t_i$ and independent, conditioned on the values of the Brownian motion at $t_{i-1}, t_i$.|\n", "2505.00648": "|**2025-05-01**|**Adaptive Nonoverlapping Preconditioners for the Helmholtz Equation**|Yi Yu, Marcus Sarkis, Guanglian Li et.al.|[2505.00648](http://arxiv.org/abs/2505.00648)|null||The Helmholtz equation poses significant computational challenges due to its oscillatory solutions, particularly for large wavenumbers. Inspired by the Schur complement system for elliptic problems, this paper presents a novel substructuring approach to mitigate the potential ill-posedness of local Dirichlet problems for the Helmholtz equation. We propose two types of preconditioners within the framework of nonoverlapping spectral additive Schwarz (NOSAS) methods. The first type of preconditioner focuses on the real part of the Helmholtz problem, while the second type addresses both the real and imaginary components, providing a comprehensive strategy to enhance scalability and reduce computational cost. Our approach is purely algebraic, which allows for adaptability to various discretizations and heterogeneous Helmholtz coefficients while maintaining theoretical convergence for thresholds close to zero. Numerical experiments confirm the effectiveness of the proposed preconditioners, demonstrating robust convergence rates and scalability, even for large wavenumbers.|\n", "2505.00625": "|**2025-05-02**|**SA-GAT-SR: Self-Adaptable Graph Attention Networks with Symbolic Regression for high-fidelity material property prediction**|Liu Junchi, Tang Ying, Tretiak Sergei et.al.|[2505.00625](http://arxiv.org/abs/2505.00625)|**[link](https://github.com/MustBeOne/SA-GAT-SR)**||Recent advances in machine learning have demonstrated an enormous utility of deep learning approaches, particularly Graph Neural Networks (GNNs) for materials science. These methods have emerged as powerful tools for high-throughput prediction of material properties, offering a compelling enhancement and alternative to traditional first-principles calculations. While the community has predominantly focused on developing increasingly complex and universal models to enhance predictive accuracy, such approaches often lack physical interpretability and insights into materials behavior. Here, we introduce a novel computational paradigm, Self-Adaptable Graph Attention Networks integrated with Symbolic Regression (SA-GAT-SR), that synergistically combines the predictive capability of GNNs with the interpretative power of symbolic regression. Our framework employs a self-adaptable encoding algorithm that automatically identifies and adjust attention weights so as to screen critical features from an expansive 180-dimensional feature space while maintaining O(n) computational scaling. The integrated SR module subsequently distills these features into compact analytical expressions that explicitly reveal quantum-mechanically meaningful relationships, achieving 23 times acceleration compared to conventional SR implementations that heavily rely on first principle calculations-derived features as input. This work suggests a new framework in computational materials science, bridging the gap between predictive accuracy and physical interpretability, offering valuable physical insights into material behavior.|\n", "2505.00548": "|**2025-05-01**|**Model order reduction of hemodynamics by space-time reduced basis and reduced fluid-structure interaction**|Riccardo Tenderini, Simone Deparis et.al.|[2505.00548](http://arxiv.org/abs/2505.00548)|null|28 pages, 8 figures. Submitted to Computer Methods in Applied   Mechanics and Engineering|In this work, we apply the space-time Galerkin reduced basis (ST-GRB) method to a reduced fluid-structure interaction model, for the numerical simulation of hemodynamics in arteries. In essence, ST-GRB extends the classical reduced basis (RB) method, exploiting a data-driven low-dimensional linear encoding of the temporal dynamics to further cut the computational costs. The current investigation brings forth two key enhancements, compared to previous works on the topic. On the one side, we model blood flow through the Navier-Stokes equations, hence accounting for convection. In this regard, we implement a hyper-reduction scheme, based on approximate space-time reduced affine decompositions, to deal with nonlinearities effectively. On the other side, we move beyond the constraint of modelling blood vessels as rigid structures, acknowledging the importance of elasticity for the accurate simulation of complex blood flow patterns. To limit computational complexity, we adopt the Coupled Momentum model, incorporating the effect of wall compliance in the fluid's equations through a generalized Robin boundary condition. In particular, we propose an efficient strategy for handling the spatio-temporal projection of the structural displacement, which ultimately configures as a by-product. The performances of ST-GRB are assessed in three different numerical experiments. The results confirm that the proposed approach can outperform the classical RB method, yielding precise approximations of high-fidelity solutions at more convenient costs. However, the computational gains of ST-GRB vanish if the number of retained temporal modes is too large, which occurs either when complex dynamics arise or if very precise solutions are sought.|\n", "2505.00529": "|**2025-05-01**|**Second-Order Adjoint Method for Quantum Optimal Control**|Harish S. Bhat et.al.|[2505.00529](http://arxiv.org/abs/2505.00529)|null|7 pages, 2 figures|We derive and implement a second-order adjoint method to compute exact gradients and Hessians for a prototypical quantum optimal control problem, that of solving for the minimal energy applied electric field that drives a molecule from a given initial state to a desired target state. For small to moderately sized systems, we demonstrate a vectorized GPU implementation of a second-order adjoint method that computes both Hessians and gradients with wall times only marginally more than those required to compute gradients via commonly used first-order adjoint methods. Pairing our second-order adjoint method with a trust region optimizer (a type of Newton method), we show that it outperforms a first-order method, requiring significantly fewer iterations and wall time to find optimal controls for four molecular systems. Our derivation of the second-order adjoint method allows for arbitrary parameterizations of the controls.|\n", "2505.00508": "|**2025-05-01**|**Weak Random Feature Method for Solving Partial Differential Equations**|Mikhail Kuvakin, Zijian Mei, Jingrun Chen et.al.|[2505.00508](http://arxiv.org/abs/2505.00508)|null||The random feature method (RFM) has demonstrated great potential in bridging traditional numerical methods and machine learning techniques for solving partial differential equations (PDEs). It retains the advantages of mesh-free approaches while achieving spectral accuracy for smooth solutions, without the need for iterative procedures. However, the implementation of RFM in the identification of weak solutions remains a subject of limited comprehension, despite crucial role of weak solutions in addressing numerous applied problems. While the direct application of RFM to problems without strong solutions is fraught with potential challenges, we propose an enhancement to the original random feature method that is specifically suited for finding weak solutions and is termed as Weak RFM. Essentially, Weak RFM reformulates the original RFM by adopting the weak form of the governing equations and constructing a new linear system through the use of carefully designed test functions, ensuring that the resulting solution satisfies the weak form by default. To rigorously evaluate the performance of the proposed method, we conduct extensive experiments on a variety of benchmark problems, including challenging three-dimensional cases, and compare its performance with state of the art machine learning-based approaches. The results demonstrate that Weak RFM achieves comparable or superior accuracy while significantly reducing computational time and memory consumption, highlighting its potential as a highly efficient and robust tool for finding weak solutions to various PDE problems.|\n", "2505.00473": "|**2025-05-01**|**Interpretable Spatial-Temporal Fusion Transformers: Multi-Output Prediction for Parametric Dynamical Systems with Time-Varying Inputs**|Shuwen Sun, Lihong Feng, Peter Benner et.al.|[2505.00473](http://arxiv.org/abs/2505.00473)|null||We explore the promising performance of a transformer model in predicting outputs of parametric dynamical systems with external time-varying input signals. The outputs of such systems vary not only with physical parameters but also with external time-varying input signals. Accurately catching the dynamics of such systems is challenging. We have adapted and extended an existing transformer model for single output prediction to a multiple-output transformer that is able to predict multiple output responses of these systems. The multiple-output transformer generalizes the interpretability of the original transformer. The generalized interpretable attention weight matrix explores not only the temporal correlations in the sequence, but also the interactions between the multiple outputs, providing explanation for the spatial correlation in the output domain. This multiple-output transformer accurately predicts the sequence of multiple outputs, regardless of the nonlinearity of the system and the dimensionality of the parameter space.|\n", "2505.00460": "|**2025-05-01**|**Subspace-Distance-Enabled Active Learning for Efficient Data-Driven Model Reduction of Parametric Dynamical Systems**|Harshit Kapadia, Peter Benner, Lihong Feng et.al.|[2505.00460](http://arxiv.org/abs/2505.00460)|null|31 pages, 10 figures, 4 tables|In situations where the solution of a high-fidelity dynamical system needs to be evaluated repeatedly, over a vast pool of parametric configurations and in absence of access to the underlying governing equations, data-driven model reduction techniques are preferable. We propose a novel active learning approach to build a parametric data-driven reduced-order model (ROM) by greedily picking the most important parameter samples from the parameter domain. As a result, during the ROM construction phase, the number of high-fidelity solutions dynamically grow in a principled fashion. The high-fidelity solution snapshots are expressed in several parameter-specific linear subspaces, with the help of proper orthogonal decomposition (POD), and the relative distance between these subspaces is used as a guiding mechanism to perform active learning. For successfully achieving this, we provide a distance measure to evaluate the similarity between pairs of linear subspaces with different dimensions, and also show that this distance measure is a metric. The usability of the proposed subspace-distance-enabled active learning (SDE-AL) framework is demonstrated by augmenting two existing non-intrusive reduced-order modeling approaches, and providing their active-learning-driven (ActLearn) extensions, namely, SDE-ActLearn-POD-KSNN, and SDE-ActLearn-POD-NN. Furthermore, we report positive results for two parametric physical models, highlighting the efficiency of the proposed SDE-AL approach.|\n", "2505.00446": "|**2025-05-01**|**Analysis of evolution equation with variable-exponent memory modeling multiscale viscoelasticity**|Yiqun Li, Xiangcheng Zheng et.al.|[2505.00446](http://arxiv.org/abs/2505.00446)|null||We investigate the well-posedness and solution regularity of an evolution equation with non-positive type variable-exponent memory, which describes multiscale viscoelasticity in materials with memory. The perturbation method is applied for model transformation, based on which the well-posedness is proved. Then the weighted solution regularity is derived, where the initial singularity is characterized by the initial value of variable exponent.|\n", "2505.00440": "|**2025-05-01**|**Error bounds for function approximation using generated sets**|Ronald Cools, Dirk Nuyens, Laurence Wilkes et.al.|[2505.00440](http://arxiv.org/abs/2505.00440)|null||This paper explores the use of \"generated sets\" $\\{ \\{ k \\boldsymbol{\\zeta} \\} : k = 1, \\ldots, n \\}$ for function approximation in reproducing kernel Hilbert spaces which consist of multi-dimensional functions with an absolutely convergent Fourier series. The algorithm is a least squares algorithm that samples the function at the points of a generated set. We show that there exist $\\boldsymbol{\\zeta} \\in [0,1]^d$ for which the worst-case $L_2$ error has the optimal order of convergence if the space has polynomially converging approximation numbers. In fact, this holds for a significant portion of the generators. Additionally we show that a restriction to rational generators is possible with a slight increase of the bound. Furthermore, we specialise the results to the weighted Korobov space, where we derive a bound applicable to low values of sample points, and state tractability results.|\n", "2505.00434": "|**2025-05-01**|**Stability of the first-order unified gas-kinetic scheme based on a linear kinetic model**|Tuowei Chen, Kun Xu et.al.|[2505.00434](http://arxiv.org/abs/2505.00434)|null||The unified gas-kinetic scheme (UGKS) is becoming increasingly popular for multiscale simulations in all flow regimes. This paper provides the first analytical study on the stability of the UGKS applied to a linear kinetic model, which is able to reproduce the one-dimensional linear scalar advection-diffusion equation via the Chapman-Enskog expansion method. Adopting periodic boundary conditions and neglecting the error from numerical integration, this paper rigorously proves the weighted $L^2$-stability of the first-order UGKS under the Courant-Friedrichs-Lewy (CFL) conditions. It is shown that the time step of the method is not constrained by being less than the particle collision time, nor is it limited by parabolic type CFL conditions typically applied in solving diffusion equations. The novelty of the proof lies in that based on the ratio of the time step to the particle collision time, the update of distribution functions is viewed as a convex combinations of sub-methods related to various physics processes, such as the particle free transport and collisions. The weighted $L^2$-stability of the sub-methods is obtained by considering them as discretizations to corresponding linear hyperbolic systems and utilizing the associated Riemann invariants. Finally, the strong stability preserving property of the UGKS leads to the desired weighted $L^2$-stability.|\n", "2505.00411": "|**2025-05-01**|**Affine matrix scrambling achieves smoothness-dependent convergence rates**|Yang Liu et.al.|[2505.00411](http://arxiv.org/abs/2505.00411)|null||We study the convergence rate of the median estimator for affine matrix scrambled digital nets applied to integrands over the unit hypercube $[0, 1]^s$. By taking the median of $(2r-1)$ independent randomized quasi-Monte Carlo (RQMC) samples, we demonstrate that the desired convergence rates can be achieved without increasing the number of randomizations $r$ as the quadrature size $N$ grows for both bounded and unbounded integrands. For unbounded integrands, our analysis assumes a boundary growth condition on the weak derivatives and also considers singularities such as kinks and jump discontinuities. Notably, when $r = 1$, the median estimator reduces to the standard RQMC estimator. By applying analytical techniques developed for median estimators, we prove that the affine matrix scrambled estimator achieves a convergence rate depending on the integrand's smoothness, and is therefore not limited by the canonical rate $\\mathcal{O}(N^{-3/2})$. However, this smoothness-dependent theoretical rate is not observed empirically in numerical experiments when the affine matrix scrambling yields a heavy-tailed sampling distribution. In contrast, the median estimator consistently reveals the theoretical rates and yields smaller integration errors than mean estimators, further highlighting its advantages.|\n", "2505.00401": "|**2025-05-01**|**Size-Dependent Tensile Behavior and Dislocation Dynamics in Cu and Ag Nanowires: A Molecular Dynamics Study**|Xiaorui Hu, Jiawei Xiong et.al.|[2505.00401](http://arxiv.org/abs/2505.00401)|null||By using molecular dynamics simulations, the research examine how copper and silver nanowires respond to tensile loading in order to clarify their nanoscale deformation mechanisms. The results demonstrate that these two metal nanowires follow notably different stress - strain trends, with silver wires exhibiting greater elastic stiffness and higher yield points at equivalent diameters - an effect likely rooted in silver's stronger atomic bonding and more stable microstructure. A pronounced size effect is observed: as the wire diameter diminishes, both the yield strength and ultimate tensile strength increase substantially, a behavior driven by the higher proportion of surface atoms that enhance dislocation nucleation and mobility. Atomistic analyses further underscore the dominant role of dislocations during plastic deformation, and in particular reveal that surface - initiated dislocations in thinner wires critically affect their fracture behavior.|\n", "2505.00384": "|**2025-05-01**|**Improving the scalability of a high-order atmospheric dynamics solver based on the deal.II library**|Giuseppe Orlando, Tommaso Benacchio, Luca Bonaventura et.al.|[2505.00384](http://arxiv.org/abs/2505.00384)|null||We present recent advances on the massively parallel performance of a numerical scheme for atmosphere dynamics applications based on the deal.II library. The implicit-explicit discontinuous finite element scheme is based on a matrix-free approach, meaning that no global sparse matrix is built and only the action of the linear operators on a vector is actually implemented. Following a profiling analysis, we focus on the performance optimization of the numerical method and describe the impact of different preconditioning and solving techniques in this framework. Moreover, we show how the use of the latest version of the \\texttt{deal.II} library and of suitable execution flags can improve the parallel performance.|\n", "2505.00376": "|**2025-05-01**|**Accurate Modeling of Interfacial Thermal Transport in van der Waals Heterostructures via Hybrid Machine Learning and Registry-Dependent Potentials**|Wenwu Jiang, Hekai Bu, Ting Liang et.al.|[2505.00376](http://arxiv.org/abs/2505.00376)|null||Two-dimensional transition metal dichalcogenides (TMDs) exhibit remarkable thermal anisotropy due to their strong intralayer covalent bonding and weak interlayer van der Waals (vdW) interactions. However, accurately modeling their thermal transport properties remains a significant challenge, primarily due to the computational limitations of density functional theory (DFT) and the inaccuracies of classical force fields in non-equilibrium regimes. To address this, we use a recently developed hybrid computational framework that combines machine learning potential (MLP) for intralayer interactions with registry-dependent interlayer potential (ILP) for anisotropic vdW interlayer interaction, achieving near quantum mechanical accuracy. This approach demonstrates exceptional agreement with DFT calculations and experimental data for TMD systems, accurately predicting key properties such as lattice constants, bulk modulus, moir\\'e reconstruction, phonon spectra, and thermal conductivities. The scalability of this method enables accurate simulations of TMD heterostructures with large-scale moir\\'e superlattices, making it a transformative tool for the design of TMD-based thermal metamaterials and devices, bridging the gap between accuracy and computational efficiency.|\n", "2505.00370": "|**2025-05-01**|**On the Schr\u00f6dingerization method for linear non-unitary dynamics with optimal dependence on matrix queries**|Shi Jin, Nana Liu, Chuwen Ma et.al.|[2505.00370](http://arxiv.org/abs/2505.00370)|null||The Schr\\\"odingerization method converts linear partial and ordinary differential equations with non-unitary dynamics into systems of Schr\\\"odinger-type equations with unitary evolution. It does so via the so-called warped phase transformation that maps the original equation into a Schr\\\"odinger-type equation in one higher dimension \\cite{Schrshort,JLY22SchrLong}. We show that by employing a smooth initialization of the warped phase transform \\cite{JLM24SchrBackward}, Schr\\\"odingerization can in fact achieve optimal scaling in matrix queries. This paper presents the detailed implementation of three smooth initializations for the Schr\\\"odingerization method: (a) the cut-off function, (b) the higher-order polynomial interpolation, and (c) the Fourier transform methods, that achieve optimality for (a) and near-optimality for (b) and (c). A detailed analysis of key parameters affecting time complexity is conducted.|\n", "2505.00353": "|**2025-05-01**|**PYSED: A tool for extracting kinetic-energy-weighted phonon dispersion and lifetime from molecular dynamics simulations**|Ting Liang, Wenwu Jiang, Ke Xu et.al.|[2505.00353](http://arxiv.org/abs/2505.00353)|null|15 pages in main text; 9 figures in main text, 4 figures in SI|Machine learning potential-driven molecular dynamics (MD) simulations have significantly enhanced the predictive accuracy of thermal transport properties across diverse materials. However, extracting phonon-mode-resolved insights from these simulations remains a critical challenge. Here, we introduce PYSED, a Python-based package built on the spectral energy density (SED) method, designed to efficiently compute kinetic-energy-weighted phonon dispersion and extract phonon lifetime from large-scale MD simulation trajectories. By integrating high-accuracy machine-learned neuroevolution potential (NEP) models, we validate and showcase the effectiveness of the implemented SED method across systems of varying dimensionalities. Specifically, the NEP-driven MD-SED accurately reveals how phonon modes are affected by strain in carbon nanotubes, as well as by interlayer coupling strength and twist angle in two-dimensional molybdenum disulfide. For three-dimensional systems, the SED method effectively establishes the thermal transport regime diagram for metal-organic frameworks, distinguishing between particlelike and wavelike propagation regions. Moreover, using bulk silicon as an example, we show that phonon SED can efficiently capture quantum dynamics based on path-integral trajectories. The PYSED package bridges MD simulations with detailed phonon-mode insights, delivering a robust tool for investigating thermal transport properties with detailed mechanisms across various materials.|\n", "2505.00351": "|**2025-05-01**|**Integral Representations of Sobolev Spaces via ReLU$^k$ Activation Function and Optimal Error Estimates for Linearized Networks**|Xinliang Liu, Tong Mao, Jinchao Xu et.al.|[2505.00351](http://arxiv.org/abs/2505.00351)|null||This paper presents two main theoretical results concerning shallow neural networks with ReLU$^k$ activation functions. We establish a novel integral representation for Sobolev spaces, showing that every function in $H^{\\frac{d+2k+1}{2}}(\\Omega)$ can be expressed as an $L^2$-weighted integral of ReLU$^k$ ridge functions over the unit sphere. This result mirrors the known representation of Barron spaces and highlights a fundamental connection between Sobolev regularity and neural network representations. Moreover, we prove that linearized shallow networks -- constructed by fixed inner parameters and optimizing only the linear coefficients -- achieve optimal approximation rates $O(n^{-\\frac{1}{2}-\\frac{2k+1}{2d}})$ in Sobolev spaces.|\n", "2505.00288": "|**2025-05-01**|**Nystr\u00f6m Type Exponential Integrators for Strongly Magnetized Charged Particle Dynamics**|Tri P. Nguyen, Ilon Joseph, Mayya Tokman et.al.|[2505.00288](http://arxiv.org/abs/2505.00288)|null||Calculating the dynamics of charged particles in electromagnetic fields (i.e. the particle pushing problem) is one of the most computationally intensive components of particle-in-cell (PIC) methods for plasma physics simulations. This task is especially challenging when the plasma is strongly magnetized, since in this case the particle motion consists of a wide range of temporal scales from highly oscillatory fast gyromotion to slow macroscopic behavior and the resulting numerical model is very stiff. Current state-of-the-art time integrators used to simulate particle motion have limitations given the severe numerical stiffness of the problem and more efficient methods are of interest. Recently, exponential integrators have been proposed as a promising new approach for these simulations and shown to offer computational advantages over commonly used schemes. Exponential methods can solve linear problems exactly and are $A$-stable. In this paper, the standard exponential algorithms framework is extended to derive Nystr\\\"om-type exponential methods that integrate the Newtonian equations of motion as a second-order differential equation. Specific Nystr\\\"om-type schemes of second and third orders are derived and applied to strongly magnetized particle pushing problems. Numerical experiments are presented to demonstrate that the Nystr\\\"om-type exponential integrators can provide significant improvement in computational efficiency over the standard exponential methods.|\n", "2505.02741": "|**2025-05-05**|**dyGRASS: Dynamic Spectral Graph Sparsification via Localized Random Walks on GPUs**|Yihang Yuan, Ali Aghdaei, Zhuo Feng et.al.|[2505.02741](http://arxiv.org/abs/2505.02741)|null||This work presents dyGRASS, an efficient dynamic algorithm for spectral sparsification of large undirected graphs that undergo streaming edge insertions and deletions. At its core, dyGRASS employs a random-walk-based method to efficiently estimate node-to-node distances in both the original graph (for decremental update) and its sparsifier (for incremental update). For incremental updates, dyGRASS enables the identification of spectrally critical edges among the updates to capture the latest structural changes. For decremental updates, dyGRASS facilitates the recovery of important edges from the original graph back into the sparsifier. To further enhance computational efficiency, dyGRASS employs a GPU-based non-backtracking random walk scheme that allows multiple walkers to operate simultaneously across various target updates. This parallelization significantly improves both the performance and scalability of the proposed dyGRASS framework. Our comprehensive experimental evaluations reveal that dyGRASS achieves approximately a 10x speedup compared to the state-of-the-art incremental sparsification (inGRASS) algorithm while eliminating the setup overhead and improving solution quality in incremental spectral sparsification tasks. Moreover, dyGRASS delivers high efficiency and superior solution quality for fully dynamic graph sparsification, accommodating both edge insertions and deletions across a diverse range of graph instances originating from integrated circuit simulations, finite element analysis, and social networks.|\n", "2505.02670": "|**2025-05-05**|**Escaping the Krylov space during finite precision Lanczos**|J. Eckseler, M. Pieper, J. Schnack et.al.|[2505.02670](http://arxiv.org/abs/2505.02670)|null|8 pages, 8 figures|The Lanczos algorithm, introduced by Cornelius Lanczos, has been known for a long time and is widely used in computational physics. While often employed to approximate extreme eigenvalues and eigenvectores of an operator, recently interest in the sequence of basis vectors produced by the algorithm rose in the context of Krylov complexity. Although it is generally accepted and partially proven that the procedure is numerically stable for approximating the eigenvalues, there are numerical problems when investigating the Krylov basis constructed via the Lanczos procedure. In this paper, we show that loss of orthogonality and the attempt of reorthoganalization fall short of understanding and addressing the problem. Instead, the numerical sequence of eigenvectors in finite precision arithmetic escapes the true vector space spanned by the exact Lanczos vectors. This poses the real threat to an interpretation in view of the operator growth hypothesis.|\n", "2505.02634": "|**2025-05-05**|**Aerodynamic and structural airfoil shape optimisation via Transfer Learning-enhanced Deep Reinforcement Learning**|David Ramos, Lucas Lacasa, Eusebio Valero et.al.|[2505.02634](http://arxiv.org/abs/2505.02634)|null|20 pages, 7 figures|The main objective of this paper is to introduce a transfer learning-enhanced, multi-objective, deep reinforcement learning (DRL) methodology that is able to optimise the geometry of any airfoil based on concomitant aerodynamic and structural criteria. To showcase the method, we aim to maximise the lift-to-drag ratio $C_L/C_D$ while preserving the structural integrity of the airfoil -- as modelled by its maximum thickness -- and train the DRL agent using a list of different transfer learning (TL) strategies. The performance of the DRL agent is compared with Particle Swarm Optimisation (PSO), a traditional gradient-free optimisation method. Results indicate that DRL agents are able to perform multi-objective shape optimisation, that the DRL approach outperforms PSO in terms of computational efficiency and shape optimisation performance, and that the TL-enhanced DRL agent achieves performance comparable to the DRL one, while further saving substantial computational resources.|\n", "2505.02594": "|**2025-05-05**|**Advances on the finite element discretization of fluid-structure interaction problems**|Najwa Alshehri, Daniele Boffi, Fabio Credali et.al.|[2505.02594](http://arxiv.org/abs/2505.02594)|null|29 pages, 12 figures, 3 tables|We review the main features of an unfitted finite element method for interface and fluid-structure interaction problems based on a distributed Lagrange multiplier in the spirit of the fictitious domain approach. We recall our theoretical findings concerning well-posedeness, stability, and convergence of the numerical schemes, and discuss the related computational challenges. In the case of elliptic interface problems, we also present a posteriori error estimates.|\n", "2505.02580": "|**2025-05-05**|**Temperature and pressure reconstruction in turbulent Rayleigh-B\u00e9nard convection by Lagrangian velocities using PINN**|R. Barta, M. -C. Volk, C. Bauer et.al.|[2505.02580](http://arxiv.org/abs/2505.02580)|null|This manuscript is also submitted to Measurments Science and   Technology|Velocity, pressure, and temperature are the key variables for understanding thermal convection, and measuring them all is a complex task. In this paper, we demonstrate a method to reconstruct temperature and pressure fields based on given Lagrangian velocity data. A physics-informed neural network (PINN) based on a multilayer perceptron architecture and a periodic sine activation function is used to reconstruct both the temperature and the pressure for two cases of turbulent Rayleigh-B\\'enard convection (Pr = 6.9, Ra = $10^9$). The first dataset is generated with DNS and it includes Lagrangian velocity data of 150000 tracer particles. The second contains a PTV experiment with the same system parameters in a water-filled cubic cell, and we observed about 50000 active particle tracks per time step with the open-source framework proPTV. A realistic temperature and pressure field could be reconstructed in both cases, which underlines the importance of PINNs also in the context of experimental data. In the case of the DNS, the reconstructed temperature and pressure fields show a 90\\% correlation over all particles when directly validated against the ground truth. Thus, the proposed method, in combination with particle tracking velocimetry, is able to provide velocity, temperature, and pressure fields in convective flows even in the hard turbulence regime. The PINN used in this paper is compatible with proPTV and is part of an open source project. It is available on request at https://github.com/DLR_AS_BOA/RBC_PINN.|\n", "2505.02559": "|**2025-05-05**|**Anomalous valley Hall effect in monolayer chromium-based triple-Q magnets**|Xiu-Cai Jiang, Li-Ya Qiao, Yu-Zhong Zhang et.al.|[2505.02559](http://arxiv.org/abs/2505.02559)|null|8 pages, 4 figures|Using the density functional theory calculations, we predict that several monolayer chromium-based materials exhibit a triple-Q tetrahedral magnetic insulating ground state. By studying the effect of biaxial strain on monolayer CrSi$\\rm{_2}$P$\\rm{_4}$ under various on-site Coulomb interactions, we reveal that this magnetic insulating state, sandwiched between the itinerant $120^{\\circ}$ coplanar noncollinear antiferromagnetic and ferromagnetic states, originates from the competition between antiferromagnetic exchange and double exchange interactions of Cr 3$d$ electrons which can also be applied to account for the ground states in other chromium-based materials. Remarkably, anomalous valley Hall effect with giant valley splitting is discovered in the magnetic states of these inversion-asymmetric systems without requiring spin-orbit coupling or net magnetization. Our findings open a new avenue towards exploring monolayer materials for valleytronics.|\n", "2505.02531": "|**2025-05-05**|**A posteriori error estimates for the finite element approximation of the convection-diffusion-reaction equation based on the variational multiscale concept**|Ramon Codina, Hauke Gravenkamp, Sheraz Ahmed Khan et.al.|[2505.02531](http://arxiv.org/abs/2505.02531)|null||In this study, we employ the variational multiscale (VMS) concept to develop a posteriori error estimates for the stationary convection-diffusion-reaction equation. The variational multiscale method is based on splitting the continuous part of the problem into a resolved scale (coarse scale) and an unresolved scale (fine scale). The unresolved scale (also known as the sub-grid scale) is modeled by choosing it proportional to the component of the residual orthogonal to the finite element space, leading to the orthogonal sub-grid scale (OSGS) method. The idea is then to use the modeled sub-grid scale as an error estimator, considering its contribution in the element interiors and on the edges. We present the results of the a priori analysis and two different strategies for the a posteriori error analysis for the OSGS method. Our proposal is to use a scaled norm of the sub-grid scales as an a posteriori error estimate in the so-called stabilized norm of the problem. This norm has control over the convective term, which is necessary for convection-dominated problems. Numerical examples show the reliable performance of the proposed error estimator compared to other error estimators belonging to the variational multiscale family.|\n", "2505.02522": "|**2025-05-05**|**Node pruning reveals compact and optimal substructures within large networks**|Manish Yadav, Merten Stender et.al.|[2505.02522](http://arxiv.org/abs/2505.02522)|null||The structural complexity of reservoir networks poses a significant challenge, often leading to excessive computational costs and suboptimal performance. In this study, we introduce a systematic, task specific node pruning framework that enhances both the efficiency and adaptability of reservoir networks. By identifying and eliminating redundant nodes, we demonstrate that large networks can be compressed while preserving or even improving performance on key computational tasks. Our findings reveal the emergence of optimal subnetwork structures from larger Erdos Renyi random networks, indicating that efficiency is governed not merely by size but by topological organization. A detailed analysis of network structure at both global and node levels uncovers the role of density distributions, special-radius and asymmetric input-output node distributions, among other graph-theoretic measures that enhance the computational capacity of pruned compact networks. We show that pruning leads to non-uniform network refinements, where specific nodes and connectivity patterns become critical for information flow and memory retention. This work offers fundamental insights into how structural optimization influences reservoir dynamics, providing a pathway toward designing more efficient, scalable, and interpretable machine learning architectures.|\n", "2505.02517": "|**2025-05-05**|**Finite difference method for nonlinear damped viscoelastic Euler-Bernoulli beam model**|Wenlin Qiu, Xiangcheng Zheng, Tao Guo et.al.|[2505.02517](http://arxiv.org/abs/2505.02517)|null||We propose and analyze the numerical approximation for a viscoelastic Euler-Bernoulli beam model containing a nonlinear strong damping coefficient. The finite difference method is used for spatial discretization, while the backward Euler method and the averaged PI rule are applied for temporal discretization. The long-time stability and the finite-time error estimate of the numerical solutions are derived for both the semi-discrete-in-space scheme and the fully-discrete scheme. Furthermore, the Leray-Schauder theorem is used to derive the existence and uniqueness of the fully-discrete numerical solutions. Finally, the numerical results verify the theoretical analysis.|\n", "2505.02422": "|**2025-05-05**|**Sampling Kantorovich operators for speckle noise reduction using a Down-Up scaling approach and gap filling in remote sensing images**|Danilo Costarelli, Mariarosaria Natale et.al.|[2505.02422](http://arxiv.org/abs/2505.02422)|null||In the literature, several approaches have been proposed for restoring and enhancing remote sensing images, including methods based on interpolation, filtering, and deep learning. In this paper, we investigate the application of multivariate sampling Kantorovich (SK) operators for image reconstruction, with a particular focus on gap filling and speckle noise reduction. To understand the accuracy performances of the proposed algorithms, we first derive a quantitative estimate in $C(\\R^n)$ for the error of approximation using the Euler-Maclaurin summation formula, which provides sharper error bounds under minimal regularity conditions. We also establish a convergence result and a quantitative estimate with respect to the dissimilarity index measured by the continuous SSIM for functions in Lebesgue spaces. Additionally, we prove a multidimensional linear prediction result, which is used to design a new SK-based reconstruction algorithm to handle missing data, that we call LP-SK algorithm. To address speckle noise, we integrate SK operators into a newly proposed Down-Up scaling approach. Numerical tests are presented on synthetic and real SAR images to validate the proposed methods. Performance is assessed using similarity metrics such as SSIM and PSNR, along with speckle-specific indexes. Comparative analysis with state-of-the-art techniques highlights the effectiveness of the proposed approaches.|\n", "2505.02345": "|**2025-05-05**|**Optimal error estimates of a second-order temporally finite element method for electrohydrodynamic equations**|Shengfeng Wang, Zeyu Xia, Maojun Li et.al.|[2505.02345](http://arxiv.org/abs/2505.02345)|null||In this work, we mainly present the optimal convergence rates of the temporally second-order finite element scheme for solving the electrohydrodynamic equation. Suffering from the highly coupled nonlinearity, the convergence analysis of the numerical schemes for such a system is rather rare, not to mention the optimal error estimates for the high-order temporally scheme. To this end, we abandon the traditional error analysis method following the process of energy estimate, which may lead to the loss of accuracy. Instead, we note that the charge density also possesses the \"energy\" decaying property directly derived by its governing equation, although it does not appear in the energy stability analysis. This fact allows us to control the error terms of the charge density more conveniently, which finally leads to the optimal convergence rates. Several numerical examples are provided to demonstrate the theoretical results, including the energy stability, mass conservation, and convergence rates.|\n", "2505.02330": "|**2025-05-05**|**On Trigonometric Interpolation and Its Applications**|Xiaorong Zou et.al.|[2505.02330](http://arxiv.org/abs/2505.02330)|null||In this paper, we propose a new trigonometric interpolation algorithm and establish relevant convergent properties. The method adjusts an existing trigonometric interpolation algorithm such that it can better leverage Fast Fourier Transform (FFT) to enhance efficiency. The algorithm can be formulated in a way such that certain cancellation effects can be effectively leveraged for error analysis, which enables us not only to obtain the desired uniform convergent rate of the approximation to a function, but desired uniform convergent rates for its derivatives as well.   We further enhance the algorithm so it can be applied to non-periodic functions defined on bounded intervals. Numerical testing results confirm decent accurate performance of the algorithm. For its application, we demonstrate how it can be applied to estimate integrals and solve linear/non-linear ordinary differential equation (ODE). The test results show that it significantly outperforms Trapezoid/Simpson method on integral and standard Runge-Kutta algorithm on ODE. In addition, we show some numerical evidences that estimation error of the algorithm likely exhibits ``local property\", i.e. error at a point tends not to propagate, which avoids significant compounding error at some other place, as a remarkable advantage compared to polynomial-based approximations.|\n", "2505.02321": "|**2025-05-05**|**Velocity-Inferred Hamiltonian Neural Networks: Learning Energy-Conserving Dynamics from Position-Only Data**|Ruichen Xu, Zongyu Wu, Luoyao Chen et.al.|[2505.02321](http://arxiv.org/abs/2505.02321)|null|22 pages, 4 figures, 4 tables; code and data to be released upon   acceptance|Data-driven modeling of physical systems often relies on learning both positions and momenta to accurately capture Hamiltonian dynamics. However, in many practical scenarios, only position measurements are readily available. In this work, we introduce a method to train a standard Hamiltonian Neural Network (HNN) using only position data, enabled by a theoretical result that permits transforming the Hamiltonian $H(q,p)$ into a form $H(q, v)$. Under certain assumptions, namely, an invertible relationship between momentum and velocity, we formally prove the validity of this substitution and demonstrate how it allows us to infer momentum from position alone. We apply our approach to canonical examples including the spring-mass system, pendulum, two-body, and three-body problems. Our results show that using only position data is sufficient for stable and energy-consistent long-term predictions, suggesting a promising pathway for data-driven discovery of Hamiltonian systems when momentum measurements are unavailable.|\n", "2505.02320": "|**2025-05-05**|**Optimally accurate operators for partial differential equations**|Nobuaki Fuji, Thibault Duretz et.al.|[2505.02320](http://arxiv.org/abs/2505.02320)|null||In this contribution, we generalize the concept of \\textit{optimally accurate operators} proposed and used in a series of studies on the simulation of seismic wave propagation, particularly based on Geller \\& Takeuchi (1995). Although these operators have been mathematically and numerically proven to be more accurate than conventional methods, the theory was specifically developed for the equations of motion in linear elastic continuous media. Furthermore, the original theory requires compensation for errors from each term due to truncation at low orders during the error estimation, which has limited its application to other types of physics described by partial differential equations.   Here, we present a new method that can automatically derive numerical operators for arbitrary partial differential equations. These operators, which involve a small number of nodes in time and space (compact operators), are more accurate than conventional ones and do not require meshing. Our method evaluates the weak formulation of the equations of motion, developed with the aid of Taylor expansions.   We establish the link between our new method and the classic optimally accurate operators, showing that they produce identical coefficients in homogeneous media. Finally, we perform a benchmark test for the 1D Poisson problem across various heterogeneous media. The benchmarks demonstrate the superiority of our method compared to conventional operators, even when using a set of linear B-spline test functions (three-point hat functions). However, the convergence rate can depend on the wavelength of the material property: when the material property has the same wavelength as that of the field, the convergence rate is O(4), whereas it can be less efficient O(2) for other models.|\n", "2505.02308": "|**2025-05-05**|**Enabling Local Neural Operators to perform Equation-Free System-Level Analysis**|Gianluca Fabiani, Hannes Vandecasteele, Somdatta Goswami et.al.|[2505.02308](http://arxiv.org/abs/2505.02308)|null|33 pages, 9 figures|Neural Operators (NOs) provide a powerful framework for computations involving physical laws that can be modelled by (integro-) partial differential equations (PDEs), directly learning maps between infinite-dimensional function spaces that bypass both the explicit equation identification and their subsequent numerical solving. Still, NOs have so far primarily been employed to explore the dynamical behavior as surrogates of brute-force temporal simulations/predictions. Their potential for systematic rigorous numerical system-level tasks, such as fixed-point, stability, and bifurcation analysis - crucial for predicting irreversible transitions in real-world phenomena - remains largely unexplored. Toward this aim, inspired by the Equation-Free multiscale framework, we propose and implement a framework that integrates (local) NOs with advanced iterative numerical methods in the Krylov subspace, so as to perform efficient system-level stability and bifurcation analysis of large-scale dynamical systems. Beyond fixed point, stability, and bifurcation analysis enabled by local in time NOs, we also demonstrate the usefulness of local in space as well as in space-time (\"patch\") NOs in accelerating the computer-aided analysis of spatiotemporal dynamics. We illustrate our framework via three nonlinear PDE benchmarks: the 1D Allen-Cahn equation, which undergoes multiple concatenated pitchfork bifurcations; the Liouville-Bratu-Gelfand PDE, which features a saddle-node tipping point; and the FitzHugh-Nagumo (FHN) model, consisting of two coupled PDEs that exhibit both Hopf and saddle-node bifurcations.|\n", "2505.02281": "|**2025-05-04**|**Minimisation of Quasar-Convex Functions Using Random Zeroth-Order Oracles**|Amir Ali Farzin, Yuen-Man Pun, Iman Shames et.al.|[2505.02281](http://arxiv.org/abs/2505.02281)|null||This study explores the performance of a random Gaussian smoothing zeroth-order (ZO) scheme for minimising quasar-convex (QC) and strongly quasar-convex (SQC) functions in both unconstrained and constrained settings. For the unconstrained problem, we establish the ZO algorithm's convergence to a global minimum along with its complexity when applied to both QC and SQC functions. For the constrained problem, we introduce the new notion of proximal-quasar-convexity and prove analogous results to the unconstrained case. Specifically, we show the complexity bounds and the convergence of the algorithm to a neighbourhood of a global minimum whose size can be controlled under a variance reduction scheme. Theoretical findings are illustrated through investigating the performance of the algorithm applied to a range of problems in machine learning and optimisation. Specifically, we observe scenarios where the ZO method outperforms gradient descent. We provide a possible explanation for this phenomenon.|\n", "2505.02276": "|**2025-05-04**|**Liapunov exponent distributions and maps for multiple parameter logistic equation. Application to DNA and RNA sequences**|Miguel Martin-Landrove, B. P. Embaid et.al.|[2505.02276](http://arxiv.org/abs/2505.02276)|null|4 pages, 4 figures|The multiple parameter logistic equation has previously been utilized to determine the global stability of ternary codes, based on the arrangement of different symbols within the code. This approach has been extended to DNA and RNA sequences, proposing a specific application in the context of reading and translation processes involved in DNA replication and RNA-mediated protein codification. To address the complexity of mapping Liapunov exponents in terms of four parameters representing the different nucleotide bases specialized mapping techniques have been developed. These include Liapunov exponent distributions for entire sequences, as well as binary maps that classify nucleotide bases based on their chemical type (purinic or pyrimidinic). Such methodologies provide a framework for examining the structural and functional properties of genetic material. The sequences analyzed encompass a wide range of DNA and RNA types, including those with and without introns, as well as codifying and noncodifying regions. This multifaceted approach offers valuable insights into the dynamic behavior and stability of nucleotide arrangements, contributing to a deeper understanding of the underlying processes that govern genetic replication and protein synthesis.|\n", "2505.02270": "|**2025-05-04**|**Hyper Boris integrators for kinetic plasma simulations**|Seiji Zenitani, Tsunehiko N. Kato et.al.|[2505.02270](http://arxiv.org/abs/2505.02270)|null|27 pages|We propose a family of numerical solvers for the nonrelativistic Newton--Lorentz equation in kinetic plasma simulations. The new solvers extend the standard 4-step Boris procedure, which has second-order accuracy in time, in three ways. First, we repeat the 4-step procedure multiple times, using an $n$-times smaller timestep ($\\Delta t/n$). We derive a formula for the arbitrary subcycling number $n$, so that we obtain the result without repeating the same calculations. Second, prior to the 4-step procedure, we apply Boris-type gyrophase corrections to the electromagnetic field. In addition to a well-known correction to the magnetic field, we correct the electric field in an anisotropic manner to achieve higher-order ($N=2,4,6 \\dots$th order) accuracy. Third, combining these two methods, we propose a family of high-accuracy particle solvers, the hyper Boris solvers, which have two hyperparameters of the subcycling number $n$ and the order of accuracy, $N$. The $n$-cycle $N$th-order solver gives a numerical error of $\\sim (\\Delta t/n)^{N}$ at affordable computational cost.|\n", "2505.02268": "|**2025-05-04**|**Phantom Domain Finite Element Method: A novel approach for heterogeneous materials**|Tianlong He, Philippe Karamian-Surville, Daniel Cho\u00ef et.al.|[2505.02268](http://arxiv.org/abs/2505.02268)|null||In this paper, we introduce the Phantom Domain Finite Element Method (PDFEM), a novel computational approach tailored for the efficient analysis of heterogeneous and composite materials. Inspired by fictitious domain methods, this method employs a structured mesh to discretize the entire material domain while utilizing separate, independent meshes for the inclusions. These inclusion meshes are coupled to the structured mesh via a substitution matrix, enabling them to act as phantom meshes that do not directly contribute to the final system of equations. This framework offers significant advantages, including enhanced flexibility in handling complex inclusion geometries and improved computational efficiency. To assess the accuracy and robustness of the proposed method, numerical experiments are conducted on structures containing inclusions of various geometries. In order to emphasize the efficiency of the PDFEM method, a numerical simulation is presented to highlight its advantages in the case of long natural fibers, such as flax and linen. These simulations are compared against FEM calculations, demonstrating the efficiency of PDFEM. Indeed, meshing such fine structures requires an extremely high number of elements, and in some cases, meshing becomes particularly challenging due to the complexity of the geometries.|\n", "2505.02155": "|**2025-05-04**|**Boundary value problem of magnetically insulated diode: existence of solutions and complex bifurcation**|Denis Sidorov, Alexander Sinitsyn, David Leguizamon et.al.|[2505.02155](http://arxiv.org/abs/2505.02155)|null||The paper focuses on the stationary self-consistent problem of magnetic insulation for a vacuum diode with space-charge limitation, described by a singularly perturbed Vlasov-Maxwell system of dimension 1.5. The case of insulated diode when the electrons are deflected back towards the cathode at the point $x^{*}$ is considered. First, the initial VM system is reduced to the nonlinear singular limit system of ODEs for the potentials of electric and magnetic fields. The second step deals with the limit system's reduction to the new nonlinear singular ODE equation for effective potential $\\Theta(x)$. The existence of non-negative solutions is proved for the last equation on the interval $[0, x^{*})$ where $\\Theta(x)>0$. The most interesting and unexplored case is when $\\Theta(x)<0$ on the interval $(x^{*}, 1]$ and corresponds to the case of an insulated diode. For the first time, a numerical analysis of complex bifurcation of solutions in insulated diode is considered for $\\Theta(x)<0$ depending on parameters and boundary conditions. Bifurcation diagrams of the dependence of solution $\\Theta(x)$ on a free point (free boundary) $x^{*}$ were constructed. Insulated diode spacing is found.|\n", "2505.04627": "|**2025-05-07**|**Is the end of Insight in Sight ?**|Jean-Michel Tucny, Mihir Durve, Sauro Succi et.al.|[2505.04627](http://arxiv.org/abs/2505.04627)|null|20 pages, 5 figures|It is shown that the weight matrices of a Physics-informed neural network (PINN)-based deep learning application to a rarefied gas dynamics problem described by the Boltzmann equation bear no evident link to the mathematical structure of the physical problem. Instead, the weights appear close to Gaussian distributed random matrices. Although significantly more work is needed to support a robust assessment in this direction, these results suggest that deep-learning and the numerical solution of the Boltzmann equation represent two equivalent, but largely distinct paths to the same physical knowledge. If so, Explainable AI might be an unrealistic target and possibly even an ill-posed one.|\n", "2505.04571": "|**2025-05-07**|**Duality-Based Algorithm and Numerical Analysis for Optimal Insulation Problems on Non-Smooth Domains**|Harbir Antil, Alex Kaltenbach, Keegan L. A. Kirk et.al.|[2505.04571](http://arxiv.org/abs/2505.04571)|null||This article develops a numerical approximation of a convex non-local and non-smooth minimization problem. The physical problem involves determining the optimal distribution, given by $h\\colon \\Gamma_I\\to [0,+\\infty)$, of a given amount $m\\in \\mathbb{N}$ of insulating material attached to a boundary part $\\Gamma_I\\subseteq \\partial\\Omega$ of a thermally conducting body $\\Omega \\subseteq \\mathbb{R}^d$, $d \\in \\mathbb{N}$, subject to conductive heat transfer. To tackle the non-local and non-smooth character of the problem, the article introduces a (Fenchel) duality framework: (a) At the continuous level, using (Fenchel) duality relations, we derive an a posteriori error identity that can handle arbitrary admissible approximations of the primal and dual formulations of the convex non-local and non-smooth minimization problem; (b) At the discrete level, using discrete (Fenchel) duality relations, we derive an a priori error identity that applies to a Crouzeix--Raviart discretization of the primal formulation and a Raviart--Thomas discretization of the dual formulation. The proposed framework leads to error decay rates that are optimal with respect to the specific regularity of a minimizer. In addition, we prove convergence of the numerical approximation under minimal regularity assumptions. Since the discrete dual formulation can be written as a quadratic program, it is solved using a primal-dual active set strategy interpreted as semismooth Newton method. A solution of the discrete primal formulation is reconstructed from the solution of the discrete dual formulation by means of an inverse generalized Marini formula. This is the first such formula for this class of convex non-local and non-smooth minimization problems.|\n", "2505.04559": "|**2025-05-07**|**MuAPBEK: An Improved Analytical Kinetic Energy Density Functional for Quantum Chemistry**|Siwoo Lee, Adji Bousso Dieng et.al.|[2505.04559](http://arxiv.org/abs/2505.04559)|null||Orbital-free density functional theory (OFDFT) offers a true realization of the Hohenberg-Kohn theorems, enabling full quantum-mechanical studies of electronic systems based solely on electron densities. However, OFDFT remains limited by the difficulty of formulating accurate kinetic-energy density functionals. In this paper, we substantially enhance the accuracy of OFDFT energies and densities by tuning, during density initialization, the parameter $\\mu$ of the APBEK functional, which arises in the second-order gradient expansion of the kinetic energy for semiclassical neutral atoms. We augment this parameterized APBEK functional with two physically-motivated, non-empirical corrections derived from Kato's cusp condition and the virial theorem. The resulting functional, which we call MuAPBEK, is benchmarked against Kohn-Sham density functional theory (KSDFT) on atoms, organic molecules from the QM9 dataset, and the anti-malarial drug artemisinin. MuAPBEK achieves much lower energy errors than standard APBEK and Thomas-Fermi-von-Weizsacker functionals, even when the latter two are evaluated on converged KSDFT densities. Its mean absolute energy errors on atoms and molecules are 161 and 122 kcal/mol, respectively, indicating that MuAPBEK's errors do not scale with system size. MuAPBEK also yields accurate densities, with a mean integrated absolute density error of 1.8 electrons for molecules. Importantly, one step of our density optimization scheme is at least ten times faster than a single KSDFT self-consistent field cycle and exhibits a lower-order computational time complexity of $O(N^{1.96})$ with respect to system size, $N$. Our results indicate that highly-accurate OFDFT for large-scale quantum simulations beyond the practical limits of KSDFT is within reach.|\n", "2505.04439": "|**2025-05-07**|**Adaptive finite element method for an unregularized semilinear optimal control problem**|Francisco Fuica, Nicolai Jork et.al.|[2505.04439](http://arxiv.org/abs/2505.04439)|null||We devise an a posteriori error estimator for an affine optimal control problem subject to a semilinear elliptic PDE and control constraints. To approximate the problem, we consider a semidiscrete scheme based on the variational discretization approach. For this solution technique, we design an a posteriori error estimator that accounts for the discretization of the state and adjoint equations, and prove, under suitable local growth conditions of optimal controls, reliability and efficiency properties of such error estimator. A simple adaptive strategy based on the devised estimator is designed and its performance is illustrated with numerical examples.|\n", "2505.04417": "|**2025-05-07**|**Localized Diffusion Models for High Dimensional Distributions Generation**|Georg A. Gottwald, Shuigen Liu, Youssef Marzouk et.al.|[2505.04417](http://arxiv.org/abs/2505.04417)|null||Diffusion models are the state-of-the-art tools for various generative tasks. However, estimating high-dimensional score functions makes them potentially suffer from the curse of dimensionality (CoD). This underscores the importance of better understanding and exploiting low-dimensional structure in the target distribution. In this work, we consider locality structure, which describes sparse dependencies between model components. Under locality structure, the score function is effectively low-dimensional, so that it can be estimated by a localized neural network with significantly reduced sample complexity. This motivates the localized diffusion model, where a localized score matching loss is used to train the score function within a localized hypothesis space. We prove that such localization enables diffusion models to circumvent CoD, at the price of additional localization error. Under realistic sample size scaling, we show both theoretically and numerically that a moderate localization radius can balance the statistical and localization error, leading to a better overall performance. The localized structure also facilitates parallel training of diffusion models, making it potentially more efficient for large-scale applications.|\n", "2505.04370": "|**2025-05-07**|**Fast Bellman algorithm for real Monge-Amp\u00e8re equation**|Aleksandra Le, Frank Wikstr\u00f6m et.al.|[2505.04370](http://arxiv.org/abs/2505.04370)|null||In this paper, we introduce a new numerical algorithm for solving the Dirichlet   problem for the real Monge-Amp\\`ere equation. The idea is to represent the   Monge-Amp\\`ere operator as an infimum of a class of linear elliptic operators   and use Bellman's principle to construct a numeric scheme for approximating   the operator attaining this infimum.   We discuss the strengths and weaknesses of the proposed algorithm and demonstrate   the performance of the method on several examples with various degrees of   degeneracy and compare the results to two existing methods. Our method runs considerably faster than the ones   used for comparison, improving the running time by a factor of 3-10 for smooth,   strictly convex examples, and by a factor of 20-100 or more for mildly degenerate   examples.|\n", "2505.04362": "|**2025-05-07**|**Adaptive Equilibration of Molecular Dynamics Simulations**|Luciano G. Silvestri, Zachary A. Johnson, Michael S. Murillo et.al.|[2505.04362](http://arxiv.org/abs/2505.04362)|null||We present a systematic framework for shortening and automating molecular dynamics equilibration through improved position initialization methods and uncertainty quantification analysis, using the Yukawa one-component plasma as an exemplar system. Our comprehensive evaluation of seven initialization approaches (uniform random, uniform random with rejection, Halton and Sobol sequences, perfect and perturbed lattices, and a Monte Carlo pair distribution method) demonstrates that initialization significantly impacts equilibration efficiency, with microfield distribution analysis providing diagnostic insights into thermal behaviors. Our results establish that initialization method selection is relatively inconsequential at low coupling strengths, while physics-informed methods demonstrate superior performance at high coupling strengths, reducing equilibration time. We establish direct relationships between temperature stability and uncertainties in transport properties (diffusion coefficient and viscosity), comparing thermostating protocols including ON-OFF versus OFF-ON duty cycles, Berendsen versus Langevin thermostats, and thermostat coupling strengths. Our findings demonstrate that weaker thermostat coupling generally requires fewer equilibration cycles, and OFF-ON thermostating sequences outperform ON-OFF approaches for most initialization methods. The methodology implements temperature forecasting as a quantitative metric for system thermalization, enabling users to determine equilibration adequacy based on specified uncertainty tolerances in desired output properties, thus transforming equilibration from a heuristic process to a rigorously quantifiable procedure with clear termination criteria.|\n", "2505.04350": "|**2025-05-07**|**On the one-dimensional SPH approximation of fractional-order operators**|Khashayar Ghorbani, Fabio Semperlotti et.al.|[2505.04350](http://arxiv.org/abs/2505.04350)|null||This work presents a theoretical formalism and the corresponding numerical techniques to obtain the approximation of fractional-order operators over a 1D domain via the smoothed particle hydrodynamics (SPH) method. The method is presented for both constant- and variable-order operators, in either integral or differential forms. Several numerical examples are presented in order to validate the theory against analytical results and to evaluate the performance of the methodology. This formalism paves the way for the solution of fractional-order continuum mechanics models via the SPH method.|\n", "2505.04304": "|**2025-05-07**|**Quantum Circuits for the Black-Scholes equations via Schr\u00f6dingerisation**|Shi Jin, Zihao Tang, Xu Yin et.al.|[2505.04304](http://arxiv.org/abs/2505.04304)|null|25 pages, 7 figures|In this paper, we construct quantum circuits for the Black-Scholes equations, a cornerstone of financial modeling, based on a quantum algorithm that overcome the cure of high dimensionality. Our approach leverages the Schr\\\"odingerisation technique, which converts linear partial and ordinary differential equations with non-unitary dynamics into a system evolved by unitary dynamics. This is achieved through a warped phase transformation that lifts the problem into a higher-dimensional space, enabling the simulation of the Black-Scholes equation on a quantum computer. We will conduct a thorough complexity analysis to highlight the quantum advantages of our approach compared to existing algorithms. The effectiveness of our quantum circuit is substantiated through extensive numerical experiments.|\n", "2505.04288": "|**2025-05-07**|**A hybridizable discontinuous Galerkin method with transmission variables for time-harmonic electromagnetic problems**|Ari E. Rappaport, Th\u00e9ophile Chaumont-Frelet, Axel Modave et.al.|[2505.04288](http://arxiv.org/abs/2505.04288)|null||The CHDG method is a hybridizable discontinuous Galerkin (HDG) finite element method suitable for the iterative solution of time-harmonic wave propagation problems. Hybrid unknowns corresponding to transmission variables are introduced at the element interfaces and the physical unknowns inside the elements are eliminated, resulting in a hybridized system with favorable properties for fast iterative solution. In this paper, we extend the CHDG method, initially studied for the Helmholtz equation, to the time-harmonic Maxwell equations. We prove that the local problems stemming from hybridization are well-posed and that the fixed-point iteration naturally associated to the hybridized system is contractive. We propose a 3D implementation with a discrete scheme based on nodal basis functions. The resulting solver and different iterative strategies are studied with several numerical examples using a high-performance parallel C++ code.|\n", "2505.04282": "|**2025-05-07**|**Critical Behavior Analysis of Pure Dipolar Triangular Lattice via Equilibrium and Non-Equilibrium Monte Carlo Simulations**|S. Ismailzadeh, M. D. Niry et.al.|[2505.04282](http://arxiv.org/abs/2505.04282)|null|10 pages. 11 figures|Magnetic thin films and 2D arrays of magnetic nanoparticles exhibit unique physical properties that make them valuable for a wide range of technological applications. In such systems, dipolar interactions play a crucial role in determining their physical behavior. However, due to the anisotropic and long-range nature of dipolar interactions, conventional Monte Carlo (MC) methods face challenges in investigating these systems near criticality. In this study, we examine the critical behavior of a triangular lattice of dipoles using the optimized Tomita MC algorithm tailored for dipolar interactions. We employ two independent computational approaches to estimate the critical temperature and exponents: equilibrium MC simulations with histogram reweighting and the non-equilibrium relaxation method. Notably, both approaches demonstrate that the critical exponents are very close to those of the 2D Ising universality class.|\n", "2505.04263": "|**2025-05-07**|**Physics-Informed DeepONets for drift-diffusion on metric graphs: simulation and parameter identification**|Jan Blechschmidt, Tom-Christian Riemer, Max Winkler et.al.|[2505.04263](http://arxiv.org/abs/2505.04263)|null||We develop a novel physics informed deep learning approach for solving nonlinear drift-diffusion equations on metric graphs. These models represent an important model class with a large number of applications in areas ranging from transport in biological cells to the motion of human crowds. While traditional numerical schemes require a large amount of tailoring, especially in the case of model design or parameter identification problems, physics informed deep operator networks (DeepONet) have emerged as a versatile tool for the solution of partial differential equations with the particular advantage that they easily incorporate parameter identification questions. We here present an approach where we first learn three DeepONet models for representative inflow, inner and outflow edges, resp., and then subsequently couple these models for the solution of the drift-diffusion metric graph problem by relying on an edge-based domain decomposition approach. We illustrate that our framework is applicable for the accurate evaluation of graph-coupled physics models and is well suited for solving optimization or inverse problems on these coupled networks.|\n", "2505.04247": "|**2025-05-07**|**A block preconditioner for thermo-poromechanics with frictional deformation of fractures**|Yury Zabegaev, Inga Berre, Eirik Keilegavlen et.al.|[2505.04247](http://arxiv.org/abs/2505.04247)|null|20 pages, 7 figures|The numerical modeling of fracture contact thermo-poromechanics is crucial for advancing subsurface engineering applications, including CO2 sequestration, production of geo-energy resources, energy storage and wastewater disposal operations. Accurately modeling this problem presents substantial challenges due to the complex physics involved in strongly coupled thermo-poromechanical processes and the frictional contact mechanics of fractures. To resolve process couplings in the resulting mathematical model, it is common to apply fully implicit time stepping. This necessitates the use of an iterative linear solver to run the model. The solver's efficiency primarily depends on a robust preconditioner, which is particularly challenging to develop because it must handle the mutual couplings between linearized contact mechanics and energy, momentum, and mass balance. In this work, we introduce a preconditioner for the problem based on the nested approximations of Schur complements. To decouple the momentum balance, we utilize the fixed-stress approximation, extended to account for both the porous media and fracture subdomains. The singularity of the contact mechanics submatrix is resolved by a linear transformation. Two variations of the algorithm are proposed to address the coupled mass and energy balance submatrix: either the Constrained Pressure Residual or the System-AMG approach. The preconditioner is evaluated through numerical experiments of fluid injection into fractured porous media, which causes thermal contraction and subsequent sliding and opening of fractures. The experiments show that the preconditioner performs robustly for a wide range of simulation regimes governed by various fracture states, friction coefficients and Peclet number. The grid refinement experiments demonstrate that the preconditioner scales well in terms of GMRES iterations, in both two and three dimensions.|\n", "2505.04236": "|**2025-05-07**|**Neural-network-based longitudinal electric field prediction in nonlinear plasma wakefield accelerators**|Xiaoning Wang, Ming Zeng, Dazhang Li et.al.|[2505.04236](http://arxiv.org/abs/2505.04236)|null||Plasma wakefield acceleration holds remarkable promise for future advanced accelerators. The design and optimization of plasma-based accelerators typically require particle-in-cell simulations, which can be computationally intensive and time consuming. In this study, we train a neural network model to obtain the on-axis longitudinal electric field distribution directly without conducting particle-in-cell simulations for designing a two-bunch plasma wakefield acceleration stage. By combining the neural network model with an advanced algorithm for achieving the minimal energy spread, the optimal normalized charge per unit length of a trailing beam leading to the optimal beam-loading can be quickly identified. This approach can reduce computation time from around 7.6 minutes in the case of using particle-in-cell simulations to under 0.1 seconds. Moreover, the longitudinal electric field distribution under the optimal beam-loading can be visually observed. Utilizing this model with the beam current profile also enables the direct extraction of design parameters under the optimal beam-loading, including the maximum decelerating electric field within the drive beam, the average accelerating electric field within the trailing beam and the transformer ratio. This model has the potential to significantly improve the efficiency of designing and optimizing the beam-driven plasma wakefield accelerators.|\n", "2505.04227": "|**2025-05-07**|**Modeling of thin plate flexural vibrations by Partition of Unity Finite Element Method**|Tong Zhou, Jean-Daniel Chazot, Emmanuel Perrey-Debain et.al.|[2505.04227](http://arxiv.org/abs/2505.04227)|null||This paper presents a conforming thin plate bending element based on the Partition of Unity Finite Element Method (PUFEM), for the simulation of steady-state forced vibration. The issue of ensuring the continuity of displacement and slope between elements is addressed by the use of cubic Hermite-type Partition of Unity (PU) functions. With appropriate PU functions, the PUFEM allows the incorporation of the special enrichment functions into the finite elements to better cope with plate oscillations in a broad frequency band. The enrichment strategies consist of the sum of a power series up to a given order and a combination of progressive flexural wave solutions with polynomials. The applicability and the effectiveness of the PUFEM plate elements is first verified via the structural frequency response. Investigation is then carried out to analyze the role of polynomial enrichment orders and enriched plane wave distributions for achieving good computational performance in terms of accuracy and data reduction. Numerical results show that the PUFEM with high-order polynomials and hybrid wave-polynomial combinations can provide highly accurate prediction results by using reduced degrees of freedom and improved rate of convergence, as compared with the classical FEM.|\n", "2505.04197": "|**2025-05-07**|**Spatial-Wavelength Multiplexing Reliable Photonic Integrated General-Purpose Analog Computing System**|Tao Zhu, Bowen Zhu, Shicheng Zhang et.al.|[2505.04197](http://arxiv.org/abs/2505.04197)|null|29pages, 10 figures, research article|In the \"post-Moore era\", the growing challenges in traditional computing have driven renewed interest in analog computing, leading to various proposals for the development of general-purpose analog computing (GPAC) systems. In this work, we present a GPAC prototype featuring a silicon photonic chip designed for fully optical analog computation. This system leverages on-chip multi-channel architectures to enable parallel processing and utilizes wavelength-division multiplexing to significantly enhance computational capacity. In addition, we have developed an error-correction algorithm to monitor processing operations in real time, ensuring the reliability of computational results. Experimentally, we demonstrate the system's capability to solve ordinary differential equations and its applications in communications, microwave photonics, and image processing. The chip's energy efficiency is evaluated to reach up to 227 tera-operations per second per watt. Through this research, we provide a novel hardware framework and innovative directions for analog photonic computing.|\n", "2505.04161": "|**2025-05-07**|**Optimization of Infectious Disease Intervention Measures Based on Reinforcement Learning -- Empirical analysis based on UK COVID-19 epidemic data**|Baida Zhang, Yakai Chen, Huichun Li et.al.|[2505.04161](http://arxiv.org/abs/2505.04161)|null||Globally, the outbreaks of infectious diseases have exerted an extremely profound and severe influence on health security and the economy. During the critical phases of epidemics, devising effective intervention measures poses a significant challenge to both the academic and practical arenas. There is numerous research based on reinforcement learning to optimize intervention measures of infectious diseases. Nevertheless, most of these efforts have been confined within the differential equation based on infectious disease models. Although a limited number of studies have incorporated reinforcement learning methodologies into individual-based infectious disease models, the models employed therein have entailed simplifications and limitations, rendering it incapable of modeling the complexity and dynamics inherent in infectious disease transmission. We establish a decision-making framework based on an individual agent-based transmission model, utilizing reinforcement learning to continuously explore and develop a strategy function. The framework's validity is verified through both experimental and theoretical approaches. Covasim, a detailed and widely used agent-based disease transmission model, was modified to support reinforcement learning research. We conduct an exhaustive exploration of the application efficacy of multiple algorithms across diverse action spaces. Furthermore, we conduct an innovative preliminary theoretical analysis concerning the issue of \"time coverage\". The results of the experiment robustly validate the effectiveness and feasibility of the methodological framework of this study. The coping strategies gleaned therefrom prove highly efficacious in suppressing the expansion of the epidemic scale and safeguarding the stability of the economic system, thereby providing crucial reference perspectives for the formulation of global public health security strategies.|\n", "2505.04155": "|**2025-05-07**|**An Adaptive Mixed Precision and Dynamically Scaled Preconditioned Conjugate Gradient Algorithm**|Yichen Guo, Eric de Sturler, Tim Warburton et.al.|[2505.04155](http://arxiv.org/abs/2505.04155)|null||We propose an adaptive mixed precision and dynamically scaled preconditioned conjugate gradient algorithm (AMP-PCG). It dynamically adjusts the precision for storing vectors and computing, exploiting low precision when appropriate, while maintaining a convergence rate and accuracy comparable to that of double precision PCG. Our mixed precision strategy consists of three main components: (1) The residual and matrix-vector product are initially computed in double precision, and the algorithm switches these to single precision based on the chosen convergence tolerance and an estimate of the residual gap. (2) Depending on the eigenvalue distribution, the preconditioned residual and search direction are either in half precision throughout the iterations or initially in double precision and then stepwise reduced to single and half precision. (3) A dynamically scaled residual is used at every iteration to mitigate underflow in half precision. We provide theoretical support for our estimates and we demonstrate the effectiveness of AMP-PCG through numerical experiments, highlighting both its robustness and the significant performance gains (1.63x speedup) achieved compared to double precision PCG on a GPU.|\n", "2505.04120": "|**2025-05-07**|**On the Crouzeix-Raviart Finite Element Approximation of Phase-Field Dependent Topology Optimization in Stokes Flow**|Bangti Jin, Jing Li, Yifeng Xu et.al.|[2505.04120](http://arxiv.org/abs/2505.04120)|null|23 pages, 10 figures|In this work, we investigate a nonconforming finite element approximation of phase-field parameterized topology optimization governed by the Stokes flow. The phase field, the velocity field and the pressure field are approximated by conforming linear finite elements, nonconforming linear finite elements (Crouzeix-Raviart elements) and piecewise constants, respectively. When compared with the standard conforming counterpart, the nonconforming FEM can provide an approximation with fewer degrees of freedom, leading to improved computational efficiency. We establish the convergence of the resulting numerical scheme in the sense that the sequences of phase-field functions and discrete velocity fields contain subsequences that converge to a minimizing pair of the continuous problem in the $H^1$-norm and a mesh-dependent norm, respectively. We present extensive numerical results to illustrate the performance of the approach, including a comparison with the popular Taylor-Hood elements.|\n", "2505.04100": "|**2025-05-07**|**Linear Analysis of Stochastic Verlet-Type Integrators for Langevin Equations**|Niels Gr\u00f8nbech-Jensen et.al.|[2505.04100](http://arxiv.org/abs/2505.04100)|null|17 pages, twelve figures|We provide an analytical framework for analyzing the quality of stochastic Verlet-type integrators for simulating the Langevin equation. Focusing only on basic objective measures, we consider the ability of an integrator to correctly simulate characteristic configurational quantities of transport, a) diffusion on a flat surface and b) drift velocity on a tilted planar surface, as well as c) statistical sampling of a harmonic potential. For any stochastic Verlet-type integrator expressed in its configurational form, we develop closed form expressions to directly assess these three most basic quantities as a function of the applied time step. The applicability of the analysis is exemplified through twelve representative integrators developed over the past five decades, and algorithm performance is conveniently visualized through the three characteristic measures for each integrator. The GJ set of integrators stands out as the only option for correctly simulating diffusion, drift, and Boltzmann distribution in linear systems, and we therefore suggest that this general method is best suited for high quality thermodynamic simulations of nonlinear and complex systems, including for relatively high time steps compared to simulations with other integrators.|\n", "2505.05420": "|**2025-05-08**|**Robustly optimal dynamics for active matter reservoir computing**|Mario U. Gaimann, Miriam Klopotek et.al.|[2505.05420](http://arxiv.org/abs/2505.05420)|null|55 pages, 30 figures. Supplementary Videos:   https://doi.org/10.18419/DARUS-4619. Replication Data:   https://doi.org/10.18419/DARUS-4620|We study the information processing abilities of active matter in the reservoir computing (RC) paradigm, using a model that is externally driven to infer the future state of a chaotic signal. The simulated system closely follows a previously reported model. We uncover an exceptional dynamical regime of agent dynamics that has been overlooked heretofore. It appears robustly optimal across varying physical parameters and inference tasks, thus providing valuable insights into computation and inference with physical systems more generally. The ability to form effective mechanisms for information processing are primarily determined by the system's own intrinsic relaxation abilities. These are identifiable when probing the system without a specific inference goal and manifest when testing minimalistic single-particle reservoirs. The regime that achieves optimal computation is situated just below the critical damping threshold, involving a microscopic dynamical relaxation with multiple stages. The optimal system is adaptable under chaotic external driving, due to a diversity in response mechanisms that emerge like rapid alternations between quasi-stationary and highly nonlinear dynamical states. Both coherent and incoherent dynamics contribute to their operation, partly at dissimilar scales of space and delay time. Correlations on agent dynamics can indicate the best-performing regimes and onsets of tight relationships between the responding system and the fluctuating driver. As this model of computation is interpretable in physical terms, it facilitates re-framing inquiries regarding learning and unconventional computing with a fresh rationale for many-body physics out of equilibrium.|\n", "2505.05416": "|**2025-05-08**|**Variable Selection for Fixed and Random Effects in Multilevel Functional Mixed Effects Models**|Rahul Ghosal, Marcos Matabuena, Enakshi Saha et.al.|[2505.05416](http://arxiv.org/abs/2505.05416)|null||We develop a new method for simultaneously selecting fixed and random effects in a multilevel functional regression model. The proposed method is motivated by accelerometer-derived physical activity data from the 2011-12 cohort of the National Health and Nutrition Examination Survey (NHANES), where we are interested in identifying age and race-specific heterogeneity in covariate effects on the diurnal patterns of physical activity across the lifespan. Existing methods for variable selection in function-on-scalar regression have primarily been designed for fixed effect selection and for single-level functional data. In high-dimensional multilevel functional regression, the presence of cluster-specific heterogeneity in covariate effects could be detected through sparsity in fixed and random effects, and for this purpose, we propose a multilevel functional mixed effects selection (MuFuMES) method. The fixed and random functional effects are modelled using splines, with spike-and-slab group lasso (SSGL) priors on the unknown parameters of interest and a computationally efficient MAP estimation approach is employed for mixed effect selection through an Expectation Conditional Maximization (ECM) algorithm. Numerical analysis using simulation study illustrates the satisfactory selection accuracy of the variable selection method in having a negligible false-positive and false-negative rate. The proposed method is applied to the accelerometer data from the NHANES 2011-12 cohort, where it effectively identifies age and race-specific heterogeneity in covariate effects on the diurnal patterns of physical activity, recovering biologically meaningful insights.|\n", "2505.05407": "|**2025-05-08**|**Neural network methods for power series problems of Perron-Frobenius operators**|T. Udomworarat, I. Brevis, M. Richter et.al.|[2505.05407](http://arxiv.org/abs/2505.05407)|null||Problems related to Perron-Frobenius operators (or transfer operators) have been extensively studied and applied across various fields. In this work, we propose neural network methods for approximating solutions to problems involving these operators. Specifically, we focus on computing the power series of non-expansive Perron-Frobenius operators under a given $L^p$-norm with a constant damping parameter in $(0,1)$. We use PINNs and RVPINNs to approximate solutions in their strong and variational forms, respectively. We provide a priori error estimates for quasi-minimizers of the associated loss functions. We present some numerical results for 1D and 2D examples to show the performance of our methods.|\n", "2505.05401": "|**2025-05-08**|**Efficient Numerical Quantification of Flettner Rotor Installations**|Niklas K\u00fchl et.al.|[2505.05401](http://arxiv.org/abs/2505.05401)|null||This paper introduces an inviscid Computational Fluid Dynamics (CFD) approach for the rapid aerodynamic assessment of Flettner rotor systems on ships. The method relies on the Eulerian flow equations, approximated utilizing a state-of-the-art Finite Volume Method with a dynamic momentum source term to enforce rotor circulation. The method offers substantial computational savings by avoiding near-wall refinement and easing time step constraints, making it ideal for early design phases such as design space exploration. Validation against potential flow theory and viscous reference simulations confirms that the method reliably predicts lift-induced forces despite its limitations in capturing parasitic drag. Three-dimensional simulations, including idealized wind tunnel setups and full-scale ship applications at high Reynolds numbers (up to ReL=1E08), demonstrate that results based on low-order convection featuring a solid numerical viscosity yield deviations with respect to viscous reference data of around O(10%). Accepting this reasonable loss of predictive accuracy provides a simulation framework with response times in the order of minutes compared to hours or even days.|\n", "2505.05393": "|**2025-05-08**|**BraWl: Simulating the thermodynamics and phase stability of multicomponent alloys using conventional and enhanced sampling techniques**|Hubert J. Naguszewski, Livia B. P\u00e1rtay, David Quigley et.al.|[2505.05393](http://arxiv.org/abs/2505.05393)|null|8 pages, 4 figures|We present BraWl, a Fortran package implementing a range of conventional and enhanced sampling algorithms for exploration of the phase space of the Bragg-Williams model, facilitating study of diffusional solid-solid transformations in binary and multicomponent alloys. These sampling algorithms include Metropolis-Hastings Monte Carlo, Wang-Landau sampling, and Nested Sampling. We demonstrate the capabilities of the package by applying it to some prototypical binary and multicomponent alloys, including high-entropy alloys.|\n", "2505.05384": "|**2025-05-08**|**Machine learning model for efficient nonthermal tuning of the charge density wave in monolayer NbSe$_2$**|Luka Beni\u0107, Federico Grasselli, Chiheb Ben Mahmoud et.al.|[2505.05384](http://arxiv.org/abs/2505.05384)|null||Understanding and controlling the charge density wave (CDW) phase diagram of transition metal dichalcogenides is a long-studied problem in condensed matter physics. However, due to complex involvement of electron and lattice degrees of freedom and pronounced anharmonicity, theoretical simulations of the CDW phase diagram at the density-functional-theory level are often numerically demanding. To reduce the computational cost of first principles modelling by orders of magnitude, we have developed an electronic free energy machine learning model for monolayer NbSe$_2$ that allows changing both electronic and ionic temperatures independently. Our approach relies on a machine learning model of the electronic density of states and zero-temperature interatomic potential. This allows us to explore the CDW phase diagram of monolayer NbSe$_2$ both under thermal and laser-induced nonthermal conditions. Our study provides an accurate estimate of the CDW transition temperature at low cost and can disentangle the role of hot electrons and phonons in nonthermal ultrafast melting process of the CDW phase in NbSe$_2$.|\n", "2505.05361": "|**2025-05-08**|**Finite element approximation for quantitative photoacoustic tomography in a diffusive regime**|Giovanni S. Alberti, Siyu Cen, Zhi Zhou et.al.|[2505.05361](http://arxiv.org/abs/2505.05361)|null|24 pages|In this paper, we focus on the numerical analysis of quantitative photoacoustic tomography. Our goal is to reconstruct the optical coefficients, i.e., the diffusion and absorption coefficients, using multiple internal observational data. The foundation of our numerical algorithm lies in solving an inverse diffusivity problem and a direct problem associated with elliptic equations. The stability of the inverse problem depends critically on a non-zero condition in the internal observations, a condition that can be met using randomly chosen boundary excitation data. Utilizing these randomly generated boundary data, we implement an output least squares formulation combined with finite element discretization to solve the inverse problem. In this scenario, we provide a rigorous error estimate in $L^2(\\Omega)$ norm for the numerical reconstruction using a weighted energy estimate, inspired by the analysis of a newly proposed conditional stability result. The resulting error estimate serves as a valuable guide for selecting appropriate regularization parameters and discretization mesh sizes according to the noise levels present in the data. Several numerical experiments are presented to support our theoretical results and illustrate the effectiveness of our numerical scheme.|\n", "2505.05282": "|**2025-05-08**|**Electronic and Optical Properties of the Recently Synthesized 2D Vivianites (Vivianenes): Insights from First-Principles Calculations**|Raphael Benjamim de Oliveira, Bruno Ipaves, Guilherme da Silva Lopes Fabris et.al.|[2505.05282](http://arxiv.org/abs/2505.05282)|null||Vivianite (Fe$_3$(PO$_4$)$_2$8H$_2$O) is a naturally occurring layered material with significant environmental and technological relevance. This work presents a comprehensive theoretical investigation of its two-dimensional (2D) counterpart, Vivianene, focusing on its structural, electronic, and optical properties. Using density functional theory (DFT) calculations and ab initio molecular dynamics (AIMD) simulations, we evaluate its thermodynamic stability, band structure, density of states, and optical response. Our results confirm that Vivianene retains the main structural features of bulk Vivianite while exhibiting enhanced thermodynamic stability at room temperature. The electronic structure analysis reveals an indirect bandgap of 3.03 eV for Vivianene, which is slightly lower than the 3.21 eV observed for bulk Vivianite, deviating from the expected quantum confinement trend in 2D materials. The projected density of states (PDOS) analysis indicates that Fe d orbitals predominantly contribute to the valence and conduction bands. Optical calculations demonstrate that Vivianene exhibits a higher optical band gap (3.6 eV) than bulk Vivianite (3.2 eV), with significant absorption in the ultraviolet region. The refractive index and reflectivity analyses suggest that most of the incident light is absorbed rather than reflected, reinforcing its potential for optoelectronic applications. These findings provide valuable insights into the fundamental properties of Vivianene and highlight its potential for advanced applications in sensing, optoelectronics, and energy-related technologies.|\n", "2505.05255": "|**2025-05-08**|**A new paradigm for computing hydrodynamic forces on particles in Euler-Lagrange point-particle simulations**|Berend van Wachem, Hani Elmestikawy, Akshay Chandran et.al.|[2505.05255](http://arxiv.org/abs/2505.05255)|null||Accurate prediction of the hydrodynamic forces on particles is central to the fidelity of Euler-Lagrange (EL) simulations of particle-laden flows. Traditional EL methods typically rely on determining the hydrodynamic forces at the positions of the individual particles from the interpolated fluid velocity field, and feed these hydrodynamic forces back to the location of the particles. This approach can introduce significant errors in two-way coupled simulations, especially when the particle diameter is not much smaller than the computational grid spacing. In this study, we propose a novel force correlation framework that circumvents the need for undisturbed velocity estimation by leveraging volume-filtered quantities available directly from EL simulations. Through a rigorous analytical derivation in the Stokes regime and extensive particle-resolved direct numerical simulations (PR-DNS) at finite Reynolds numbers, we formulate force correlations that depend solely on the volume-filtered fluid velocity and local volume fraction, parametrized by the filter width. These correlations are shown to recover known drag laws in the appropriate asymptotic limits and exhibit a good agreement with analytical and high-fidelity numerical benchmarks for single particle cases, and, compared to existing correlations, an improved agreement for the drag force on particles in particle assemblies. The proposed framework significantly enhances the accuracy of hydrodynamic force predictions for both isolated particles and dense suspensions, without incurring the prohibitive computational costs associated with reconstructing undisturbed flow fields. This advancement lays the foundation for robust, scalable, and high-fidelity EL simulations of complex particulate flows across a wide range of industrial and environmental applications.|\n", "2505.05245": "|**2025-05-08**|**Machine learning-enabled atomistic insights into phase boundary engineering of solid-solution ferroelectrics**|Weiru Wen, Fan-Da Zeng, Ben Xu et.al.|[2505.05245](http://arxiv.org/abs/2505.05245)|null||Atomistic control of phase boundaries is crucial for optimizing the functional properties of solid-solution ferroelectrics, yet their microstructural mechanisms remain elusive. Here, we harness machine-learning-driven molecular dynamics to resolve the phase boundary behavior in the KNbO3-KTaO3 (KNTO) system. Our simulations reveal that chemical composition and ordering enable precise modulation of polymorphic phase boundaries (PPBs), offering a versatile pathway for materials engineering. Diffused PPBs and polar nano regions, predicted by our model, highly match with experiments, underscoring the fidelity of the machine-learning atomistic simulation. Crucially, we identify elastic and electrostatic mismatches between ferroelectric KNbO3 and paraelectric KTaO3 as the driving forces behind complex microstructural evolution. This work not only resolves the longstanding microstructural debate but also establishes a generalizable framework for phase boundary engineering toward next-generation high-performance ferroelectrics.|\n", "2505.05244": "|**2025-05-08**|**Three dimensional seepage analysis using a polyhedral scaled boundary finite element method**|Mingjiao Yan, Yang Yang, Dengmiao Hao et.al.|[2505.05244](http://arxiv.org/abs/2505.05244)|null||This paper presents a novel polyhedral scaled boundary finite element method (PSBFEM) for three-dimensional seepage analysis. The proposed method combines the semi-analytical capabilities of the SBFEM with the geometric flexibility of polyhedral and octree meshes, making it well-suited for complex seepage problems. Wachspress shape functions are employed to construct shape functions over arbitrary polyhedral elements, enabling accurate approximation along complex boundaries. The method is implemented within the ABAQUS UEL framework to support steady-state, transient, and free-surface seepage simulations. A series of numerical examples are provided to verify the accuracy, efficiency, and convergence of the proposed method, including benchmark problems and geometrically complex domains. The results demonstrate that the PSBFEM achieves higher accuracy and faster convergence compared to conventional FEM, particularly when using hybrid octree meshes with localized refinement. This framework provides a robust and efficient tool for 3D seepage analysis in geotechnical and hydraulic engineering applications.|\n", "2505.05234": "|**2025-05-08**|**Weighting operators for sparsity regularization**|Ole L\u00f8seth Elvetun, Bj\u00f8rn Fredrik Nielsen, Niranjana Sudheer et.al.|[2505.05234](http://arxiv.org/abs/2505.05234)|null||Standard regularization methods typically favor solutions which are in, or close to, the orthogonal complement of the null space of the forward operator/matrix $\\mathsf{A}$. This particular biasedness might not be desirable in applications and can lead to severe challenges when $\\mathsf{A}$ is non-injective.   We have therefore, in a series of papers, investigated how to \"remedy\" this fact, relative to a chosen basis and in a certain mathematical sense: Based on a weighting procedure, it turns out that it is possible to modify both Tikhonov and sparsity regularization such that each member of the chosen basis can be almost perfectly recovered from their image under $\\mathsf{A}$. In particular, we have studied this problem for the task of using boundary data to identify the source term in an elliptic PDE. However, this weighting procedure involves $\\mathsf{A}^\\dagger \\mathsf{A}$, where $\\mathsf{A}^\\dagger$ denotes the pseudo inverse of $\\mathsf{A}$, and can thus be CPU-demanding and lead to undesirable error amplification.   We therefore, in this paper, study alternative weighting approaches and prove that some of the recovery results established for the methodology involving $\\mathsf{A}$ hold for a broader class of weighting schemes. In fact, it turns out that \"any\" linear operator $\\mathsf{B}$ has an associated proper weighting defined in terms of images under $\\mathsf{B}\\mathsf{A}$. We also present a series of numerical experiments, employing different choices of $\\mathsf{B}$.|\n", "2505.05228": "|**2025-05-08**|**On the stability and conditioning of a fictitious domain formulation for fluid-structure interaction problems**|Daniele Boffi, Fabio Credali, Lucia Gastaldi et.al.|[2505.05228](http://arxiv.org/abs/2505.05228)|null|26 pages, 13 figures, 1 table|We consider a distributed Lagrange multiplier formulation for fluid-structure interaction problems in the spirit of the fictitious domain approach. Our previous studies showed that the formulation is unconditionally stable in time and that its mixed finite element discretization is well-posed. In this paper, we analyze the behavior of the condition number with respect to mesh refinement. Moreover, we observe that our formulation does not need any stabilization term in presence of small cut cells and conditioning is not affected by the interface position.|\n", "2505.05207": "|**2025-05-08**|**A Fourier-based inference method for learning interaction kernels in particle systems**|Grigorios A. Pavliotis, Andrea Zanoni et.al.|[2505.05207](http://arxiv.org/abs/2505.05207)|null||We consider the problem of inferring the interaction kernel of stochastic interacting particle systems from observations of a single particle. We adopt a semi-parametric approach and represent the interaction kernel in terms of a generalized Fourier series. The basis functions in this expansion are tailored to the problem at hand and are chosen to be orthogonal polynomials with respect to the invariant measure of the mean-field dynamics. The generalized Fourier coefficients are obtained as the solution of an appropriate linear system whose coefficients depend on the moments of the invariant measure, and which are approximated from the particle trajectory that we observe. We quantify the approximation error in the Lebesgue space weighted by the invariant measure and study the asymptotic properties of the estimator in the joint limit as the observation interval and the number of particles tend to infinity, i.e. the joint large time-mean field limit. We also explore the regime where an increasing number of generalized Fourier coefficients is needed to represent the interaction kernel. Our theoretical results are supported by extensive numerical simulations.|\n", "2505.05134": "|**2025-05-08**|**Matrices over a Hilbert space and their low-rank approximation**|Stanislav Budzinskiy et.al.|[2505.05134](http://arxiv.org/abs/2505.05134)|null||Matrices are typically considered over fields or rings. Motivated by applications in parametric differential equations and data-driven modeling, we suggest to study matrices with entries from a Hilbert space and present an elementary theory of them: from basic properties to low-rank approximation. Specifically, we extend the idea of cross approximation to such matrices and propose an analogue of the adaptive cross approximation algorithm. Our numerical experiments show that this approach can achieve quasioptimal approximation and be integrated with the existing computational software for partial differential equations.|\n", "2505.05128": "|**2025-05-08**|**Constraints Preserving Lax-Wendroff Flux Reconstruction for Relativistic Hydrodynamics with General Equations of State**|Sujoy Basak, Arpit Babbar, Harish Kumar et.al.|[2505.05128](http://arxiv.org/abs/2505.05128)|null||In the realm of relativistic astrophysics, the ideal equation of state with a constant adiabatic index provides a poor approximation due to its inconsistency with relativistic kinetic theory. However, it is a common practice to use it for relativistic fluid flow equations due to its simplicity. Here we develop a high-order Lax-Wendroff flux reconstruction method on Cartesian grids for solving special relativistic hydrodynamics equations with several general equations of state available in the literature. We also study the conversion from conservative to primitive variables, which depends on the equation of state in use, and provide an alternative method of conversion when the existing approach does not succeed. For the admissibility of the solution, we blend the high-order method with a low-order method on sub-cells and prove its physical admissible property in the case of all the equations of state used here. Lastly, we validate the scheme by several test cases having strong discontinuities, large Lorentz factor, and low density or pressure in one and two dimensions.|\n", "2505.05117": "|**2025-05-08**|**Spin-glass quantum phase transition in amorphous arrays of Rydberg atoms**|L. Brodoloni, J. Vovrosh, S. Juli\u00e0-Farr\u00e9 et.al.|[2505.05117](http://arxiv.org/abs/2505.05117)|null|10 pages, 7 figures, 1 table|The experiments performed with neutral atoms trapped in optical tweezers and coherently coupled to the Rydberg state allow quantum simulations of paradigmatic Hamiltonians for quantum magnetism. Previous studies have focused mainly on periodic arrangements of the optical tweezers, which host various spatially ordered magnetic phases. Here, we perform unbiased quantum Monte Carlo simulations of the ground state of quantum Ising models for amorphous arrays of Rydberg atoms. These models are designed to feature well-controlled local structural properties in the absence of long-range order. Notably, by determining the Edwards-Anderson order parameter, we find evidence of a quantum phase transition from a paramagnetic to a spin-glass phase. The magnetic structure factor indicates short-range isotropic antiferromagnetic correlations. For the feasible sizes, the spin-overlap distribution features a nontrivial structure with two broad peaks and a sizable weight at zero overlap. The comparison against results for the clean kagome lattice, which features local structural properties similar to those of our amorphous arrays, highlights the important role of the absence of long-range structural order of the underlying array. Our findings indicate a route to experimentally implement the details of a Hamiltonian which hosts a quantum spin-glass phase.|\n", "2505.05109": "|**2025-05-08**|**Structure determination from single-molecule X-ray scattering images using stochastic gradient ascent**|Steffen Schultze, D. Russell Luke, Helmut Grubm\u00fcller et.al.|[2505.05109](http://arxiv.org/abs/2505.05109)|null|24 pages, 6 figures|Scattering experiments using ultrashort X-ray free electron laser (XFEL) pulses have opened a new path for structure determination of a wide variety of specimens, including nano-crystals and entire viruses, approaching atomistic spatial and femtoseconds time resolution. However, random and unknown sample orientations as well as low signal to noise ratios have so far prevented a successful application to smaller specimens like single biomolecules. We here present resolution-annealed stochastic gradient ascent (RASTA), a new approach for direct atomistic electron density determination, which utilizes our recently developed rigorous Bayesian treatment of single-particle X-ray scattering. We demonstrate electron density determination at 2\\r{A} resolution of various small proteins from synthetic scattering images with as low as 15 photons per image.|\n", "2505.05105": "|**2025-05-08**|**Multigrid methods for the ghost finite element approximation of elliptic problems**|Hridya Dilip, Armando Coco et.al.|[2505.05105](http://arxiv.org/abs/2505.05105)|null||We present multigrid methods for solving elliptic partial differential equations on arbitrary domains using the nodal ghost finite element method, an unfitted boundary approach where the domain is implicitly defined by a level-set function. This method achieves second-order accuracy and offers substantial computational advantages over both direct solvers and finite-difference-based multigrid methods. A key strength of the ghost finite element framework is its variational formulation, which naturally enables consistent transfer operators and avoids residual splitting across grid levels. We provide a detailed construction of the multigrid components in both one and two spatial dimensions, including smoothers, transfer operators, and coarse grid operators. The choice of the stabilization parameter plays a crucial role in ensuring well-posedness and optimal convergence of the multigrid method. We derive explicit algebraic expressions for this parameter based on the geometry of cut cells. In the two-dimensional setting, we further improve efficiency by performing additional smoothing exclusively on cut cells, reducing computational cost without compromising convergence. Numerical results validate the proposed method across a range of geometries and confirm its robustness and scalability.|\n", "2505.05085": "|**2025-05-08**|**Learning dynamically inspired invariant subspaces for Koopman and transfer operator approximation**|Gary Froyland, Kevin K\u00fchl et.al.|[2505.05085](http://arxiv.org/abs/2505.05085)|null|20 pages, 13 figures|Transfer and Koopman operator methods offer a framework for representing complex, nonlinear dynamical systems via linear transformations, enabling for a deeper understanding of the underlying dynamics. The spectrum of these operators provide important insights into system predictability and emergent behaviour, although efficiently estimating them from data can be challenging. We tackle this issue through the lens of general operator and representational learning, in which we approximate these linear operators using efficient finite-dimensional representations. Specifically, we machine-learn orthonormal, locally supported basis functions that are dynamically tailored to the system. This learned basis provides a particularly accurate approximation of the operator's action as well as a nearly invariant finite-dimensional subspace. We illustrate our approach with examples that showcase the retrieval of spectral properties from the estimated operator, and emphasise the dynamically adaptive quality of the machine-learned basis.|\n", "2505.06195": "|**2025-05-09**|**Stable fully practical finite element methods for axisymmetric Willmore flow**|Harald Garcke, Robert N\u00fcrnberg, Quan Zhao et.al.|[2505.06195](http://arxiv.org/abs/2505.06195)|null||We consider fully discrete numerical approximations for axisymmetric Willmore flow that are unconditionally stable and work reliably without remeshing. We restrict our attention to surfaces without boundary, but allow for spontaneous curvature effects. The axisymmetric setting allows us to formulate our schemes in terms of the generating curve of the considered surface. We propose a novel weak formulation, that combines an evolution equation for the surface's mean curvature and the curvature identity of the generating curve. The mean curvature is used to describe the gradient flow structure, which enables an unconditional stability result for the discrete solutions. The generating curve's curvature, on the other hand, describes the surface's in-plane principal curvature and plays the role of a Lagrange multiplier for an equidistribution property on the discrete level. We introduce two fully discrete schemes and prove their unconditional stability. Numerical results are provided to confirm the convergence, stability and equidistribution properties of the introduced schemes.|\n", "2505.06194": "|**2025-05-09**|**ProME: An Integrated Computational Platform for Material Properties at Extremes and Its Application in Multicomponent Alloy Design**|Xingyu Gao, William Yi Wang, Xin Chen et.al.|[2505.06194](http://arxiv.org/abs/2505.06194)|null||We have built an integrated computational platform for material properties at extreme conditions, ProME (Professional Materials at Extremes) v1.0, which enables integrated calculations for multicomponent alloys, covering high temperatures up to tens of thousands of Kelvin, high pressures up to millions of atmospheres, and high strain rates up to millions per second. A series of software packages have been developed and integrated into ProME v1.0, including ABC (AI-Based Crystal search) for crystal structure search under pressure, SAE (Similar Atomic Environment) for disordered configuration modeling, MFP$^2$ (Multiphase Fast Previewer by Mean-Field Potential) for multiphase thermodynamic properties, HTEM (High-throughput Toolkit for Elasticity Modeling) for thermo-elastic properties, TREX (TRansport at Extremes) for electrical and thermal conductivity, Hippos (High plastic phase model software) for phase-field simulation of microstructure evolution under high strain rates, and AutoCalphad for modeling and optimization of phase diagrams with variable compositions. ProME v1.0 has been applied to design the composition of the quaternary alloys Platinum-Iridium-Aluminum-Chromium (Pt-Ir-Al-Cr) for engine nozzles of aerospace attitude-orbit control, achieving high-temperature strength comparable to the currently used Pt-Ir alloys but with significantly reduced costs for raw materials. ProME offers crucial support for advancing both fundamental scientific understanding and industrial innovation in materials research and development.|\n", "2505.06189": "|**2025-05-09**|**Efficient time-domain scattering synthesis via frequency-domain singularity subtraction**|Oscar P. Bruno, Manuel A. Santana et.al.|[2505.06189](http://arxiv.org/abs/2505.06189)|null||Fourier-transform-based methods enable accurate, dispersion-free   simulations of time-domain scattering problems by evaluating   solutions to the Helmholtz equation at a discrete set of   frequencies, sufficient to approximate the inverse Fourier   transform. However, in the case of scattering by trapping obstacles,   the Helmholtz solution exhibits nearly-real complex   resonances -- which significantly slows the convergence of numerical   inverse transform. To address this difficulty this paper introduces   a frequency-domain singularity subtraction technique that   regularizes the integrand of the inverse transform and efficiently   computes the singularity contribution via a combination of a   straightforward and inexpensive numerical technique together with a   large-time asymptotic expansion. Crucially, all relevant   complex resonances and their residues are determined via rational   approximation of integral equation solutions at real   frequencies. An adaptive algorithm is employed to ensure that all   relevant complex resonances are properly identified.|\n", "2505.06160": "|**2025-05-09**|**A Convergent Inexact Abedin-Kitagawa Iteration Method for Monge-Amp\u00e8re Eigenvalue Problems**|Liang Chen, Youyicun Lin, Junqi Yang et.al.|[2505.06160](http://arxiv.org/abs/2505.06160)|null||In this paper, we propose an inexact Aleksandrov solution based Abedin-Kitagawa iteration (AKI) method for solving (real) Monge-Amp{\\`e}re eigenvalue problems. The proposed approach utilizes the convergent Rayleigh inverse iterative formulation introduced by Abedin and Kitagawa as the prototype. More importantly, it employs an error tolerance criterion of inexact Aleksandrov solutions to approximately solve the subproblems without spoiling the convergence, which becomes the most crucial issue for the efficient implementation of the iterative method. For the two-dimensional case, by properly taking advantage of the flexibility rendered by the proposed inexact approach and a convergent fixed-point-based approach to solve the subproblems, considerable advancements in computational efficiency can be achieved by the inexact AKI method with its convergence under the ${\\cal C}^{2,\\alpha}$ boundary condition being rigorously established. Numerical experiments are conducted to demonstrate the efficiency of the proposed inexact AKI method. The numerical results suggest that the inexact AKI method can be more than eight times faster than the original AKI method, at least for all the tested problems.|\n", "2505.06141": "|**2025-05-09**|**Fast recovery of parametric eigenvalues depending on several parameters and location of high order exceptional points**|Benoit Nennig, Martin Ghienne, Emmanuel Perrey-Debain et.al.|[2505.06141](http://arxiv.org/abs/2505.06141)|null||A numerical algorithm is proposed to deal with parametric eigenvalue problems involving non-Hermitian matrices and is exploited to find location of defective eigenvalues in the parameter space of non-hermitian parametric eigenvalue problems. These non-Hermitian degeneracies also called exceptional points (EP) have raised considerable attention in the scientific community as these can have a great impact in a variety of physical problems. The method first requires the computation of high order derivatives of a few selected eigenvalues with respect to each parameter involved. The second step is to recombine these quantities to form new coefficients associated with a partial characteristic polynomial (PCP). By construction, these coefficients are regular functions in a large domain of the parameter space which means that the PCP allows one to recover the selected eigenvalues as well as the localization of high order EPs by simply using standard root-finding algorithms. The versatility of the proposed approach is tested on several applications, from mass-spring systems to guided acoustic waves with absorbing walls and rooms acoustics. The scalability of the method to large sparse matrices arising from conventional discretization techniques such as the finite element method is demonstrated. The proposed approach can be extended to a large number of applications where EPs play an important role in quantum mechanics, optics and photonics or in mechanical engineering.|\n", "2505.06138": "|**2025-05-09**|**Role of defects in atom probe analysis of sol-gel silica**|Gustav Eriksson, Matteo De Tullio, Francesco Carnovale et.al.|[2505.06138](http://arxiv.org/abs/2505.06138)|null|41 pages, 19 figures|Silica is a suitable material to encapsulate proteins at room temperature, enabling their analysis at the atomic level using laser-assisted atom probe tomography (La- APT). In this study, we show that UV and deep-UV lasers can achieve a high success rate in La-APT of silica in terms of chemical resolution and three-dimensional image volume, with both lasers providing comparable results. Since the La-APT analyses are driven by photon absorption, in order to understand the mechanisms behind the enhanced absorption of UV light, we performed density functional theory calculations to model the electronic and optical properties of amorphous silica matrices generated using a Monte Carlo approach to structural optimisation. In particular, we have investigated the role of various defects introduced during sample preparation, such as substitutional and interstitial carbon, sodium and gallium ions, and hydrogen. Our results show that the presence of defects increases the absorption of silica in the UV and deep-UV range and thus improves the La-APT capabilities of the material. However, due to the low density of free charge carriers resulting from the absorption of laser energy by defects, deviations from the nominal chemical composition and suboptimal chemical resolution may occur, potentially limiting the optimal acquisition of APT mass spectra.|\n", "2505.06106": "|**2025-05-09**|**Unconditionally local bounds preserving numerical scheme based on inverse Lax-Wendroff procedure for advection on networks**|Peter Frolkovi\u010d, Svetlana Kri\u0161kov\u00e1, Katar\u00edna Lackov\u00e1 et.al.|[2505.06106](http://arxiv.org/abs/2505.06106)|null||We derive an implicit numerical scheme for the solution of advection equation where the roles of space and time variables are exchanged using the inverse Lax-Wendroff procedure. The scheme contains a linear weight for which it is always second order accurate in time and space, and the stencil in the implicit part is fully upwinded for any value of the weight, enabling a direct computation of numerical solutions by forward substitution. To fulfill the local bounds for the solution represented by the discrete minimum and maximum principle (DMP), we use a predicted value obtained with the linear weight and check a priori if the DMP is valid. If not, we can use either a nonlinear weight or a limiter function that depends on Courant number and apply such a high-resolution version of the scheme to obtain a corrected value. The advantage of the scheme obtained with the inverse Lax-Wendroff procedure is that only in the case of too small Courant numbers, the limiting is towards the first order accurate scheme, which is not a situation occurring in numerical simulations with implicit schemes very often. In summary, the local bounds are satisfied up to rounding errors unconditionally for any Courant numbers, and the formulas for the predictor and the corrector are explicit. The high-resolution scheme can be extended straightforwardly for advection with nonlinear retardation coefficient with numerical solutions satisfying the DMP, and a scalar nonlinear algebraic equation has to be solved to obtain each predicted and corrected value. In numerical experiments, including transport on a sewer network, we can confirm the advantageous properties of numerical solutions for several representative examples.|\n", "2505.06043": "|**2025-05-09**|**Triangular preconditioners for double saddle point linear systems arising in the mixed form of poroelasticity equations**|Luca Bergamaschi, Massimiliano Ferronato, Angeles Martinez et.al.|[2505.06043](http://arxiv.org/abs/2505.06043)|null||In this paper, we study a class of inexact block triangular preconditioners for double saddle-point symmetric linear systems arising from the mixed finite element and mixed hybrid finite element discretization of Biot's poroelasticity equations. We develop a spectral analysis of the preconditioned matrix, showing that the complex eigenvalues lie in a circle of center $(1,0)$ and radius smaller than 1. In contrast, the real eigenvalues are described in terms of the roots of a third-degree polynomial with real coefficients. The results of numerical experiments are reported to show the quality of the theoretical bounds and illustrate the efficiency of the proposed preconditioners used with GMRES, especially in comparison with similar block diagonal preconditioning strategies along with the MINRES iteration.|\n", "2505.06024": "|**2025-05-09**|**Discretization of Dirac systems and port-Hamiltonian systems: the role of the constraint algorithm**|Mar\u00eda Barbero-Li\u00f1\u00e1n, Juan Manuel L\u00f3pez Medel, David Mart\u00edn de Diego et.al.|[2505.06024](http://arxiv.org/abs/2505.06024)|null|29 images, 3 figures|We study the discretization of (almost-)Dirac structures using the notion of retraction and discretization maps on manifolds. Additionally, we apply the proposed discretization techniques to obtain numerical integrators for port-Hamiltonian systems and we discuss how to merge the discretization procedure and the constraint algorithm associated to systems of implicit differential equations.|\n", "2505.05985": "|**2025-05-09**|**Discontinuous Galerkin time integration for second-order differential problems: formulations, analysis, and analogies**|Gabriele Ciaramella, Martin J. Gander, Ilario Mazzieri et.al.|[2505.05985](http://arxiv.org/abs/2505.05985)|null||We thoroughly investigate Discontinuous Galerkin (DG) discretizations as time integrators for second-order oscillatory systems, considering both second-order and first-order formulations of the original problem. Key contributions include new convergence analyses for the second-order formulation and equivalence proofs between DG and classical time-stepping schemes (such as Newmark schemes and general linear methods). In addition, the chapter provides a detailed review and convergence analysis for the first-order formulation, alongside comparisons of the proposed schemes in terms of accuracy, consistency, and computational cost.|\n", "2505.05978": "|**2025-05-09**|**A review of discontinuous Galerkin time-stepping methods for wave propagation problems**|Paola F. Antonietti, Alberto Artoni, Gabriele Ciaramella et.al.|[2505.05978](http://arxiv.org/abs/2505.05978)|null||This chapter reviews and compares discontinuous Galerkin time-stepping methods for the numerical approximation of second-order ordinary differential equations, particularly those stemming from space finite element discretization of wave propagation problems. Two formulations, tailored for second- and first-order systems of ordinary differential equations, are discussed within a generalized framework, assessing their stability, accuracy, and computational efficiency. Theoretical results are supported by various illustrative examples that validate the findings, enhancing the understanding and applicability of these methods in practical scenarios.|\n", "2505.05940": "|**2025-05-09**|**Fast Differentiable Modal Simulation of Non-linear Strings, Membranes, and Plates**|Rodrigo Diaz, Mark Sandler et.al.|[2505.05940](http://arxiv.org/abs/2505.05940)|null|accepted to DAFx 2025|Modal methods for simulating vibrations of strings, membranes, and plates are widely used in acoustics and physically informed audio synthesis. However, traditional implementations, particularly for non-linear models like the von K\\'arm\\'an plate, are computationally demanding and lack differentiability, limiting inverse modelling and real-time applications. We introduce a fast, differentiable, GPU-accelerated modal framework built with the JAX library, providing efficient simulations and enabling gradient-based inverse modelling. Benchmarks show that our approach significantly outperforms CPU and GPU-based implementations, particularly for simulations with many modes. Inverse modelling experiments demonstrate that our approach can recover physical parameters, including tension, stiffness, and geometry, from both synthetic and experimental data. Although fitting physical parameters is more sensitive to initialisation compared to other methods, it provides greater interpretability and more compact parameterisation. The code is released as open source to support future research and applications in differentiable physical modelling and sound synthesis.|\n", "2505.05915": "|**2025-05-09**|**On removing orders from amplitude equations**|David Juhasz, Per Kristen Jakobsen et.al.|[2505.05915](http://arxiv.org/abs/2505.05915)|null|56 pages, 12 figures|In this paper, we introduce a modified version of the renormalization group (RG) method and test its numerical accuracy. It has been tested on numerous scalar ODEs and systems of ODEs. Our method is primarily motivated by the possibility of simplifying amplitude equations. The key feature of our method is the introduction of a new homogeneous function at each order of the perturbation hierarchy, which is then used to remove terms from the amplitude equations. We have shown that there is a limit to how many terms can be removed, as doing so beyond a certain point would reintroduce linear growth. There is thus a \\textit{core} in the amplitude equation, which consists of the terms that cannot be removed while avoiding linear growth. Using our modified RG method, higher accuracy can also be achieved while maintaining the same level of complexity in the amplitude equation.|\n", "2505.05908": "|**2025-05-09**|**TTNOpt: Tree tensor network package for high-rank tensor compression**|Ryo Watanabe, Hidetaka Manabe, Toshiya Hikihara et.al.|[2505.05908](http://arxiv.org/abs/2505.05908)|null|18pages, 11 figures|We have developed TTNOpt, a software package that utilizes tree tensor networks (TTNs) for quantum spin systems and high-dimensional data analysis. TTNOpt provides efficient and powerful TTN computations by locally optimizing the network structure, guided by the entanglement pattern of the target tensors. For quantum spin systems, TTNOpt searches for the ground state of Hamiltonians with bilinear spin interactions and magnetic fields, and computes physical properties of these states, including the variational energy, bipartite entanglement entropy (EE), single-site expectation values, and two-site correlation functions. Additionally, TTNOpt can target the lowest-energy state within a specified subspace, provided that the Hamiltonian conserves total magnetization. For high-dimensional data analysis, TTNOpt factorizes complex tensors into TTN states that maximize fidelity to the original tensors by optimizing the tensors and the network. When a TTN is provided as input, TTNOpt reconstructs the network based on the EE without referencing the fidelity of the original state. We present three demonstrations of TTNOpt: (1) Ground-state search for the hierarchical chain model with a system size of $256$. The entanglement patterns of the ground state manifest themselves in a tree structure, and TTNOpt successfully identifies the tree. (2) Factorization of a quantic tensor of the $2^{24}$ dimensions representing a three-variable function where each variant has a weak bit-wise correlation. The optimized TTN shows that its structure isolates the variables from each other. (3) Reconstruction of the matrix product network representing a $16$-variable normal distribution characterized by a tree-like correlation structure. TTNOpt can reveal hidden correlation structures of the covariance matrix.|\n", "2505.05891": "|**2025-05-09**|**Elastic properties of transition metal dichalcogenides**|S. Azadi, A. Azhar, R. V. Belosludov et.al.|[2505.05891](http://arxiv.org/abs/2505.05891)|null||We present a comprehensive first-principles study of the structural and elastic properties of 2H-MX$_2$ transition metal dichalcogenides (TMDs) (M = W, Mo, Ta, Nb; X = S, Se). Using density functional theory with various van der Waals exchange-correlation functionals, we systematically investigate the influence of nonlocal interactions on lattice parameters, elastic constants, and mechanical moduli. Our results reveal a fundamental distinction between semiconducting and metallic TMDs: metallic compounds exhibit larger in-plane lattice parameters and reduced interlayer spacing, consistent with their bonding characteristics. We find that metallic TMDs display significantly lower in-plane stiffness and shear modulus compared to their semiconducting counterparts. We discuss this behaviour in the context of the observed charge density waves. In addition, we establish clear trends in the bulk, Young's, and shear moduli, demonstrating the role of atomic number and chemical composition in determining mechanical stability.|\n", "2505.05869": "|**2025-05-09**|**Generative Discovery of Partial Differential Equations by Learning from Math Handbooks**|Hao Xu, Yuntian Chen, Rui Cao et.al.|[2505.05869](http://arxiv.org/abs/2505.05869)|null||Data driven discovery of partial differential equations (PDEs) is a promising approach for uncovering the underlying laws governing complex systems. However, purely data driven techniques face the dilemma of balancing search space with optimization efficiency. This study introduces a knowledge guided approach that incorporates existing PDEs documented in a mathematical handbook to facilitate the discovery process. These PDEs are encoded as sentence like structures composed of operators and basic terms, and used to train a generative model, called EqGPT, which enables the generation of free form PDEs. A loop of generation evaluation optimization is constructed to autonomously identify the most suitable PDE. Experimental results demonstrate that this framework can recover a variety of PDE forms with high accuracy and computational efficiency, particularly in cases involving complex temporal derivatives or intricate spatial terms, which are often beyond the reach of conventional methods. The approach also exhibits generalizability to irregular spatial domains and higher dimensional settings. Notably, it succeeds in discovering a previously unreported PDE governing strongly nonlinear surface gravity waves propagating toward breaking, based on real world experimental data, highlighting its applicability to practical scenarios and its potential to support scientific discovery.|\n", "2505.05836": "|**2025-05-09**|**Computational Homogenization in 3D Magnetostatics using E3C Hyper-Reduction**|Hauke Goldbeck, Stephan Wulfinghoff et.al.|[2505.05836](http://arxiv.org/abs/2505.05836)|null||The recently published hyper-reduction method \"Empirically Corrected Cluster Cubature\" (E3C) is for the first time applied in three dimensions (here magnetostatics). The method is verified to give accurate results even for a small number of integration points, such as 15 for 3D microstructure simulations. The influence of the number of snapshots and modes, as well as the number of integration points, is investigated and the set with the best performance is selected, showing hyper-reduction errors of less than 1%. Exemplary simulations, including a two-scale simulation are considered illustrating the performance of the E3C method for 3D simulations.|\n", "2505.05792": "|**2025-05-09**|**On the Stability Barrier of Hermite Type Discretizations of Advection Equations**|Xianyi Zeng et.al.|[2505.05792](http://arxiv.org/abs/2505.05792)|null|29 pages, 7 figures|In this paper we establish a stability barrier of a class of high-order Hermite-type discretization of 1D advection equations underlying the hybrid-variable (HV) and active flux (AF) methods. These methods seek numerical approximations to both cell-averages and nodal solutions and evolves them in time simultaneously. It was shown in earlier work that the HV methods are supraconvergent, providing that the discretization uses more unknowns in the upwind direction than the downwind one, similar to the \"upwind condition\" of classical finite-difference schemes. Although it is well known that the stencil of finite-difference methods could not be too biased towards the upwind direction for stability consideration, known as \"stability barrier\", such a barrier has not been established for Hermite-type methods. In this work, we first show by numerical evidence that a similar barrier exists for HV methods and make a conjecture on the sharp bound on the stencil. Next, we prove the existence of stability barrier by showing that the semi-discretized HV methods are unstable given a stencil sufficiently biased towards the upwind direction. Tighter barriers are then proved using combinatorical tools, and finally we extend the analysis to studying other Hermite-type methods built on approximating nodal solutions and derivatives, such as those widely used in Hermite WENO methods.|\n", "2505.05781": "|**2025-05-09**|**A LightGBM-Incorporated Absorbing Boundary Conditions for the Wave-Equation-Based Meshless Method**|Qi-Chang Dong, Zhizhang David Chen et.al.|[2505.05781](http://arxiv.org/abs/2505.05781)|null|NA|A LightGBM-Incorporated absorbing boundary condition (ABC) computation approach for the wave-equation-based the radial point interpolation meshless (RPIM) method is proposed to simulate wave propagation in open space during the computation process. Different strageties are implemented for replacing the conventional perfectly matched layers (PMLs) in the computational domain. In this work, the model is used to predict the field components on the boundary at each time step to improve computational efficiency. The effectiveness and high efficiency of our method is verified by numerical experiments.|\n", "2505.05749": "|**2025-05-09**|**A Mechanism-Guided Inverse Engineering Framework to Unlock Design Principles of H-Bonded Organic Frameworks for Gas Separation**|Yong Qiu, Lei Wang, Letian Chen et.al.|[2505.05749](http://arxiv.org/abs/2505.05749)|null||The diverse combinations of novel building blocks offer a vast design space for hydrogen-boned frameworks (HOFs), rendering it a great promise for gas separation and purification. However, the underlying separation mechanism facilitated by their unique hydrogen-bond networks has not yet been fully understood. In this work, a comprehensive understanding of the separation mechanisms was achieved through an iterative data-driven inverse engineering approach established upon a hypothetical HOF database possessing nearly 110,000 structures created by a material genomics method. Leveraging a simple yet universal feature extracted from hydrogen bonding information with unambiguous physical meanings, the entire design space was exploited to rapidly identify the optimization route towards novel HOF structures with superior Xe/Kr separation performance (selectivity >103). This work not only provides the first large-scale HOF database, but also demonstrates the enhanced machine learning interpretability of our model-driven iterative inverse design framework, offering new insights into the rational design of nanoporous materials for gas separation.|\n", "2505.08783": "|**2025-05-13**|**CodePDE: An Inference Framework for LLM-driven PDE Solver Generation**|Shanda Li, Tanya Marwah, Junhong Shen et.al.|[2505.08783](http://arxiv.org/abs/2505.08783)|null||Partial differential equations (PDEs) are fundamental to modeling physical systems, yet solving them remains a complex challenge. Traditional numerical solvers rely on expert knowledge to implement and are computationally expensive, while neural-network-based solvers require large training datasets and often lack interpretability. In this work, we frame PDE solving as a code generation task and introduce CodePDE, the first inference framework for generating PDE solvers using large language models (LLMs). Leveraging advanced inference-time algorithms and scaling strategies, CodePDE unlocks critical capacities of LLM for PDE solving: reasoning, debugging, selfrefinement, and test-time scaling -- all without task-specific tuning. CodePDE achieves superhuman performance across a range of representative PDE problems. We also present a systematic empirical analysis of LLM generated solvers, analyzing their accuracy, efficiency, and numerical scheme choices. Our findings highlight the promise and the current limitations of LLMs in PDE solving, offering a new perspective on solver design and opportunities for future model development. Our code is available at https://github.com/LithiumDA/CodePDE.|\n", "2505.08715": "|**2025-05-13**|**Robust computation of higher-dimensional invariant tori from individual trajectories**|Maximilian Ruth, Jackson Kulik, Joshua Burby et.al.|[2505.08715](http://arxiv.org/abs/2505.08715)|null||We present a method for computing invariant tori of dimension greater than one. The method uses a single short trajectory of a dynamical system without any continuation or initial guesses. No preferred coordinate system is required, meaning the method is practical for physical systems where the user does not have much \\textit{a priori} knowledge. Three main tools are used to obtain the rotation vector of the invariant torus: the reduced rank extrapolation method, Bayesian maximum a posteriori estimation, and a Korkine-Zolatarev lattice basis reduction. The parameterization of the torus is found via a least-squares approach. The robustness of the algorithm is demonstrated by accurately computing many two-dimensional invariant tori of a standard map example. Examples of islands and three-dimensional invariant tori are shown as well.|\n", "2505.08708": "|**2025-05-13**|**A Reynolds-semi-robust H(div)-conforming method for unsteady incompressible non-Newtonian flows**|Louren\u00e7o Beir\u00e3o da Veiga, Daniele A. Di Pietro, Kirubell B. Haile et.al.|[2505.08708](http://arxiv.org/abs/2505.08708)|null||In this work, we prove what appear to be the first Reynolds-semi-robust and pressure-robust velocity error estimates for an H(div)-conforming approximation of unsteady incompressible flows of power-law type fluids. The proposed methods hinges on a discontinuous Galerkin approximation of the viscous term and a reinforced upwind-type stabilization of the convective term. The derived velocity error estimates account for pre-asymptotic orders of convergence observed in convection-dominated flows through regime-dependent estimates of the error contributions. A complete set of numerical results validate the theoretical findings.|\n", "2505.08631": "|**2025-05-13**|**Learning cardiac activation and repolarization times with operator learning**|Edoardo Centofanti, Giovanni Ziarelli, Nicola Parolini et.al.|[2505.08631](http://arxiv.org/abs/2505.08631)|null|31 pages, 17 figures|Solving partial or ordinary differential equation models in cardiac electrophysiology is a computationally demanding task, particularly when high-resolution meshes are required to capture the complex dynamics of the heart. Moreover, in clinical applications, it is essential to employ computational tools that provide only relevant information, ensuring clarity and ease of interpretation. In this work, we exploit two recently proposed operator learning approaches, namely Fourier Neural Operators (FNO) and Kernel Operator Learning (KOL), to learn the operator mapping the applied stimulus in the physical domain into the activation and repolarization time distributions. These data-driven methods are evaluated on synthetic 2D and 3D domains, as well as on a physiologically realistic left ventricle geometry. Notably, while the learned map between the applied current and activation time has its modelling counterpart in the Eikonal model, no equivalent partial differential equation (PDE) model is known for the map between the applied current and repolarization time. Our results demonstrate that both FNO and KOL approaches are robust to hyperparameter choices and computationally efficient compared to traditional PDE-based Monodomain models. These findings highlight the potential use of these surrogate operators to accelerate cardiac simulations and facilitate their clinical integration.|\n", "2505.08587": "|**2025-05-13**|**Two-Level Sketching Alternating Anderson acceleration for Complex Physics Applications**|Nicol\u00e1s A. Barnafi, Massimiliano Lupo Pasini et.al.|[2505.08587](http://arxiv.org/abs/2505.08587)|null||We present a novel two-level sketching extension of the Alternating Anderson-Picard (AAP) method for accelerating fixed-point iterations in challenging single- and multi-physics simulations governed by discretized partial differential equations. Our approach combines a static, physics-based projection that reduces the least-squares problem to the most informative field (e.g., via Schur-complement insight) with a dynamic, algebraic sketching stage driven by a backward stability analysis under Lipschitz continuity. We introduce inexpensive estimators for stability thresholds and cache-aware randomized selection strategies to balance computational cost against memory-access overhead. The resulting algorithm solves reduced least-squares systems in place, minimizes memory footprints, and seamlessly alternates between low-cost Picard updates and Anderson mixing. Implemented in Julia, our two-level sketching AAP achieves up to 50% time-to-solution reductions compared to standard Anderson acceleration-without degrading convergence rates-on benchmark problems including Stokes, p-Laplacian, Bidomain, and Navier-Stokes formulations at varying problem sizes. These results demonstrate the method's robustness, scalability, and potential for integration into high-performance scientific computing frameworks. Our implementation is available open-source in the AAP.jl library.|\n", "2505.08577": "|**2025-05-13**|**Modeling Quantum Links for the Exploration of Distributed Quantum Computing Systems**|Sahar Ben Rached, Zezhou Sun, Junaid Khan et.al.|[2505.08577](http://arxiv.org/abs/2505.08577)|null|Invited Paper for ICTON 2025|Quantum computing offers the potential to solve certain complex problems, yet, scaling monolithic processors remains a major challenge. Modular and distributed architectures are proposed to build large-scale quantum systems while bringing the security advantages of quantum communication. At present, this requires accurate and computationally efficient models of quantum links across different scales to advance system design and guide experimental prototyping. In this work, we review protocols and models for estimating latency, losses, and fidelity in quantum communication primitives relying on quantum state distribution via microwave photons. We also propose a scalable simulation framework to support the design and evaluation of future distributed quantum computing systems.|\n", "2505.08572": "|**2025-05-13**|**Entropy numbers of classes defined by integral operators**|V. Temlyakov et.al.|[2505.08572](http://arxiv.org/abs/2505.08572)|null||In this paper we develop the following general approach. We study asymptotic behavior of the entropy numbers not for an individual smoothness class, how it is usually done, but for the collection of classes, which are defined by integral operators with kernels coming from a given class of functions. Earlier, such approach was realized for the Kolmogorov widths.|\n", "2505.08526": "|**2025-05-14**|**Improving Data Fidelity via Diffusion Model-based Correction and Super-Resolution**|Wuzhe Xu, Yulong Lu, Sifan Wang et.al.|[2505.08526](http://arxiv.org/abs/2505.08526)|null||We propose a unified diffusion model-based correction and super-resolution method to enhance the fidelity and resolution of diverse low-quality data through a two-step pipeline. First, the correction step employs a novel enhanced stochastic differential editing technique based on an imbalanced perturbation and denoising process, ensuring robust and effective bias correction at the low-resolution level. The robustness and effectiveness of this approach are validated theoretically and experimentally. Next, the super-resolution step leverages cascaded conditional diffusion models to iteratively refine the corrected data to high-resolution. Numerical experiments on three PDE problems and a climate dataset demonstrate that the proposed method effectively enhances low-fidelity, low-resolution data by correcting numerical errors and noise while simultaneously improving resolution to recover fine-scale structures.|\n", "2505.08511": "|**2025-05-13**|**Numerical Analysis of Stabilization for Random Hyperbolic Systems of Balance Laws**|Shaoshuai Chu, Michael Herty, Alexander Kurganov et.al.|[2505.08511](http://arxiv.org/abs/2505.08511)|null||This paper extends the deterministic Lyapunov-based stabilization framework to random hyperbolic systems of balance laws, where uncertainties arise in boundary controls and initial data. Building on the finite volume discretization method from [{\\sc M. Banda and M. Herty}, Math. Control Relat. Fields., 3 (2013), pp. 121--142], we introduce a stochastic discrete Lyapunov function to prove the exponential decay of numerical solutions for systems with random perturbations. For linear systems, we derive explicit decay rates, which depend on boundary control parameters, grid resolutions, and the statistical properties of the random inputs. Theoretical decay rates are verified through numerical examples, including boundary stabilization of the linear wave equations and linearized shallow-water flows with random perturbations. We also demonstrate the decay rates for a nonlinear example.|\n", "2505.08491": "|**2025-05-13**|**Numerical Solution of Mixed-Dimensional PDEs Using a Neural Preconditioner**|Nunzio Dimola, Nicola Rares Franco, Paolo Zunino et.al.|[2505.08491](http://arxiv.org/abs/2505.08491)|null|27 pages, 7 figures|Mixed-dimensional partial differential equations (PDEs) are characterized by coupled operators defined on domains of varying dimensions and pose significant computational challenges due to their inherent ill-conditioning. Moreover, the computational workload increases considerably when attempting to accurately capture the behavior of the system under significant variations or uncertainties in the low-dimensional structures such as fractures, fibers, or vascular networks, due to the inevitable necessity of running multiple simulations. In this work, we present a novel preconditioning strategy that leverages neural networks and unsupervised operator learning to design an efficient preconditioner specifically tailored to a class of 3D-1D mixed-dimensional PDEs. The proposed approach is capable of generalizing to varying shapes of the 1D manifold without retraining, making it robust to changes in the 1D graph topology. Moreover, thanks to convolutional neural networks, the neural preconditioner can adapt over a range of increasing mesh resolutions of the discrete problem, enabling us to train it on low resolution problems and deploy it on higher resolutions. Numerical experiments validate the effectiveness of the preconditioner in accelerating convergence in iterative solvers, demonstrating its appeal and limitations over traditional methods. This study lays the groundwork for applying neural network-based preconditioning techniques to a broader range of coupled multi-physics systems.|\n", "2505.08482": "|**2025-05-13**|**Reconstructing initial pressure and speed of sound distributions simultaneously in photoacoustic tomography**|Miika Suhonen, Felix Lucka, Aki Pulkkinen et.al.|[2505.08482](http://arxiv.org/abs/2505.08482)|null||Image reconstruction in photoacoustic tomography relies on an accurate knowledge of the speed of sound in the target. However, the speed of sound distribution is not generally known, which may result in artefacts in the reconstructed distribution of initial pressure. Therefore, reconstructing the speed of sound simultaneously with the initial pressure would be valuable for accurate imaging in photoacoustic tomography. Furthermore, the speed of sound distribution could provide additional valuable information about the imaged target. In this work, simultaneous reconstruction of initial pressure and speed of sound in photoacoustic tomography is studied. This inverse problem is known to be highly ill-posed. To overcome this, we study an approach where the ill-posedness is alleviated by utilising multiple photoacoustic data sets that are generated by different initial pressure distributions within the same imaged target. Then, these initial pressure distributions are reconstructed simultaneously with the speed of sound distribution. A methodology for solving this minimisation problem is formulated using a gradient-based iterative approach equipped with bound constraints and a multigrid approach. The methodology was evaluated with numerical simulations. Different approaches for generating multiple initial pressure distributions and their effect on the solution of the image reconstruction problem were studied. The results show that initial pressure and speed of sound can be simultaneously reconstructed from photoacoustic data. Furthermore, utilising multiple initial pressure distributions improves the reconstructions such that the locations of initial pressure and speed of sound inhomogeneities can be better distinguished and image artifacts are reduced.|\n", "2505.08461": "|**2025-05-13**|**An Optimal and Robust Nonconforming Finite Element Method for the Strain Gradient Elasticity**|Jianguo Huang, Xuehai Huang, Zheqian Tang et.al.|[2505.08461](http://arxiv.org/abs/2505.08461)|null|23 pages|An optimal and robust low-order nonconforming finite element method is developed for the strain gradient elasticity (SGE) model in arbitrary dimension. An $H^2$-nonconforming quadratic vector-valued finite element in arbitrary dimension is constructed, which together with an $H^1$-nonconforming scalar finite element and the Nitsche's technique, is applied for solving the SGE model. The resulting nonconforming finite element method is optimal and robust with respect to the Lam\\'{e} coefficient $\\lambda$ and the size parameter $\\iota$, as confirmed by numerical results. Additionally, nonconforming finite element discretization of the smooth Stokes complex in two and three dimensions is devised.|\n", "2505.08427": "|**2025-05-13**|**Lower bounds for the reach and applications**|Daniel Platt, Ra\u00fal S\u00e1nchez Gal\u00e1n et.al.|[2505.08427](http://arxiv.org/abs/2505.08427)|null|32 pages, 4 figures|The reach of a submanifold of $\\mathbb{R}^N$ is defined as the largest radius of a tubular neighbourhood around the submanifold that avoids self-intersections. While essential in geometric and topological applications, computing the reach explicitly is notoriously difficult. In this paper, we introduce a rigorous and practical method to compute a guaranteed lower bound for the reach of a submanifold described as the common zero-set of finitely many smooth functions, not necessarily polynomials. Our algorithm uses techniques from numerically verified proofs and is particularly suitable for high-performance parallel implementations.   We illustrate the utility of this method through several applications. Of special note is a novel algorithm for computing the homology groups of planar curves, achieved by constructing a cubical complex that deformation retracts onto the curve--an approach potentially extendable to higher-dimensional manifolds. Additional applications include an improved comparison inequality between intrinsic and extrinsic distances for submanifolds of $\\mathbb{R}^N$, lower bounds for the first eigenvalue of the Laplacian on algebraic varieties and explicit bounds on how much smooth varieties can be deformed without changing their diffeomorphism type.|\n", "2505.08400": "|**2025-05-13**|**A Fourier finite volume approach for the optical inverse problem of quantitative photoacoustic tomography**|David J. Chappell et.al.|[2505.08400](http://arxiv.org/abs/2505.08400)|null|15 pages, 7 figures|A new approach for solving the optical inverse problem of quantitative photoacoustic tomography is introduced, which interpolates between the well-known diffusion approximation and a radiative transfer equation based model. The proposed formulation combines a spatial finite volume scheme with a truncated Fourier expansion in the direction variable for the radiative transfer equation. The finite volume scheme provides a natural and simple approach for representing piecewise constant image data modelled using transport equations. The truncated Fourier expansion in the direction variable facilitates the interpolation between the diffusion approximation at low order, and the full radiative transfer model as the truncation limit $N\\rightarrow\\infty$. It is therefore possible to tune the precision of the model to the demands of the imaging application, taking $N=1$ for cases when the diffusion approximation would suffice and increasing the number of terms otherwise. We will then utilise the non-linear optimisation functionality of Matlab to address the corresponding large-scale nonlinear inverse problem using gradient based quasi-Newton minimisation via the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Numerical experiments for two test-cases of increasing complexity and resolution will be presented, and the effect of logarithmically rescaling the problem data on the accuracy of the reconstructed solutions will be investigated. We will focus on cases where the diffusion approximation is not sufficient to demonstrate that our approach can provide significant accuracy gains with only a modest increase in the number of Fourier terms included.|\n", "2505.08387": "|**2025-05-13**|**The Lax--Wendroff theorem for Patankar-type methods applied to hyperbolic conservation laws**|Janina Bender, Thomas Izgin, Philipp \u00d6ffner et.al.|[2505.08387](http://arxiv.org/abs/2505.08387)|null|25 pages, 6 figures, submitted to Computers and Fluids, VSI: HONOM   2024|For hyperbolic conservation laws, the famous Lax--Wendroff theorem delivers sufficient conditions for the limit of a convergent numerical method to be a weak (entropy) solution. This theorem is a fundamental result, and many investigations have been done to verify its validity for finite difference, finite volume, and finite element schemes, using either explicit or implicit linear time-integration methods. Recently, the use of modified Patankar (MP) schemes as time-integration methods for the discretization of hyperbolic conservation laws has gained increasing interest. These schemes are unconditionally conservative and positivity-preserving and only require the solution of a linear system. However, MP schemes are by construction nonlinear, which is why the theoretical investigation of these schemes is more involved. We prove an extension of the Lax--Wendroff theorem for the class of MP methods. This is the first extension of the Lax--Wendroff theorem to nonlinear time integration methods with just an additional hypothesis on the total time variation boundness of the numerical solutions. We provide some numerical simulations that validate the theoretical observations.|\n", "2505.08381": "|**2025-05-13**|**Elastic properties of silicene: Spinodal instabilities**|Carlos P. Herrero, Rafael Ramirez et.al.|[2505.08381](http://arxiv.org/abs/2505.08381)|null|13 pages, 10 figures|Silicene, a two-dimensional (2D) allotrope of silicon, has attracted significant interest for its electronic and mechanical properties, alongside its compatibility with various substrates. In this study, we investigate the structural and elastic characteristics of silicene using molecular dynamics simulations based on a tight-binding Hamiltonian, calibrated to align with density-functional theory calculations. We focus particularly on the material's elastic properties and mechanical stability, analyzing its behavior under extensive compressive and tensile in-plane stresses and across temperatures up to 1000 K. Key properties examined include in-plane area, Si--Si bond length, atomic mean-square displacements, elastic constants, and 2D compression modulus. Our findings reveal a notable reduction in stiffness elastic constants, Poisson's ratio, and compression modulus with increasing temperature. Additionally, we identify mechanical instabilities in the silicene structure at specific compressive and tensile biaxial stresses, signaling the material's stability limits or spinodal points. At the corresponding spinodal pressures, structural and elastic properties exhibit anomalies or divergences.|\n", "2505.08368": "|**2025-05-13**|**Matched Asymptotic Expansions-Based Transferable Neural Networks for Singular Perturbation Problems**|Zhequan Shen, Lili Ju, Liyong Zhu et.al.|[2505.08368](http://arxiv.org/abs/2505.08368)|null||In this paper, by utilizing the theory of matched asymptotic expansions, an efficient and accurate neural network method, named as \"MAE-TransNet\", is developed for solving singular perturbation problems in general dimensions, whose solutions usually change drastically in some narrow boundary layers. The TransNet is a two-layer neural network with specially pre-trained hidden-layer neurons. In the proposed MAE-TransNet, the inner and outer solutions produced from the matched asymptotic expansions are first approximated by a TransNet with nonuniform hidden-layer neurons and a TransNet with uniform hidden-layer neurons, respectively. Then, these two solutions are combined with a matching term to obtain the composite solution, which approximates the asymptotic expansion solution of the singular perturbation problem. This process enables the MAE-TransNet method to retain the precision of the matched asymptotic expansions while maintaining the efficiency and accuracy of TransNet. Meanwhile, the rescaling of the sharp region allows the same pre-trained network parameters to be applied to boundary layers with various thicknesses, thereby improving the transferability of the method. Notably, for coupled boundary layer problems, a computational framework based on MAE-TransNet is also constructed to effectively address issues resulting from the lack of relevant matched asymptotic expansion theory in such problems. Our MAE-TransNet is compared with TransNet, PINN, and Boundary-Layer PINN on various benchmark problems including 1D linear and nonlinear problems with boundary layers, the 2D Couette flow problem, a 2D coupled boundary layer problem, and the 3D Burgers vortex problem. Numerical results demonstrate that MAE-TransNet significantly outperforms other neural network methods in capturing the characteristics of boundary layers, improving the accuracy, and reducing the computational cost.|\n", "2505.08324": "|**2025-05-13**|**An incremental algorithm for non-convex AI-enhanced medical image processing**|Elena Morotti et.al.|[2505.08324](http://arxiv.org/abs/2505.08324)|null||Solving non-convex regularized inverse problems is challenging due to their complex optimization landscapes and multiple local minima. However, these models remain widely studied as they often yield high-quality, task-oriented solutions, particularly in medical imaging, where the goal is to enhance clinically relevant features rather than merely minimizing global error. We propose incDG, a hybrid framework that integrates deep learning with incremental model-based optimization to efficiently approximate the $\\ell_0$-optimal solution of imaging inverse problems. Built on the Deep Guess strategy, incDG exploits a deep neural network to generate effective initializations for a non-convex variational solver, which refines the reconstruction through regularized incremental iterations. This design combines the efficiency of Artificial Intelligence (AI) tools with the theoretical guarantees of model-based optimization, ensuring robustness and stability. We validate incDG on TpV-regularized optimization tasks, demonstrating its effectiveness in medical image deblurring and tomographic reconstruction across diverse datasets, including synthetic images, brain CT slices, and chest-abdomen scans. Results show that incDG outperforms both conventional iterative solvers and deep learning-based methods, achieving superior accuracy and stability. Moreover, we confirm that training incDG without ground truth does not significantly degrade performance, making it a practical and powerful tool for solving non-convex inverse problems in imaging and beyond.|\n", "2505.08291": "|**2025-05-13**|**Multireference error mitigation for quantum computation of chemistry**|Hang Zou, Erika Magnusson, Hampus Brunander et.al.|[2505.08291](http://arxiv.org/abs/2505.08291)|null||Quantum error mitigation (QEM) strategies are essential for improving the precision and reliability of quantum chemistry algorithms on noisy intermediate-scale quantum devices. Reference-state error mitigation (REM) is a cost-effective chemistry-inspired QEM method that performs exceptionally well for weakly correlated problems. However, the effectiveness of REM is often limited when applied to strongly correlated systems. Here, we introduce multireference-state error mitigation (MREM), an extension of REM that systematically captures quantum hardware noise in strongly correlated ground states by utilizing multireference states. A pivotal aspect of MREM is using Givens rotations to efficiently construct quantum circuits to generate multireference states. To strike a balance between circuit expressivity and noise sensitivity, we employ compact wavefunctions composed a few dominant Slater determinants. These truncated multireference states, engineered to exhibit substantial overlap with the target ground state, can effectively enhance error mitigation in variational quantum eigensolver experiments. We demonstrate the effectiveness of MREM through comprehensive simulations of molecular systems $\\mathrm{H_2O, ~N_2, ~and ~F_2}$, underscoring its ability to realize significant improvements in computational accuracy compared to the original REM method. MREM broadens the scope of error mitigation to encompass a wider variety of molecular systems, including those exhibiting pronounced electron correlation.|\n", "2505.08289": "|**2025-05-13**|**Nonlinear optical response in kagome lattice with inversion symmetry breaking**|Xiangyang Liu, Junwen Lai, Jie Zhan et.al.|[2505.08289](http://arxiv.org/abs/2505.08289)|null||The kagome lattice is a fundamental model structure in condensed matter physics and materials science featuring symmetry-protected flat bands, saddle points, and Dirac points. This structure has emerged as an ideal platform for exploring various quantum physics. By combining effective model analysis and first-principles calculations, we propose that the synergy among inversion symmetry breaking, flat bands, and saddle point-related van Hove singularities within the kagome lattice holds significant potential for generating strong second-order nonlinear optical response. This property provides an inspiring insight into the practical application of the kagome-like materials, which is helpful for a comprehensive understanding of kagome lattice-related physics. Moreover, this work offers an alternative approach for designing materials with strong a second-order nonlinear optical response.|\n", "2505.10553": "|**2025-05-15**|**Flowing Through Hilbert Space: Quantum-Enhanced Generative Models for Lattice Field Theory**|Jehu Martinez, Andrea Delgado et.al.|[2505.10553](http://arxiv.org/abs/2505.10553)|null|8 pages, 6 figures, Submitted to QCE Quantum Week 2025|Sampling from high-dimensional and structured probability distributions is a fundamental challenge in computational physics, particularly in the context of lattice field theory (LFT), where generating field configurations efficiently is critical, yet computationally intensive. In this work, we apply a previously developed hybrid quantum-classical normalizing flow model to explore quantum-enhanced sampling in such regimes. Our approach embeds parameterized quantum circuits within a classical normalizing flow architecture, leveraging amplitude encoding and quantum entanglement to enhance expressivity in the generative process. The quantum circuit serves as a trainable transformation within the flow, while classical networks provide adaptive coupling and compensate for quantum hardware imperfections. This design enables efficient density estimation and sample generation, potentially reducing the resources required compared to purely classical methods. While LFT provides a representative and physically meaningful application for benchmarking, our focus is on improving the sampling efficiency of generative models through quantum components. This work contributes toward the development of quantum-enhanced generative modeling frameworks that address the sampling bottlenecks encountered in physics and beyond.|\n", "2505.10511": "|**2025-05-15**|**Learning Nonlinear Dynamics in Physical Modelling Synthesis using Neural Ordinary Differential Equations**|Victor Zheleznov, Stefan Bilbao, Alec Wright et.al.|[2505.10511](http://arxiv.org/abs/2505.10511)|**[link](https://github.com/victorzheleznov/dafx25)**|Accepted for publication in Proceedings of the 28th International   Conference on Digital Audio Effects (DAFx25), Ancona, Italy, September 2025|Modal synthesis methods are a long-standing approach for modelling distributed musical systems. In some cases extensions are possible in order to handle geometric nonlinearities. One such case is the high-amplitude vibration of a string, where geometric nonlinear effects lead to perceptually important effects including pitch glides and a dependence of brightness on striking amplitude. A modal decomposition leads to a coupled nonlinear system of ordinary differential equations. Recent work in applied machine learning approaches (in particular neural ordinary differential equations) has been used to model lumped dynamic systems such as electronic circuits automatically from data. In this work, we examine how modal decomposition can be combined with neural ordinary differential equations for modelling distributed musical systems. The proposed model leverages the analytical solution for linear vibration of system's modes and employs a neural network to account for nonlinear dynamic behaviour. Physical parameters of a system remain easily accessible after the training without the need for a parameter encoder in the network architecture. As an initial proof of concept, we generate synthetic data for a nonlinear transverse string and show that the model can be trained to reproduce the nonlinear dynamics of the system. Sound examples are presented.|\n", "2505.10450": "|**2025-05-15**|**Genetic algorithm demystified for cosmological parameter estimation**|Reginald Christian Bernardo, Yun Chen et.al.|[2505.10450](http://arxiv.org/abs/2505.10450)|null|12 pages + refs, 6 figures, comments welcome|Genetic algorithm (GA) belongs to a class of nature-inspired evolutionary algorithms that leverage concepts from natural selection to perform optimization tasks. In cosmology, the standard method for estimating parameters is the Markov chain Monte Carlo (MCMC) approach, renowned for its reliability in determining cosmological parameters. This paper presents a pedagogical examination of GA as a potential corroborative tool to MCMC for cosmological parameter estimation. Utilizing data sets from cosmic chronometers and supernovae with a curved $\\Lambda$CDM model, we explore the impact of GA's key hyperparameters -- such as the fitness function, crossover rate, and mutation rate -- on the population of cosmological parameters determined by the evolutionary process. We compare the results obtained with GA to those by MCMC, analyzing their effectiveness and viability for cosmological application.|\n", "2505.10404": "|**2025-05-15**|**A general regularization strategy for singular Stokes problems and convergence analysis for corresponding discretization and iterative solution**|Weizhang Huang, Zhuoran Wang et.al.|[2505.10404](http://arxiv.org/abs/2505.10404)|null|26 pages|A general regularization strategy is considered for the efficient iterative solution of the lowest-order weak Galerkin approximation of singular Stokes problems. The strategy adds a rank-one regularization term to the zero (2,2) block of the underlying singular saddle point system. This strategy includes the existing pressure pinning and mean-zero enforcement regularization as special examples. It is shown that the numerical error maintains the optimal-order convergence provided that the nonzero Dirichlet boundary datum is approximated numerically with sufficient accuracy. Inexact block diagonal and triangular Schur complement preconditioners are considered for the regularized system. The convergence analysis for MINRES and GMRES with corresponding block preconditioners is provided for different choices of the regularization term. Numerical experiments in two and three dimensions are presented to verify the theoretical findings and the effectiveness of the preconditioning for solving the regularized system.|\n", "2505.10299": "|**2025-05-15**|**Nature-inspired optimization, the Philippine Eagle, and cosmological parameter estimation**|Reginald Christian Bernardo, Erika Antonette Enriquez, Renier Mendoza et.al.|[2505.10299](http://arxiv.org/abs/2505.10299)|null|24 pages + refs, 13 figures, comments welcome|Precise and accurate estimation of cosmological parameters is crucial for understanding the Universe's dynamics and addressing cosmological tensions. In this methods paper, we explore bio-inspired metaheuristic algorithms, including the Improved Multi-Operator Differential Evolution scheme and the Philippine Eagle Optimization Algorithm (PEOA), alongside the relatively known genetic algorithm, for cosmological parameter estimation. Using mock data that underlay a true fiducial cosmology, we test the viability of each optimization method to recover the input cosmological parameters with confidence regions generated by bootstrapping on top of optimization. We compare the results with Markov chain Monte Carlo (MCMC) in terms of accuracy and precision, and show that PEOA performs comparably well under the specific circumstances provided. Understandably, Bayesian inference and optimization serve distinct purposes, but comparing them highlights the potential of nature-inspired algorithms in cosmological analysis, offering alternative pathways to explore parameter spaces and validate standard results.|\n", "2505.10298": "|**2025-05-15**|**Discrete Geodesic Calculus in the Space of Sobolev Curves**|Sascha Beutler, Florine Hartwig, Martin Rumpf et.al.|[2505.10298](http://arxiv.org/abs/2505.10298)|null|54 pages, 11 figures|The Riemannian manifold of curves with a Sobolev metric is an important and frequently studied model in the theory of shape spaces. Various numerical approaches have been proposed to compute geodesics, but so far elude a rigorous convergence theory. By a slick modification of a temporal Galerkin discretization we manage to preserve coercivity and compactness properties of the continuous model and thereby are able to prove convergence for the geodesic boundary value problem. Likewise, for the numerical analysis of the geodesic initial value problem we are able to exploit the geodesic completeness of the underlying continuous model for the error control of a time-stepping approximation. In fact, we develop a convergent discretization of a comprehensive Riemannian calculus that in addition includes parallel transport, covariant differentiation, the Riemann curvature tensor, and sectional curvature, all important tools to explore the geometry of the space of curves. Selected numerical examples confirm the theoretical findings and show the qualitative behaviour. To this end, a low-dimensional submanifold of Sobolev curves with explicit formulas for ground truth covariant derivatives and curvatures are considered.|\n", "2505.10215": "|**2025-05-15**|**Extension of the consistent $\u03b4^{+}$-SPH model for multiphase flows considering the compressibility of different phases**|Xiao-Ting Huang, Peng-Nan Sun, Hong-Guan Lyu et.al.|[2505.10215](http://arxiv.org/abs/2505.10215)|null|49 pages, 33 figures|In hydrodynamic problems involving wave impact on structures, air compressibility is crucial for accurate pressure prediction when an air bubble is entrapped. In this work, the consistent $\\delta^{+}$-SPH model, originally developed for single-phase scenarios, is extended to multiphase contexts. Although the consistent $\\delta^{+}$-SPH model shows good performance for single phase and viscous flow simulations, extending it to multiphase scenarios presents challenges, such as proper implementation of particle shifting for multiphase interfaces. Therefore, within the framework of the consistent $\\delta^{+}$-SPH, we introduce the following enhancements: firstly, new strategy for handling $\\delta \\boldmath{u}$-terms given by the particle shifting technique at multiphase interfaces are proposed to maintain stability and conservation. Secondly, for modeling of incompressible phases, like water, an acoustic damper term is introduced to alleviate acoustic waves resulting from the weakly-compressible assumption, which is expected to achieve smooth pressure field comparable to truly-incompressible hypothesis, thereby reducing the nonphysical pressure wave during the violent impact state; for modeling compressible phases like air, a physical sound speed is adopted in the equation of state to accurately model real gas phase compressibility. To test and validate the present multiphase SPH model, simulations were conducted for six scenarios. In particular, except for sloshing with two-layer liquids, the other scenarios fully consider air pressure oscillations when air is entrapped, compressed, or expanded by surrounding flows. The results demonstrate significant advantages of the present SPH model in simulating multiphase problems involving strong liquid impact and different phase compressibility.|\n", "2505.10211": "|**2025-05-15**|**PyLIT: Reformulation and implementation of the analytic continuation problem using kernel representation methods**|Alexander Benedix Robles, Phil-Alexander Hofmann, Thomas Chuna et.al.|[2505.10211](http://arxiv.org/abs/2505.10211)|null|29 pages, 11 figures, submitted to Computer Physics Communications|Path integral Monte Carlo (PIMC) simulations are a cornerstone for studying quantum many-body systems. The analytic continuation (AC) needed to estimate dynamic quantities from these simulations is an inverse Laplace transform, which is ill-conditioned. If this inversion were surmounted, then dynamical observables (e.g. dynamic structure factor (DSF) $S(q,\\omega)$) could be extracted from the imaginary-time correlation functions estimates.   Although of important, the AC problem remains challenging due to its ill-posedness. To address this challenge, we express the DSF as a linear combination of kernel functions with known Laplace transforms that have been tailored to satisfy its physical constraints. We use least-squares optimization regularized with a Bayesian prior to determine the coefficients of this linear combination. We explore various regularization term, such as the commonly used entropic regularizer, as well as the Wasserstein distance and $L^2$-distance as well as techniques for setting the regularization weight. A key outcome is the open-source package PyLIT (\\textbf{Py}thon \\textbf{L}aplace \\textbf{I}nverse \\textbf{T}ransform), which leverages Numba and unifies the presented formulations. PyLIT's core functionality is kernel construction and optimization.   In our applications, we find PyLIT's DSF estimates share qualitative features with other more established methods. We identify three key findings. Firstly, independent of the regularization choice, utilizing non-uniform grid point distributions reduced the number of unknowns and thus reduced our space of possible solutions. Secondly, the Wasserstein distance, a previously unexplored regularizer, performs as good as the entropic regularizer while benefiting from its linear gradient. Thirdly, future work can meaningfully combine regularized and stochastic optimization.   (text cut for char. limit)|\n", "2505.10178": "|**2025-05-15**|**The finiteness conjecture for $3 \\times 3$ binary matrices**|Thomas Mejstrik et.al.|[2505.10178](http://arxiv.org/abs/2505.10178)|null||The invariant polytope algorithm was a breakthrough in the joint spectral radius computation, allowing to find the exact value of the joint spectral radius for most matrix families~\\cite{GP2013,GP2016}. This algorithm found many applications in problems of functional analysis, approximation theory, combinatorics, etc. In this paper we propose a modification of the invariant polytope algorithm enlarging the class of problems to which it is applicable. Precisely, we introduce mixed numeric and symbolic computations. A further minor modification of augmenting the input set with additional matrices speeds up the algorithm in certain cases. With this modifications we are able to automatically prove the finiteness conjecture for all pairs of binary $3\\times 3$ matrices and sign $2\\times 2$ matrices.|\n", "2505.10108": "|**2025-05-15**|**A generalized discontinuous Hamilton Monte Carlo for transdimensional sampling**|Lei Li, Xiangxian Luo, Yinchen Luo et.al.|[2505.10108](http://arxiv.org/abs/2505.10108)|null||In this paper, we propose a discontinuous Hamilton Monte Carlo (DHMC) to sample from dimensional varying distributions, and particularly the grand canonical ensemble. The DHMC was proposed in [Biometrika, 107(2)] for discontinuous potential where the variable has a fixed dimension. When the dimension changes, there is no clear explanation of the volume-preserving property, and the conservation of energy is also not necessary. We use a random sampling for the extra dimensions, which corresponds to a measure transform. We show that when the energy is corrected suitably for the trans-dimensional Hamiltonian dynamics, the detailed balance condition is then satisfied. For the grand canonical ensemble, such a procedure can be explained very naturally to be the extra free energy change brought by the newly added particles, which justifies the rationality of our approach. To sample the grand canonical ensemble for interacting particle systems, the DHMC is then combined with the random batch method to yield an efficient sampling method. In experiments, we show that the proposed DHMC combined with the random batch method generates samples with much less correlation when compared with the traditional Metropolis-Hastings method.|\n", "2505.10106": "|**2025-05-15**|**Numerical Analysis of Pure and Blended Fuel Sonic Jets in a Mach 2 Crossflow**|Radouan Boukharfane et.al.|[2505.10106](http://arxiv.org/abs/2505.10106)|null|19|The injection of transverse jets into supersonic compressible crossflows represents a fundamental configuration relevant to a spectrum of high-speed applications. The intricate interactions arising between the crossflow and the injected jet induce complex flow phenomena, including shock waves and vortical structures, the characteristics of which are significantly contingent upon the thermophysical properties of the injected fuel. While prior investigations have addressed the influence of various fuels, a knowledge gap persists concerning the behaviour of alternative and synthetic multicomponent fuels within this flow regime. The present work employs high-fidelity large-eddy simulations (LES) to examine the impact of ten distinct fuels-hydrogen, methane, ethylene, ammonia, syngas mixture, and a synthetic blend, alongside several NH3/H2/N2 mixtures-on the macroscopic flow structures and mixing attributes within a transverse sonic jet immersed in a Mach 2 crossflow. By maintaining a uniform momentum flux ratio across the investigated cases, the study aims to isolate the influence of the unique thermophysical properties of each fuel on the windward mixing layer, a region critically important for initial entrainment processes. The investigation quantifies the effects of molecular weight, heat capacity ratio, and density on the development and evolution of coherent structures through a detailed examination of instantaneous flow fields, vortex dynamics, scalar distributions, and turbulence statistics. The results are expected to provide pertinent insights into fuel-dependent mixing mechanisms in supersonic flows, thereby contributing to the advancement of more efficient and versatile propulsion systems.|\n", "2505.10095": "|**2025-05-15**|**Error Estimates and Graded Mesh Refinement for Isogeometric Analysis on Polar Domains with Corners**|Thomas Apel, Philipp Zilk et.al.|[2505.10095](http://arxiv.org/abs/2505.10095)|null|44 pages, 11 figures|Isogeometric analysis (IGA) enables exact representations of computational geometries and higher-order approximation of PDEs. In non-smooth domains, however, singularities near corners limit the effectiveness of IGA, since standard methods typically fail to achieve optimal convergence rates. These constraints can be addressed through local mesh refinement, but existing approaches require breaking the tensor-product structure of splines, which leads to increased implementation complexity.   This work introduces a novel local refinement strategy based on a polar parameterization, in which one edge of the parametric square is collapsed into the corner. By grading the standard mesh toward the collapsing edge, the desired locality near the singularity is obtained while maintaining the tensor-product structure. A mathematical analysis and numerical tests show that the resulting isogeometric approximation achieves optimal convergence rates with suitable grading parameters.   Polar parameterizations, however, suffer from a lack of regularity at the polar point, making existing standard isogeometric approximation theory inapplicable. To address this, a new framework is developed for deriving error estimates on polar domains with corners. This involves the construction of polar function spaces on the parametric domain and a modified projection operator onto the space of $C^0$-smooth polar splines. The theoretical results are verified by numerical experiments confirming both the accuracy and efficiency of the proposed approach.|\n", "2505.09964": "|**2025-05-15**|**On the critical length conjecture for spherical Bessel functions in CAGD**|Ognyan Kounchev, Hermann Render et.al.|[2505.09964](http://arxiv.org/abs/2505.09964)|null|23 pages, 1 figure|A conjecture of J.M. Carnicer, E. Mainar and J.M. Pe\\~{n}a states that the critical length of the space $P_{n}\\odot C_{1}$ generated by the functions $x^{k}\\sin x$ and $x^{k}\\cos x$ for $k=0,...n$ is equal to the first positive zero $j_{n+\\frac{1}{2},1}$ of the Bessel function $J_{n+\\frac{1}{2}}$ of the first kind. It is known that the conjecture implies the following statement (D3): the determinant of the Hankel matrix \\begin{equation} \\left( \\begin{array} [c]{ccc} f & f^{\\prime} & f^{\\prime\\prime}\\\\ f^{\\prime} & f^{\\prime\\prime} & f^{\\left( 3\\right) }\\\\ f^{\\prime\\prime} & f^{\\prime\\prime\\prime} & f^{\\left( 4\\right) } \\end{array} \\right) \\label{eqabstract} \\end{equation} does not have a zero in the interval $(0,j_{n+\\frac{1}{2},1})$ whenever $f=f_{n}$ is given by $f_{n}\\left( x\\right) =\\sqrt{\\frac{\\pi}{2}} x^{n+\\frac{1}{2}}J_{n+\\frac{1}{2}}\\left( x\\right) .$ In this paper we shall prove (D3) and various generalizations.|\n", "2505.09911": "|**2025-05-15**|**Discontinuous hybrid neural networks for the one-dimensional partial differential equations**|Xiaoyu Wang, Long Yuan, Yao Yu et.al.|[2505.09911](http://arxiv.org/abs/2505.09911)|null|17page, 23 figures|A feedforward neural network, including hidden layers, motivated by nonlinear functions (such as Tanh, ReLU, and Sigmoid functions), exhibits uniform approximation properties in Sobolev space, and discontinuous neural networks can reduce computational complexity. In this work, we present a discontinuous hybrid neural network method for solving the partial differential equations, construct a new hybrid loss functional that incorporates the variational of the approximation equation, interface jump stencil and boundary constraints. The RMSprop algorithm and discontinuous Galerkin method are employed to update the nonlinear parameters and linear parameters in neural networks, respectively. This approach guarantees the convergence of the loss functional and provides an approximate solution with high accuracy.|\n", "2505.09876": "|**2025-05-15**|**Topological photonics of generalized and nonlinear eigenvalue equations**|Takuma Isobe, Tsuneya Yoshida, Yasuhiro Hatsugai et.al.|[2505.09876](http://arxiv.org/abs/2505.09876)|null||Topological photonics is developed based on the analogy of Schr\\\"{o}dinger equation which is mathematically reduced to a standard eigenvalue equation. Notably, several photonic systems are beyond the standard topological band theory as they are described by generalized or nonlinear eigenvalue equations. In this article, we review the topological band theory of this category. In the first part, we discuss topological photonics of generalized eigenvalue equations where the band structure may take complex values even when the involved matrices are Hermitian. These complex bands explain the characteristic dispersion relation of hyperbolic metamaterials. In addition, our numerical analysis predicts the emergence of symmetry-protected exceptional points in a photonic crystal composed of negative index media. In the second part, by introducing auxiliary bands, we establish the nonlinear bulk-edge correspondence under ``weak\" nonlinearity of eigenvalues. The nonlinear bulk-edge correspondence elucidates the robustness of chiral edge modes in photonic systems where the permittivity and permeability are frequency dependent.|\n", "2505.09857": "|**2025-05-16**|**High-Order Hermite Optimization: Fast and Exact Gradient Computation in Open-Loop Quantum Optimal Control using a Discrete Adjoint Approach**|Spencer Lee, Daniel Appelo et.al.|[2505.09857](http://arxiv.org/abs/2505.09857)|null||This work introduces the High-Order Hermite Optimization (HOHO) method, an open-loop discrete adjoint method for quantum optimal control. Our method is the first of its kind to efficiently compute exact (discrete) gradients when using continuous, parameterized control pulses while solving the forward equations (e.g. Schrodinger's equation or the Linblad master equation) with an arbitrarily high-order Hermite Runge-Kutta method. The HOHO method is implemented in [QuantumGateDesign.jl](https://github.com/leespen1/QuantumGateDesign.jl), an open-source software package for the Julia programming language, which we use to perform numerical experiments comparing the method to [Juqbox.jl](https://github.com/LLNL/Juqbox.jl). For realistic model problems we observe speedups up to 775x.|\n", "2505.09788": "|**2025-05-14**|**Strontium I, III, IV and V: Electron Impact Excitation Data for Kilonovae and White Dwarf Diagnostic Applications**|David J. Dougan, Niall E. McElroy, Connor P. Ballance et.al.|[2505.09788](http://arxiv.org/abs/2505.09788)|null|18 Pages, 15 Figures, 14 Tables|Strontium (Sr) emissions have been observed across a wide range of astrophysical phenomena, from kilonovae (KNe) events to white dwarf (WD) stars. Precise and extensive atomic data for low ionisation stages of Sr is required for accurate theoretical modelling and to improve our understanding of evolutionary pathways. We calculated energy levels, Einstein A coefficients and electron-impact excitation collision strengths for Sr I, Sr III, Sr IV and Sr V at the temperature and density ranges of interest in KNe and WD research. We developed new target structures using the GRASP0 and AUTOSTRUCTURE packages. The energies and A-values arising from the new structures were found to be in good agreement with experimental and theoretical equivalents reported in the literature. Maxwellian averaged electron impact collision strengths were calculated using the R-matrix approach, as applied through the DARC and RMBP coding packages. These are presented in adf04 file format. The new data sets allowed us to construct synthetic spectra for the first five ionisation stages of Sr and probe possible density and temperature diagnostic lines. The synthetic spectra within the KNe regime revealed possible Sr IV and Sr V candidate lines at 1027.69nm and 1203.35nm respectively. These may provide useful benchmarks for determining the extent of Sr ionisation that can be reached in an evolving KNe event. Additional diagnostic lines were found to be poor across the Sr ion stages for both KNe and WD regimes due to most levels being in either coronal or Local Thermodynamic Equilibrium (LTE) conditions.|\n", "2505.09770": "|**2025-05-14**|**Efficient Calculation of Modified Bessel Functions of the First Kind, $I_\u03bd (z)$, for Real Orders and Complex Arguments: Fortran Implementation with Double and Quadruple Precision**|Mofreh R. Zaghloul, Steven G. Johnson et.al.|[2505.09770](http://arxiv.org/abs/2505.09770)|null||We present an efficient self-contained algorithm for computing the modified Bessel function of the first kind $I_{\\nu}(z)$, implemented in a robust Fortran code supporting double and quadruple (quad) precision. The algorithm overcomes the limitations of Algorithm 644, which is restricted to double precision and applies overly conservative underflow and overflow thresholds, leading to failures in large parameter regions. Accuracy is validated against high-precision Maple calculations, and benchmarking shows execution time reductions to 54%-80% of Algorithm 644 (in double precision). Quad precision enhances numerical stability and broadens the domain of computations, making the implementation well suited for high-precision applications in physics and engineering. This work also provides a foundation for the development of efficient algorithms for other Bessel functions.|\n", "2505.09766": "|**2025-05-14**|**On the Well-Posedness of Green's Function Reconstruction via the Kirchhoff-Helmholtz Equation for One-Speed Neutron Diffusion**|Roberto Ponciroli et.al.|[2505.09766](http://arxiv.org/abs/2505.09766)|null||This work presents a methodology for reconstructing the spatial distribution of the neutron flux in a nuclear reactor, leveraging real-time measurements obtained from ex-core detectors. The Kirchhoff-Helmholtz (K-H) equation inherently defines the problem of estimating a scalar field within a domain based on boundary data, making it a natural mathematical framework for this task. The main challenge lies in deriving the Green's function specific to the domain and the neutron diffusion process. While analytical solutions for Green's functions exist for simplified geometries, their derivation of complex, heterogeneous domains-such as a nuclear reactor-requires a numerical approach. The objective of this work is to demonstrate the well-posedness of the data-driven Green's function approximation by formulating and solving the K-H equation as an inverse problem. After establishing the symmetry properties that the Green's function must satisfy, the K-H equation is derived from the one-speed neutron diffusion model. This is followed by a comprehensive description of the procedure for interpreting sensor readings and implementing the neutron flux reconstruction algorithm. Finally, the existence and uniqueness of the Green's function inferred from the sampled data are demonstrated, ensuring the reliability of the proposed method and its predictions.|\n", "2505.09765": "|**2025-05-14**|**Connections between convex optimization algorithms and subspace correction methods**|Boou Jiang, Jongho Park, Jinchao Xu et.al.|[2505.09765](http://arxiv.org/abs/2505.09765)|null|60 pages, 0 figures|We show that a broad range of convex optimization algorithms, including alternating projection, operator splitting, and multiplier methods, can be systematically derived from the framework of subspace correction methods via convex duality. To formalize this connection, we introduce the notion of dualization, a process that transforms an iterative method for the dual problem into an equivalent method for the primal problem. This concept establishes new connections across these algorithmic classes, encompassing both well-known and new methods. In particular, we show that classical algorithms such as the von Neumann, Dykstra, Peaceman--Rachford, and Douglas--Rachford methods can be interpreted as dualizations of subspace correction methods applied to appropriate dual formulations. Beyond unifying existing methods, our framework enables the systematic development of new algorithms for convex optimization. For instance, we derive parallel variants of alternating projection and operator splitting methods, as dualizations of parallel subspace correction methods, that are well-suited for large-scale problems on modern computing architectures and offer straightforward convergence guarantees. We also propose new alternating direction method of multipliers-type algorithms, derived as dualizations of certain operator splitting methods. These algorithms naturally ensure convergence even in the multi-block setting, where the conventional method does not guarantee convergence when applied to more than two blocks. This unified perspective not only facilitates algorithm design and the transfer of theoretical results but also opens new avenues for research and innovation in convex optimization.|\n", "2505.09846": "|**2025-05-14**|**Deep-Learning Atomistic Pseudopotential Model for Nanomaterials**|Kailai Lin, Matthew J. Coley-O'Rourke, Eran Rabani et.al.|[2505.09846](http://arxiv.org/abs/2505.09846)|null||The semi-empirical pseudopotential method (SEPM) has been widely applied to provide computational insights into the electronic structure, photophysics, and charge carrier dynamics of nanoscale materials. We present \"DeepPseudopot\", a machine-learned atomistic pseudopotential model that extends the SEPM framework by combining a flexible neural network representation of the local pseudopotential with parameterized non-local and spin-orbit coupling terms. Trained on bulk quasiparticle band structures and deformation potentials from GW calculations, the model captures many-body and relativistic effects with very high accuracy across diverse semiconducting materials, as illustrated for silicon and group III-V semiconductors. DeepPseudopot's accuracy, efficiency, and transferability make it well-suited for data-driven in silico design and discovery of novel optoelectronic nanomaterials.|\n", "2505.13401": "|**2025-05-19**|**Unraveling superradiance: entanglement and mutual information in collective decay**|Xin H. H. Zhang, Daniel Malz, Peter Rabl et.al.|[2505.13401](http://arxiv.org/abs/2505.13401)|null|7+3+2 pages|We study the collective decay of an initially inverted ensemble of two-level emitters into a squeezed photonic reservoir. By using a quantum-state diffusion approach to unravel the emission process, we investigate entanglement and classical correlations along individual quantum trajectories over time. This numerical analysis shows that despite an accelerated initial build-up of entanglement and a significant amount of spin squeezing in the steady state, the essential features of the superradiant burst are well described by averages over fully factorizable states. We explain this observation in terms of an almost complete factorization of all 2-local observables, which we identify as a generic property of superradiant decay. Based on this insight, we provide a purely classical theory for the burst in squeezed superradiance, which is both intuitive and exact for a large number of emitters. Moreover, we show that our numerical approach is also applicable for the investigation of subradiant states, which dominate the slow residual decay of non-uniform ensembles at very long times.|\n", "2505.13397": "|**2025-05-19**|**Learning by solving differential equations**|Benoit Dherin, Michael Munn, Hanna Mazzawi et.al.|[2505.13397](http://arxiv.org/abs/2505.13397)|null||Modern deep learning algorithms use variations of gradient descent as their main learning methods. Gradient descent can be understood as the simplest Ordinary Differential Equation (ODE) solver; namely, the Euler method applied to the gradient flow differential equation. Since Euler, many ODE solvers have been devised that follow the gradient flow equation more precisely and more stably. Runge-Kutta (RK) methods provide a family of very powerful explicit and implicit high-order ODE solvers. However, these higher-order solvers have not found wide application in deep learning so far. In this work, we evaluate the performance of higher-order RK solvers when applied in deep learning, study their limitations, and propose ways to overcome these drawbacks. In particular, we explore how to improve their performance by naturally incorporating key ingredients of modern neural network optimizers such as preconditioning, adaptive learning rates, and momentum.|\n", "2505.13374": "|**2025-05-19**|**Structure-preserving schemes conserving entropy and kinetic energy**|Kunal Bahuguna, Ramesh Kolluru, S. V. Raghurama Rao et.al.|[2505.13374](http://arxiv.org/abs/2505.13374)|null||This paper presents a novel structure-preserving scheme for Euler equations, focusing on the numerical conservation of entropy and kinetic energy. Explicit flux functions engineered to conserve entropy are introduced within the finite-volume framework. Further, discrete kinetic energy conservation too is introduced. A systematic inquiry is presented, commencing with an overview of numerical entropy conservation and formulation of entropy-conserving and kinetic energy-preserving fluxes, followed by the study of their properties and efficacy. A novelty introduced is to associate numerical entropy conservation to the discretization of the energy conservation equation. Furthermore, an entropy-stable shock-capturing diffusion method and a hybrid approach utilizing the entropy distance to manage smooth regions effectively are also introduced. The addition of artificial viscosity in appropriate regions ensures entropy generation sufficient to prevent numerical instabilities. Various test cases, showcasing the efficacy and stability of the proposed methodology, are presented.|\n", "2505.13356": "|**2025-05-19**|**Quantum Hardware-in-the-Loop for Optimal Power Flow in Renewable-Integrated Power Systems**|Zeynab Kaseb, Rahul Rane, Aleksandra Lekic et.al.|[2505.13356](http://arxiv.org/abs/2505.13356)|null||This paper presents a proof-of-concept for integrating quantum hardware with real-time digital simulator (RTDS) to model and control modern power systems, including renewable energy resources. Power flow (PF) analysis and optimal power flow (OPF) studies are conducted using RTDS coupled with Fujitsu's CMOS Digital Annealer and D-Wave's Advantage quantum processors. The adiabatic quantum power flow (AQPF) and adiabatic quantum optimal power flow (AQOPF) algorithms are used to perform PF and OPF, respectively, on quantum and quantum-inspired hardware. The experiments are performed on the IEEE 9-bus test system and a modified version that includes solar and wind farms. The findings demonstrate that the AQPF and AQOPF algorithms can accurately perform PF and OPF, yielding results that closely match those of classical Newton-Raphson (NR) solvers while also exhibiting robust convergence. Furthermore, the integration of renewable energy sources (RES) within the AQOPF framework proves effective in maintaining system stability and performance, even under variable generation conditions. These findings highlight the potential of quantum computing to significantly enhance the modeling and control of future power grids, particularly in systems with high renewable energy penetration.|\n", "2505.13305": "|**2025-05-19**|**Kinematic dynamos and resolution limits for Smoothed Particle Magnetohydrodynamics**|Nikyta Shchutskyi, Matthieu Schaller, Orestis A. Karapiperis et.al.|[2505.13305](http://arxiv.org/abs/2505.13305)|null|18 pages, 15 figures, submitted to MNRAS, comments welcome|Understanding the origin and evolution of magnetic fields on cosmological scales opens up a window into the physics of the early Universe. Numerical simulations of such fields require a careful treatment to faithfully solve the equations of magnetohydrodynamics (MHD) without introducing numerical artefacts. In this paper, we study the growth of the magnetic fields in controlled kinematic dynamo setups using both smoothed particle hydrodynamics implementations in the SWIFT code. We assess the quality of the reconstructed solution in the Roberts flow case against the reference implementation in the Pencil code and find generally a good agreement. Similarly, we reproduce the known features of the more complex ABC flow. Using a simple induction-diffusion balance model to analyse the results, we construct an \"overwinding\" trigger metric to locally detect regions where the magnetic diffusion cannot counteract the expected induction because of limitations in the method's ability to resolve magnetic field gradients. This metric is then used to identify the necessary resolution and resistivity levels to counteract the overwinding problem. We finally apply this metric to adiabatic cosmological simulations and discuss the resolution requirements needed to resolve the growth of the primordial fields without artefacts.|\n", "2505.13283": "|**2025-05-19**|**Accelerating Bayesian Optimal Experimental Design via Local Radial Basis Functions: Application to Soft Material Characterization**|Tianyi Chu, Jonathan B. Estrada, Spencer H. Bryngelson et.al.|[2505.13283](http://arxiv.org/abs/2505.13283)|null||We develop a computational approach that significantly improves the efficiency of Bayesian optimal experimental design (BOED) using local radial basis functions (RBFs). The presented RBF--BOED method uses the intrinsic ability of RBFs to handle scattered parameter points, a property that aligns naturally with the probabilistic sampling inherent in Bayesian methods. By constructing accurate deterministic surrogates from local neighborhood information, the method enables high-order approximations with reduced computational overhead. As a result, computing the expected information gain (EIG) requires evaluating only a small uniformly sampled subset of prior parameter values, greatly reducing the number of expensive forward-model simulations needed. For demonstration, we apply RBF--BOED to optimize a laser-induced cavitation (LIC) experimental setup, where forward simulations follow from inertial microcavitation rheometry (IMR) and characterize the viscoelastic properties of hydrogels. Two experimental design scenarios, single- and multi-constitutive-model problems, are explored. Results show that EIG estimates can be obtained at just 8% of the full computational cost in a five-model problem within a two-dimensional design space. This advance offers a scalable path toward optimal experimental design in soft and biological materials.|\n", "2505.13185": "|**2025-05-19**|**Filtering in a hazard rate change-point model with financial and life-insurance applications**|Matteo Buttarazzi, Claudia Ceci et.al.|[2505.13185](http://arxiv.org/abs/2505.13185)|null|28 pages, 3 figures|This paper develops a continuous-time filtering framework for estimating a hazard rate subject to an unobservable change-point. This framework arises naturally in both financial and insurance applications, where the default intensity of a firm or the mortality rate of an individual may experience a sudden jump at an unobservable time, representing, for instance, a shift in the firm's risk profile or a deterioration in an individual's health status. By employing a progressive enlargement of filtration, we integrate noisy observations of the hazard rate with default-related information. We characterise the filter, i.e. the conditional probability of the change-point given the information flow, as the unique strong solution to a stochastic differential equation driven by the innovation process enriched with the discontinuous component. A sensitivity analysis and a comparison of the filter's behaviour under various information structures are provided. Our framework further allows for the derivation of an explicit formula for the survival probability conditional on partial information. This result applies to the pricing of credit-sensitive financial instruments such as defaultable bonds, credit default swaps, and life insurance contracts. Finally, a numerical analysis illustrates how partial information leads to delayed adjustments in the estimation of the hazard rate and consequently to mispricing of credit-sensitive instruments when compared to a full-information setting.|\n", "2505.13179": "|**2025-05-19**|**Lattice thermal conductivity of 16 elemental metals from molecular dynamics simulations with a unified neuroevolution potential**|Shuo Cao, Ao Wang, Zheyong Fan et.al.|[2505.13179](http://arxiv.org/abs/2505.13179)|null|10 pages, 8 figures|Metals play a crucial role in heat management in electronic devices, such as integrated circuits, making it vital to understand heat transport in elementary metals and alloys. In this work, we systematically study phonon thermal transport in 16 metals using the efficient homogeneous nonequilibrium molecular dynamics (HNEMD) method and the recently developed unified neuroevolution potential version 1 (UNEP-v1) for 16 metals and their alloys. We compare our results with existing ones based on the Boltzmann transport equation (BTE) approach and find that our HNEMD results align well with BTE results obtained by considering phonon-phonon scattering only. By contrast, HNEMD results based on the conventional embedded-atom method potential show less satisfactory agreement with BTE ones. Given the high accuracy of the UNEP-v1 model demonstrated in various metal alloys, we anticipate that the HNEMD method combined with the UNEP-v1 model will be a promising tool for exploring phonon thermal transport properties in complex systems such as high-entropy alloys.|\n", "2505.13165": "|**2025-05-19**|**A parametric finite element method for a degenerate multi-phase Stefan problem with triple junctions**|Tokuhiro Eto, Harald Garcke, Robert N\u00fcrnberg et.al.|[2505.13165](http://arxiv.org/abs/2505.13165)|null|32 pages, 49 figures|In this study, we propose a parametric finite element method for a degenerate multi-phase Stefan problem with triple junctions. This model describes the energy-driven motion of a surface cluster whose distributional solution was studied by Garcke and Sturzenhecker. We approximate the weak formulation of this sharp interface model by an unfitted finite element method that uses parametric elements for the representation of the moving interfaces. We establish existence and uniqueness of the discrete solution and prove unconditional stability of the proposed scheme. Moreover, a modification of the original scheme leads to a structure-preserving variant, in that it conserves the discrete analogue of a quantity that is preserved by the classical solution. Some numerical results demonstrate the applicability of our introduced schemes.|\n", "2505.13132": "|**2025-05-19**|**Ocean wave spectrum reconstruction from HF radar data and its application to wave height estimation**|Kaede Watanabe, Toshiaki Yachimura, Tsubasa Terada et.al.|[2505.13132](http://arxiv.org/abs/2505.13132)|null||Real-time estimation of ocean wave heights using high-frequency (HF) radar has attracted great attention. This method offers the benefit of easy maintenance by virtue of its ground-based installation. However, it is adversely affected by issues such as low estimation accuracy. As described herein, we propose an algorithm based on the nonnegative sparse regularization method to estimate the energy distribution of the component waves, known as the ocean wave spectrum, from HF radar data. After proving a stability estimate of this algorithm, we perform numerical simulations to verify the proposed method's effectiveness.|\n", "2505.13097": "|**2025-05-19**|**An implicit regularized enthalpy Lattice Boltzmann Method for the Stefan problem**|Francky Luddens, Corentin Lothod\u00e9, Ionut Danaila et.al.|[2505.13097](http://arxiv.org/abs/2505.13097)|null||Solving the Stefan problem, also referred as the heat conduction problem with phase change, is a necessary step to solve phase change problems with convection. In this article, we are interested in using the Lattice Boltzmann Method (LBM) to solve the Stefan problem using a regularized total enthalpy model. The liquid fraction is treated as a nonlinear source/sink term, that involves the time derivative of the solution. The resulting non-linear system is solved using a Newton algorithm. By conserving the locality of the problem, this method is highly scalable, while keeping a high accuracy. The newly developed scheme is analyzed theoretically through a Chapman-Enskog expansion and illustrated numerically with 1D and 2D benchmarks.|\n", "2505.13086": "|**2025-05-19**|**Coupled integral equations method with open boundary conditions for calculation the characteristics of structured waveguides**|M. I. Ayzatsky et.al.|[2505.13086](http://arxiv.org/abs/2505.13086)|null|7 pages, 7 figures, 1 table|The results of modification of the CASCIE code aimed at implementing open boundary conditions are presented. The accelerator section developed at CERN was chosen as a prototype for the structured waveguide under testing. Results of testing the CASCIE-M code confirms that the implementation of matrix open boundary conditions gives possibility to consider the structure in which waves enter and exit without additional reflections from couplers. It was shown that the dependence of the reflection coefficient on frequency differs from the similar dependence for a waveguide with couplers. It does not have a regular sequence of minimum and maximum values associated with reflections from the couplers and the formation of resonance conditions. This indicates that the reflections are of a different nature and are associated with inhomogeneity. The proposed modification of the coupled integral equation method allows us to investigate the accuracy of the field expansion on which coupled mode theory can be constructed that describes structured waveguides.|\n", "2505.13068": "|**2025-05-19**|**Revisiting the Slip Boundary Condition: Surface Roughness as a Hidden Tuning Parameter**|Matthias Maier, Peter Munch, Murtazo Nazarov et.al.|[2505.13068](http://arxiv.org/abs/2505.13068)|null||In this paper, we investigate the effect of boundary surface roughness on numerical simulations of incompressible fluid flow past a cylinder in two and three spatial dimensions furnished with slip boundary conditions. The governing equations are approximated using a continuous finite element method, stabilized with a Galerkin least-squares approach.   Through a series of numerical experiments, we demonstrate that: $(i)$ the introduction of surface roughness through numerical discretization error, or mesh distortion, makes the potential flow solution unstable; $(ii)$ when numerical surface roughness and mesh distortion are minimized by using high-order isoparametric geometry mappings, a stable potential flow is obtained in both two and three dimensions; $(iii)$ numerical surface roughness, mesh distortion and refinement level can be used as control parameters to manipulate drag and lift forces resulting in numerical values spanning more than an order of magnitude.   Our results cast some doubt on the predictive capability of the slip boundary condition for wall modeling in turbulent simulations of incompressible flow.|\n", "2505.13042": "|**2025-05-19**|**An introduction to Neural Networks for Physicists**|G. Caf\u00e9 de Miranda, Gubio G. de Lima, Tiago de S. Farias et.al.|[2505.13042](http://arxiv.org/abs/2505.13042)|null|This work was submitted to the journal \"The Revista Brasileira de   Ensino de F\\'isica - RBEF\"|Machine learning techniques have emerged as powerful tools to tackle various challenges. The integration of machine learning methods with Physics has led to innovative approaches in understanding, controlling, and simulating physical phenomena. This article aims to provide a practical introduction to neural network and their basic concepts. It presents some perspectives on recent advances at the intersection of machine learning models with physical systems. We introduce practical material to guide the reader in taking their first steps in applying neural network to Physics problems. As an illustrative example, we provide four applications of increasing complexity for the problem of a simple pendulum, namely: parameter fitting of the pendulum's ODE for the small-angle approximation; Application of Physics-Inspired Neural Networks (PINNs) to find solutions of the pendulum's ODE in the small-angle regime; Autoencoders applied to an image dataset of the pendulum's oscillations for estimating the dimensionality of the parameter space in this physical system; and the use of Sparse Identification of Non-Linear Dynamics (SINDy) architectures for model discovery and analytical expressions for the nonlinear pendulum problem (large angles).|\n", "2505.12990": "|**2025-05-19**|**From Theory to Practice: Analyzing VQPM for Quantum Optimization of QUBO Problems**|Ammar Daskin et.al.|[2505.12990](http://arxiv.org/abs/2505.12990)|null|The simulation code that generates figures in the paper can be   downloaded from https://github.com/adaskin/vqpm|The variational quantum power method (VQPM), which adapts the classical power iteration algorithm for quantum settings, has shown promise for eigenvector estimation and optimization on quantum hardware. In this work, we provide a comprehensive theoretical and numerical analysis of VQPM by investigating its convergence, robustness, and qubit locking mechanisms. We present detailed strategies for applying VQPM to QUBO problems by leveraging these locking mechanisms. Based on the simulations for each strategy we have carried out, we give systematic guidelines for their practical applications. We also offer a simple numerical comparison with the quantum approximate optimization algorithm (QAOA) by running both algorithms on a set of trial problems. Our results indicate that VQPM can be employed as an effective quantum optimization algorithm on quantum computers for QUBO problems, and this work can serve as an initial guideline for such applications.|\n", "2505.12977": "|**2025-05-19**|**Regularized Model Predictive Control**|Komeil Nosrati, Juri Belikov, Aleksei Tepljakov et.al.|[2505.12977](http://arxiv.org/abs/2505.12977)|null||In model predictive control (MPC), the choice of cost-weighting matrices and designing the Hessian matrix directly affects the trade-off between rapid state regulation and minimizing the control effort. However, traditional MPC in quadratic programming relies on fixed design matrices across the entire horizon, which can lead to suboptimal performance. This letter presents a Riccati equation-based method for adjusting the design matrix within the MPC framework, which enhances real-time performance. We employ a penalized least-squares (PLS) approach to derive a quadratic cost function for a discrete-time linear system over a finite prediction horizon. Using the method of weighting and enforcing the constraint equation by introducing a large penalty parameter, we solve the constrained optimization problem and generate control inputs for forward-shifted horizons. This process yields a recursive PLS-based Riccati equation that updates the design matrix as a regularization term in each shift, forming the foundation of the regularized MPC (Re-MPC) algorithm. To accomplish this, we provide a convergence and stability analysis of the developed algorithm. Numerical analysis demonstrates its superiority over traditional methods by allowing Riccati equation-based adjustments.|\n", "2505.12958": "|**2025-05-19**|**Nanoindentation simulations for copper and tungsten with adaptive-precision potentials**|David Immel, Matous Mrovec, Ralf Drautz et.al.|[2505.12958](http://arxiv.org/abs/2505.12958)|null|16 pages, 16 figures|We perform nanoindentation simulations for both the prototypical face-centered cubic metal copper and the body-centered cubic metal tungsten with a new adaptive-precision description of interaction potentials including different accuracy and computational costs: We combine both a computationally efficient embedded atom method (EAM) potential and a precise but computationally less efficient machine learning potential based on the atomic cluster expansion (ACE) into an adaptive-precision (AP) potential tailored for the nanoindentation. The numerically expensive ACE potential is employed selectively only in regions of the computational cell where large accuracy is required. The comparison with pure EAM and pure ACE simulations shows that for Cu, all potentials yield similar dislocation morphologies under the indenter with only small quantitative differences. In contrast, markedly different plasticity mechanisms are observed for W in simulations performed with the central-force EAM potential compared to results obtained using the ACE potential which is able to describe accurately the angular character of bonding in W due to its half-filled d-band. All ACE-specific mechanisms are reproduced in the AP nanoindentation simulations, however, with a significant speedup of 20-30 times compared to the pure ACE simulations. Hence, the AP potential overcomes the performance gap between the precise ACE and the fast EAM potential by combining the advantages of both potentials.|\n", "2505.12944": "|**2025-05-19**|**CALM-PDE: Continuous and Adaptive Convolutions for Latent Space Modeling of Time-dependent PDEs**|Jan Hagnberger, Daniel Musekamp, Mathias Niepert et.al.|[2505.12944](http://arxiv.org/abs/2505.12944)|**[link](https://github.com/jhagnberger/calm-pde)**||Solving time-dependent Partial Differential Equations (PDEs) using a densely discretized spatial domain is a fundamental problem in various scientific and engineering disciplines, including modeling climate phenomena and fluid dynamics. However, performing these computations directly in the physical space often incurs significant computational costs. To address this issue, several neural surrogate models have been developed that operate in a compressed latent space to solve the PDE. While these approaches reduce computational complexity, they often use Transformer-based attention mechanisms to handle irregularly sampled domains, resulting in increased memory consumption. In contrast, convolutional neural networks allow memory-efficient encoding and decoding but are limited to regular discretizations. Motivated by these considerations, we propose CALM-PDE, a model class that efficiently solves arbitrarily discretized PDEs in a compressed latent space. We introduce a novel continuous convolution-based encoder-decoder architecture that uses an epsilon-neighborhood-constrained kernel and learns to apply the convolution operator to adaptive and optimized query points. We demonstrate the effectiveness of CALM-PDE on a diverse set of PDEs with both regularly and irregularly sampled spatial domains. CALM-PDE is competitive with or outperforms existing baseline methods while offering significant improvements in memory and inference time efficiency compared to Transformer-based methods.|\n", "2505.12940": "|**2025-05-19**|**Multi-Level Monte Carlo Training of Neural Operators**|James Rowbottom, Stefania Fresca, Pietro Lio et.al.|[2505.12940](http://arxiv.org/abs/2505.12940)|null|18 pages, 8 figures|Operator learning is a rapidly growing field that aims to approximate nonlinear operators related to partial differential equations (PDEs) using neural operators. These rely on discretization of input and output functions and are, usually, expensive to train for large-scale problems at high-resolution. Motivated by this, we present a Multi-Level Monte Carlo (MLMC) approach to train neural operators by leveraging a hierarchy of resolutions of function dicretization. Our framework relies on using gradient corrections from fewer samples of fine-resolution data to decrease the computational cost of training while maintaining a high level accuracy. The proposed MLMC training procedure can be applied to any architecture accepting multi-resolution data. Our numerical experiments on a range of state-of-the-art models and test-cases demonstrate improved computational efficiency compared to traditional single-resolution training approaches, and highlight the existence of a Pareto curve between accuracy and computational time, related to the number of samples per resolution.|\n", "2505.12919": "|**2025-05-19**|**RGNMR: A Gauss-Newton method for robust matrix completion with theoretical guarantees**|Eilon Vaknin Laufer, Boaz Nadler et.al.|[2505.12919](http://arxiv.org/abs/2505.12919)|null||Recovering a low rank matrix from a subset of its entries, some of which may be corrupted, is known as the robust matrix completion (RMC) problem. Existing RMC methods have several limitations: they require a relatively large number of observed entries; they may fail under overparametrization, when their assumed rank is higher than the correct one; and many of them fail to recover even mildly ill-conditioned matrices. In this paper we propose a novel RMC method, denoted $\\texttt{RGNMR}$, which overcomes these limitations. $\\texttt{RGNMR}$ is a simple factorization-based iterative algorithm, which combines a Gauss-Newton linearization with removal of entries suspected to be outliers. On the theoretical front, we prove that under suitable assumptions, $\\texttt{RGNMR}$ is guaranteed exact recovery of the underlying low rank matrix. Our theoretical results improve upon the best currently known for factorization-based methods. On the empirical front, we show via several simulations the advantages of $\\texttt{RGNMR}$ over existing RMC methods, and in particular its ability to handle a small number of observed entries, overparameterization of the rank and ill-conditioned matrices.|\n", "2505.15777": "|**2025-05-21**|**Projection-Based Correction for Enhancing Deep Inverse Networks**|Jorge Bacca et.al.|[2505.15777](http://arxiv.org/abs/2505.15777)|null||Deep learning-based models have demonstrated remarkable success in solving illposed inverse problems; however, many fail to strictly adhere to the physical constraints imposed by the measurement process. In this work, we introduce a projection-based correction method to enhance the inference of deep inverse networks by ensuring consistency with the forward model. Specifically, given an initial estimate from a learned reconstruction network, we apply a projection step that constrains the solution to lie within the valid solution space of the inverse problem. We theoretically demonstrate that if the recovery model is a well-trained deep inverse network, the solution can be decomposed into range-space and null-space components, where the projection-based correction reduces to an identity transformation. Extensive simulations and experiments validate the proposed method, demonstrating improved reconstruction accuracy across diverse inverse problems and deep network architectures.|\n", "2505.15771": "|**2025-05-21**|**Elasto-acoustic wave propagation in geophysical media using hybrid high-order methods on general meshes**|Romain Mottier, Alexandre Ern, Laurent Guillot et.al.|[2505.15771](http://arxiv.org/abs/2505.15771)|null||Hybrid high-order (HHO) methods are numerical methods characterized by several interesting properties such as local conservativity, geometric flexibility and high-order accuracy. Here, HHO schemes are studied for the space semi-discretization of coupled elasto-acoustic waves in the time domain using a first-order formulation. Explicit and singly diagonal implicit Runge--Kutta (ERK & SDIRK) schemes are used for the time discretization. We show that an efficient implementation of explicit (resp. implicit) time schemes calls for a static condensation of the face (resp. cell) unknowns. Crucially, both static condensation procedures only involve block-diagonal matrices. Then, we provide numerical estimates for the CFL stability limit of ERK schemes and present a comparative study on the efficiency of explicit versus implicit schemes. Our findings indicate that implicit time schemes remain competitive in many situations. Finally, simulations in a 2D realistic geophysical configuration are performed, illustrating the geometrical flexibility of the HHO method: both hybrid (triangular and quadrangular) and nonconforming (with hanging nodes) meshes are easily handled, delivering results of comparable accuracy to a reference spectral element software based on tensorized elements.|\n", "2505.15665": "|**2025-05-21**|**Analysis and Simulation of Generalized Langevin Equations with Non-Gaussian Orthogonal Forces**|Henrik Kiefer, Benjamin J. A. H\u00e9ry, Lucas Tepper et.al.|[2505.15665](http://arxiv.org/abs/2505.15665)|null|10 pages, 3 figures, 18 pages of Supplementary Material attached at   the end|The generalized Langevin equation (GLE) is a useful framework for analyzing and modeling the dynamics of many-body systems in terms of low-dimensional reaction coordinates, with its specific form determined by the choice of projection formalism. We compare parameters derived from different GLE formulations using molecular dynamics simulations of butane's dihedral angle dynamics. Our analysis reveals non-Gaussian contributions of the orthogonal force in different GLEs, being most enhanced for the Mori-GLE, where all non-linearities are relegated to the orthogonal force. We establish a simulation technique that correctly accounts for non-Gaussian orthogonal forces, which is critical for accurately predicting dihedral-angle mean first-passage times. We find that the accuracy of GLE simulations depends significantly on the chosen GLE formalism; the Mori-GLE offers the most numerically robust framework for capturing the statistical observables of the dihedral angle dynamics, provided the correct non-Gaussian orthogonal force distribution is used.|\n", "2505.15661": "|**2025-05-21**|**Deep greedy unfolding: Sorting out argsorting in greedy sparse recovery algorithms**|Sina Mohammad-Taheri, Matthew J. Colbrook, Simone Brugiapaglia et.al.|[2505.15661](http://arxiv.org/abs/2505.15661)|null||Gradient-based learning imposes (deep) neural networks to be differentiable at all steps. This includes model-based architectures constructed by unrolling iterations of an iterative algorithm onto layers of a neural network, known as algorithm unrolling. However, greedy sparse recovery algorithms depend on the non-differentiable argsort operator, which hinders their integration into neural networks. In this paper, we address this challenge in Orthogonal Matching Pursuit (OMP) and Iterative Hard Thresholding (IHT), two popular representative algorithms in this class. We propose permutation-based variants of these algorithms and approximate permutation matrices using \"soft\" permutation matrices derived from softsort, a continuous relaxation of argsort. We demonstrate -- both theoretically and numerically -- that Soft-OMP and Soft-IHT, as differentiable counterparts of OMP and IHT and fully compatible with neural network training, effectively approximate these algorithms with a controllable degree of accuracy. This leads to the development of OMP- and IHT-Net, fully trainable network architectures based on Soft-OMP and Soft-IHT, respectively. Finally, by choosing weights as \"structure-aware\" trainable parameters, we connect our approach to structured sparse recovery and demonstrate its ability to extract latent sparsity patterns from data.|\n", "2505.15640": "|**2025-05-21**|**Damping optimization of discrete mechanical systems -- rod/string model**|Ninoslav Truhar, Kre\u0161imir Veseli\u0107 et.al.|[2505.15640](http://arxiv.org/abs/2505.15640)|null||This paper investigates two optimization criteria for damping optimization in a multi-body oscillator system with arbitrary degrees of freedom ($n$), resembling string/rod free vibrations. The total average energy over all possible initial data and the total average displacement over all possible initial data. Our first result shows that both criteria are equivalent to the trace minimization of the solution of the Lyapunov equation with different right-hand sides. As the second result, we prove that in the case of damping with one damper, for the discrete system, the minimal trace for each criterion can be expressed as a linear or cubic function of the dimension $n$. Consequently, the optimal damping position is determined solely by the number of dominant eigenfrequencies and the optimal viscosity, independent of the dimension $n$, offering efficient damping optimization in discrete systems. The paper concludes with numerical examples illustrating the presented theoretical framework and results.|\n", "2505.15584": "|**2025-05-21**|**Improved power methods for computing eigenvalues of dual quaternion Hermitian matrices**|Yongjun Chen, Liping Zhang et.al.|[2505.15584](http://arxiv.org/abs/2505.15584)|null||This paper investigates the eigenvalue computation problem of the dual quaternion Hermitian matrix closely related to multi-agent group control. Recently, power method was proposed by Cui and Qi in Journal of Scientific Computing, 100 (2024) to solve such problem. Recognizing that the convergence rate of power method is slow due to its dependence on the eigenvalue distribution, we propose two improved versions of power method based on dual complex adjoint matrices and Aitken extrapolation, named DCAM-PM and ADCAM-PM. They achieve notable efficiency improvements and demonstrate significantly faster convergence. However, power method may be invalid for dual quaternion Hermitian matrices with eigenvalues having identical standard parts but distinct dual parts. To overcome this disadvantage, utilizing the eigen-decomposition properties of dual complex adjoint matrix, we propose a novel algorithm EDDCAM-EA which surpasses the power method in both accuracy and speed. Application to eigenvalue computations of dual quaternion Hermitian matrices in multi-agent formation control and numerical experiments highlight the remarkable accuracy and speed of our proposed algorithms.|\n", "2505.15538": "|**2025-05-21**|**Machine learning-based parameter optimization for M\u00fcntz spectral methods**|Wei Zeng, Chuanju Xu, Yiming Lu et.al.|[2505.15538](http://arxiv.org/abs/2505.15538)|null||Spectral methods employing non-standard polynomial bases, such as M\\\"untz polynomials, have proven effective for accurately solving problems with solutions exhibiting low regularity, notably including sub-diffusion equations. However, due to the absence of theoretical guidance, the key parameters controlling the exponents of M\\\"untz polynomials are usually determined empirically through extensive numerical experiments, leading to a time-consuming tuning process. To address this issue, we propose a novel machine learning-based optimization framework for the M\\\"untz spectral method. As an illustrative example, we optimize the parameter selection for solving time-fractional partial differential equations (PDEs). Specifically, an artificial neural network (ANN) is employed to predict optimal parameter values based solely on the time-fractional order as input. The ANN is trained by minimizing solution errors on a one-dimensional time-fractional convection-diffusion equation featuring manufactured exact solutions that manifest singularities of varying intensity, covering a comprehensive range of sampled fractional orders. Numerical results for time-fractional PDEs in both one and two dimensions demonstrate that the ANN-based parameter prediction significantly improves the accuracy of the M\\\"untz spectral method. Moreover, the trained ANN generalizes effectively from one-dimensional to two-dimensional cases, highlighting its robustness across spatial dimensions. Additionally, we verify that the ANN substantially outperforms traditional function approximators, such as spline interpolation, in both prediction accuracy and training efficiency. The proposed optimization framework can be extended beyond fractional PDEs, offering a versatile and powerful approach for spectral methods applied to various low-regularity problems.|\n", "2505.15535": "|**2025-05-21**|**Matrix-Free Methods for Finite-Strain Elasticity: Automatic Code Generation with No Performance Overhead**|Micha\u0142 Wichrowski, Mohsen Rezaee-Hajidehi, Jo\u017ee Korelc et.al.|[2505.15535](http://arxiv.org/abs/2505.15535)|null||This study explores matrix-free tangent evaluations in finite-strain elasticity with the use of automatically-generated code for the quadrature-point level calculations. The code generation is done via automatic differentiation (AD) with AceGen. We compare hand-written and AD-generated codes under two computing strategies: on-the-fly evaluation and caching intermediate results. The comparison reveals that the AD-generated code achieves superior performance in matrix-free computations.|\n", "2505.15407": "|**2025-05-21**|**Efficient Differentiable Approximation of Generalized Low-rank Regularization**|Naiqi Li, Yuqiu Xie, Peiyuan Liu et.al.|[2505.15407](http://arxiv.org/abs/2505.15407)|**[link](https://github.com/naiqili/edlrr)**|Accepted by IJCAI-25|Low-rank regularization (LRR) has been widely applied in various machine learning tasks, but the associated optimization is challenging. Directly optimizing the rank function under constraints is NP-hard in general. To overcome this difficulty, various relaxations of the rank function were studied. However, optimization of these relaxed LRRs typically depends on singular value decomposition, which is a time-consuming and nondifferentiable operator that cannot be optimized with gradient-based techniques. To address these challenges, in this paper we propose an efficient differentiable approximation of the generalized LRR. The considered LRR form subsumes many popular choices like the nuclear norm, the Schatten-$p$ norm, and various nonconvex relaxations. Our method enables LRR terms to be appended to loss functions in a plug-and-play fashion, and the GPU-friendly operations enable efficient and convenient implementation. Furthermore, convergence analysis is presented, which rigorously shows that both the bias and the variance of our rank estimator rapidly reduce with increased sample size and iteration steps. In the experimental study, the proposed method is applied to various tasks, which demonstrates its versatility and efficiency. Code is available at https://github.com/naiqili/EDLRR.|\n", "2505.15283": "|**2025-05-21**|**Quantization of Probability Distributions via Divide-and-Conquer: Convergence and Error Propagation under Distributional Arithmetic Operations**|Bilgesu Arif Bilgin, Olof Hallqvist Elias, Michael Selby et.al.|[2505.15283](http://arxiv.org/abs/2505.15283)|null|31 pages, 8 figures. Comments welcome!|This article studies a general divide-and-conquer algorithm for approximating continuous one-dimensional probability distributions with finite mean. The article presents a numerical study that compares pre-existing approximation schemes with a special focus on the stability of the discrete approximations when they undergo arithmetic operations. The main results are a simple upper bound of the approximation error in terms of the Wasserstein-1 distance that is valid for all continuous distributions with finite mean. In many use-cases, the studied method achieve optimal rate of convergence, and numerical experiments show that the algorithm is more stable than pre-existing approximation schemes in the context of arithmetic operations.|\n", "2505.15106": "|**2025-05-21**|**A coupled HDG discretization for the interaction between acoustic and elastic waves**|Fernando Artaza-Covarrubias, Tonatiuh S\u00e1nchez-Vizuet, Manuel Solano et.al.|[2505.15106](http://arxiv.org/abs/2505.15106)|null||We propose and analyze an HDG scheme for the Laplace-domain interaction between a transient acoustic wave and a bounded elastic solid embedded in an unbounded fluid medium. Two mixed variables (the stress tensor and the velocity of the acoustic wave) are included while the symmetry of the stress tensor is imposed weakly by considering the antisymmetric part of the strain tensor (the spin or vorticity tensor) as an additional unknown. The optimal convergence of the method is demonstrated theoretically and numerical results confirming the theoretical prediction are presented.|\n", "2505.15099": "|**2025-05-21**|**Runge-Kutta Methods and Stiff Order Conditions for Semilinear ODEs**|Steven B. Roberts, David Shirokoff, Abhijit Biswas et.al.|[2505.15099](http://arxiv.org/abs/2505.15099)|null||Classical convergence theory of Runge-Kutta methods assumes that the time step is small relative to the Lipschitz constant of the ordinary differential equation (ODE). For stiff problems, that assumption is often violated, and a problematic degradation in accuracy, known as order reduction, can arise. High stage order methods can avoid order reduction, but they must be fully implicit. For linear problems, weaker stiff order conditions exist and are compatible with computationally efficient methods, i.e., explicit or diagonally implicit. This work develops a new theory of stiff order conditions and convergence for semilinear ODEs, consisting of a stiff linear term and a non-stiff nonlinear term. New semilinear order conditions are formulated in terms of orthogonality relations enumerated by rooted trees. Novel, optimized diagonally implicit methods are constructed that satisfy these semilinear conditions. Numerical results demonstrate that for a broad class of relevant nonlinear test problems, these new methods successfully mitigate order reduction and yield highly accurate numerical approximations.|\n", "2505.14926": "|**2025-05-20**|**Pathobiological Dictionary Defining Pathomics and Texture Features: Addressing Understandable AI Issues in Personalized Liver Cancer; Dictionary Version LCP1.0**|Mohammad R. Salmanpour, Seyed Mohammad Piri, Somayeh Sadat Mehrnia et.al.|[2505.14926](http://arxiv.org/abs/2505.14926)|null|29 pages, 4 figures and 1 table|Artificial intelligence (AI) holds strong potential for medical diagnostics, yet its clinical adoption is limited by a lack of interpretability and generalizability. This study introduces the Pathobiological Dictionary for Liver Cancer (LCP1.0), a practical framework designed to translate complex Pathomics and Radiomics Features (PF and RF) into clinically meaningful insights aligned with existing diagnostic workflows. QuPath and PyRadiomics, standardized according to IBSI guidelines, were used to extract 333 imaging features from hepatocellular carcinoma (HCC) tissue samples, including 240 PF-based-cell detection/intensity, 74 RF-based texture, and 19 RF-based first-order features. Expert-defined ROIs from the public dataset excluded artifact-prone areas, and features were aggregated at the case level. Their relevance to the WHO grading system was assessed using multiple classifiers linked with feature selectors. The resulting dictionary was validated by 8 experts in oncology and pathology. In collaboration with 10 domain experts, we developed a Pathobiological dictionary of imaging features such as PFs and RF. In our study, the Variable Threshold feature selection algorithm combined with the SVM model achieved the highest accuracy (0.80, P-value less than 0.05), selecting 20 key features, primarily clinical and pathomics traits such as Centroid, Cell Nucleus, and Cytoplasmic characteristics. These features, particularly nuclear and cytoplasmic, were strongly associated with tumor grading and prognosis, reflecting atypia indicators like pleomorphism, hyperchromasia, and cellular orientation.The LCP1.0 provides a clinically validated bridge between AI outputs and expert interpretation, enhancing model transparency and usability. Aligning AI-derived features with clinical semantics supports the development of interpretable, trustworthy diagnostic tools for liver cancer pathology.|\n", "2505.14909": "|**2025-05-20**|**Fast Newton Transform: Interpolation in Downward Closed Polynomial Spaces**|Phil-Alexander Hofmann, Damar Wicaksono, Michael Hecht et.al.|[2505.14909](http://arxiv.org/abs/2505.14909)|null||We present the Fast Newton Transform (FNT), an algorithm for performing $m$-variate Newton interpolation in downward closed polynomial spaces with time complexity $\\mathcal{O}(|A|m\\overline{n})$. Here, $A$ is a downward closed set of cardinality $|A|$ equal to the dimension of the associated downward closed polynomial space $\\Pi_A$, where $\\overline{n}$ denotes the mean of the maximum polynomial degrees across the spatial dimensions. For functions being analytic in an open Bernstein poly-ellipse, geometric approximation rates apply when interpolating in non-tensorial Leja-ordered Chebyshev-Lobatto grids or Leja nodes. To mitigate the curse of dimensionality, we utilize $\\ell^p$-sets, with the Euclidean case $(p=2)$ turning out to be the pivotal choice, leading to $|A|/(n+1)^m \\in \\mathcal{O}(e^{-m})$. Expanding non-periodic functions, the FNT complements the approximation capabilities of the Fast Fourier Transform (FFT). Choosing $\\ell^2$-sets for $A$ renders the FNT time complexity to be less than the FFT time complexity $\\mathcal{O}((n+1)^m m \\log(n))$ in a range of $n$, behaving as $\\mathcal{O}(m e^m)$. Maintaining this advantage true for the differentials, the FNT sets a new standard in $m$-variate interpolation and approximation practice.|\n", "2505.14606": "|**2025-05-20**|**Electrostatics from Laplacian Eigenbasis for Neural Network Interatomic Potentials**|Maksim Zhdanov, Vladislav Kurenkov et.al.|[2505.14606](http://arxiv.org/abs/2505.14606)|**[link](https://github.com/dunnolab/phi-module)**||Recent advances in neural network interatomic potentials have emerged as a promising research direction. However, popular deep learning models often lack auxiliary constraints grounded in physical laws, which could accelerate training and improve fidelity through physics-based regularization. In this work, we introduce $\\Phi$-Module, a universal plugin module that enforces Poisson's equation within the message-passing framework to learn electrostatic interactions in a self-supervised manner. Specifically, each atom-wise representation is encouraged to satisfy a discretized Poisson's equation, making it possible to acquire a potential $\\boldsymbol{\\phi}$ and a corresponding charge density $\\boldsymbol{\\rho}$ linked to the learnable Laplacian eigenbasis coefficients of a given molecular graph. We then derive an electrostatic energy term, crucial for improved total energy predictions. This approach integrates seamlessly into any existing neural potential with insignificant computational overhead. Experiments on the OE62 and MD22 benchmarks confirm that models combined with $\\Phi$-Module achieve robust improvements over baseline counterparts. For OE62 error reduction ranges from 4.5\\% to 17.8\\%, and for MD22, baseline equipped with $\\Phi$-Module achieves best results on 5 out of 14 cases. Our results underscore how embedding a first-principles constraint in neural interatomic potentials can significantly improve performance while remaining hyperparameter-friendly, memory-efficient and lightweight in training. Code will be available at \\href{https://github.com/dunnolab/phi-module}{dunnolab/phi-module}.|\n", "2505.14555": "|**2025-05-20**|**Physics-Guided Learning of Meteorological Dynamics for Weather Downscaling and Forecasting**|Yingtao Luo, Shikai Fang, Binqing Wu et.al.|[2505.14555](http://arxiv.org/abs/2505.14555)|**[link](https://github.com/yingtaoluo/phydl-nwp)**|Published/Accepted in KDD 2025 (February Cycle)|Weather forecasting is essential but remains computationally intensive and physically incomplete in traditional numerical weather prediction (NWP) methods. Deep learning (DL) models offer efficiency and accuracy but often ignore physical laws, limiting interpretability and generalization. We propose PhyDL-NWP, a physics-guided deep learning framework that integrates physical equations with latent force parameterization into data-driven models. It predicts weather variables from arbitrary spatiotemporal coordinates, computes physical terms via automatic differentiation, and uses a physics-informed loss to align predictions with governing dynamics. PhyDL-NWP enables resolution-free downscaling by modeling weather as a continuous function and fine-tunes pre-trained models with minimal overhead, achieving up to 170x faster inference with only 55K parameters. Experiments show that PhyDL-NWP improves both forecasting performance and physical consistency.|\n", "2505.14399": "|**2025-05-20**|**Accelerating multigrid with streaming chiral SVD for Wilson fermions in lattice QCD**|Travis Whyte, Andreas Stathopoulos, Eloy Romero et.al.|[2505.14399](http://arxiv.org/abs/2505.14399)|null|16 pages, 4 algorithms, 7 figures, 3 tables. arXiv admin note:   substantial text overlap with arXiv:2502.03091|A modification to the setup algorithm for the multigrid preconditioner of Wilson fermions in lattice QCD is presented. A larger basis of test vectors than that used in conventional multigrid is calculated by the smoother and truncated by singular value decomposition on the chiral components of the test vectors. The truncated basis is used to form the prolongation and restriction matrices of the multigrid hierarchy. This modification of the setup method is demonstrated to increase the convergence of linear solvers on an anisotropic lattice with $m_{\\pi} \\approx 239$ MeV from the Hadron Spectrum Collaboration and an isotropic lattice with $m_{\\pi} \\approx 220$ MeV from the MILC Collaboration. The lattice volume dependence of the method is also examined. Increasing the number of test vectors improves speedup up to a point, but storing these vectors becomes impossible in limited memory resources such as GPUs. To address storage cost, we implement a \\emph{streaming} singular value decomposition of the basis of test vectors on the chiral components and demonstrate a decrease in the number of fine level iterations by a factor of 1.7 for $m_q \\approx m_{crit}$.|\n", "2505.14373": "|**2025-05-20**|**Time domain analysis of microstructured materials through the reduced relaxed micromorphic model**|Gianluca Rizzi, Angela Madeo et.al.|[2505.14373](http://arxiv.org/abs/2505.14373)|null|28 pages|Microstructured materials, such as architected metamaterials and phononic crystals, exhibit complex wave propagation phenomena due to their internal structure. While full-scale numerical simulations can capture these effects, they are computationally demanding, especially in time-domain analyses. To overcome this limitation, effective continuum models have been developed to approximate the macroscopic behavior of these materials while retaining key microscale effects. In this work we investigate the time-domain dynamic response of microstructured materials and focus on their effective micromorphic counterparts. We compare direct numerical simulations of discrete microstructures with predictions from micromorphic models to assess their accuracy in capturing transient wave phenomena. Our findings provide new insights into the applicability and limitations of micromorphic models in time-dependent analyses, contributing to the development of improved predictive tools for metamaterial design and engineering applications.|\n", "2505.14372": "|**2025-05-20**|**phaser: An all-in-one package for (multislice) electron ptychography**|Colin Gilgenbach, James M. LeBeau et.al.|[2505.14372](http://arxiv.org/abs/2505.14372)|**[link](https://github.com/hexane360/phaser)**||Electron ptychography is a groundbreaking technique for the advanced characterization of materials. Successful ptychography relies on robust implementations of reconstruction algorithms with process raw data into phase images. Current software has enabled deep sub-angstrom multislice electron ptychographic reconstructions, but reconstructions remain challenging and computationally expensive. In addition, current software generally lacks the modularity to act as a platform for the development and testing of new solver components and algorithms. For instance, algorithms based on gradient descent and autodifferentiation promise faster reconstructions with higher quality, but it is difficult to implement these algorithms in existing software. Here, we present \\texttt{phaser}, an open source Python package which provides a unified interface to both conventional and gradient descent based ptychographic algorithms. Reconstructions are specified in a declarative format, and can be run from a command line, Jupyter notebook, or web interface. Multiple computational backends are supported to provide maximum flexibility. With the JAX computational backend, a 6x improvement in iteration speed is achieved over a state-of-the-art package, \\texttt{fold\\_slice}. Additionally, \\texttt{phaser}'s gradient descent solver achieves improved reconstruction quality.|\n", "2505.14262": "|**2025-05-20**|**Strong convergence in the infinite horizon of numerical methods for stochastic delay differential equations**|Yudong Wang, Hongjiong Tian et.al.|[2505.14262](http://arxiv.org/abs/2505.14262)|null|31 pages,6 figures. arXiv admin note: text overlap with   arXiv:2505.12883|In this work, we present a general technique for establishing the strong convergence of numerical methods for stochastic delay differential equations (SDDEs) in the infinite horizon. This technique can also be extended to analyze certain continuous function-valued segment processes associated with the numerical methods, facilitating the numerical approximation of invariant measures of SDDEs. To illustrate the application of these results, we specifically investigate the backward and truncated Euler-Maruyama methods. Several numerical experiments are provided to demonstrate the theoretical results.|\n", "2505.17004": "|**2025-05-22**|**Guided Diffusion Sampling on Function Spaces with Applications to PDEs**|Jiachen Yao, Abbas Mammadov, Julius Berner et.al.|[2505.17004](http://arxiv.org/abs/2505.17004)|null||We propose a general framework for conditional sampling in PDE-based inverse problems, targeting the recovery of whole solutions from extremely sparse or noisy measurements. This is accomplished by a function-space diffusion model and plug-and-play guidance for conditioning. Our method first trains an unconditional discretization-agnostic denoising model using neural operator architectures. At inference, we refine the samples to satisfy sparse observation data via a gradient-based guidance mechanism. Through rigorous mathematical analysis, we extend Tweedie's formula to infinite-dimensional Hilbert spaces, providing the theoretical foundation for our posterior sampling approach. Our method (FunDPS) accurately captures posterior distributions in function spaces under minimal supervision and severe data scarcity. Across five PDE tasks with only 3% observation, our method achieves an average 32% accuracy improvement over state-of-the-art fixed-resolution diffusion baselines while reducing sampling steps by 4x. Furthermore, multi-resolution fine-tuning ensures strong cross-resolution generalizability. To the best of our knowledge, this is the first diffusion-based framework to operate independently of discretization, offering a practical and flexible solution for forward and inverse problems in the context of PDEs. Code is available at https://github.com/neuraloperator/FunDPS|\n", "2505.16992": "|**2025-05-22**|**PICT -- A Differentiable, GPU-Accelerated Multi-Block PISO Solver for Simulation-Coupled Learning Tasks in Fluid Dynamics**|Aleksandra Franz, Hao Wei, Luca Guastoni et.al.|[2505.16992](http://arxiv.org/abs/2505.16992)|null|Source code at https://github.com/tum-pbs/PICT|Despite decades of advancements, the simulation of fluids remains one of the most challenging areas of in scientific computing. Supported by the necessity of gradient information in deep learning, differentiable simulators have emerged as an effective tool for optimization and learning in physics simulations. In this work, we present our fluid simulator PICT, a differentiable pressure-implicit solver coded in PyTorch with Graphics-processing-unit (GPU) support. We first verify the accuracy of both the forward simulation and our derived gradients in various established benchmarks like lid-driven cavities and turbulent channel flows before we show that the gradients provided by our solver can be used to learn complicated turbulence models in 2D and 3D. We apply both supervised and unsupervised training regimes using physical priors to match flow statistics. In particular, we learn a stable sub-grid scale (SGS) model for a 3D turbulent channel flow purely based on reference statistics. The low-resolution corrector trained with our solver runs substantially faster than the highly resolved references, while keeping or even surpassing their accuracy. Finally, we give additional insights into the physical interpretation of different solver gradients, and motivate a physically informed regularization technique. To ensure that the full potential of PICT can be leveraged, it is published as open source: https://github.com/tum-pbs/PICT.|\n", "2505.16989": "|**2025-05-22**|**Spin adaptation of the cumulant expansions of reduced density matrices**|Julia Liebert, Christian Schilling, David A. Mazziotti et.al.|[2505.16989](http://arxiv.org/abs/2505.16989)|null||We develop a systematic framework for the spin adaptation of the cumulants of p-particle reduced density matrices (RDMs), with explicit constructions for p = 1 to 3. These spin-adapted cumulants enable rigorous treatment of both S_z and S^2 symmetries in quantum systems, providing a foundation for spin-resolved electronic structure methods. We show that complete spin adaptation -- referred to as complete S-representability -- can be enforced by constraining the variances of S_z and S^2, which require the 2-RDM and 4-RDM, respectively. Importantly, the cumulants of RDMs scale linearly with system size -- size-extensive -- making them a natural object for incorporating spin symmetries in scalable electronic structure theories. The developed formalism is applicable to density-based methods (DFT), one-particle RDM functional theories (RDMFT), and two-particle RDM methods. We further extend the approach to spin-orbit-coupled systems via total angular momentum adaptation. Beyond spin, the framework enables the adaptation of RDM theories to additional symmetries through the construction of suitable irreducible tensor operators.|\n", "2505.16970": "|**2025-05-22**|**Horospherically Convex Optimization on Hadamard Manifolds Part I: Analysis and Algorithms**|Christopher Criscitiello, Jungbin Kim et.al.|[2505.16970](http://arxiv.org/abs/2505.16970)|null||Geodesic convexity (g-convexity) is a natural generalization of convexity to Riemannian manifolds. However, g-convexity lacks many desirable properties satisfied by Euclidean convexity. For instance, the natural notions of half-spaces and affine functions are themselves not g-convex. Moreover, recent studies have shown that the oracle complexity of geodesically convex optimization necessarily depends on the curvature of the manifold (Criscitiello and Boumal, 2022; Criscitiello and Boumal, 2023; Hamilton and Moitra, 2021), a computational bottleneck for several problems, e.g., tensor scaling. Recently, Lewis et al. (2024) addressed this challenge by proving curvature-independent convergence of subgradient descent, assuming horospherical convexity of the objective's sublevel sets. Using a similar idea, we introduce a generalization of convex functions to Hadamard manifolds, utilizing horoballs and Busemann functions as building blocks (as proxies for half-spaces and affine functions). We refer to this new notion as horospherical convexity (h-convexity). We provide algorithms for both nonsmooth and smooth h-convex optimization, which have curvature-independent guarantees exactly matching those from Euclidean space; this includes generalizations of subgradient descent and Nesterov's accelerated method. Motivated by applications, we extend these algorithms and their convergence rates to minimizing a sum of horospherically convex functions, assuming access to a weighted-Fr\\'echet-mean oracle.|\n", "2505.16937": "|**2025-05-22**|**Quasi-optimal hierarchically semi-separable matrix approximation**|Noah Amsel, Tyler Chen, Feyza Duman Keles et.al.|[2505.16937](http://arxiv.org/abs/2505.16937)|null||We present a randomized algorithm for producing a quasi-optimal hierarchically semi-separable (HSS) approximation to an $N\\times N$ matrix $A$ using only matrix-vector products with $A$ and $A^T$. We prove that, using $O(k \\log(N/k))$ matrix-vector products and ${O}(N k^2 \\log(N/k))$ additional runtime, the algorithm returns an HSS matrix $B$ with rank-$k$ blocks whose expected Frobenius norm error $\\mathbb{E}[\\|A - B\\|_F^2]$ is at most $O(\\log(N/k))$ times worse than the best possible approximation error by an HSS rank-$k$ matrix. In fact, the algorithm we analyze in a simple modification of an empirically effective method proposed by [Levitt & Martinsson, SISC 2024]. As a stepping stone towards our main result, we prove two results that are of independent interest: a similar guarantee for a variant of the algorithm which accesses $A$'s entries directly, and explicit error bounds for near-optimal subspace approximation using projection-cost-preserving sketches. To the best of our knowledge, our analysis constitutes the first polynomial-time quasi-optimality result for HSS matrix approximation, both in the explicit access model and the matrix-vector product query model.|\n", "2505.16932": "|**2025-05-22**|**The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm**|Noah Amsel, David Persson, Christopher Musco et.al.|[2505.16932](http://arxiv.org/abs/2505.16932)|null||Computing the polar decomposition and the related matrix sign function, has been a well-studied problem in numerical analysis for decades. More recently, it has emerged as an important subroutine in deep learning, particularly within the Muon optimization framework. However, the requirements in this setting differ significantly from those of traditional numerical analysis. In deep learning, methods must be highly efficient and GPU-compatible, but high accuracy is often unnecessary. As a result, classical algorithms like Newton-Schulz (which suffers from slow initial convergence) and methods based on rational functions (which rely on QR decompositions or matrix inverses) are poorly suited to this context. In this work, we introduce Polar Express, a GPU-friendly algorithm for computing the polar decomposition. Like classical polynomial methods such as Newton-Schulz, our approach uses only matrix-matrix multiplications, making it GPU-compatible. Motivated by earlier work of Chen & Chow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule at each iteration by solving a minimax optimization problem, and we prove that it enjoys a strong worst-case optimality guarantee. This property ensures both rapid early convergence and fast asymptotic convergence. We also address finite-precision issues, making it stable in bfloat16 in practice. We apply Polar Express within the Muon optimization framework and show consistent improvements in validation loss on large-scale models such as GPT-2, outperforming recent alternatives across a range of learning rates.|\n", "2505.16906": "|**2025-05-22**|**Higher order Jacobi method for solving a system of linear equations**|Nithin Kumar Goona, Lama Tarsissi et.al.|[2505.16906](http://arxiv.org/abs/2505.16906)|null|12 pages, 3 figures|This work proposes a higher-order iterative framework for solving matrix equations, inspired by the structure and functionality of neural networks. A modification of the classical Jacobi method is introduced to compute higher-order coefficient matrices through matrix-matrix multiplications. The resulting method, termed the higher-order Jacobi method, structurally resembles a shallow linear network and allows direct computation of the inverse of the coefficient matrix. Building on this, an iterative scheme is developed that, once the network parameters are determined for a known system, enables efficient resolution of system variations without re-computing the coefficients. This iterative process naturally assumes the form of a deep recurrent neural network. The proposed approach moves beyond conventional Physics-Informed Neural Networks (PINNs) by providing an explicit, training-free definition of network parameters rooted in physical and mathematical formulations. Computational analysis demonstrates improved order of complexity, suggesting a promising direction for algorithmic efficiency in solving linear systems. This methodology opens avenues for interpretable and scalable solutions to physically motivated problems in computational science.|\n", "2505.16905": "|**2025-05-22**|**Accurate crystal field Hamiltonians of single-ion magnets at mean-field cost**|Linqing Peng, Shuanglong Liu, Xing Zhang et.al.|[2505.16905](http://arxiv.org/abs/2505.16905)|null||The effective crystal field Hamiltonian provides the key description of the electronic properties of single-ion magnets, but obtaining its parameters from ab initio computation is challenging. We introduce a simple approach to derive the effective crystal field Hamiltonian through density functional calculations of randomly rotated mean-field states within the low-energy manifold. In benchmarks on five lanthanide-based complexes, we find that we compute with mean-field cost an effective crystal field Hamiltonian that matches the state-of-the-art from much more expensive multi-configurational quantum chemistry methods. In addition, we are able to reproduce the experimental low-energy spectrum and magnetic properties with an accuracy exceeding prior attempts. Due to its low cost, our approach provides a crucial ingredient in the computational design of single-ion magnets with tailored physical properties and low-energy spectra.|\n", "2505.16812": "|**2025-05-22**|**Lp boundedness, r-nuclearity and approximation of pseudo-differential operators on $\\hbar\\mathbb{Z}^n$**|Juan Pablo Lopez et.al.|[2505.16812](http://arxiv.org/abs/2505.16812)|null||In this work sufficient conditions on the order of the symbol are developed to ensure boundedness, compactness and r-nuclearity of pseudo-differential operators in $\\hbar\\mathbb{Z}^n$. In addition, these conditions allow us to obtain growth estimates for the eigenvalues of some elliptic operators, in particular perturbed discrete Schr\\\"odinger operator.|\n", "2505.16762": "|**2025-05-22**|**A Riemannian Optimization Approach for Finding the Nearest Reversible Markov Chain**|Fabio Durastante, Miryam Gnazzo, Beatrice Meini et.al.|[2505.16762](http://arxiv.org/abs/2505.16762)|null|20 pages, 8 figures|We address the algorithmic problem of determining the reversible Markov chain $\\tilde X$ that is closest to a given Markov chain $X$, with an identical stationary distribution. More specifically, $\\tilde X$ is the reversible Markov chain with the closest transition matrix, in the Frobenius norm, to the transition matrix of $X$. To compute the transition matrix of $\\tilde X$, we propose a novel approach based on Riemannian optimization. Our method introduces a modified multinomial manifold endowed with a prescribed stationary vector, while also satisfying the detailed balance conditions, all within the framework of the Fisher metric. We evaluate the performance of the proposed approach in comparison with an existing quadratic programming method and demonstrate its effectiveness through a series of synthetic experiments, as well as in the construction of a reversible Markov chain from transition count data obtained via direct estimation from a stochastic differential equation.|\n", "2505.16487": "|**2025-05-22**|**Implicit Neural Shape Optimization for 3D High-Contrast Electrical Impedance Tomography**|Junqing Chen, Haibo Liu et.al.|[2505.16487](http://arxiv.org/abs/2505.16487)|null||We present a novel implicit neural shape optimization framework for 3D high-contrast Electrical Impedance Tomography (EIT), addressing scenarios where conductivity exhibits sharp discontinuities across material interfaces. These high-contrast cases, prevalent in metallic implant monitoring and industrial defect detection, challenge traditional reconstruction methods due to severe ill-posedness. Our approach synergizes shape optimization with implicit neural representations, introducing key innovations including a shape derivative-based optimization scheme that explicitly incorporates high-contrast interface conditions and an efficient latent space representation that reduces variable dimensionality. Through rigorous theoretical analysis of algorithm convergence and extensive numerical experiments, we demonstrate substantial performance improvements, establishing our framework as promising for practical applications in medical imaging with metallic implants and industrial non-destructive testing.|\n", "2505.16468": "|**2025-05-22**|**Local projection stabilization methods for $\\boldsymbol{H}({\\rm curl})$ and $\\boldsymbol{H}({\\rm div})$ advection problems**|Yangfan Luo, Jindong Wang, Shuonan Wu et.al.|[2505.16468](http://arxiv.org/abs/2505.16468)|null||We devise local projection stabilization (LPS) methods for advection problems in the $\\boldsymbol{H}$(curl) and $\\boldsymbol{H}$(div) spaces, employing conforming finite element spaces of arbitrary order within a unified framework. The key ingredient is a local inf-sup condition, enabled by enriching the approximation space with appropriate $\\boldsymbol{H}$(d) bubble functions (with d = curl or div). This enrichment allows for the construction of modified interpolation operators, which are crucial for establishing optimal a priori error estimates in the energy norm. Numerical examples are presented to verify both the theoretical results and the stabilization properties of the proposed method.|\n", "2505.16443": "|**2025-05-22**|**Stochastic collocation schemes for Neural Field Equations with random data**|Daniele Avitabile, Francesca Cavallini, Svetlana Dubinkina et.al.|[2505.16443](http://arxiv.org/abs/2505.16443)|null||We develop and analyse numerical schemes for uncertainty quantification in neural field equations subject to random parametric data in the synaptic kernel, firing rate, external stimulus, and initial conditions. The schemes combine a generic projection method for spatial discretisation to a stochastic collocation scheme for the random variables. We study the problem in operator form, and derive estimates for the total error of the schemes, in terms of the spatial projector. We give conditions on the projected random data which guarantee analyticity of the semi-discrete solution as a Banach-valued function. We illustrate how to verify hypotheses starting from analytic random data and a choice of spatial projection. We provide evidence that the predicted convergence rates are found in various numerical experiments for linear and nonlinear neural field problems.|\n", "2505.16378": "|**2025-05-22**|**Observing dynamics of distinct structural transitions in trapped-ion clusters**|Akhil Ayyadevara, Anand Prakash, Shovan Dutta et.al.|[2505.16378](http://arxiv.org/abs/2505.16378)|null||Interacting many-particle systems can self-organize into a rich variety of crystalline structures. While symmetry considerations provide a powerful framework for predicting whether structural transitions between crystal states are continuous or discontinuous, collective lattice dynamics offer complementary insights into the microscopic mechanisms that drive these transitions. Laser-cooled ions in electromagnetic traps present a pristine and highly controllable, few-body system for examining this interplay of symmetry and dynamics. Here, we use real-time fluorescence imaging while deforming the trapping potential to observe a variety of structural transitions in three-dimensional unit-cell-sized ion clusters. We probe their distinct dynamical signatures: mode softening indicating a lattice instability at a symmetry-breaking continuous transition, stochastic switching between stable structures at a discontinuous transition, and hysteresis near a spinodal point. Notably, we observe a triple point-like feature where a discontinuous symmetry-changing transition and a continuous symmetry-breaking transition occur simultaneously. Analytical calculations based on symmetry considerations and numerical analysis of collective modes provide a comprehensive understanding of the observed dynamics. Our results demonstrate tunable Coulomb clusters as a versatile platform to investigate how symmetry, energy landscapes, and dynamical pathways govern structural transitions in confined few-body systems.|\n", "2505.16365": "|**2025-05-22**|**A collaborative constrained graph diffusion model for the generation of realistic synthetic molecules**|Manuel Ruiz-Botella, Marta Sales-Pardo, Roger Guimer\u00e0 et.al.|[2505.16365](http://arxiv.org/abs/2505.16365)|null|28 pages, 10 figures, 4 tables|Developing new molecular compounds is crucial to address pressing challenges, from health to environmental sustainability. However, exploring the molecular space to discover new molecules is difficult due to the vastness of the space. Here we introduce CoCoGraph, a collaborative and constrained graph diffusion model capable of generating molecules that are guaranteed to be chemically valid. Thanks to the constraints built into the model and to the collaborative mechanism, CoCoGraph outperforms state-of-the-art approaches on standard benchmarks while requiring up to an order of magnitude fewer parameters. Analysis of 36 chemical properties also demonstrates that CoCoGraph generates molecules with distributions more closely matching real molecules than current models. Leveraging the model's efficiency, we created a database of 8.2M million synthetically generated molecules and conducted a Turing-like test with organic chemistry experts to further assess the plausibility of the generated molecules, and potential biases and limitations of CoCoGraph.|\n", "2505.16345": "|**2025-05-22**|**Convergence analysis of GMRES applied to Helmholtz problems near resonances**|Victorita Dolean, Pierre Marchand, Axel Modave et.al.|[2505.16345](http://arxiv.org/abs/2505.16345)|null||In this work we study how the convergence rate of GMRES is influenced by the properties of linear systems arising from Helmholtz problems near resonances or quasi-resonances. We extend an existing convergence bound to demonstrate that the approximation of small eigenvalues by harmonic Ritz values plays a key role in convergence behavior. Next, we analyze the impact of deflation using carefully selected vectors and combine this with a Complex Shifted Laplacian preconditioner. Finally, we apply these tools to two numerical examples near (quasi-)resonant frequencies, using them to explain how the convergence rate evolves.|\n", "2505.16343": "|**2025-05-22**|**Neural Field Equations with random data**|Daniele Avitabile, Francesca Cavallini, Svetlana Dubinkina et.al.|[2505.16343](http://arxiv.org/abs/2505.16343)|null||We study neural field equations, which are prototypical models of large-scale cortical activity, subject to random data. We view this spatially-extended, nonlocal evolution equation as a Cauchy problem on abstract Banach spaces, with randomness in the synaptic kernel, firing rate function, external stimuli, and initial conditions. We determine conditions on the random data that guarantee existence, uniqueness, and measurability of the solution in an appropriate Banach space, and examine the regularity of the solution in relation to the regularity of the inputs. We present results for linear and nonlinear neural fields, and for the two most common functional setups in the numerical analysis of this problem. In addition to the continuous problem, we analyse in abstract form neural fields that have been spatially discretised, setting the foundations for analysing uncertainty quantification (UQ) schemes.|\n", "2505.16296": "|**2025-05-22**|**A finite element solver for a thermodynamically consistent electrolyte model**|Jan Habscheid, Satyvir Singh, Lambert Theisen et.al.|[2505.16296](http://arxiv.org/abs/2505.16296)|null|29 pages, 15 figures|In this study, we present a finite element solver for a thermodynamically consistent electrolyte model that accurately captures multicomponent ionic transport by incorporating key physical phenomena such as steric effects, solvation, and pressure coupling. The model is rooted in the principles of non-equilibrium thermodynamics and strictly enforces mass conservation, charge neutrality, and entropy production. It extends beyond classical frameworks like the Nernst-Planck system by employing modified partial mass balances, the electrostatic Poisson equation, and a momentum balance expressed in terms of electrostatic potential, atomic fractions, and pressure, thereby enhancing numerical stability and physical consistency. Implemented using the FEniCSx platform, the solver efficiently handles one- and two-dimensional problems with varied boundary conditions and demonstrates excellent convergence behavior and robustness. Validation against benchmark problems confirms its improved physical fidelity, particularly in regimes characterized by high ionic concentrations and strong electrochemical gradients. Simulation results reveal critical electrolyte phenomena, including electric double layer formation, rectification behavior, and the effects of solvation number, Debye length, and compressibility. The solver's modular variational formulation facilitates its extension to complex electrochemical systems involving multiple ionic species with asymmetric valences.|\n", "2505.16243": "|**2025-05-22**|**A novel splitting method for Vlasov-Ampere**|James A. Rossmanith, Christine Vaughan et.al.|[2505.16243](http://arxiv.org/abs/2505.16243)|null|9 pages, 3 figures, 1 table|Vlasov equations model the dynamics of plasma in the collisionless regime. A standard approach for numerically solving the Vlasov equation is to operator split the spatial and velocity derivative terms, allowing simpler time-stepping schemes to be applied to each piece separately (known as the Cheng-Knorr method). One disadvantage of such an operator split method is that the order of accuracy of fluid moments (e.g., mass, momentum, and energy) is restricted by the order of the operator splitting (second-order accuracy in the Cheng-Knorr case). In this work, we develop a novel approach that first represents the particle density function on a velocity mesh with a local fluid approximation in each discrete velocity band and then introduces an operator splitting that splits the inter-velocity band coupling terms from the dynamics within the discrete velocity band. The advantage is that the inter-velocity band coupling terms are only needed to achieve consistency of the full distribution functions, but the local fluid models within each band are sufficient to achieve high-order accuracy on global moments such as mass, momentum, and energy. The resulting scheme is verified on several standard Vlasov-Poisson test cases.|\n", "2505.16045": "|**2025-05-21**|**Regularizing Ill-Posed Inverse Problems: Deblurring Barcodes**|Mark Embree et.al.|[2505.16045](http://arxiv.org/abs/2505.16045)|null||This manuscript is designed to introduce students in applied mathematics and data science to the concept of regularization for ill-posed inverse problems. Construct a mathematical model that describes how an image gets blurred. Convert a calculus problem into a linear algebra problem by discretization. Inverting the blurring process should sharpen up an image; this requires the solution of a system of linear algebraic equations. Solving this linear system of equations turns out to be delicate, as deblurring is an example of an ill-posed inverse problem. To address this challenge, recast the system as a regularized least squares problem (also known as ridge regression).|\n", "2505.18100": "|**2025-05-23**|**A new generation of effective core potentials: Selected lanthanides and heavy elements II**|Omar Madany, Benjamin Kincaid, Aqsa Shaikh et.al.|[2505.18100](http://arxiv.org/abs/2505.18100)|null|16 pages, 17 figures and 5 tables|We present a new set of correlation-consistent effective core potentials (ccECPs) for selected heavy $s$, $p$, $d$, and $f$-block elements significant in materials science and chemistry (Rb, Sr, Cs, Ba, In, Sb, Pb, Ru, Cd, La, Ce, and Eu). The ccECPs are designed using minimal Gaussian parameterization to achieve smooth and bounded potentials. They are expressed as a combination of averaged relativistic effective potentials (AREP) and effective spin-orbit (SO) terms, developed within a relativistic coupled-cluster framework. The optimization is driven by correlated all-electron (AE) atomic spectra, norm-conservation, and spin-orbit splittings, with considerations for plane wave cut-offs to ensure accuracy and viability across various electronic configurations. Transferability of these ccECPs is validated through testing on molecular oxides and hydrides, emphasizing discrepancies in molecular binding energies across a spectrum of bond lengths and electronic environments. The ccECPs demonstrate excellent agreement with AE reference calculations, attaining chemical accuracy in bond dissociation energies and equilibrium bond lengths, even in systems characterized by substantial relativistic and correlation effects. These ccECPs provide accurate and transferable framework for valence-only calculations.|\n", "2505.18072": "|**2025-05-23**|**Recovering Hidden Degrees of Freedom Using Gaussian Processes**|Georg Diez, Nele Dethloff, Gerhard Stock et.al.|[2505.18072](http://arxiv.org/abs/2505.18072)|null||Dimensionality reduction represents a crucial step in extracting meaningful insights from Molecular Dynamics (MD) simulations. Conventional approaches, including linear methods such as principal component analysis as well as various autoencoder architectures, typically operate under the assumption of independent and identically distributed data, disregarding the sequential nature of MD simulations. Here, we introduce a physics-informed representation learning framework that leverages Gaussian Processes combined with variational autoencoders to exploit the temporal dependencies inherent in MD data. Time-dependent kernel functions--such as the Mat\\'ern kernel--directly impose impose the temporal correlation structure of the input coordinates onto a low-dimensional space, preserving Markovianity in the reduced representation while faithfully capturing the essential dynamics. Using a three-dimensional toy model, we demonstrate that this approach can successfully identify and separate dynamically distinct states that are geometrically indistinguishable due to hidden degrees of freedom. The resulting embedding features enhance metastability, facilitating the construction of Markov state models with smaller lag times and better convergence of implied timescales. This time-aware perspective provides a promising framework for understanding complex biomolecular systems, in which conventional collective variables may fail to capture the full dynamical picture.|\n", "2505.18050": "|**2025-05-23**|**Inference of Substructured Reduced-Order Models for Dynamic Contact from Contact-free Simulations**|Diana Manvelyan-Stroot, Yevgeniya Filanova, Igor Pontes Duff et.al.|[2505.18050](http://arxiv.org/abs/2505.18050)|null||In this paper, we propose an operator-inference-based reduction approach for contact problems, leveraging snapshots from simulations without active contact. Contact problems are solved using adjoint methods, by switching to the dual system, where the corresponding Lagrange multipliers represent the contact pressure. The Craig-Bampton-like substructuring method is incorporated into the inference process to provide the reduced system matrices and the coupling of the contact and interior nodes. The maximum possible set of contact nodes must be known a priori. Characteristic properties of the inferred matrices, such as symmetry and positive definiteness, are enforced by appending additional constraints to the underlying least-squares problem. The resulting dual system, which forms a linear complementarity problem, is well-defined and can be effectively solved using methods such as Lemke's algorithm. The performance of the proposed method is validated on three-dimensional finite element models.|\n", "2505.18042": "|**2025-05-23**|**A novel parameter-free and locking-free enriched Galerkin method for linear elasticity**|Shuai Su, Xiurong Yan, Qian Zhang et.al.|[2505.18042](http://arxiv.org/abs/2505.18042)|null|20 pages, 11 figures|We propose a novel parameter-free and locking-free enriched Galerkin (EG) method for solving the linear elasticity problem in both two and three dimensions. Unlike existing locking-free EG methods, our method enriches the first-order continuous Galerkin (CG) space with piecewise constants along edges in two dimensions or faces in three dimensions. This enrichment acts as a correction to the normal component of the CG space, ensuring the locking-free property and delivering an oscillation-free stress approximation without requiring post-processing. Our theoretical analysis establishes the well-posedness of the method and derives optimal error estimates. Numerical experiments further demonstrate the accuracy, efficiency, and robustness of the proposed method.|\n", "2505.17969": "|**2025-05-23**|**A new class of finite difference methods: The zigzag schemes**|Lorenzo Poggioni, Didier Clamond, Yves D'Angelo et.al.|[2505.17969](http://arxiv.org/abs/2505.17969)|null|Main author : Lorenzo Poggioni|We introduce a novel class of finite difference approximations, termed zigzag schemes, that employ a hybrid stencil that is neither symmetrical, nor fully one-sided. These zigzag schemes often enjoy more permissive stability constraints and see their coefficients vanish as the order tends to infinity. This property permits the formulation of higher order schemes. An explicit formula is given for both collocated and staggered grids for an arbitrary order and a closed-form expression for the infinite-order scheme is also provided. A linear stability analysis indicates that the zigzag scheme offer a broader range of conditional stability compared to the centred and upwind schemes, sometimes being the only stable scheme. Additionally, the asymmetrical structure of the stencil of zigzag schemes prevents some issues such as the formation of ``ghost solutions''. Moreover, implementing zigzag schemes is relatively easy when a code using classical finite differences is available, that is an important feature for well-tested legacy codes. Overall, zigzag schemes provide a compelling alternative for finite differences methods by enabling faster and more stable numerical simulations without sacrificing accuracy or ease of use.|\n", "2505.17849": "|**2025-05-23**|**A sparse $hp$-finite element method for piecewise-smooth differential equations with periodic boundary conditions**|Daniel VandenHeuvel, Sheehan Olver et.al.|[2505.17849](http://arxiv.org/abs/2505.17849)|null|43 pages, 10 figures|We develop an efficient $hp$-finite element method for piecewise-smooth differential equations with periodic boundary conditions, using orthogonal polynomials defined on circular arcs. The operators derived from this basis are banded and achieve optimal complexity regardless of $h$ or $p$, both for building the discretisation and solving the resulting linear system in the case where the operator is symmetric positive definite. The basis serves as a useful alternative to other bases such as the Fourier or integrated Legendre bases, especially for problems with discontinuities. We relate the convergence properties of these bases to regions of analyticity in the complex plane, and further use several differential equation examples to demonstrate these properties. The basis spans the low order eigenfunctions of constant coefficient differential operators, thereby achieving better smoothness properties for time-evolution partial differential equations.|\n", "2505.17756": "|**2025-05-23**|**Qiskit Machine Learning: an open-source library for quantum machine learning tasks at scale on quantum hardware and classical simulators**|M. Emre Sahin, Edoardo Altamura, Oscar Wallis et.al.|[2505.17756](http://arxiv.org/abs/2505.17756)|null|6 pages, 1 figure. Qiskit Machine Learning is open-source and   available at https://github.com/qiskit-community/qiskit-machine-learning|We present Qiskit Machine Learning (ML), a high-level Python library that combines elements of quantum computing with traditional machine learning. The API abstracts Qiskit's primitives to facilitate interactions with classical simulators and quantum hardware. Qiskit ML started as a proof-of-concept code in 2019 and has since been developed to be a modular, intuitive tool for non-specialist users while allowing extensibility and fine-tuning controls for quantum computational scientists and developers. The library is available as a public, open-source tool and is distributed under the Apache version 2.0 license.|\n", "2505.17753": "|**2025-05-23**|**Application of troubled-cells to finite volume methods -- an optimality study using a novel monotonicity parameter**|R Shivananda Rao, M Ramakrishna et.al.|[2505.17753](http://arxiv.org/abs/2505.17753)|null|arXiv admin note: text overlap with arXiv:2504.11056|We adapt a troubled-cell indicator from discontinuous Galerkin (DG) methods to finite volume methods (FVM) with MUSCL reconstruction and using a novel monotonicity parameter show there is a trade-off between convergence and quality of the solution. Employing two dimensional compressible Euler equations for flows with oblique shocks, this trade-off is studied by varying the number of troubled-cells systematically. An oblique shock is characterized primarily by the upstream Mach number, the shock angle $\\beta$, and the deflection angle $\\theta$. We study these factors and their combinations and find that the degree of the shock misalignment with the grid determines the optimal number of troubled-cells. On each side of the shock, the optimal set consists of three troubled-cells for aligned shocks, and the troubled-cells identified by tracing the shock and four lines parallel to it, separated by the grid spacing, for nonaligned shocks. We show that the adapted troubled-cell indicator identifies a set of cells that is close to and contains the optimal set of cells for a threshold constant $K = 0.05$, and consequently, produces a solution close to that obtained by limiting everywhere, but with improved convergence.|\n", "2505.17751": "|**2025-05-23**|**Computational Math with Neural Networks is Hard**|Michael Feischl, Fabian Zehetgruber et.al.|[2505.17751](http://arxiv.org/abs/2505.17751)|null||We show that under some widely believed assumptions, there are no higher-order algorithms for basic tasks in computational mathematics such as: Computing integrals with neural network integrands, computing solutions of a Poisson equation with neural network source term, and computing the matrix-vector product with a neural network encoded matrix. We show that this is already true for very simple feed-forward networks with at least three hidden layers, bounded weights, bounded realization, and sparse connectivity, even if the algorithms are allowed to access the weights of the network. The fundamental idea behind these results is that it is already very hard to check whether a given neural network represents the zero function. The non-locality of the problems above allow us to reduce the approximation setting to deciding whether the input is zero or not. We demonstrate sharpness of our results by providing fast quadrature algorithms for one-layer networks and giving numerical evidence that quasi-Monte Carlo methods achieve the best possible order of convergence for quadrature with neural networks.|\n", "2505.17740": "|**2025-05-23**|**A tensor network approach for chaotic time series prediction**|Rodrigo Mart\u00ednez-Pe\u00f1a, Rom\u00e1n Or\u00fas et.al.|[2505.17740](http://arxiv.org/abs/2505.17740)|null|12 pages, 3 figures. Comments are welcome!|Making accurate predictions of chaotic time series is a complex challenge. Reservoir computing, a neuromorphic-inspired approach, has emerged as a powerful tool for this task. It exploits the memory and nonlinearity of dynamical systems without requiring extensive parameter tuning. However, selecting and optimizing reservoir architectures remains an open problem. Next-generation reservoir computing simplifies this problem by employing nonlinear vector autoregression based on truncated Volterra series, thereby reducing hyperparameter complexity. Nevertheless, the latter suffers from exponential parameter growth in terms of the maximum monomial degree. Tensor networks offer a promising solution to this issue by decomposing multidimensional arrays into low-dimensional structures, thus mitigating the curse of dimensionality. This paper explores the application of a previously proposed tensor network model for predicting chaotic time series, demonstrating its advantages in terms of accuracy and computational efficiency compared to conventional echo state networks. Using a state-of-the-art tensor network approach enables us to bridge the gap between the tensor network and reservoir computing communities, fostering advances in both fields.|\n", "2505.17719": "|**2025-05-23**|**Stage-Parallel Implicit Runge--Kutta methods via low-rank matrix equation corrections**|Fabio Durastante, Mariarosa Mazza et.al.|[2505.17719](http://arxiv.org/abs/2505.17719)|null||Implicit Runge--Kutta (IRK) methods are highly effective for solving stiff ordinary differential equations (ODEs) but can be computationally expensive for large-scale problems due to the need of solving coupled algebraic equations at each step. This study improves IRK efficiency by leveraging parallelism to decouple stage computations and reduce communication overhead, specifically we stably decouple a perturbed version of the stage system of equations and recover the exact solution by solving a Sylvester matrix equation with an explicitly known low-rank right-hand side. Two IRK families -- symmetric methods and collocation methods -- are analyzed, with extensions to nonlinear problems using a simplified Newton method. Implementation details, shared memory parallel code, and numerical examples, particularly for ODEs from spatially discretized PDEs, demonstrate the efficiency of the proposed IRK technique.|\n", "2505.17678": "|**2025-05-23**|**Optimal control of variable-exponent subdiffusion**|Yiqun Li, Mengmeng Liu, Wenlin Qiu et.al.|[2505.17678](http://arxiv.org/abs/2505.17678)|null||This work investigates the optimal control of the variable-exponent subdiffusion, which extends the work [Gunzburger and Wang, {\\it SIAM J. Control Optim.} 2019] to the variable-exponent case to account for the multiscale and crossover diffusion behavior. To resolve the difficulties caused by the leading variable-exponent operator, we adopt the convolution method to reformulate the model into an equivalent but more tractable form, and then prove the well-posedness and weighted regularity of the optimal control. As the convolution kernels in reformulated models are indefinite-sign, non-positive-definite, and non-monotonic, we adopt the discrete convolution kernel approach in numerical analysis to show the $O(\\tau(1+|\\ln\\tau|)+h^2)$ accuracy of the schemes for state and adjoint equations. Numerical experiments are performed to substantiate the theoretical findings.|\n", "2505.17606": "|**2025-05-23**|**An insight on some properties of high order nonstandard linear multistep methods**|B\u00e1lint Tak\u00e1cs et.al.|[2505.17606](http://arxiv.org/abs/2505.17606)|null|33 pages, 10 figures|In this paper, nonstandard multistep methods are observed. It is shown that under some (sufficient and necessary) conditions, these methods attain the same order as their standard counterparts - to prove this statement, a nonstandard version of Taylor's series is constructed. The preservation of some qualitative properties (boundedness, a weak form of monotonicity, and the linear combination of the components) is also proven for all step sizes. The methods are applied to a one-dimensional equation and a system of equations, in which the numerical experiments confirm the theoretical results.|\n", "2505.17535": "|**2025-05-23**|**Equilibrium boundary conditions for vectorial multi-dimensional lattice Boltzmann schemes**|Denise Aregba-Driollet, Thomas Bellotti et.al.|[2505.17535](http://arxiv.org/abs/2505.17535)|null||The concept of equilibrium is a general tool to fill the gap between macroscopic and mesoscopic information, both within kinetic systems and kinetic schemes. This work explores the use of equilibria to devise numerical boundary conditions for multi-dimensional vectorial lattice Boltzmann schemes tackling systems of hyperbolic conservation laws. In the scalar case, we prove convergence for schemes with monotone relaxation to the weak entropy solution by Bardos, Leroux, and N{\\'e}delec [Commun. Partial Differ. Equ., 4 (9), 1979], following the path by Crandall and Majda [Math. Comput., 34, 149 (1980)]. Numerical experiments are conducted both for scalar and vectorial problems, and demonstrate the effectiveness of equilibrium boundary conditions in capturing significant physical phenomena.|\n", "2505.17516": "|**2025-05-23**|**Increasing the Resistance of Magnetic Flux Concentrator during Generation of Strong Pulsed Magnetic Fields**|P. A. Russkikh, G. Sh. Boltachev et.al.|[2505.17516](http://arxiv.org/abs/2505.17516)|null|10 pages, 8 figures|The possibility of significant increase of generated pulsed magnetic fields by the inductor system of a single-turn solenoid and magnetic flux concentrator without initialization of low-cycle fatigue mechanism is theoretically studied by varying the size of the inductor system, the material of the concentrator and the parameters of the discharge circuit. The analysis is carried out on the basis of self-consistent solution of the equation, which describes dynamics of the discharge electric circuit, with equations describing spatial distributions of magnetic and temperature fields, mechanical stresses and deformations in the inductor and concentrator. It is shown that for traditionally used steel concentrators by varying the electrical resistance of the circuit it is possible to increase the amplitude of generated pulsed magnetic fields without the threat of concentrator destruction by about 25~\\%, from 32 to 40~T.|\n", "2505.17504": "|**2025-05-23**|**The GMRES method for solving the large indefinite least squares problem via an accelerated preconditioner**|Jun Li, Lingsheng Meng et.al.|[2505.17504](http://arxiv.org/abs/2505.17504)|null||In this research, to solve the large indefinite least squares problem, we firstly transform its normal equation into a sparse block three-by-three linear systems, then use GMRES method with an accelerated preconditioner to solve it. The construction idea of the preconditioner comes from the thought of Luo et.al [Luo, WH., Gu, XM., Carpentieri, B., BIT 62, 1983-2004(2022)], and the advantage of this is that the preconditioner is closer to the coefficient matrix of the block three-by-three linear systems when the parameter approachs zero. Theoretically, the iteration method under the preconditioner satisfies the conditional convergence, and all eigenvalues of the preconditioned matrix are real numbers and gathered at point $(1,0)$ as parameter is close to $0$. In the end, numerical results reflect that the theoretical results is correct and the proposed preconditioner is effective by comparing with serval existing preconditioners.|\n", "2505.17396": "|**2025-05-23**|**Cross-scale Modeling of Polymer Topology Impact on Extrudability through Molecular Dynamics and Computational Fluid Dynamics**|Yawei Gao, Jan Michael Carrillo, Logan T. Kearney et.al.|[2505.17396](http://arxiv.org/abs/2505.17396)|null||Understanding how polymer topology influences melt extrudability is critical for advancing material design in extrusion-based additive manufacturing. In this work, we develop a bottom-up, cross-scale modeling framework that integrates coarse-grained molecular dynamics (CGMD) and continuum-scale computational fluid dynamics (CFD) to quantitatively assess the effects of polymer architecture on extrudability A range of branched polydimethylsiloxane (PDMS) polymers are systematically designed by varying backbone length, sidechain length, grafting density, grafted block ratio, and periodicity of grafted-ungrafted segments. CGMD simulations are used to compute zero-shear viscosity and relaxation times, which are then incorporated into the Phan-Thien-Tanner (PTT) model within a computational fluid dynamics (CFD) model to predict pressure drop of PDMS during extrusion through printer nozzle. Qualitative analysis reveals that polymers with concentrated grafted blocks exhibit significantly higher zero-shear viscosity than stochastically branched analogs, while sidechain inertia drives longer relaxation time. However, for untangled and weakly entangled PDMS, relaxation time remains in the nanosecond range, making shear-thinning and elastic effects negligible. Consequently, zero-shear viscosity emerges as the primary determinant of extrudability. This cross-scale modeling strategy provides a predictive framework for guiding the rational design of extrudable polymer materials with tailored topologies.|\n", "2505.17376": "|**2025-05-23**|**Internal dynamics and fission of pure-quartic soliton molecules**|Zhixiang Deng, Rui Ma, Chunxiang Zhang et.al.|[2505.17376](http://arxiv.org/abs/2505.17376)|null|10 pages and 9 figures; To be published in Physical Review A|We address the weak interaction of a pair of well-separated pure-quartic solitons (PQSs), which are solutions to a generalized nonlinear Schrodinger equation (NLSE) with the quartic-only dispersion. An asymptotic technique is applied to derive equations for the slow evolution of the temporal separation and phase difference of the PQSs interacting through the overlapping of their exponentially decaying oscillating tails. Based on this approach, various stationary states of bound PQS (soliton molecules) with distinct phase differences are predicted. Their stability is addressed via the numerical calculation of the eigenvalue spectrum of small perturbations, showing instability of the bound states. A systematic numerical analysis demonstrates that the parameter space of the PQS bound states is organized as a self-similar fractal structure, composed of regions populated by robustly oscillating or splitting two-soliton states. The analytical method and results reported here can be extended for bound states of two or several weakly interacting modes in other conservative and dissipative systems.|\n", "2505.17341": "|**2025-05-22**|**TI-DeepONet: Learnable Time Integration for Stable Long-Term Extrapolation**|Dibyajyoti Nayak, Somdatta Goswami et.al.|[2505.17341](http://arxiv.org/abs/2505.17341)|null|18 pages, 8 figures|Accurate temporal extrapolation presents a fundamental challenge for neural operators in modeling dynamical systems, where reliable predictions must extend significantly beyond the training time horizon. Conventional Deep Operator Network (DeepONet) approaches employ two inherently limited training paradigms - fixed-horizon rollouts that predict complete spatiotemporal solutions while disregarding temporal causality, and autoregressive formulations that accumulate errors through sequential predictions. We introduce TI-DeepONet, a framework that integrates neural operators with adaptive numerical time-stepping techniques to preserve the Markovian structure of dynamical systems while mitigating error propagation in extended temporal forecasting. Our approach reformulates the learning objective from direct state prediction to the approximation of instantaneous time-derivative fields, which are then integrated using established numerical schemes. This architecture supports continuous-time prediction and enables deployment of higher-precision integrators during inference than those used during training, balancing computational efficiency with predictive accuracy. We further develop TI(L)-DeepONet, which incorporates learnable coefficients for intermediate slopes in the integration process, adapting to solution-specific variations and enhancing fidelity. Evaluation across three canonical PDEs shows that TI(L)-DeepONet marginally outperforms TI-DeepONet, with both reducing relative L2 extrapolation errors: approximately 81% over autoregressive and 70% over fixed-horizon methods. Notably, both maintain prediction stability for temporal domains extending to about twice the training interval. This research establishes a physics-aware operator learning paradigm that bridges neural approximation with numerical analysis while preserving the causal structure of dynamical systems.|\n", "2505.17305": "|**2025-05-22**|**Data-driven Closure Strategies for Parametrized Reduced Order Models via Deep Operator Networks**|Anna Ivagnes, Giovanni Stabile, Gianluigi Rozza et.al.|[2505.17305](http://arxiv.org/abs/2505.17305)|null|arXiv admin note: substantial text overlap with arXiv:2406.04169|In this paper, we propose an equation-based parametric Reduced Order Model (ROM), whose accuracy is improved with data-driven terms added into the reduced equations. These additions have the aim of reintroducing contributions that in standard reduced-order approaches are not taken into account. In particular, in this work we focus on a Proper Orthogonal Decomposition (POD)-based formulation and our goal is to build a closure or correction model, aimed to re-introduce the contribution of the discarded modes. The approach has been investigated in previous works, and the goal of this manuscript is to extend the model to a parametric setting making use of machine learning procedures, and, in particular, of deep operator networks. More in detail, we model the closure terms through a deep operator network taking as input the reduced variables and the parameters of the problem. We tested the methods on three test cases with different behaviors: the periodic turbulent flow past a circular cylinder, the unsteady turbulent flow in a channel-driven cavity, and the geometrically-parametrized backstep flow. The performance of the machine learning-enhanced ROM is deeply studied in different modal regimes, and considerably improved the pressure and velocity accuracy with respect to the standard POD-Galerkin approach.|\n", "2505.21424": "|**2025-05-27**|**A Hyperbolic Approximation of the Nonlinear Schr\u00f6dinger Equation**|Abhijit Biswas, Laila S. Busaleh, David I. Ketcheson et.al.|[2505.21424](http://arxiv.org/abs/2505.21424)|null||We study a first-order hyperbolic approximation of   the nonlinear Schr\\\"odinger (NLS) equation. We show that the system   is strictly hyperbolic and possesses a modified Hamiltonian structure, along with   at least three conserved quantities that approximate those of NLS.   We provide families of explicit standing-wave solutions to the hyperbolic system,   which are shown to converge uniformly to ground-state solutions   of NLS in the relaxation limit.   The system is formally equivalent to NLS in the relaxation limit, and we   develop asymptotic preserving discretizations that tend to a consistent discretization   of NLS in that limit, while also conserving mass.   Examples for both the focusing and defocusing regimes demonstrate that the   numerical discretization provides an accurate approximation of the NLS   solution.|\n", "2505.21308": "|**2025-05-27**|**Dissipative Preparation of Many-Body Quantum States: Towards Practical Quantum Advantage**|Lin Lin et.al.|[2505.21308](http://arxiv.org/abs/2505.21308)|null||While dissipation has traditionally been viewed as an obstacle to quantum coherence, it is increasingly recognized as a powerful computational resource. Dissipative protocols can prepare complex many-body quantum states by leveraging engineered system-environment interactions. This essay focuses on a class of algorithms that utilize algorithmically constructed Lindblad generators, and highlight recent advances enabling the preparation of ground and thermal states for certain non-commuting Hamiltonians with rigorous performance guarantees. We also propose extensions of these protocols to prepare excited and resonance states, which may offer new pathways toward realizing practical quantum advantage on early fault-tolerant quantum computing platforms.|\n", "2505.21293": "|**2025-05-27**|**Strain controlled g- to d-wave transition in altermagnetic CrSb**|Bennet Karetta, Xanthe H. Verbeek, Rodrigo Jaeschke-Ubiergo et.al.|[2505.21293](http://arxiv.org/abs/2505.21293)|null||The possibility of a strain-induced transformation from $g$-wave to $d$-wave altermagnetism was recently recently proposed for MnTe using a $k\\cdot p$ perturbative model. In this work, we demonstrate such a transition in CrSb for a wider array of strains, using a combination of a minimal model and first-principles calculations. Starting from a symmetry perspective, we analyze the spin elastoconductivity tensor, and determine the strain types which allow for a change in the altermagnetic symmetry. We obtain three strain directions, which allow for a $d$-wave type splitting, and one in which a net magnetic moment emerges. Using first-principles calculations in the absence of spin-orbit coupling (SOC), we confirm these symmetry predictions. Furthermore, these results do not alter qualitatively in the presence of SOC. Finally, we reveal that the resulting spin currents give rise to a spin-splitter effect of up to 5\\% under realistic strains of 1\\%, confirming strain as a powerful tool for tuning altermagnetic properties.|\n", "2505.21287": "|**2025-05-27**|**A mathematical analysis of the discretized IPT-DMFT equations**|E. Canc\u00e8s, A. Kirsch, S. Perrin--Roussel et.al.|[2505.21287](http://arxiv.org/abs/2505.21287)|null||In a previous contribution (E. Canc\\`es, A. Kirsch and S. Perrin--Roussel, arXiv:2406.03384), we have proven the existence of a solution to the Dynamical Mean-Field Theory (DMFT) equations under the Iterated Perturbation Theory (IPT-DMFT) approximation. In view of numerical simulations, these equations need to be discretized. In this article, we are interested in a discretization of the \\acrshort{ipt}-\\acrshort{dmft} functional equations, based on the restriction of the hybridization function and local self-energy to a finite number of points in the upper half-plane $\\left(i\\omega_n\\right)_{n \\in |[0,N_\\omega]|}$, where $\\omega_n=(2n+1)\\pi / \\beta$ is the $n$-th Matsubara frequency and $N_\\omega \\in \\mathbb N$. We first prove the existence of solutions to the discretized equations in some parameter range depending on $N_\\omega$. We then prove uniqueness for a smaller range of parameters. We also study more in depth the case of bipartite systems exhibiting particle-hole symmetry. In this case, the discretized IPT-DMFT equations have purely imaginary solutions, which can be obtained by solving a real algebraic system of $(N_\\omega+1)$ equations with $(N_\\omega+1)$ variables. We provide a complete characterization of the solutions for $N_\\omega=0$ and some results for $N_\\omega=1$ in the simple case of the Hubbard dimer. We finally present some numerical simulations on the Hubbard dimer.|\n", "2505.21247": "|**2025-05-27**|**Kernel Ridge Regression for conformer ensembles made easy with Structured Orthogonal Random Features**|Konstantin Karandashev et.al.|[2505.21247](http://arxiv.org/abs/2505.21247)|null||A computationally efficient protocol for machine learning in chemical space using Boltzmann ensembles of conformers as input is proposed; the method is based on rewriting Kernel Ridge Regression expressions in terms of Structured Orthogonal Random Features, yielding physics-motivated trigonometric neural networks. The resulting method is tested on experimental datasets of oxidation potentials in acetonitrile and hydration energies, using several popular molecular representations to demonstrate the method's flexibility. Despite only using computationally cheap forcefield calculations for conformer generation, we observe systematic decrease of machine learning error with increased training set size in all cases, with experimental accuracy reached after training on hundreds of molecules and prediction errors being comparable to state-of-the-art machine learning approaches. We also present novel versions of Huber and LogCosh loss functions that made hyperparameter optimization of the new approach more convenient.|\n", "2505.21246": "|**2025-05-28**|**Physics Computational Literacy: Programming, modeling and collaboration at the journeyman level**|Karl Henrik Fredly, Tor Ole B. Odden, Benjamin M. Zwickl et.al.|[2505.21246](http://arxiv.org/abs/2505.21246)|null||Computation has become an integral part of physics research. However, little is known about how students learn to productively use computation as a tool beyond the introductory level, especially as they transition into physics research. In this study, we apply the theory of Physics Computational Literacy and the Novice-Expert framework to describe the development of expertise in computational physics, as students transition from novice to journeymen computational physicists. We base this description on a thematic analysis of interviews with 13 computational physics master's students with extensive experience using computation. We first describe the most important elements driving the development of computational physics expertise, identifying two distinct transitions of competence during their studies, driven by experience with large computational projects and professional research. We then present an overview of the various skills students attain on this path toward the journeyman level of computational physics expertise. Based on these results, we argue for the need to assist students in collaborative coding and the learning of new tools, as well as the importance of large, scaffolded, computational projects for helping students develop the advanced skills needed for computational research.|\n", "2505.21235": "|**2025-05-27**|**From Polyhedra to Crystals: A Graph-Theoretic Framework for Crystal Structure Generation**|Tomoyasu Yokoyama, Kazuhide Ichikawa, Hisashi Naito et.al.|[2505.21235](http://arxiv.org/abs/2505.21235)|null|11 pages, 10 figures|Crystal structures can be viewed as assemblies of space-filling polyhedra, which play a critical role in determining material properties such as ionic conductivity and dielectric constant. However, most conventional crystal structure prediction methods rely on random structure generation and do not explicitly incorporate polyhedral tiling, limiting their efficiency and interpretability. In this highlight, we introduced a novel crystal structure generation method based on discrete geometric analysis of polyhedral information. The geometry and topology of space-filling polyhedra are encoded as a dual periodic graph, and the corresponding crystal structure is obtained via the standard realization of this graph. We demonstrate the effectiveness of our approach by reconstructing face-centered cubic (FCC), hexagonal close-packed (HCP), and body-centered cubic (BCC) structures from their dual periodic graphs. This method offers a new pathway for systematically generating crystal structures based on target polyhedra, potentially accelerating the discovery of novel materials for applications in electronics, energy storage, and beyond.|\n", "2505.21203": "|**2025-05-27**|**Quantum Optimal Control Using MAGICARP: Combining Pontryagin's Maximum Principle and Gradient Ascent**|Denis Jankovi\u0107, Jean-Gabriel Hartmann, Paul-Louis Etienney et.al.|[2505.21203](http://arxiv.org/abs/2505.21203)|null||We introduce the MAGICARP algorithm, a numerical optimization method for quantum optimal control problems that combines the structure provided by Pontryagin's Maximum Principle (PMP) and the robustness of gradient ascent techniques, such as GRAPE. MAGICARP is formulated as a \"shooting technique\", aiming to determine the appropriate initial adjoint momentum to realize a target quantum gate. This method naturally incorporates time and energy optimal constraints through a PMP-informed pulse structure. We demonstrate MAGICARP's effectiveness through illustrative numerical examples, comparing its performance to GRAPE and highlighting its advantages in specific scenarios.|\n", "2505.21031": "|**2025-05-27**|**Reactive molecular dynamics approach to PFAS plasma oxidation in water**|Axel Richard, Pascal Brault, Nicolas Froloff et.al.|[2505.21031](http://arxiv.org/abs/2505.21031)|null||This work establishes a protocol to study via Molecular Dynamics simulation the degradation of Per-and Polyfluoroalkyl Substances (PFAS) in water by hydroxyl radical. To achieve this, molecular dynamics simulations are carried out, using ReaxFF reactive interaction potential. Simulations are carried out under a temperature ramp for determining all possible products. Using this methodology, reaction pathways of perfluorooctanoic acid (PFOA) and perfluorooctanesulfonic acid (PFOS) are identified.|\n", "2505.20950": "|**2025-05-27**|**Scattering Networks on Noncommutative Finite Groups**|Maria Teresa Arias, Davide Barbieri, Eugenio Hern\u00e1ndez et.al.|[2505.20950](http://arxiv.org/abs/2505.20950)|null||Scattering Networks were initially designed to elucidate the behavior of early layers in Convolutional Neural Networks (CNNs) over Euclidean spaces and are grounded in wavelets. In this work, we introduce a scattering transform on an arbitrary finite group (not necessarily abelian) within the context of group-equivariant convolutional neural networks (G-CNNs). We present wavelets on finite groups and analyze their similarity to classical wavelets. We demonstrate that, under certain conditions in the wavelet coefficients, the scattering transform is non-expansive, stable under deformations, preserves energy, equivariant with respect to left and right group translations, and, as depth increases, the scattering coefficients are less sensitive to group translations of the signal, all desirable properties of convolutional neural networks. Furthermore, we provide examples illustrating the application of the scattering transform to classify data with domains involving abelian and nonabelian groups.|\n", "2505.20818": "|**2025-05-27**|**Domain Decomposition Subspace Neural Network Method for Solving Linear and Nonlinear Partial Differential Equations**|Zhenxing Fu, Hongliang Liu, Zhiqiang Sheng et.al.|[2505.20818](http://arxiv.org/abs/2505.20818)|null||This paper proposes a domain decomposition subspace neural network method for efficiently solving linear and nonlinear partial differential equations. By combining the principles of domain decomposition and subspace neural networks, the method constructs basis functions using neural networks to approximate PDE solutions. It imposes $C^k$ continuity conditions at the interface of subdomains, ensuring smoothness across the global solution. Nonlinear PDEs are solved using Picard and Newton iterations, analogous to classical methods. Numerical experiments demonstrate that our method achieves exceptionally high accuracy, with errors reaching up to $10^{-13}$, while significantly reducing computational costs compared to existing approaches, including PINNs, DGM, DRM. The results highlight the method's superior accuracy and training efficiency.|\n", "2505.20721": "|**2025-05-27**|**Recurrent Neural Operators: Stable Long-Term PDE Prediction**|Zaijun Ye, Chen-Song Zhang, Wansheng Wang et.al.|[2505.20721](http://arxiv.org/abs/2505.20721)|null||Neural operators have emerged as powerful tools for learning solution operators of partial differential equations. However, in time-dependent problems, standard training strategies such as teacher forcing introduce a mismatch between training and inference, leading to compounding errors in long-term autoregressive predictions. To address this issue, we propose Recurrent Neural Operators (RNOs)-a novel framework that integrates recurrent training into neural operator architectures. Instead of conditioning each training step on ground-truth inputs, RNOs recursively apply the operator to their own predictions over a temporal window, effectively simulating inference-time dynamics during training. This alignment mitigates exposure bias and enhances robustness to error accumulation. Theoretically, we show that recurrent training can reduce the worst-case exponential error growth typical of teacher forcing to linear growth. Empirically, we demonstrate that recurrently trained Multigrid Neural Operators significantly outperform their teacher-forced counterparts in long-term accuracy and stability on standard benchmarks. Our results underscore the importance of aligning training with inference dynamics for robust temporal generalization in neural operator learning.|\n", "2505.20719": "|**2025-05-27**|**A Nested Krylov Method Using Half-Precision Arithmetic**|Kengo Suzuki, Takeshi Iwashita et.al.|[2505.20719](http://arxiv.org/abs/2505.20719)|null|16 pages, 6 figures|Low-precision computing is essential for efficiently utilizing memory bandwidth and computing cores. While many mixed-precision algorithms have been developed for iterative sparse linear solvers, effectively leveraging half-precision (fp16) arithmetic remains challenging. This study introduces a novel nested Krylov approach that integrates the flexible GMRES and Richardson methods in a deeply nested structure, progressively reducing precision from double-precision to fp16 toward the innermost solver. To avoid meaningless computations beyond precision limits, the low-precision inner solvers perform only a few iterations per invocation, while the nested structure ensures their frequent execution. Numerical experiments show that using fp16 in the approach directly enhances solver performance without compromising convergence, achieving speedups of up to 1.65x and 2.42x over double-precision and double-single mixed-precision implementations, respectively. Moreover, the proposed method outperforms or matches other standard Krylov solvers, including restarted GMRES, CG, and BiCGStab methods.|\n", "2505.20696": "|**2025-05-27**|**An Empirical Study of Conjugate Gradient Preconditioners for Solving Symmetric Positive Definite Systems of Linear Equations**|Marc A. Tunnell, David F. Gleich et.al.|[2505.20696](http://arxiv.org/abs/2505.20696)|null||Despite hundreds of papers on preconditioned linear systems of equations, there remains a significant lack of comprehensive performance benchmarks comparing various preconditioners for solving symmetric positive definite (SPD) systems. In this paper, we present a comparative study of 79 matrices using a broad range of preconditioners. Specifically, we evaluate 10 widely used preconditoners across 108 configurations to assess their relative performance against using no preconditioner. Our focus is on preconditioners that are commonly used in practice, are available in major software packages, and can be utilized as black-box tools without requiring significant \\textit{a priori} knowledge. In addition, we compare these against a selection of classical methods. We primarily compare them without regards to effort needed to compute the preconditioner. Our results show that symmetric positive definite systems are mostly likely to benefit from incomplete symmetric factorizations, such as incomplete Cholesky (IC). Multigrid methods occasionally do exceptionally well. Simple classical techniques, symmetric Gauss Seidel and symmetric SOR, are not productive. We find that including preconditioner construction costs significantly diminishes the advantages of iterative methods compared to direct solvers; although, tuned IC methods often still outperform direct methods. Additionally, ordering strategies such as approximate minimum degree significantly enhance IC effectiveness. We plan to expand the benchmark with larger matrices, additional solvers, and detailed metrics to provide actionable information on SPD preconditioning.|\n", "2505.20618": "|**2025-05-28**|**An Operator-Splitting Scheme for Viscosity Solutions of Constrained Second-Order PDEs**|Po-Yi Wu et.al.|[2505.20618](http://arxiv.org/abs/2505.20618)|null||This work presents a novel operator-splitting scheme for approximating viscosity solutions of constrained second-order partial differential equations (PDEs) with low-regularity solutions in \\( C(\\overline{\\Omega}_T) \\cap H^1(\\Omega_T) \\). By decoupling PDE evolution and constraint enforcement, the scheme leverages stabilized finite elements and implicit Euler time-stepping to ensure consistency, stability, and monotonicity, guaranteeing convergence to the unique viscosity solution via the Barles-Souganidis framework. The method supports vector-valued constraints and unstructured meshes, addressing challenges in traditional approaches such as restrictive stability conditions and ill-conditioned systems. Theoretical analysis demonstrates a convergence rate of \\( O(h^{1-\\epsilon}) \\) with a proper chosen time step. Applications to Hamilton-Jacobi equations, reaction-diffusion systems, and two-phase Navier-Stokes flows highlight the scheme's versatility and robustness, positioning it as a significant advancement in numerical methods for constrained nonlinear PDEs.|\n", "2505.20602": "|**2025-05-27**|**Connecting randomized iterative methods with Krylov subspaces**|Yonghan Sun, Deren Han, Jiaxin Xie et.al.|[2505.20602](http://arxiv.org/abs/2505.20602)|null||Randomized iterative methods, such as the randomized Kaczmarz method, have gained significant attention for solving large-scale linear systems due to their simplicity and efficiency. Meanwhile, Krylov subspace methods have emerged as a powerful class of algorithms, known for their robust theoretical foundations and rapid convergence properties. Despite the individual successes of these two paradigms, their underlying connection has remained largely unexplored. In this paper, we develop a unified framework that bridges randomized iterative methods and Krylov subspace techniques, supported by both rigorous theoretical analysis and practical implementation. The core idea is to formulate each iteration as an adaptively weighted linear combination of the sketched normal vector and previous iterates, with the weights optimally determined via a projection-based mechanism. This formulation not only reveals how subspace techniques can enhance the efficiency of randomized iterative methods, but also enables the design of a new class of iterative-sketching-based Krylov subspace algorithms. We prove that our method converges linearly in expectation and validate our findings with numerical experiments.|\n", "2505.20596": "|**2025-05-27**|**Seeing through the light cone: Visualizing electromagnetic fields in special relativity**|Daiju Nakayama, Kin-ya Oda, Koichiro Yasuda et.al.|[2505.20596](http://arxiv.org/abs/2505.20596)|null|25 pages, 5 figures; second half of Sec. 2 and entirety of Sec. 5   from arXiv:2408.05481v2 separated into this implementation paper following   referee's suggestion|The theoretical framework of electromagnetism played a foundational role in Einstein's development of special relativity. To support conceptual understanding, we present a fully special relativistic computer simulation that visualizes electromagnetic fields from the perspective of a moving observer. In this simulation, the user observes electromagnetic phenomena through their past light cone and directly experiences the Lorentz force acting at that spacetime point. The electromagnetic field is computed from the subluminal motion of point charges at the intersection of their worldlines with the observer's past light cone, ensuring causal consistency and Lorentz covariance. This approach offers an interactive and intuitive representation of relativistic electromagnetism. It provides insight into how electric and magnetic fields transform across inertial frames, and serves as a bridge between abstract formalism and physical intuition. The simulation also lends itself to pedagogical use in courses on special relativity or electrodynamics.|\n", "2505.20574": "|**2025-05-26**|**xChemAgents: Agentic AI for Explainable Quantum Chemistry**|Can Polat, Mehmet Tuncel, Hasan Kurban et.al.|[2505.20574](http://arxiv.org/abs/2505.20574)|null|Submitted to ICML 2025 Workshop on MAS|Recent progress in multimodal graph neural networks has demonstrated that augmenting atomic XYZ geometries with textual chemical descriptors can enhance predictive accuracy across a range of electronic and thermodynamic properties. However, naively appending large sets of heterogeneous descriptors often degrades performance on tasks sensitive to molecular shape or symmetry, and undermines interpretability. xChemAgents proposes a cooperative agent framework that injects physics-aware reasoning into multimodal property prediction. xChemAgents comprises two language-model-based agents: a Selector, which adaptively identifies a sparse, weighted subset of descriptors relevant to each target, and provides a natural language rationale; and a Validator, which enforces physical constraints such as unit consistency and scaling laws through iterative dialogue. On standard benchmark datasets, xChemAgents achieves up to a 22\\% reduction in mean absolute error over strong baselines, while producing faithful, human-interpretable explanations. Experiment results highlight the potential of cooperative, self-verifying agents to enhance both accuracy and transparency in foundation-model-driven materials science. The implementation and accompanying dataset are available anonymously at https://github.com/KurbanIntelligenceLab/xChemAgents.|\n", "2505.20560": "|**2025-05-26**|**A minimax method for the spectral fractional Laplacian and related evolution problems**|Jos\u00e9 A. Carrillo, Stefano Fronzoni, Yuji Nakatsukasa et.al.|[2505.20560](http://arxiv.org/abs/2505.20560)|null||We present a numerical method for the approximation of the inverse of the fractional Laplacian $(-\\Delta)^{s}$, based on its spectral definition, using rational functions to approximate the fractional power $A^{-s}$ of a matrix $A$, for $0<s<1$. The proposed numerical method is fast and accurate, benefiting from the fact that the matrix $A$ arises from a finite element approximation of the Laplacian $-\\Delta$, which makes it applicable to a wide range of domains with potentially irregular shapes. We make use of state-of-the-art software to compute the best rational approximation of a fractional power. We analyze the convergence rate of our method and validate our findings through a series of numerical experiments with a range of exponents $s \\in (0,1)$. Additionally, we apply the proposed numerical method to different evolution problems that involve the fractional Laplacian through an interaction potential: the fractional porous medium equation and the fractional Keller-Segel equation. We then investigate the accuracy of the resulting numerical method, focusing in particular on the accurate reproduction of qualitative properties of the associated analytical solutions to these partial differential equations.|\n", "2505.20528": "|**2025-05-26**|**Superfast 1-Norm Estimation**|Soo Go, Victor Y. Pan et.al.|[2505.20528](http://arxiv.org/abs/2505.20528)|null|21 pages, 9 figures|A matrix algorithm is said to be superfast (that is, runs at sublinear cost) if it involves much fewer scalars and flops than the input matrix has entries. Such algorithms have been extensively studied and widely applied in modern computations for matrices with low displacement rank and more recently for low-rank approximation of matrices, even though they are known to fail on worst-case inputs in the latter application. We devise novel superfast algorithms that consistently produce accurate 1-norm estimates for real-world matrices and discuss some promising extensions of our surprisingly simple techniques. With further testing and refinement, our algorithms can potentially be adopted in practical computations.|\n", "2505.23753": "|**2025-05-29**|**Efficient sampling for sparse Bayesian learning using hierarchical prior normalization**|Jan Glaubitz, Youssef Marzouk et.al.|[2505.23753](http://arxiv.org/abs/2505.23753)|null|25 pages, 17 figures|We introduce an approach for efficient Markov chain Monte Carlo (MCMC) sampling for challenging high-dimensional distributions in sparse Bayesian learning (SBL). The core innovation involves using hierarchical prior-normalizing transport maps (TMs), which are deterministic couplings that transform the sparsity-promoting SBL prior into a standard normal one. We analytically derive these prior-normalizing TMs by leveraging the product-like form of SBL priors and Knothe--Rosenblatt (KR) rearrangements. These transform the complex target posterior into a simpler reference distribution equipped with a standard normal prior that can be sampled more efficiently. Specifically, one can leverage the standard normal prior by using more efficient, structure-exploiting samplers. Our numerical experiments on various inverse problems -- including signal deblurring, inverting the non-linear inviscid Burgers equation, and recovering an impulse image -- demonstrate significant performance improvements for standard MCMC techniques.|\n", "2505.23717": "|**2025-05-29**|**Computerized Modeling of Electrophysiology and Pathoelectrophysiology of the Atria -- How Much Detail is Needed?**|Olaf D\u00f6ssel, Axel Loewe et.al.|[2505.23717](http://arxiv.org/abs/2505.23717)|null||This review focuses on the computerized modeling of the electrophysiology of the human atria, emphasizing the simulation of common arrhythmias such as atrial flutter (AFlut) and atrial fibrillation (AFib). Which components of the model are necessary to accurately model arrhythmogenic tissue modifications, including remodeling, cardiomyopathy, and fibrosis, to ensure reliable simulations? The central question explored is the level of detail required for trustworthy simulations for a specific context of use. The review discusses the balance between model complexity and computational efficiency, highlighting the risks of oversimplification and excessive detail. It covers various aspects of atrial modeling, from cellular to whole atria levels, including the influence of atrial geometry, fiber direction, anisotropy, and wall thickness on simulation outcomes. The article also examines the impact of different modeling approaches, such as volumetric 3D models, bilayer models, and single surface models, on the realism of simulations. In addition, it reviews the latest advances in the modeling of fibrotic tissue and the verification and validation of atrial models. The intended use of these models in planning and optimization of atrial ablation strategies is discussed, with a focus on personalized modeling for individual patients and cohort-based approaches for broader applications. The review concludes by emphasizing the importance of integrating experimental data and clinical validation to enhance the utility of computerized atrial models to improve patient outcomes.|\n", "2505.23702": "|**2025-05-29**|**(U)NFV: Supervised and Unsupervised Neural Finite Volume Methods for Solving Hyperbolic PDEs**|Nathan Lichtl\u00e9, Alexi Canesse, Zhe Fu et.al.|[2505.23702](http://arxiv.org/abs/2505.23702)|null||We introduce (U)NFV, a modular neural network architecture that generalizes classical finite volume (FV) methods for solving hyperbolic conservation laws. Hyperbolic partial differential equations (PDEs) are challenging to solve, particularly conservation laws whose physically relevant solutions contain shocks and discontinuities. FV methods are widely used for their mathematical properties: convergence to entropy solutions, flow conservation, or total variation diminishing, but often lack accuracy and flexibility in complex settings. Neural Finite Volume addresses these limitations by learning update rules over extended spatial and temporal stencils while preserving conservation structure. It supports both supervised training on solution data (NFV) and unsupervised training via weak-form residual loss (UNFV). Applied to first-order conservation laws, (U)NFV achieves up to 10x lower error than Godunov's method, outperforms ENO/WENO, and rivals discontinuous Galerkin solvers with far less complexity. On traffic modeling problems, both from PDEs and from experimental highway data, (U)NFV captures nonlinear wave dynamics with significantly higher fidelity and scalability than traditional FV approaches.|\n", "2505.23699": "|**2025-05-29**|**Dyn-HTE: High-temperature expansion of the dynamic Matsubara spin correlator**|Ruben Burkard, Benedikt Schneider, Bj\u00f6rn Sbierski et.al.|[2505.23699](http://arxiv.org/abs/2505.23699)|null|Code: https://github.com/bsbierski/Dyn-HTE Companion letter:   arXiv:2505.14571|The high-temperature series expansion for quantum spin models is a well-established tool to compute thermodynamic quantities and equal-time spin correlations, in particular for frustrated interactions. We extend the scope of this expansion to the dynamic Matsubara spin-spin correlator and develop a fully analytic algorithm to compute its expansion coefficients. We focus on Heisenberg models with a single coupling constant J and spin lengths S=1/2,1. The expansion coefficients up to 12th order in J/T are precomputed on all possible ~10^6 graphs embeddable in arbitrary lattices and are provided under https://github.com/bsbierski/Dyn-HTE. This enables calculation of static momentum-resolved susceptibilities for arbitrary site-pairs or wavevectors. We test our results for the S=1/2 Heisenberg chain and on the triangular lattice model. Moreover, the analytic frequency dependence in the expansion allows for stable analytic continuation to the real-frequency dynamic structure factor. This important application is discussed in a companion letter.|\n", "2505.23652": "|**2025-05-29**|**Optimization-Free Diffusion Model -- A Perturbation Theory Approach**|Yuehaw Khoo, Mathias Oster, Yifan Peng et.al.|[2505.23652](http://arxiv.org/abs/2505.23652)|null|36 pages, 6 figures|Diffusion models have emerged as a powerful framework in generative modeling, typically relying on optimizing neural networks to estimate the score function via forward SDE simulations. In this work, we propose an alternative method that is both optimization-free and forward SDE-free. By expanding the score function in a sparse set of eigenbasis of the backward Kolmogorov operator associated with the diffusion process, we reformulate score estimation as the solution to a linear system, avoiding iterative optimization and time-dependent sample generation. We analyze the approximation error using perturbation theory and demonstrate the effectiveness of our method on high-dimensional Boltzmann distributions and real-world datasets.|\n", "2505.23582": "|**2025-05-29**|**$S^{\\top\\!}S$-SVD and the Nearest Sketched Orthogonal Matrix**|Davide Palitta, Valeria Simoncini et.al.|[2505.23582](http://arxiv.org/abs/2505.23582)|null||Sketching techniques have gained popularity in numerical linear algebra to accelerate the solution of least squares problems. The so-called $\\varepsilon$-subspace embedding property of a sketching matrix $S$ has been largely used to characterize the problem residual norm, since the procedure is no longer optimal in terms of the (classical) Frobenius or Euclidean norm. By building on available results on the SVD of the sketched matrix $SA$ derived by Gilbert, Park, and Wakin (Proc. of SPARS-2013), a novel decomposition of $A$, the $S^{\\top\\!} S$-SVD, is proposed, which is exact with high probability, and in which the left singular vectors are orthonormal with respect to a (semi-)norm defined by the sketching matrix $S$. The new decomposition is less expensive to compute, while preserving the singular values with probabilistic confidence. The $S^{\\top\\!} S$-SVD appears to be the right tool to analyze the quality of several sketching techniques in the literature, for which examples are reported. For instance, it is possible to simply bound the distance from (standard) orthogonality of sketched orthogonal matrices in state-of-the-art randomized QR algorithms. As an application, the classical problem of the nearest orthogonal matrix is generalized to the new $S^{\\top\\!} S$-orthogonality, and the $S^{\\top\\!} S$-SVD is used to solve it. Probabilistic bounds on the quality of the solution are also derived.|\n", "2505.23565": "|**2025-05-29**|**DRO: A Python Library for Distributionally Robust Optimization in Machine Learning**|Jiashuo Liu, Tianyu Wang, Henry Lam et.al.|[2505.23565](http://arxiv.org/abs/2505.23565)|null||We introduce dro, an open-source Python library for distributionally robust optimization (DRO) for regression and classification problems. The library implements 14 DRO formulations and 9 backbone models, enabling 79 distinct DRO methods. Furthermore, dro is compatible with both scikit-learn and PyTorch. Through vectorization and optimization approximation techniques, dro reduces runtime by 10x to over 1000x compared to baseline implementations on large-scale datasets. Comprehensive documentation is available at https://python-dro.org.|\n", "2505.23456": "|**2025-05-29**|**Particle exchange Monte Carlo methods for eigenfunction and related nonlinear problems**|Paul Dupuis, Benjamin J. Zhang et.al.|[2505.23456](http://arxiv.org/abs/2505.23456)|null||We introduce and develop a novel particle exchange Monte Carlo method. Whereas existing methods apply to eigenfunction problems where the eigenvalue is known (e.g., integrals with respect to a Gibbs measure, which can be interpreted as corresponding to eigenvalue zero), here the focus is on problems where the eigenvalue is not known a priori. To obtain an appropriate particle exchange rule we must consider a pair of processes, with one evolving forward in time and the other backward. Applications to eigenfunction problems corresponding to quasistationary distributions and ergodic stochastic control are discussed.|\n", "2505.23429": "|**2025-05-29**|**An additive two-level parallel variant of the DMRG algorithm with coarse-space correction**|Laura Grigori, Muhammad Hassan et.al.|[2505.23429](http://arxiv.org/abs/2505.23429)|null|35 pages (including supplementary materials), 8 figures|The density matrix renormalization group (DMRG) algorithm is a popular alternating minimization scheme for solving high-dimensional optimization problems in the tensor train format. Classical DMRG, however, is based on sequential minimization, which raises challenges in its implementation on parallel computing architectures. To overcome this, we propose a novel additive two-level DMRG algorithm that combines independent, local minimization steps with a global update step using a subsequent coarse-space minimization. Our proposed algorithm, which is directly inspired by additive Schwarz methods from the domain decomposition literature, is particularly amenable to implementation on parallel, distributed architectures since both the local minimization steps and the construction of the coarse-space can be performed in parallel. Numerical experiments on strongly correlated molecular systems demonstrate that the method achieves competitive convergence rates while achieving significant parallel speedups.|\n", "2505.23356": "|**2025-05-29**|**Revolutionising Antibacterial Warfare: Machine Learning and Molecular Dynamics Unveiling Potential Gram-Negative Bacteria Inhibitors**|Pritish Joshi, Niladri Patra et.al.|[2505.23356](http://arxiv.org/abs/2505.23356)|null||Diseases caused by bacteria have been a threat to human civilisation for centuries. Despite the availability of numerous antibacterial drugs today, bacterial diseases continue to pose life-threatening challenges. The credit for this goes to Gram-Negative bacteria, which have developed multi-drug resistant properties towards \\b{eta}-lactams, chloramphenicols, fluoroquinolones, tetracyclines, carbapenems, and macrolide antibiotics. V arious mechanisms of bacterial defence contribute to drug resistance, with Multi-Drug Efflux Pumps and Enzymatic degradation being the major ones. An effective approach to cope with this resistance is to target and inhibit the activity of efflux pumps and esterases. Even though various Efflux Pump Inhibitors and Esterase resistant macrolide drugs have been proposed in the literature, none of them has achieved FDA approval due to several side effects. This research has provided valuable insights into the mechanism of drug resistance by RND efflux pump and Erythromycin esterase. A handful of potential efflux pump inhibitors have been predicted through machine learning and molecular dynamics.|\n", "2505.23344": "|**2025-05-29**|**A Descriptor Is All You Need: Accurate Machine Learning of Nonadiabatic Coupling Vectors**|Jakub Martinka, Lina Zhang, Yi-Fan Hou et.al.|[2505.23344](http://arxiv.org/abs/2505.23344)|null||Nonadiabatic couplings (NACs) play a crucial role in modeling photochemical and photophysical processes with methods such as the widely used fewest-switches surface hopping (FSSH). There is therefore a strong incentive to machine learn NACs for accelerating simulations. However, this is challenging due to NACs' vectorial, double-valued character and the singularity near a conical intersection seam. For the first time, we design NAC-specific descriptors based on our domain expertise and show that they allow learning NACs with never-before-reported accuracy of $R^2$ exceeding 0.99. The key to success is also our new ML phase-correction procedure. We demonstrate the efficiency and robustness of our approach on a prototypical example of fully ML-driven FSSH simulations of fulvene targeting the SA-2-CASSCF(6,6) electronic structure level. This ML-FSSH dynamics leads to an accurate description of $S_1$ decay while reducing error bars by allowing the execution of a large ensemble of trajectories. Our implementations are available in open-source MLatom.|\n", "2505.23245": "|**2025-05-29**|**A posteriori error estimates and adaptivity for locally conservative methods. Inexpensive implementation and evaluation, polytopal meshes, iterative linearization and algebraic solvers, and applications to complex porous media flows**|Martin Vohral\u00edk, Soleiman Yousef et.al.|[2505.23245](http://arxiv.org/abs/2505.23245)|null||A posteriori estimates give bounds on the error between the unknown solution of a partial differential equation and its numerical approximation. We present here the methodology based on H1-conforming potential and H(div)-conforming equilibrated flux reconstructions, where the error bounds are guaranteed and fully computable. We consider any lowest-order locally conservative method of the finite volume type and treat general polytopal meshes. We start by a pure diffusion problem and first address the discretization error. We then progressively pass to more complicated model problems, up to complex multiphase multicomponent flow in porous media, and also take into account the errors arising in iterative linearization of nonlinear problems and in algebraic resolution of systems of linear algebraic equations. We focus on the ease of implementation and evaluation of the estimates. In particular, the evaluation of our estimates is explicit and inexpensive, since it merely consists in some local matrix-vector multiplications. Here, on each mesh element, the matrices are either directly inherited from the given numerical method, or easily constructed from the element geometry, while the vectors are the algebraic unknowns of the flux and potential approximations on the given element. Our mtehodology leads to an easy-to-implement and fast-to-run adaptive algorithm with guaranteed overall precision, adaptive stopping criteria for nonlinear and linear solvers, and adaptive space and time mesh refinements and derefinements. Progressively along the theoretical exposition, numerical experiments on academic benchmarks as well as on real-life problems in two and three space dimensions illustrate the performance of the derived methodology. The presentation is largely self-standing, developing all the details and recalling all necessary basic notions.|\n", "2505.23216": "|**2025-05-29**|**Trefftz Discontinuous Galerkin methods for scattering by periodic structures**|Andrea Moiola, Armando Maria Monforte et.al.|[2505.23216](http://arxiv.org/abs/2505.23216)|null|30 pages, 16 figures|We propose a Trefftz discontinuous Galerkin (TDG) method for the approximation of plane wave scattering by periodic diffraction gratings, modelled by the two-dimensional Helmholtz equation. The periodic obstacle may include penetrable and impenetrable regions. The TDG method requires the approximation of the Dirichlet-to-Neumann (DtN) operator on the periodic cell faces, and relies on plane wave discrete spaces. For polygonal meshes, all linear-system entries can be computed analytically. Using a Rellich identity, we prove a new explicit stability estimate for the Helmholtz solution, which is robust in the small material jump limit.|\n", "2505.23215": "|**2025-05-29**|**Trajectory Generator Matching for Time Series**|T. Jahn, J. Chemseddine, P. Hagemann et.al.|[2505.23215](http://arxiv.org/abs/2505.23215)|null||Accurately modeling time-continuous stochastic processes from irregular observations remains a significant challenge. In this paper, we leverage ideas from generative modeling of image data to push the boundary of time series generation. For this, we find new generators of SDEs and jump processes, inspired by trajectory flow matching, that have the marginal distributions of the time series of interest. Specifically, we can handle discontinuities of the underlying processes by parameterizing the jump kernel densities by scaled Gaussians that allow for closed form formulas of the corresponding Kullback-Leibler divergence in the loss. Unlike most other approaches, we are able to handle irregularly sampled time series.|\n", "2505.23144": "|**2025-05-29**|**Flux Globalization Based Well-Balanced Path-Conservative Central-Upwind Schemes for Shallow Water Linearized Moment Equations**|Yangyang Cao, Qian Huang, Julian Koellermeier et.al.|[2505.23144](http://arxiv.org/abs/2505.23144)|null||We develop second-order path-conservative central-upwind (PCCU) schemes for the hyperbolic shallow water linearized moment equations (HSWLME), which are an extension of standard depth-averaged models for free-surface flows. The proposed PCCU schemes are constructed via flux globalization strategies adapted to the nonconservative form via a path-conservative finite-volume method. The resulting scheme is well-balanced (WB) in the sense that it is capable of exactly preserving physically relevant steady states including moving-water ones. We validate the proposed scheme on several benchmarks, including smooth solutions, small perturbation of steady states, and dam-break scenarios. These results demonstrate that our flux globalization based WB PCCU schemes provide a reliable framework for computing solutions of shallow water moment models with nonlinear and nonconservative features.|\n", "2505.23106": "|**2025-05-29**|**Neural Interpretable PDEs: Harmonizing Fourier Insights with Attention for Scalable and Interpretable Physics Discovery**|Ning Liu, Yue Yu et.al.|[2505.23106](http://arxiv.org/abs/2505.23106)|null||Attention mechanisms have emerged as transformative tools in core AI domains such as natural language processing and computer vision. Yet, their largely untapped potential for modeling intricate physical systems presents a compelling frontier. Learning such systems often entails discovering operators that map between functional spaces using limited instances of function pairs -- a task commonly framed as a severely ill-posed inverse PDE problem. In this work, we introduce Neural Interpretable PDEs (NIPS), a novel neural operator architecture that builds upon and enhances Nonlocal Attention Operators (NAO) in both predictive accuracy and computational efficiency. NIPS employs a linear attention mechanism to enable scalable learning and integrates a learnable kernel network that acts as a channel-independent convolution in Fourier space. As a consequence, NIPS eliminates the need to explicitly compute and store large pairwise interactions, effectively amortizing the cost of handling spatial interactions into the Fourier transform. Empirical evaluations demonstrate that NIPS consistently surpasses NAO and other baselines across diverse benchmarks, heralding a substantial leap in scalable, interpretable, and efficient physics learning. Our code and data accompanying this paper are available at https://github.com/fishmoon1234/Nonlocal-Attention-Operator.|\n", "2505.23046": "|**2025-05-29**|**Revisit CP Tensor Decomposition: Statistical Optimality and Fast Convergence**|Runshi Tang, Julien Chhor, Olga Klopp et.al.|[2505.23046](http://arxiv.org/abs/2505.23046)|null||Canonical Polyadic (CP) tensor decomposition is a fundamental technique for analyzing high-dimensional tensor data. While the Alternating Least Squares (ALS) algorithm is widely used for computing CP decomposition due to its simplicity and empirical success, its theoretical foundation, particularly regarding statistical optimality and convergence behavior, remain underdeveloped, especially in noisy, non-orthogonal, and higher-rank settings.   In this work, we revisit CP tensor decomposition from a statistical perspective and provide a comprehensive theoretical analysis of ALS under a signal-plus-noise model. We establish non-asymptotic, minimax-optimal error bounds for tensors of general order, dimensions, and rank, assuming suitable initialization. To enable such initialization, we propose Tucker-based Approximation with Simultaneous Diagonalization (TASD), a robust method that improves stability and accuracy in noisy regimes. Combined with ALS, TASD yields a statistically consistent estimator. We further analyze the convergence dynamics of ALS, identifying a two-phase pattern-initial quadratic convergence followed by linear refinement. We further show that in the rank-one setting, ALS with an appropriately chosen initialization attains optimal error within just one or two iterations.|\n", "2505.23002": "|**2025-05-29**|**Deep asymptotic expansion method for solving singularly perturbed time-dependent reaction-advection-diffusion equations**|Q. Zhu, D. Chaikovskii, B. Jin et.al.|[2505.23002](http://arxiv.org/abs/2505.23002)|null|24 pages|Physics-informed neural network (PINN) has shown great potential in solving differential equations. However, it faces challenges when dealing with problems involving steep gradients. For singularly perturbed time-dependent reaction-advection-diffusion equations, which exhibit internal transition layers with sharp gradients, we propose a deep asymptotic expansion (DAE) method that leverages deep learning to obtain explicit smooth approximate solutions. Inspired by asymptotic analysis, we first derive the governing equations for transition layers and then solve them using PINN. Numerical experiments show that DAE outperforms PINN, gPINN and PINN with adaptive sampling. We also show its robustness with respect to training point distributions, network architectures, and random seeds.|\n", "2505.22955": "|**2025-05-29**|**Diverse edge states of nanoribbons and excitonic insulator states of the monolayer Ta2Ni3Te5**|Hong Tang, Jiang Wei, Gabor I. Csonka et.al.|[2505.22955](http://arxiv.org/abs/2505.22955)|null|10 Figures|Ta2Ni3Te5, a layered transition metal chalcogenide with quasi-one-dimensional electronic states, exhibits rich topological and correlated phenomena. Using first-principles calculations, we explore Ta2Ni3Te5 nanoribbons, demonstrating tunable electronic and magnetic properties-ranging from metallic to semimetallic and semiconducting (band gaps of 29.7-60.8 meV), and from ferromagnetic to antiferromagnetic-controlled by edge (Ni or Ta), ribbon width, and H/F saturation. Additionally, GW and Bethe-Salpeter equation (BSE) calculations, complemented by metaGGA-based modified BSE, reveal that the Ta2Ni3Te5 monolayer is an excitonic insulator, with an exciton binding energy exceeding its band gap. These diverse properties position Ta2Ni3Te5 nanoribbons and monolayers as promising candidates for nanoelectronics, spintronics, and optoelectronics, motivating further experimental exploration.|\n", "2505.22904": "|**2025-05-28**|**Defining Foundation Models for Computational Science: A Call for Clarity and Rigor**|Youngsoo Choi, Siu Wun Cheung, Youngkyu Kim et.al.|[2505.22904](http://arxiv.org/abs/2505.22904)|null|26 pages, 2 tables, 7 figures|The widespread success of foundation models in natural language processing and computer vision has inspired researchers to extend the concept to scientific machine learning and computational science. However, this position paper argues that as the term \"foundation model\" is an evolving concept, its application in computational science is increasingly used without a universally accepted definition, potentially creating confusion and diluting its precise scientific meaning. In this paper, we address this gap by proposing a formal definition of foundation models in computational science, grounded in the core values of generality, reusability, and scalability. We articulate a set of essential and desirable characteristics that such models must exhibit, drawing parallels with traditional foundational methods, like the finite element and finite volume methods. Furthermore, we introduce the Data-Driven Finite Element Method (DD-FEM), a framework that fuses the modular structure of classical FEM with the representational power of data-driven learning. We demonstrate how DD-FEM addresses many of the key challenges in realizing foundation models for computational science, including scalability, adaptability, and physics consistency. By bridging traditional numerical methods with modern AI paradigms, this work provides a rigorous foundation for evaluating and developing novel approaches toward future foundation models in computational science.|\n", "2505.24861": "|**2025-05-30**|**A localized consensus-based sampling algorithm**|Arne Bouillon, Alexander Bodard, Panagiotis Patrinos et.al.|[2505.24861](http://arxiv.org/abs/2505.24861)|null||We develop a novel interacting-particle method for sampling from non-Gaussian distributions. As a first step, we propose a new way to derive the consensus-based sampling (CBS) algorithm, starting from ensemble-preconditioned Langevin diffusions. We approximate the target potential by its Moreau envelope, such that the gradient in the Langevin equation can be replaced by a proximal operator. We then approximate the proximal operator by a weighted mean, and finally assume that the initial and target distributions are Gaussian, resulting in the CBS dynamics. If we keep only those approximations that can be justified in the non-Gaussian setting, the result is a new interacting-particle method for sampling, which we call localized consensus-based sampling. We prove that our algorithm is affine-invariant and exact for Gaussian distributions in the mean-field setting. Numerical tests illustrate that localized CBS compares favorably to alternative methods in terms of affine-invariance and performance on non-Gaussian distributions.|\n", "2505.24839": "|**2025-05-30**|**Novel methodology to obtain transonic solutions for dissipative flows around compact objects**|Shilpa Sarkar et.al.|[2505.24839](http://arxiv.org/abs/2505.24839)|null|18 pages, 10 figures, 1 table, Accepted in Physical Review D after   minor revision|A novel methodology to obtain global transonic solutions around compact objects is reported here. A unified methodology to obtain accretion as well as wind solutions around these objects has been presented. Flows around compact objects are dissipative, and the conservation equations are therefore stiff. In such conditions, obtaining of sonic point(s) and hence, the transonic solution is not trivial. The conserved equations of motion fail to integrate in the presence of realistic viscosity, thereby making it difficult to obtain a global solution. This inhibits one from getting an actual picture of an astrophysical flow. The current work addresses this long-standing issue of obtaining solutions for both accretion and wind. The methodology developed utilises the inner boundary conditions and takes recourse to implicit-explicit (ImEx) integration schemes, to obtain general global transonic accretion and wind solutions. This is the first time such an attempt has been made. Current work considers the different cooling processes like bremsstrahlung, synchrotron and their inverse-Comptonizations, which are found to affect the thermodynamics of the flow. This methodology could successfully generate all topologies of global solutions, multiple sonic point regime, as well as shocks. A broad parameter space study has been done in this work. In an upcoming part II of the paper, a detailed discussion on the spectra and luminosity of the accretion and wind solutions has been presented.|\n", "2505.24807": "|**2025-05-30**|**PySEQM 2.0: Accelerated Semiempirical Excited State Calculations on Graphical Processing Units**|Vishikh Athavale, Nikita Fedik, William Colglazier et.al.|[2505.24807](http://arxiv.org/abs/2505.24807)|null||We report the implementation of electronic excited states for semi-empirical quantum chemical methods at the configuration interaction singles (CIS) and time-dependent Hartree-Fock (TDHF) level of theory in the PySEQM software. Built on PyTorch, this implementation leverages GPU acceleration to significantly speed up molecular property calculations. Benchmark tests demonstrate that our approach can compute excited states for molecules with nearly a thousand atoms in under a minute. Additionally, the implementation also includes a machine learning interface to enable parameters re-optimization and neural network training for future machine learning applications for excited state dynamics.|\n", "2505.24793": "|**2025-05-30**|**AFIRE: Accurate and Fast Image Reconstruction Algorithm for Geometric-inconsistency Multispectral CT**|Yu Gao, Chong Chen et.al.|[2505.24793](http://arxiv.org/abs/2505.24793)|null|36 pages, 15 figures, 1 table|For nonlinear multispectral computed tomography (CT), accurate and fast image reconstruction is challenging when the scanning geometries under different X-ray energy spectra are inconsistent or mismatched. Motivated by this, we propose an accurate and fast algorithm named AFIRE to address such problem in the case of mildly full scan. We discover that the derivative operator (gradient) of the involved nonlinear mapping at some special points, for example, at zero, can be represented as a composition (block multiplication) of a diagonal operator (matrix) composed of X-ray transforms (projection matrices) and a very small-scale matrix. Based on the insights, the AFIRE is proposed respectively from the continuous, discrete and actual-use perspectives by leveraging the simplified Newton method. Under proper conditions, we establish the convergence theory of the proposed algorithm. Furthermore, numerical experiments are also carried out to verify that the proposed algorithm can accurately and effectively reconstruct the basis images in completely geometric-inconsistency dual-energy CT with noiseless and noisy projection data. Particularly, the proposed algorithm significantly outperforms some state-of-the-art methods in terms of accuracy and efficiency. Finally, the flexibility and extensibility of the proposed algorithm are also demonstrated.|\n", "2505.24610": "|**2025-05-30**|**Potential Effects of Loading Terminal Locations on Surface Trajectories of Oil Spill Transport**|Shoshana Reich, Edward Buskey, Clint Dawson et.al.|[2505.24610](http://arxiv.org/abs/2505.24610)|null||We present an investigation comparing the potential impacts of offshore and onshore crude oil loading sites on surface trajectories of spilled oil particles in the regions near the Port of Corpus Christi, Texas. Oil transport is established in a two step procedure. First, the circulation and flow characteristics of seawater throughout the coastal ocean are established for various flow conditions, including current and proposed channel depth, seasonality changes, and extreme weather events. Then, spilled oil is modeled as distinct particles released at either the proposed onshore or offshore loading locations. The particle trajectories are tracked and used to assess the spread into diverse coastal ecosystems with extensive plant, sea, and land life. The models indicate that the extent of spread of these simulated oil spills to ecologically significant regions is greater when initiated at the onshore loading site than at the offshore site.|\n", "2505.24563": "|**2025-05-30**|**Molecular Chiral Response Enhanced by Crosstalking Quasi-Bound States in the Continuum**|Diana Shakirova, Adri\u00e0 Can\u00f3s Valero, Daniil Riabov et.al.|[2505.24563](http://arxiv.org/abs/2505.24563)|null||Identifying the handedness of chiral molecules is of fundamental importance in chemistry, biology, pharmacy, and medicine. Nanophotonic structures allow us to control light at the nanoscale and offer powerful tools for chiral sensing, enabling the detection of small analyte volumes and low molecular concentrations by harnessing optical resonances. Most existing strategies rely on intuitive concepts such as strong local field enhancement or large local optical chirality, often achieved by engineering electric and magnetic Mie resonances in dielectric or plasmonic nanostructures. Recent insights, however, reveal that the chiroptical response of resonant systems is governed not only by local field effects, but also by less obvious mechanisms such as modal crosstalk. In this work, we present a dielectric metasurface engineered to amplify the modal crosstalk by supporting two nearly degenerate, high-quality-factor resonant states known as quasi-bound states in the continuum. Our theoretical and numerical analysis predicts a pronounced differential transmittance that exceeds the detection threshold of standard spectrometers. In particular, the differential transmittance reaches up to $10^{-2}$ for the Pasteur parameter $\\kappa = 1\\cdot10^{-4}$. These findings advance the capabilities of nanophotonic sensors for chiral detection, paving the way toward ultrasensitive identification of molecular handedness in increasingly smaller volumes and concentrations at the experimentally visible level.|\n", "2505.24504": "|**2025-05-30**|**Jacobian-free Multigrid Preconditioner for Discontinuous Galerkin Methods applied to Numerical Weather Prediction**|Philipp Birken, Andreas Dedner, Robert Kl\u00f6fkorn et.al.|[2505.24504](http://arxiv.org/abs/2505.24504)|null|24 pages, 10 figures|Discontinuous Galerkin (DG) methods are promising high order discretizations for unsteady compressible flows. Here, we focus on Numerical Weather Prediction (NWP). These flows are characterized by a fine resolution in $z$-direction and low Mach numbers, making the system stiff. Thus, implicit time integration is required and for this a fast, highly parallel, low-memory iterative solver for the resulting algebraic systems. As a basic framework, we use inexact Jacobian-Free Newton-GMRES with a preconditioner.   For low order finite volume discretizations, multigrid methods have been successfully applied to steady and unsteady fluid flows. However, for high order DG methods, such solvers are currently lacking. %The lack of efficient solvers suitable for contemporary computer architectures inhibits wider adoption of DG methods. This motivates our research to construct a Jacobian-free precondtioner for high order DG discretizations. The preconditioner is based on a multigrid method constructed for a low order finite volume discretization defined on a subgrid of the DG mesh. We design a computationally efficient and mass conservative mapping between the grids. As smoothers, explicit Runge-Kutta pseudo time iterations are used, which can be implemented in parallel in a Jacobian-free low-memory manner.   We consider DG Methods for the Euler equations and for viscous flow equations in 2D, both with gravity, in a well balanced formulation. Numerical experiments in the software framework DUNE-FEM on atmospheric flow problems show the benefit of this approach.|\n", "2505.24468": "|**2025-05-30**|**Decoupling Local Electrostatic Potential and Temperature-Driven Atomistic Forming Mechanisms in TaOx/HfO2-Based ReRAMs using Reactive Molecular Dynamics Simulations**|Simanta Lahkar, Valeria Bragaglia, Behnaz Bagheri et.al.|[2505.24468](http://arxiv.org/abs/2505.24468)|null||Resistive random access memories (ReRAMs) with a bilayer TaOx/HfO2 stack structure have been shown to possess uniquely promising resistive switching characteristics. However, the key atomistic forming mechanisms and the physical processes that govern the behavior of this kind of device remain to be clarified. Here, we present a detailed analysis of the physical mechanisms underlying its forming at the atomistic level through molecular dynamics (MD) simulations using an extended charge equilibration scheme to describe the effects of applied voltage with the charge transfer ionic potential formalism. The displacement of the tantalum ions was found to be the highest, followed by that of the hafnium ions, in response to a sufficiently high applied voltage across the electrodes, whereas the oxygen ions had a relatively minor voltage-driven response. This led to the formation of a tantalum-depleted, oxygen-rich zone near the positive top electrode acting as the anode and the clustering of oxygen vacancies that nucleated into the filament near the negative bottom electrode, or the cathode. This process resulted in partial shielding of the bulk dielectric from the applied voltage. We found a minimum threshold voltage was required to initiate vacancy clustering to form the filament. Filament growth during forming is attributed to a localized mechanism, driven by thermally activated generation of oxygen vacancy defects, which get stabilized by the local electric fields near the edge of the nucleated filament at the cathode.|\n", "2505.24405": "|**2025-05-30**|**Variation of Bose surface by Filling in Cooper pair Bose metal**|Jiahao Su, Ji Liu, Jianyu Li et.al.|[2505.24405](http://arxiv.org/abs/2505.24405)|null|12 pages, 9 figures,|The Cooper pair Bose metal (CPBM) is a non-superfluid quantum phase in which uncondensed fermion pairs form a \"Bose surface\" in momentum space. We investigate the CPBM in the two-dimensional spin-anisotropic attractive Hubbard model by tuning the next-nearest-neighbor (NNN) hopping t', carrier filling n, and spin anisotropy alpha, using large-scale constrained-path quantum Monte Carlo simulations. A moderate NNN hopping (t'/t = 0.2) substantially enlarges the CPBM region: the phase extends into weaker anisotropy regimes and coexists with a commensurate charge-density wave (CDW) near half-filling (n > 0.95), where CDW order would otherwise dominate at t' = 0. Interestingly, t' suppresses the overall CDW peak amplitude and introduces a geometric correlation between the orientations of the Fermi and Bose surfaces: for weak Fermi-surface rotations, the Bose surface remains aligned with the lattice axes, while larger distortions drive both surfaces to rotate in tandem. Momentum-resolved pairing distributions reveal that the bosonic pairing channels are jointly controlled by t' and carrier filling n. For small t', d_xy-wave correlations dominate across the entire filling range. In contrast, for larger t', the dominant pairing symmetry varies with n, reflecting a nontrivial interplay between frustration and density. These findings establish carrier filling and NNN hopping as complementary levers for manipulating CPBM stability and provide concrete criteria for identifying non-superfluid bosonic matter in cold-atom and correlated-electron systems.|\n", "2505.24384": "|**2025-05-30**|**Provably convergent stochastic fixed-point algorithm for free-support Wasserstein barycenter of continuous non-parametric measures**|Zeyi Chen, Ariel Neufeld, Qikun Xiang et.al.|[2505.24384](http://arxiv.org/abs/2505.24384)|null||We propose a provably convergent algorithm for approximating the 2-Wasserstein barycenter of continuous non-parametric probability measures. Our algorithm is inspired by the fixed-point iterative scheme of \\'Alvarez-Esteban et al. (2016) whose convergence to the 2-Wasserstein barycenter relies on obtaining exact optimal transport (OT) maps. However, typically in practice, OT maps are only approximately computed and exact computation of OT maps between continuous probability measures is only tractable for certain restrictive parametric families. To circumvent the need to compute exact OT maps between general non-parametric measures, we develop a tailored iterative scheme that utilizes consistent estimators of the OT maps instead of the exact OT maps. This gives rise to a computationally tractable stochastic fixed-point algorithm which is provably convergent to the 2-Wasserstein barycenter. Our algorithm remarkably does not restrict the support of the 2-Wasserstein barycenter to be any fixed finite set and can be implemented in a distributed computing environment, which makes it suitable for large-scale data aggregation problems. In our numerical experiments, we propose a method of generating non-trivial instances of 2-Wasserstein barycenter problems where the ground-truth barycenter measure is known. Our numerical results showcase the capability of our algorithm in developing high-quality approximations of the 2-Wasserstein barycenter, as well as its superiority over state-of-the-art methods based on generative neural networks in terms of accuracy, stability, and efficiency.|\n", "2505.24328": "|**2025-05-30**|**Identifiability through special linear measurements**|Fulvio Gesmundo, Alexandros Grosdos, Andr\u00e9 Uschmajew et.al.|[2505.24328](http://arxiv.org/abs/2505.24328)|null|12 pages, 1 figure. Comments are welcome|We show that one can always identify a point on an algebraic variety $X$ uniquely with $\\dim X +1$ generic linear measurements taken themselves from a variety under minimal assumptions. As illustrated by several examples the result is sharp, that is, $\\dim X$ measurements are in general not enough for unique identifiability.|\n", "2505.24288": "|**2025-05-30**|**Factorization method for near-field inverse scattering problems in elastodynamics**|Chun Liu, Guanghui Hu, Tao Yin et.al.|[2505.24288](http://arxiv.org/abs/2505.24288)|null|20 pages, 20 figures|Consider a time-harmonic elastic point source incident on a bounded obstacle which is embedded in an open space filled with a homogeneous and isotropic elastic medium. This paper is concerned with the inverse problem of recovering the location and shape of the obstacle from near-field data generated by infinitely many incident point source waves at a fixed energy. The incident point sources and the receivers for recording scattered signals are both located on a spherical closed surface, on which an outgoing-to-incoming operator is defined for facilitating the factorization of the near-field operator. Numerical examples in 2D are presented to show the validity and accuracy of the inversion algorithm.|\n", "2505.24270": "|**2025-05-30**|**Nonlinear PDEs with modulated dispersion IV: normal form approach and unconditional uniqueness**|Massimiliano Gubinelli, Guopeng Li, Jiawei Li et.al.|[2505.24270](http://arxiv.org/abs/2505.24270)|null|36 pages|We study the modulated Korteweg-de~Vries equation (KdV) on the circle with a time non-homogeneous modulation acting on the linear dispersion term. By adapting the normal form approach to the modulated setting, we prove sharp unconditional uniqueness of solutions to the modulated KdV in $L^2(\\mathbb T)$ if a modulation is sufficiently irregular. For example, this result implies that if the modulation is given by a sample path of a fractional Brownian motion with Hurst index $0 < H < \\frac 25$, the modulated KdV on the circle is unconditionally well-posed in $L^2(\\mathbb T)$. Our normal form approach provides the construction of solutions to the modulated KdV (and the associated nonlinear Young integral) {\\it without} assuming any positive regularity in time. As an interesting byproduct of our normal form approach, we extend the construction of the nonlinear Young integral to a much larger class of functions, and obtain an improved Euler approximation scheme as compared to the classical sewing lemma approach. We also establish analogous sharp unconditional uniqueness results for the modulated Benjamin-Ono equation and the modulated derivative nonlinear Schr\\\"odinger equation (NLS) with a quadratic nonlinearity. In the appendix, we prove sharp unconditional uniqueness of the cubic modulated NLS on the circle in $H^{\\frac 16}(\\mathbb T)$.|\n", "2505.24210": "|**2025-05-30**|**STORK: Improving the Fidelity of Mid-NFE Sampling for Diffusion and Flow Matching Models**|Zheng Tan, Weizhen Wang, Andrea L. Bertozzi et.al.|[2505.24210](http://arxiv.org/abs/2505.24210)|null||Diffusion models (DMs) have demonstrated remarkable performance in high-fidelity image and video generation. Because high-quality generations with DMs typically require a large number of function evaluations (NFEs), resulting in slow sampling, there has been extensive research successfully reducing the NFE to a small range (<10) while maintaining acceptable image quality. However, many practical applications, such as those involving Stable Diffusion 3.5, FLUX, and SANA, commonly operate in the mid-NFE regime (20-50 NFE) to achieve superior results, and, despite the practical relevance, research on the effective sampling within this mid-NFE regime remains underexplored. In this work, we propose a novel, training-free, and structure-independent DM ODE solver called the Stabilized Taylor Orthogonal Runge--Kutta (STORK) method, based on a class of stiff ODE solvers with a Taylor expansion adaptation. Unlike prior work such as DPM-Solver, which is dependent on the semi-linear structure of the DM ODE, STORK is applicable to any DM sampling, including noise-based and flow matching-based models. Within the 20-50 NFE range, STORK achieves improved generation quality, as measured by FID scores, across unconditional pixel-level generation and conditional latent-space generation tasks using models like Stable Diffusion 3.5 and SANA. Code is available at https://github.com/ZT220501/STORK.|\n", "2505.24194": "|**2025-05-30**|**Energy-Embedded Neural Solvers for One-Dimensional Quantum Systems**|Yi-Qiang Wu, Xuan Liu, Hanlin Li et.al.|[2505.24194](http://arxiv.org/abs/2505.24194)|null|27 pages, 7figures|Physics-informed neural networks (PINN) have been widely used in computational physics to solve partial differential equations (PDEs). In this study, we propose an energy-embedding-based physics-informed neural network method for solving the one-dimensional time-independent Schr\\\"{o}dinger equation to obtain ground- and excited-state wave functions, as well as energy eigenvalues by incorporating an embedding layer to generate process-driven data. The method demonstrates high accuracy for several well-known potentials, such as the infinite potential well, harmonic oscillator potential, Woods-Saxon potential, and double-well potential. Further validation shows that the method also performs well in solving the radial Coulomb potential equation, showcasing its adaptability and extensibility. The proposed approach can be extended to solve other partial differential equations beyond the Schr\\\"{o}dinger equation and holds promise for applications in high-dimensional quantum systems.|\n", "2505.24191": "|**2025-05-30**|**Benchmarking Quantum Heuristics: Non-Variational QWOA for Weighted Maxcut**|Tavis Bennett, Aidan Smith, Edric Matwiejew et.al.|[2505.24191](http://arxiv.org/abs/2505.24191)|null|14 pages, 6 figures|We present benchmarking results for the non-variational Quantum Walk Optimisation Algorithm (non-variational QWOA) applied to the weighted maxcut problem, using classical simulations for problem sizes up to $n = 31$. The amplified quantum state, prepared using a quadratic number of alternating unitaries, achieves a constant average-case measurement probability for globally optimal solutions across these problem sizes. This behaviour contrasts with that of classical heuristics, which, for NP-hard optimisation problems, typically exhibit solve probabilities that decay as problem size increases. Performance comparisons with two local-search heuristics on the same benchmark instances suggest that the non-variational QWOA may offer a meaningful advantage by scaling more favourably with problem size. These results provide supporting evidence for the potential of this quantum heuristic to achieve quantum advantage, though further work is needed to assess whether the observed performance scaling persists at larger problem sizes, and to confirm whether similar performance trends are observed for the other problem classes to which the non-variational QWOA is designed to generalise.|\n", "2505.24186": "|**2025-05-30**|**Photostriction-tunable Polarization and Structural Dynamics in Interlayer Sliding Ferroelectrics**|Kun Yang, Jianxin Yu, Jia Zhang et.al.|[2505.24186](http://arxiv.org/abs/2505.24186)|null|4 figures|Two-dimensional ferroelectrics with robust polarization offer promising opportunities for non-volatile memory, field-effect transistors, and optoelectronic devices. However, the impact of lattice deformation on polarization and photoinduced structural response remains poorly understood. Here, we employ first-principles calculations to demonstrate photodoping-induced lattice expansion in rhombohedrally stacked bilayer MoS2, revealing a strong coupling between photodoping carrier and lattice structure. We identify a pronounced photostrictive response in sliding ferroelectrics, wherein electron-hole excitation leads to substantial in-plane expansion, increased interlayer spacing, and enhanced ferroelectric polarization. This strain-induced modulation drives significant bandgap renormalization. The photostriction-tunable polarization and structural dynamics arise from the strong electromechanical coupling inherent to the non-centrosymmetric rhombohedral stacking. The findings provide critical insights into the nonthermal lattice expansion governing sliding ferroelectrics at atomic-scale timescales, while simultaneously laying the groundwork for next-generation electronic and memory technologies by leveraging lattice-tunable polarization switching.|\n", "2505.24170": "|**2025-05-30**|**A Method for Analytical Solutions in the Lattice Boltzmann Method**|Jordan Larson, Alexander J. Wagner et.al.|[2505.24170](http://arxiv.org/abs/2505.24170)|null|13 pages main document, 2 pages supplemental material; 11 figures; to   be submitted to Physical Review E|Analytical solutions to the lattice Boltzmann Equation make it possible to study the method itself, explore the properties of its collision operator, and identify implementations of boundary conditions. In this paper, we propose a method to find analytical solutions where the macroscopic flow profile is known. We test this method on bulk Couette flow aligned and inclined to the simulation lattice with the quadratic and entropic equilibrium distributions. Our method indeed provides an analytical solution to these flows when using the quadratic distribution. When the flow is aligned to the lattice, our method provides an analytical solution using the entropic distribution for practical relaxation times and shear rates. We show that a small even order truncation of the formal solution is optimal for accuracy-compute-time trade-off. In the inclined case, our method does not conserve momentum, by a small relative error, when using the entropic distribution. We also discover that entropic lattice Boltzmann method is not compatible with the angled Couette flow. We discuss the application of our method to more complicated flows.|\n", "2505.24132": "|**2025-05-30**|**Information-theoretic machine learning for time-varying mode decomposition of separated airfoil wakes**|Kai Fukami, Ryo Araki et.al.|[2505.24132](http://arxiv.org/abs/2505.24132)|null||We perform an information-theoretic mode decomposition for separated wakes around a wing. The current data-driven approach based on a neural network referred to as deep sigmoidal flow enables the extraction of an informative component from a given flow field snapshot with respect to a target variable at a future time stamp, thereby capturing the causality as a time-varying modal structure. We consider three examples of separated flows around a NACA0012 airfoil, namely, 1. laminar periodic wake at post-stall angles of attack, 2. strong vortex-airfoil interactions, and 3. a turbulent wake in a spanwise-periodic domain. The present approach reveals informative vortical structures associated with a time-varying lift response. For the periodic shedding cases, the informative structures vary in time corresponding to the fluctuation level from their mean values. With the second example of vortex-airfoil interactions, how the effect of vortex gust on a wing emerges in the lift response over time is identified in an interpretable manner. Furthermore, for the case of turbulent wake, the present model highlights structures near the wing and vortex cores as informative components based solely on the information metric without any prior knowledge of aerodynamics and length scales. This study provides causality-based insights into a range of unsteady aerodynamic problems.|\n", "2505.24058": "|**2025-05-29**|**A positivity-preserving hybrid DDG method for Poisson--Nernst--Planck systems**|Hailiang Liu, Zhongming Wang, Peimeng Yin et.al.|[2505.24058](http://arxiv.org/abs/2505.24058)|null||In earlier work [H. Liu and Z. Wang, J. Comput. Phys., 328(2017)], an arbitrary high-order conservative and energy-dissipative direct discontinuous Galerkin (DDG) scheme was developed. Although this scheme enforced solution positivity using cell averages as reference values, it lacked a theoretical guarantee for the positivity of those cell averages. In this study, we develop a novel arbitrary high-order DDG method with rigorously proven positivity-preserving properties. Specifically, the positivity of the cell averages is ensured through a modified numerical flux in combination with forward Euler time discretization. To achieve point-wise positivity of ion concentrations, we introduce a hybrid algorithm that integrates a positivity-preserving limiter. The proposed method is further extended to higher-dimensional problems with rectangular meshes. Numerical results confirm the scheme's high-order accuracy, guaranteed positivity preservation, and consistent discrete energy dissipation.|\n", "2506.03124": "|**2025-06-03**|**Simulating dynamics of correlated matter with neural quantum states**|Markus Schmitt, Markus Heyl et.al.|[2506.03124](http://arxiv.org/abs/2506.03124)|null||While experimental advancements continue to expand the capabilities to control and probe non-equilibrium quantum matter at an unprecedented level, the numerical simulation of the dynamics of correlated quantum systems remains a pivotal challenge - especially in intermediate spatial dimensions. Neural quantum states are emerging as a new computational tool to investigate the time evolution of many-body quantum systems in previously inaccessible regimes. We review the recent progress in the field with a focus on the different time propagation methods, an overview of the reported applications, and a discussion of the major current challenges.|\n", "2506.03081": "|**2025-06-03**|**A structure-preserving and thermodynamically compatible cell-centered Lagrangian finite volume scheme for continuum mechanics**|Walter Boscheri, Michael Dumbser, Raphael Loub\u00e8re et.al.|[2506.03081](http://arxiv.org/abs/2506.03081)|null||In this work we present a novel structure-preserving scheme for the discretization of the Godunov-Peshkov-Romenski (GPR) model of continuum mechanics written in Lagrangian form. This model admits an extra conservation law for the total energy (first principle of thermodynamics) and satisfies the entropy inequality (second principle of thermodynamics). Furthermore, in the absence of algebraic source terms, the distortion field of the continuum and the specific thermal impulse satisfy a curl-free condition, provided the initial data are curl-free. Last but not least, the determinant of the distortion field is related to the density of the medium, i.e. the system is also endowed with a nonlinear algebraic constraint.   The objective of this work is to construct and analyze a new semi-discrete thermodynamically compatible cell-centered Lagrangian finite volume scheme on moving unstructured meshes that satisfies the following structural properties of the governing PDE exactly at the discrete level: i) compatibility with the first law of thermodynamics, i.e. discrete total energy conservation; ii) compatibility with the second law of thermodynamics, i.e. discrete entropy inequality; iii) exact discrete compatibility between the density and the determinant of the distortion field; iv) exact preservation of the curl-free property of the distortion field and of the specific thermal impulse in the absence of algebraic source terms. We show that it is possible to achieve all above properties simultaneously. Unlike in existing schemes, we choose to directly discretize the entropy inequality, hence obtaining total energy conservation as a consequence of an appropriate and thermodynamically compatible discretization of all the other equations.|\n", "2506.03070": "|**2025-06-03**|**GPU-Parallelizable Randomized Sketch-and-Precondition for Linear Regression using Sparse Sign Sketches**|Tyler Chen, Pradeep Niroula, Archan Ray et.al.|[2506.03070](http://arxiv.org/abs/2506.03070)|null||A litany of theoretical and numerical results have established the sketch-and-precondition paradigm as a powerful approach to solving large linear regression problems in standard computing environments. Perhaps surprisingly, much less work has been done on understanding how sketch-and-precondition performs on graphics processing unit (GPU) systems. We address this gap by benchmarking an implementation of sketch-and-precondition based on sparse sign-sketches on single and multi-GPU systems. In doing so, we describe a novel, easily parallelized, rejection-sampling based method for generating sparse sign sketches. Our approach, which is particularly well-suited for GPUs, is easily adapted to a variety of computing environments. Taken as a whole, our numerical experiments indicate that sketch-and-precondition with sparse sign sketches is particularly well-suited for GPUs, and may be suitable for use in black-box least-squares solvers.|\n", "2506.03039": "|**2025-06-03**|**Rates of convergence of finite element approximations of second-order mean field games with nondifferentiable Hamiltonians**|Yohance A. P. Osborne, Iain Smears et.al.|[2506.03039](http://arxiv.org/abs/2506.03039)|null|31 pages|We prove a rate of convergence for finite element approximations of stationary, second-order mean field games with nondifferentiable Hamiltonians posed in general bounded polytopal Lipschitz domains with strongly monotone running costs. In particular, we obtain a rate of convergence in the $H^1$-norm for the value function approximations and in the $L^2$-norm for the approximations of the density. We also establish a rate of convergence for the error between the exact solution of the MFG system with a nondifferentiable Hamiltonian and the finite element discretizations of the corresponding MFG system with a regularized Hamiltonian.|\n", "2506.03027": "|**2025-06-03**|**Accuracy and scalability of asynchronous compressible flow solver for transitional flows**|Aswin Kumar Arumugam, Shubham Kumar Goswami, Nagabhushana Rao Vadlamani et.al.|[2506.03027](http://arxiv.org/abs/2506.03027)|null||To overcome the communication bottlenecks observed in state-of-the-art parallel time-dependent flow solvers at extreme scales, an asynchronous computing approach that relaxes communication and synchronization at a mathematical level was previously developed. This approach preserves high-order accuracy of computations near processing element boundaries using asynchrony-tolerant (AT) schemes while significantly improving the scalability. The numerical properties of the AT schemes were studied based on simple linear and nonlinear partial differential equations (PDEs) in previous works. Allowing asynchrony in numerical schemes can minimize communication overheads in a parallel setting in two ways: one that avoids communication over a few predetermined time steps, and the other that initiates communications without enforcing synchronization. In this study, the asynchronous algorithms are incorporated into the high-order compressible flow solver COMP-SQUARE, which solves practically relevant flow problems in complex geometries in a multi-block framework. The numerical efficacy and scalability of the two asynchronous algorithms are demonstrated for three test cases: isentropic advection of a vortex, the Taylor-Green vortex, and a much more sensitive case of the flow transitioning on a NACA0012 airfoil. Speed-ups of up to $4\\times$ with respect to the baseline synchronous algorithm are observed in the scaling experiments performed on up to 18,432 cores. The results of this study demonstrate the applicability of AT schemes on established CFD solvers to improve scalability at extreme scales as the scientific computing environment moves to the exascale era.|\n", "2506.03025": "|**2025-06-03**|**Bivariate polynomial histopolation techniques on Padua, Fekete and Leja triangles**|Ludovico Bruni Bruno, Francesco Dell'Accio, Wolfgang Erb et.al.|[2506.03025](http://arxiv.org/abs/2506.03025)|null|24 pages, 15 figures|This paper explores the reconstruction of a real-valued function $f$ defined over a domain $\\Omega \\subset \\mathbb{R}^2$ using bivariate polynomials that satisfy triangular histopolation conditions. More precisely, we assume that only the averages of $f$ over a given triangulation $\\mathcal{T}_N$ of $\\Omega$ are available and seek a bivariate polynomial that approximates $f$ using a histopolation approach, potentially flanked by an additional regression technique. This methodology relies on the selection of a subset of triangles $\\mathcal{T}_M \\subset \\mathcal{T}_N$ for histopolation, ensuring both the solvability and the well-conditioning of the problem. The remaining triangles can potentially be used to enhance the accuracy of the polynomial approximation through a simultaneous regression. We will introduce histopolation and combined histopolation-regression methods using the Padua points, discrete Leja sequences, and approximate Fekete nodes. The proposed algorithms are implemented and evaluated through numerical experiments that demonstrate their effectiveness in function approximation.|\n", "2506.03014": "|**2025-06-03**|**Convergence and efficiency proof of quantum imaginary time evolution for bounded order systems**|Tobias Hartung, Karl Jansen et.al.|[2506.03014](http://arxiv.org/abs/2506.03014)|null|15 pages|Many current and near-future applications of quantum computing utilise parametric families of quantum circuits and variational methods to find optimal values for these parameters. Solving a quantum computational problem with such variational methods relies on minimising some cost function, e.g., the energy of a physical system. As such, this is similar to the training process in machine learning and variational quantum simulations can therefore suffer from similar problems encountered in machine learning training. This includes non-convergence to the global minimum due to local minima as well as critical slowing down. In this article, we analyse the imaginary time evolution as a means of compiling parametric quantum circuits and finding optimal parameters, and show that it guarantees convergence to the global minimum without critical slowing down. We also show that the compilation process, including the task of finding optimal parameters, can be performed efficiently up to an arbitrary error threshold if the underlying physical system is of bounded order. This includes many relevant computational problems, e.g., local physical theories and combinatorial optimisation problems such as the flight-to-gate assignment problem. In particular, we show a priori estimates on the success probability for these combinatorial optimisation problems. There seem to be no known classical methods with similar efficiency and convergence guarantees. Meanwhile the imaginary time evolution method can be implemented on current quantum computers.|\n", "2506.03003": "|**2025-06-03**|**Newtonian potentials of Legendre polynomials on rectangles have displacement structure**|Sheehan Olver et.al.|[2506.03003](http://arxiv.org/abs/2506.03003)|null||Particular solutions of the Poisson equation can be constructed via Newtonian potentials, integrals involving the corresponding Green's function which in two-dimensions has a logarithmic singularity. The singularity represents a significant challenge for computing the integrals, which is typically overcome via specially designed quadrature methods involving a large number of evaluations of the function and kernel. We present an attractive alternative: we show that Newtonian potentials (and their gradient) applied to (tensor products of) Legendre polynomials can be expressed in terms of complex integrals which satisfy simple and explicit recurrences that can be utilised to exactly compute singular integrals, i.e., singular integral quadrature is completely avoided. The inhomogeneous part of the recurrence has low rank structure (its rank is at most three for the Newtonian potential) and hence these recurrences have displacement structure. Using the recurrence directly is a fast approach for evaluation on or near the integration domain that remains accurate for low degree polynomial approximations, while high-precision arithmetic allows accurate use of the approach for moderate degree polynomials.|\n", "2506.02947": "|**2025-06-03**|**Real and finite field versions of Chebotarev's theorem**|Tarek Emmrich, Stefan Kunis et.al.|[2506.02947](http://arxiv.org/abs/2506.02947)|null||Chebotarev's theorem on roots of unity states that all minors of the Fourier matrix of prime size are non-vanishing. This result has been rediscovered several times and proved via different techniques. We follow the proof of Evans and Isaacs and generalize the original result to a real version and a version over finite fields. For the latter, we are able to remove an order condition between the characteristic of the field and the size of the matrix as well as decrease a sufficient lower bound on the characteristic by Zhang considerably. Direct applications include a specific real phase retrieval problem as well as a recent result for Riesz bases of exponentials.|\n", "2506.02816": "|**2025-06-03**|**Eigenvalue bounds for preconditioned symmetric multiple saddle-point matrices**|L. Bergamaschi, A. Martinez, J. W. Pearson et.al.|[2506.02816](http://arxiv.org/abs/2506.02816)|null||We develop eigenvalue bounds for symmetric, block tridiagonal multiple saddle-point linear systems, preconditioned with block diagonal matrices. We extend known results for $3 \\times 3$ block systems [Bradley and Greif, IMA J.\\ Numer. Anal. 43 (2023)] and for $4 \\times 4$ systems [Pearson and Potschka, IMA J. Numer. Anal. 44 (2024)] to an arbitrary number of blocks. Moreover, our results generalize the bounds in [Sogn and Zulehner, IMA J. Numer. Anal. 39 (2018)], developed for an arbitrary number of blocks with null diagonal blocks. Extension to the bounds when the Schur complements are approximated is also provided, using perturbation arguments. Practical bounds are also obtained for the double saddle-point linear system. Numerical experiments validate our findings.|\n", "2506.02815": "|**2025-06-03**|**The Bayesian Finite Element Method in Inverse Problems: a Critical Comparison between Probabilistic Models for Discretization Error**|Anne Poot, Iuri Rocha, Pierre Kerfriden et.al.|[2506.02815](http://arxiv.org/abs/2506.02815)|null|20 pages, 9 figures|When using the finite element method (FEM) in inverse problems, its discretization error can produce parameter estimates that are inaccurate and overconfident. The Bayesian finite element method (BFEM) provides a probabilistic model for the epistemic uncertainty due to discretization error. In this work, we apply BFEM to various inverse problems, and compare its performance to the random mesh finite element method (RM-FEM) and the statistical finite element method (statFEM), which serve as a frequentist and inference-based counterpart to BFEM. We find that by propagating this uncertainty to the posterior, BFEM can produce more accurate parameter estimates and prevent overconfidence, compared to FEM. Because the BFEM covariance operator is designed to leave uncertainty only in the appropriate space, orthogonal to the FEM basis, BFEM is able to outperform RM-FEM, which does not have such a structure to its covariance. Although inferring the discretization error via a model misspecification component is possible as well, as is done in statFEM, the feasibility of such an approach is contingent on the availability of sufficient data. We find that the BFEM is the most robust way to consistently propagate uncertainty due to discretization error to the posterior of a Bayesian inverse problem.|\n", "2506.02807": "|**2025-06-03**|**An open-source finite element toolbox for anisotropic creep and irradiation growth: Application to tube and spacer grid assembly**|Fabrizio E. Aguzzi, Santiago M. Rabazzi, Mart\u00edn S. Armoa et.al.|[2506.02807](http://arxiv.org/abs/2506.02807)|null|preprint on review|This work presents an open-source interface that couples the viscoplastic self-consistent (VPSC) model capable of simulating anisotropic creep and irradiation growth in polycrystalline materials with the finite element solver Code_Aster. The interface enables the simulation of the micromechanical response of irradiated zirconium alloy components by integrating grain-level constitutive behavior into a structural FEM framework. A key feature is the automated rotation of stress and strain tensors between the global FEM frame and the local crystallographic axes, a transformation not natively supported by Code_Aster. The elastic strain is recovered analytically using the inverse of the self-consistently stiffness tensor provided by VPSC. As a demonstration, the framework is applied to an actual model of a pressurized water reactor (PWR) spacer grid, based on a patented design, capturing nonlinear contact and the anisotropic response of the cladding and grid. Simulations reveal the micromechanisms controlling the evolution of clearance between components and highlight the role of crystallographic texture in mitigating wear. In particular, a texture with a high fraction of prismatic planes oriented in the normal direction of the grid appears to be the most suitable for spacer design, as it minimizes clearance and contributes to wear resistance. The interface offers a flexible, extensible platform for high-fidelity simulations in nuclear fuel performance analysis.|\n", "2506.02792": "|**2025-06-03**|**Exploring metrics for analyzing dynamic behavior in MPI programs via a coupled-oscillator model**|Ayesha Afzal, Georg Hager, Gerhard Wellen et.al.|[2506.02792](http://arxiv.org/abs/2506.02792)|null||We propose a novel, lightweight, and physically inspired approach to modeling the dynamics of parallel distributed-memory programs. Inspired by the Kuramoto model, we represent MPI processes as coupled oscillators with topology-aware interactions, custom coupling potentials, and stochastic noise. The resulting system of nonlinear ordinary differential equations opens a path to modeling key performance phenomena of parallel programs, including synchronization, delay propagation and decay, bottlenecks, and self-desynchronization.   This paper introduces interaction potentials to describe memory- and compute-bound workloads and employs multiple quantitative metrics -- such as an order parameter, synchronization entropy, phase gradients, and phase differences -- to evaluate phase coherence and disruption. We also investigate the role of local noise and show that moderate noise can accelerate resynchronization in scalable applications. Our simulations align qualitatively with MPI trace data, showing the potential of physics-informed abstractions to predict performance patterns, which offers a new perspective for performance modeling and software-hardware co-design in parallel computing.|\n", "2506.02778": "|**2025-06-03**|**Nonsmooth data error estimates for exponential Runge--Kutta methods and applications to split exponential integrators**|Qiumei Huang, Alexander Ostermann, Gangfan Zhong et.al.|[2506.02778](http://arxiv.org/abs/2506.02778)|null||We derive error bounds for exponential Runge-Kutta discretizations of parabolic equations with nonsmooth initial data. Our analysis is carried out in a framework of abstract semilinear evolution equations with operators having non-dense domain. In particular, we investigate nonsmooth data error estimates for the Allen-Cahn and the Burgers' equation. As an application, we apply these nonsmooth data error estimates to split exponential integrators and derive a convergence result in terms of the data.|\n", "2506.02747": "|**2025-06-03**|**A priori error estimates for the $\u03b8$-method for the flow of nonsmooth velocity fields**|Gennaro Ciampa, Tommaso Cortopassi, Gianluca Crippa et.al.|[2506.02747](http://arxiv.org/abs/2506.02747)|null||Velocity fields with low regularity (below the Lipschitz threshold) naturally arise in many models from mathematical physics, such as the inhomogeneous incompressible Navier-Stokes equations, and play a fundamental role in the analysis of nonlinear PDEs. The DiPerna-Lions theory ensures existence and uniqueness of the flow associated with a divergence-free velocity field with Sobolev regularity. In this paper, we establish a priori error estimates showing a logarithmic rate of convergence of numerical solutions, constructed via the $\\theta$-method, towards the exact (analytic) flow for a velocity field with Sobolev regularity. In addition, we derive analogous a priori error estimates for Lagrangian solutions of the associated transport equation, exhibiting the same logarithmic rate of convergence. Our theoretical results are supported by numerical experiments, which confirm the predicted logarithmic behavior.|\n", "2506.02716": "|**2025-06-03**|**Reentrant localization in a quasiperiodic chain with correlated hopping sequences**|Sourav Karmakar, Sudin Ganguly, Santanu K. Maiti et.al.|[2506.02716](http://arxiv.org/abs/2506.02716)|null|7 pages, 5 figures. Comments are welcome|Quasiperiodic systems are known to exhibit localization transitions in low dimensions, wherein all electronic states become localized beyond a critical disorder strength. Interestingly, recent studies have uncovered a reentrant localization (RL) phenomenon: upon further increasing the quasiperiodic disorder strength beyond the localization threshold, a subset of previously localized states can become delocalized again within a specific parameter window. While RL transitions have been primarily explored in systems with simple periodic modulations, such as dimerized or long-range hopping integrals, the impact of more intricate or correlated hopping structures on RL behavior remains largely elusive. In this work, we investigate the localization behavior in a one-dimensional lattice featuring staggered, correlated on-site potentials following the Aubry-Andr\\'{e}-Harper model, along with off-diagonal hopping modulations structured according to quasiperiodic Fibonacci and Bronze Mean sequences. By systematically analyzing the fractal dimension, inverse participation ratio, and normalized participation ratio, we demonstrate the occurrence of RL transitions induced purely by the interplay between quasiperiodic on-site disorder and correlated hopping. Our findings highlight the crucial role of underlying structural correlations in governing localization-delocalization transitions in low-dimensional quasiperiodic systems, where the correlated disorder manifests in both diagonal and off-diagonal terms.|\n", "2506.02678": "|**2025-06-03**|**TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression**|Zhong-Zhi Li, Xiao Liang, Zihao Tang et.al.|[2506.02678](http://arxiv.org/abs/2506.02678)|null||Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon.|\n", "2506.02663": "|**2025-06-03**|**Fourth-order Adaptive Mesh Refinement both in space and in time for incompressible Navier-Stokes equations with Dirichlet boundary conditions**|Shubo Zhao, Qinghai Zhang et.al.|[2506.02663](http://arxiv.org/abs/2506.02663)|null||We present a fourth-order projection method with adaptive mesh refinement (AMR) for numerically solving the incompressible Navier-Stokes equations (INSE) with subcycling in time. Our method features (i) a reformulation of INSE so that the velocity divergence decays exponentially on the coarsest level, (ii) a derivation of coarse-fine interface conditions that preserves the decay of velocity divergence on any refinement level of the AMR hierarchy, (iii) an approximation of the coarse-fine interface conditions via spatiotemporal interpolations to facilitate subcycling in time, (iv) enforcing to machine precision solvability conditions of elliptic equations over each connected component of the subdomain covered by any refinement level, (v) a composite projection for synchronizing multiple levels, and (vi) geometric multigrid for solving linear systems with optimal complexity. Different from current block-structured AMR algorithms, our method never adopts refluxing at the coarse-fine interface, nor is fine-to-coarse averaging applied to projected velocities. Results of numerical tests demonstrate the high accuracy and efficiency of the proposed method.|\n", "2506.02647": "|**2025-06-03**|**Multilevel Stochastic Gradient Descent for Optimal Control Under Uncertainty**|Niklas Baumgarten, David Schneiderhan et.al.|[2506.02647](http://arxiv.org/abs/2506.02647)|null||We present a multilevel stochastic gradient descent method for the optimal control of systems governed by partial differential equations under uncertain input data. The gradient descent method used to find the optimal control leverages a parallel multilevel Monte Carlo method as stochastic gradient estimator. As a result, we achieve precise control over the stochastic gradient's bias, introduced by numerical approximation, and its sampling error, arising from the use of incomplete gradients, while optimally managing computational resources. We show that the method exhibits linear convergence in the number of optimization steps while avoiding the cost of computing the full gradient at the highest fidelity. Numerical experiments demonstrate that the method significantly outperforms the standard (mini-) batched stochastic gradient descent method in terms of convergence speed and accuracy. The method is particularly well-suited for high-dimensional control problems, taking advantage of parallel computing resources and a distributed multilevel data structure. Additionally, we evaluate and implement different step size strategies, optimizer schemes, and budgeting techniques. The method's performance is studied using a two-dimensional elliptic subsurface diffusion problem with log-normal coefficients and Mat\\'ern covariance.|\n", "2506.02644": "|**2025-06-04**|**Non-exchangeable evolutionary and mean field games and their applications**|H. Yoshioka, M. Tsujimura, T. Tanaka et.al.|[2506.02644](http://arxiv.org/abs/2506.02644)|null||A replicator dynamic for non-exchangeable agents in a continuous action space is formulated and its well-posedness is proven in a space of probability measures. The non-exchangeability allows for the analysis of evolutionary games involving agents with distinct (and possibly infinitely many) types. We also explicitly connect this replicator dynamic to a stationary mean field game, which determines the pairwise actions of the heterogeneous agents. Moreover, as a byproduct of our theoretical results, we show that a class of nonlinear voter models, recently the subject of increasing interest, called q-voter models, can be viewed as a replicator dynamic driven by a utility that is a power of the probability density. This implies that non-exchangeable and/or mean-field game formulations of these models can also be constructed. We also present computational examples of evolutionary and mean field game models using a finite difference method, focusing on tragedy of the commons and the q-voter model with non-exchangeable agents, of which are interesting cases from theoretical and computational perspectives.|\n", "2506.05293": "|**2025-06-05**|**Reduction of Outflow Boundary Influence on Aerodynamic Performance using Neural Networks**|Mario Christopher Bedrunka, Dirk Reith, Holger Foysi et.al.|[2506.05293](http://arxiv.org/abs/2506.05293)|null||The accurate treatment of outflow boundary conditions remains a critical challenge in computational fluid dynamics when predicting aerodynamic forces and/or acoustic emissions. This is particularly evident when employing the lattice Boltzmann method (LBM) as the numerical solution technique, which often suffers from inaccuracies induced by artificial reflections from outflow boundaries. This paper investigates the use of neural networks (NN) to mitigate these adverse boundary effects and enable truncated domain requirements. Two distinct NN-based approaches are proposed: (1) direct reconstruction of unknown particle distribution functions at the outflow boundary; and (2) enhancement of established characteristic boundary conditions (CBC) by dynamically tuning their parameters. The direct reconstruction model was trained on data generated from a 2D flow over a cylindrical obstruction. The drag, lift, and Strouhal number were used to test the new boundary condition. We analyzed results for various Reynolds numbers and restricted domain sizes where it demonstrated significantly improved predictions when compared with the traditional Zou & He boundary condition. To examine the robustness of the NN-based reconstruction, the same condition was applied to the simulation of a NACA0012 airfoil, again providing accurate aerodynamic performance predictions. The neural-enhanced CBC were evaluated on a 2D convected vortex benchmark and showed superior performance in minimizing density errors compared to CBCs with fixed parameters. These findings highlight the potential of NN-integrated boundary conditions to improve accuracy and reduce computational expense of aerodynamic and acoustic emissions simulations with the LBM.|\n", "2506.05292": "|**2025-06-05**|**Learning Beyond Experience: Generalizing to Unseen State Space with Reservoir Computing**|Declan A. Norton, Yuanzhao Zhang, Michelle Girvan et.al.|[2506.05292](http://arxiv.org/abs/2506.05292)|null|15 pages, 9 figures|Machine learning techniques offer an effective approach to modeling dynamical systems solely from observed data. However, without explicit structural priors -- built-in assumptions about the underlying dynamics -- these techniques typically struggle to generalize to aspects of the dynamics that are poorly represented in the training data. Here, we demonstrate that reservoir computing -- a simple, efficient, and versatile machine learning framework often used for data-driven modeling of dynamical systems -- can generalize to unexplored regions of state space without explicit structural priors. First, we describe a multiple-trajectory training scheme for reservoir computers that supports training across a collection of disjoint time series, enabling effective use of available training data. Then, applying this training scheme to multistable dynamical systems, we show that RCs trained on trajectories from a single basin of attraction can achieve out-of-domain generalization by capturing system behavior in entirely unobserved basins.|\n", "2506.05174": "|**2025-06-05**|**Norming Sets for Tensor and Polynomial Sketching**|Yifan Zhang, Joe Kileel et.al.|[2506.05174](http://arxiv.org/abs/2506.05174)|null|16 pages|This paper develops the sketching (i.e., randomized dimension reduction) theory for real algebraic varieties and images of polynomial maps, including, e.g., the set of low rank tensors and tensor networks. Through the lens of norming sets, we provide a framework for controlling the sketching dimension for \\textit{any} sketch operator used to embed said sets, including sub-Gaussian, fast Johnson-Lindenstrauss, and tensor structured sketch operators. Leveraging norming set theory, we propose a new sketching method called the median sketch. It embeds such a set $V$ using only $\\widetilde{\\mathcal{O}}(\\dim V)$ tensor structured or sparse linear measurements.|\n", "2506.05122": "|**2025-06-05**|**Reactive Transport Simulation of Silicate-Rich Shale Rocks when Exposed to CO2 Saturated Brine Under High Pressure and High Temperature**|Shaziya A. Banu, Venkata R. S. B. Varanasi, Arash Noshadravan et.al.|[2506.05122](http://arxiv.org/abs/2506.05122)|null||This study examines the feasibility of carbon dioxide storage in shale rocks and the reliability of reactive transport models in achieving accurate replication of the chemo-mechanical interactions and transport processes transpiring in these rocks when subjected to CO2 saturated brine. Owing to the heterogeneity of rocks, experimental testing for adequate deductions and findings, could be an expensive and time-intensive process. Therefore, this study proposes utilization of reactive transport modeling to replicate the pore-scale chemo-mechanical reactions and transport processes occurring in silicate-rich shale rocks in the presence of CO2 saturated brine under high pressure and high temperature. For this study, Crunch Tope has been adopted to simulate a one-dimensional reactive transport model of a Permian rock specimen exposed to the acidic brine at a temperature of 100 {\\deg}C and pressure of 12.40 MPa (1800 psi) for a period of 14 and 28 days. The results demonstrated significant dissolution followed by precipitation of quartz rich phases, precipitation and swelling of clay rich phases, and dissolution of feldspar rich phases closer to the acidic brine-rock interface. Moreover, porosity against reaction depth curve showed nearly 1.00% mineral precipitation occur at 14 and 28 days, which is insufficient to completely fill the pore spaces.|\n", "2506.05105": "|**2025-06-05**|**Classification and enumeration of solid-solid phase transition mechanisms**|Fang-Cheng Wang, Qi-Jun Ye, Yu-Cheng Zhu et.al.|[2506.05105](http://arxiv.org/abs/2506.05105)|null|22 pages, 14 figures|Crystal-structure match (CSM), the atom-to-atom correspondence between two crystalline phases, is used extensively to describe solid-solid phase transition (SSPT) mechanisms. However, existing computational methods cannot account for all possible CSMs. Here, we propose a formalism to classify all CSMs into a tree structure, which is independent of the choices of unit cell and supercell. We rigorously proved that only a finite number of noncongruent CSMs are of practical interest. By representing CSMs as integer matrices, we introduce the crystmatch method to exhaustively enumerate them, which uncontroversially solves the CSM optimization problem under any geometric criterion. For most SSPTs, crystmatch can reproduce all known deformation mechanisms and CSMs within 10 CPU minutes, while also revealing thousands of new candidates. The resulting database can be further used for comparing experimental phenomena, high-throughput energy barrier calculations, or machine learning.|\n", "2506.05031": "|**2025-06-05**|**Quantum simulation of the Hubbard model on a graphene hexagon: Strengths of IQPE and noise constraints**|Mohammad Mirzakhani, Kyungsun Moon et.al.|[2506.05031](http://arxiv.org/abs/2506.05031)|null|14 pages, 10 figures|Quantum computing offers transformative potential for simulating real-world materials, providing a powerful platform to investigate complex quantum systems across quantum chemistry and condensed matter physics. In this work, we leverage this capability to simulate the Hubbard model on a six-site graphene hexagon using Qiskit, employing the Iterative Quantum Phase Estimation (IQPE) and adiabatic evolution algorithms to determine its ground-state properties. Noiseless simulations yield accurate ground-state energies (GSEs), charge and spin densities, and correlation functions, all in excellent agreement with exact diagonalization, underscoring the precision and reliability of quantum simulation for strongly correlated electron systems. However, deploying IQPE and adiabatic evolution on today's noisy quantum hardware remains highly challenging. To examine these limitations, we utilize the Qiskit Aer simulator with a custom noise model tailored to the characteristics of IBM's real backend. This model includes realistic depolarizing gate errors, thermal relaxation, and readout noise, allowing us to explore how these factors degrade simulation accuracy. Preliminary hardware runs on IBM devices further expose discrepancies between simulated and real-world noise, emphasizing the gap between ideal and practical implementations. Overall, our results highlight the promise of quantum computing for simulating correlated quantum materials, while also revealing the significant challenges posed by hardware noise in achieving accurate and reliable physical predictions using current quantum devices.|\n", "2506.04969": "|**2025-06-05**|**Probability of Collision with Tethered Spacecraft**|Yema Paul, Emmanuel Delande, Francois Vinet et.al.|[2506.04969](http://arxiv.org/abs/2506.04969)|null|13 pages, 2 figures, Engineering Note|This Engineering Note addresses the challenge of estimating the probability of collision for tethered spacecraft during close encounters with other space objects. Standard probability of collision methods, based on spherical hard-body assumptions, tend to be overly conservative when applied to long tether systems. We introduce a method that accounts for the tether's spatial extent and configuration uncertainty by maximizing the probability of collision over all physically admissible tether shapes. Applied to real-world conjunction events involving a kilometer-scale flexible inextensible tether, the method yields more realistic risk estimates. This approach improves the ability to distinguish hazardous from benign encounters, thereby supporting more informed collision avoidance decisions.|\n", "2506.04898": "|**2025-06-05**|**Uncertainty quantification and stability of neural operators for prediction of three-dimensional turbulence**|Xintong Zou, Zhijie Li, Yunpeng Wang et.al.|[2506.04898](http://arxiv.org/abs/2506.04898)|null||Turbulence poses challenges for numerical simulation due to its chaotic, multiscale nature and high computational cost. Traditional turbulence modeling often struggles with accuracy and long-term stability. Recent scientific machine learning (SciML) models, such as Fourier Neural Operators (FNO), show promise in solving PDEs, but are typically limited to one-step-ahead predictions and often fail over long time horizons, especially in 3D turbulence. This study proposes a framework to assess the reliability of neural operator models in turbulent flows. Using three-dimensional forced homogeneous isotropic turbulence (HIT) as a benchmark, we evaluate models in terms of uncertainty quantification (UQ), error propagation, and sensitivity to initial perturbations. Statistical tools such as error distribution analysis and autocorrelation functions (ACF) are used to assess predictive robustness and temporal coherence. Our proposed model, the factorized-implicit FNO (F-IFNO), improves long-term stability and accuracy by incorporating implicit factorization into the prediction process. It outperforms conventional LES and other FNO-based models in balancing accuracy, stability, and efficiency. The results highlight the importance of prediction constraints, time interval selection, and UQ in developing robust neural operator frameworks for turbulent systems.|\n", "2506.04880": "|**2025-06-05**|**Numerical analysis for constrained and unconstrained Q-tensor energies for liquid crystals**|Heiko Gimperlein, Ruma R. Maity et.al.|[2506.04880](http://arxiv.org/abs/2506.04880)|null||This paper introduces a comprehensive finite element approximation framework for three-dimensional Landau-de Gennes $Q$-tensor energies for nematic liquid crystals, with a particular focus on the anisotropy of the elastic energy and the Ball-Majumdar singular potential. This potential imposes essential physical constraints on the eigenvalues of the $Q$-tensor, ensuring realistic modeling. We address the approximation of regular solutions to nonlinear elliptic partial differential equations with non-homogeneous boundary conditions associated with Landau-de Gennes energies. The well-posedness of the discrete linearized problem is rigorously demonstrated. The existence and local uniqueness of the discrete solution is derived using the Newton-Kantorovich theorem. Furthermore, we demonstrate an optimal order convergence rate in the energy norm and discuss the impact of eigenvalue constraints on the a priori error analysis.|\n", "2506.04857": "|**2025-06-05**|**Active flux for ideal magnetohydrodynamics: A positivity-preserving scheme with the Godunov-Powell source term**|Junming Duan, Praveen Chandrashekar, Christian Klingenberg et.al.|[2506.04857](http://arxiv.org/abs/2506.04857)|null|27 pages, 12 figures|The Active Flux (AF) is a compact, high-order finite volume scheme that allows more flexibility by introducing additional point value degrees of freedom at cell interfaces. This paper proposes a positivity-preserving (PP) AF scheme for solving the ideal magnetohydrodynamics, where the Godunov-Powell source term is employed to deal with the divergence-free constraint. For the evolution of the cell average, apart from the standard conservative finite volume method for the flux derivative, the nonconservative source term is built on the quadratic reconstruction in each cell, which maintains the compact stencil in the AF scheme. For the point value update, the local Lax-Friedrichs (LLF) flux vector splitting is adopted for the flux derivative, originally proposed in [Duan, Barsukow, and Klingenberg, SIAM Journal on Scientific Computing, 47(2), A811--A837, 2025], and a central difference is used to discretize the divergence in the source term. A parametrized flux limiter and a scaling limiter are presented to preserve the density and pressure positivity by blending the AF scheme with the first-order PP LLF scheme with the source term. To suppress oscillations, a new shock sensor considering the divergence error is proposed, which is used to compute the blending coefficients for the cell average. Several numerical tests are conducted to verify the third-order accuracy, PP property, and shock-capturing ability of the scheme. The key role of the Godunov-Powell source term and its suitable discretization in controlling divergence error is also validated.|\n", "2506.04840": "|**2025-06-05**|**Efficient randomized algorithms for the fixed Tucker-rank problem of Tucker decomposition with adaptive shifts**|Maolin Che, Yimin Wei, Chong Wu et.al.|[2506.04840](http://arxiv.org/abs/2506.04840)|null|41 pages, 43 figures|Randomized numerical linear algebra is proved to bridge theoretical advancements to offer scalable solutions for approximating tensor decomposition. This paper introduces fast randomized algorithms for solving the fixed Tucker-rank problem of Tucker decomposition, through the integration of adaptive shifted power iterations. The proposed algorithms enhance randomized variants of truncated high-order singular value decomposition (T-HOSVD) and sequentially T-HOSVD (ST-HOSVD) by incorporating dynamic shift strategies, which accelerate convergence by refining the singular value gap and reduce the number of required power iterations while maintaining accuracy. Theoretical analyses provide probabilistic error bounds, demonstrating that the proposed methods achieve comparable or superior accuracy compared to deterministic approaches. Numerical experiments on synthetic and real-world datasets validate the efficiency and robustness of the proposed algorithms, showing a significant decline in runtime and approximation error over state-of-the-art techniques.|\n", "2506.04835": "|**2025-06-05**|**Thermoplasmonics of Gold-Core Silica-Shell Colloidal Nanoparticles under Pulse Illumination**|Julien El Hajj, Gilles Ledoux, Samy Merabia et.al.|[2506.04835](http://arxiv.org/abs/2506.04835)|null||Core-shell nanoparticles, particularly those having a gold core, have emerged as a highly promising class of materials due to their unique optical and thermal properties, which underpin a wide range of applications in photothermal therapy, imaging, and biosensing. In this study, we present a comprehensive study of the thermal dynamics of gold-core silica-shell nanoparticles immersed in water under pulse illumination. The plasmonic response of the core-shell nanoparticle is described by incorporating Mie theory with electronic temperature corrections to the refractive indices of gold, based on a Drude Lorentz formulation. The thermal response of the core-shell nanoparticles is modeled by coupling the two temperature model with molecular dynamics simulations, providing an atomistic description of nanoscale heat transfer. We investigate nanoparticles with both dense and porous silica shells (with 50% porosity) under laser pulse durations of 100 fs, 10 ps, and 1 ns, and over a range of fluences between 0.05 and 5mJ/cm2. We show that nanoparticles with a thin dense silica shell (5 nm) exhibit significantly faster water heating compared to bare gold nanoparticles. This behavior is attributed to enhanced electron-phonon coupling at the gold silica interface and to the relatively high thermal conductance between silica and water. These findings provide new insights into optimizing nanoparticle design for efficient photothermal applications and establish a robust framework for understanding energy transfer mechanisms in heterogeneous metal dielectric nanostructures.|\n", "2506.04809": "|**2025-06-05**|**Numerical solution of the wave equation outside a sphere**|Michael J. Carley et.al.|[2506.04809](http://arxiv.org/abs/2506.04809)|null||A method is presented for the fast evaluation of the transient acoustic field generated outside a spherical surface by sources inside the surface. The method employs Lebedev quadratures, which are the optimal method for spatial integration, and Lagrange interpolation and differentiation in an advanced time algorithm for the evaluation of the transient field. Numerical testing demonstrates that the approach gives near machine-precision accuracy and a speed-up in evaluation time which depends on the order of quadrature rule employed but breaks even with direct evaluation at a number of field points about 1.15 times the number of surface quadrature nodes.|\n", "2506.04791": "|**2025-06-05**|**Tensor-based multivariate function approximation: methods benchmarking and comparison**|Athanasios C. Antoulas, Ion Victor Gosea, Charles Poussot-Vassal et.al.|[2506.04791](http://arxiv.org/abs/2506.04791)|null|Report with a collection of examples, aimed at being regularly   updated. Associated GIT: https://github.com/cpoussot/mLF|In this note, we evaluate the performances, the features and the user-experience of some methods (and their implementations) designed for tensor- (or data-) based multivariate function construction and approximation. To this aim, a collection of multivariate functions extracted from contributive works coming from different communities, is suggested. First, these functions with varying complexity (e.g. number and degree of the variables) and nature (e.g. rational, irrational, differentiable or not, symmetric, etc.) are used to construct tensors, each of different dimension and size on the disk. Second, grounded on this tensor, we inspect performances of each considered method (e.g. the accuracy, the computational time, the parameters tuning impact, etc.). Finally, considering the \"best\" parameter tuning set, we compare each method using multiple evaluation criteria. The purpose of this note is not to rank the methods but rather to evaluate as fairly as possible the different available strategies, with the idea in mind to guide users to understand the process, the possibilities, the advantages and the limits brought by each tools. The contribution claimed is to suggest a complete benchmark collection of some available tools for tensor approximation by surrogate models (e.g. rational functions, networks, etc.). In addition, as contributors of the multivariate Loewner Framework (mLF) approach (and its side implementation in MDSPACK), attention and details of the latter are more explicitly given, in order to provide readers a digest of this contributive work and some details with simple examples.|\n", "2506.04781": "|**2025-06-05**|**Deep learning image burst stacking to reconstruct high-resolution ground-based solar observations**|Christoph Schirninger, Robert Jarolim, Astrid M. Veronig et.al.|[2506.04781](http://arxiv.org/abs/2506.04781)|null||Large aperture ground based solar telescopes allow the solar atmosphere to be resolved in unprecedented detail. However, observations are limited by Earths turbulent atmosphere, requiring post image corrections. Current reconstruction methods using short exposure bursts face challenges with strong turbulence and high computational costs. We introduce a deep learning approach that reconstructs 100 short exposure images into one high quality image in real time. Using unpaired image to image translation, our model is trained on degraded bursts with speckle reconstructions as references, improving robustness and generalization. Our method shows an improved robustness in terms of perceptual quality, especially when speckle reconstructions show artifacts. An evaluation with a varying number of images per burst demonstrates that our method makes efficient use of the combined image information and achieves the best reconstructions when provided with the full image burst.|\n", "2506.04763": "|**2025-06-05**|**A highly scalable numerical framework for reservoir simulation on UG4 platform**|Shuai Lu et.al.|[2506.04763](http://arxiv.org/abs/2506.04763)|null||The modeling and simulation of multiphase fluid flow receive significant attention in reservoir engineering. Many time discretization schemes for multiphase flow equations are either explicit or semi-implicit, relying on the decoupling between the saturation equation and the pressure equation. In this study, we delve into a fully coupled and fully implicit framework for simulating multiphase flow in heterogeneous porous media, considering gravity and capillary effects. We utilize the Vertex-Centered Finite Volume Method for spatial discretization and propose an efficient implementation of interface conditions for heterogeneous porous media within the current scheme. Notably, we introduce the Linearly Implicit Extrapolation Method (LIMEX) with an error estimator, adapted for the first time to multiphase flow problems. To solve the resulting linear system, we employ the BiCGSTAB method with the Geometric Multigrid (GMG) preconditioner. The implementations of models and methods are based on the open-source software: UG4. The results from parallel computations on the supercomputer demonstrate that the scalability of our proposed framework is sufficient, supporting a scale of thousands of processors with Degrees of Freedom (DoF) extending up to billions.|\n", "2506.04732": "|**2025-06-05**|**A Fast, Accurate and Oscillation-free Spectral Collocation Solver for High-dimensional Transport Problems**|Nicola Cavallini, Gianmarco Manzini, Daniele Funaro et.al.|[2506.04732](http://arxiv.org/abs/2506.04732)|null||Transport phenomena-describing the movement of particles, energy, or other physical quantities-are fundamental in various scientific disciplines, including nuclear physics, plasma physics, astrophysics, engineering, and the natural sciences.   However, solving the associated seven-dimensional transport equations poses a significant computational challenge due to the curse of dimensionality.   We introduce the Tensor Train Superconsistent Spectral (T${^2}$S${^2}$) solver to address this challenge, integrating Spectral Collocation for exponential convergence, Superconsistency for stabilization in transport-dominated regimes, and Tensor Train format for substantial data compression. T${^2}$S${^2}$ enforces a dimension-wise superconsistent condition compatible with tensor structures, achieving extremely low compression ratios, in the order of $(10^{-12})$, while preserving spectral accuracy. Numerical experiments on linear problems demonstrate that T${^2}$S${^2}$ can solve high-dimensional transport problems in minutes on standard hardware, making previously intractable problems computationally feasible. This advancement opens new avenues for efficiently and accurately modeling complex transport phenomena.|\n", "2506.04710": "|**2025-06-05**|**An Array Decomposition Method for Finite Arrays with Electrically Connected Elements for fast Toeplitz Solvers**|Lucas \u00c5kerstedt, Harald Hultin, B. L. G. Jonsson et.al.|[2506.04710](http://arxiv.org/abs/2506.04710)|null|12 pages, 17 figures|A large part of the geometry of array antennas is often partially defined by finite translational symmetries. Applying the method of moments (MoM) with the RWG-like element on an appropriately structured mesh to these arrays results in an impedance matrix where the main part exhibits a multilevel block Toeplitz structure. This article introduces a memory-efficient construction method that effectively represents and reuses impedance calculations. The proposed method, applicable to electrically connected elements, also accounts for all non-symmetric parts of the array. The core idea involves nine distinct electrically connectable components from which the array can be assembled. The derived multilevel block Toeplitz matrix is further utilized by an in-house inverse solver to achieve faster and more memory-efficient MoM current vector calculations. We demonstrate the method by computing the far-field of a 32x32 array and the scattering parameters of two tightly coupled 9x9 arrays. This approach reduces the memory allocation from $\\mathcal{O}(N_x^2 N_y^2)$ to $\\mathcal{O}(N_x N_y)$, for an $N_x \\times N_y$ array.|\n", "2506.04655": "|**2025-06-05**|**Inverse elastic obstacle scattering problems by monotonicity method**|Mengjiao Bai, Huaian Diao, Weisheng Zhou et.al.|[2506.04655](http://arxiv.org/abs/2506.04655)|null||We consider the elastic wave scattering problem involving rigid obstacles. This work addresses the inverse problem of reconstructing the position and shape of such obstacles using far-field measurements. A novel monotonicity-based approach is developed for this purpose. By factorizing the far-field operator and utilizing the existence of localized wave functions, we derive a shape characterization criterion for the obstacle boundary. The proposed method employs monotonicity tests to determine the geometric relationship between any given test domain and the actual scatterer. As a result, the shape and location of rigid elastic obstacles can be uniquely identified without requiring any initial guesses or prior knowledge of the physical parameters of the homogeneous background medium.|\n", "2506.04523": "|**2025-06-05**|**Perturbative Gradient Training: A novel training paradigm for bridging the gap between deep neural networks and physical reservoir computing**|Cliff B. Abbott, Mark Elo, Dmytro A. Bozhko et.al.|[2506.04523](http://arxiv.org/abs/2506.04523)|null|7 pages, 8 figures, submitted to IEEE Transactions on Neural Netowrks   and Learning Systems|We introduce Perturbative Gradient Training (PGT), a novel training paradigm that overcomes a critical limitation of physical reservoir computing: the inability to perform backpropagation due to the black-box nature of physical reservoirs. Drawing inspiration from perturbation theory in physics, PGT uses random perturbations in the network's parameter space to approximate gradient updates using only forward passes. We demonstrate the feasibility of this approach on both simulated neural network architectures, including a dense network and a transformer model with a reservoir layer, and on experimental hardware using a magnonic auto-oscillation ring as the physical reservoir. Our results show that PGT can achieve performance comparable to that of standard backpropagation methods in cases where backpropagation is impractical or impossible. PGT represents a promising step toward integrating physical reservoirs into deeper neural network architectures and achieving significant energy efficiency gains in AI training.|\n", "2506.05012": "|**2025-06-05**|**A Unified Framework for Simulating Strongly-Coupled Fluid-Robot Multiphysics**|Jeong Hun Lee, Junzhe Hu, Sofia Kwok et.al.|[2506.05012](http://arxiv.org/abs/2506.05012)|null||We present a framework for simulating fluid-robot multiphysics as a single, unified optimization problem. The coupled manipulator and incompressible Navier-Stokes equations governing the robot and fluid dynamics are derived together from a single Lagrangian using the principal of least action. We then employ discrete variational mechanics to derive a stable, implicit time-integration scheme for jointly simulating both the fluid and robot dynamics, which are tightly coupled by a constraint that enforces the no-slip boundary condition at the fluid-robot interface. Extending the classical immersed boundary method, we derive a new formulation of the no-slip constraint that is numerically well-conditioned and physically accurate for multibody systems commonly found in robotics. We demonstrate our approach's physical accuracy on benchmark computational fluid-dynamics problems, including Poiseuille flow and a disc in free stream. We then design a locomotion policy for a novel swimming robot in simulation and validate results on real-world hardware, showcasing our framework's sim-to-real capability for robotics tasks.|\n"}}